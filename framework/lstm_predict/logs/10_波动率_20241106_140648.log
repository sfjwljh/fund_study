2024-11-06 14:06:49,930 - INFO - Epoch [1/300], Batch [1/43], Training Loss: 0.05372647
2024-11-06 14:06:50,041 - INFO - Epoch [1/300], Batch [2/43], Training Loss: 0.05165815
2024-11-06 14:06:50,045 - INFO - Epoch [1/300], Batch [3/43], Training Loss: 0.05004047
2024-11-06 14:06:50,049 - INFO - Epoch [1/300], Batch [4/43], Training Loss: 0.04795082
2024-11-06 14:06:50,052 - INFO - Epoch [1/300], Batch [5/43], Training Loss: 0.04457780
2024-11-06 14:06:50,056 - INFO - Epoch [1/300], Batch [6/43], Training Loss: 0.04224044
2024-11-06 14:06:50,060 - INFO - Epoch [1/300], Batch [7/43], Training Loss: 0.04169046
2024-11-06 14:06:50,064 - INFO - Epoch [1/300], Batch [8/43], Training Loss: 0.03888153
2024-11-06 14:06:50,067 - INFO - Epoch [1/300], Batch [9/43], Training Loss: 0.03578983
2024-11-06 14:06:50,071 - INFO - Epoch [1/300], Batch [10/43], Training Loss: 0.03371737
2024-11-06 14:06:50,075 - INFO - Epoch [1/300], Batch [11/43], Training Loss: 0.03197926
2024-11-06 14:06:50,079 - INFO - Epoch [1/300], Batch [12/43], Training Loss: 0.02837560
2024-11-06 14:06:50,083 - INFO - Epoch [1/300], Batch [13/43], Training Loss: 0.02487480
2024-11-06 14:06:50,086 - INFO - Epoch [1/300], Batch [14/43], Training Loss: 0.02269302
2024-11-06 14:06:50,088 - INFO - Epoch [1/300], Batch [15/43], Training Loss: 0.02102270
2024-11-06 14:06:50,091 - INFO - Epoch [1/300], Batch [16/43], Training Loss: 0.01727078
2024-11-06 14:06:50,095 - INFO - Epoch [1/300], Batch [17/43], Training Loss: 0.01572372
2024-11-06 14:06:50,099 - INFO - Epoch [1/300], Batch [18/43], Training Loss: 0.01244401
2024-11-06 14:06:50,103 - INFO - Epoch [1/300], Batch [19/43], Training Loss: 0.01026173
2024-11-06 14:06:50,107 - INFO - Epoch [1/300], Batch [20/43], Training Loss: 0.00705975
2024-11-06 14:06:50,111 - INFO - Epoch [1/300], Batch [21/43], Training Loss: 0.00548671
2024-11-06 14:06:50,115 - INFO - Epoch [1/300], Batch [22/43], Training Loss: 0.00345020
2024-11-06 14:06:50,118 - INFO - Epoch [1/300], Batch [23/43], Training Loss: 0.00199709
2024-11-06 14:06:50,122 - INFO - Epoch [1/300], Batch [24/43], Training Loss: 0.00091298
2024-11-06 14:06:50,126 - INFO - Epoch [1/300], Batch [25/43], Training Loss: 0.00017601
2024-11-06 14:06:50,129 - INFO - Epoch [1/300], Batch [26/43], Training Loss: 0.00010790
2024-11-06 14:06:50,133 - INFO - Epoch [1/300], Batch [27/43], Training Loss: 0.00057581
2024-11-06 14:06:50,136 - INFO - Epoch [1/300], Batch [28/43], Training Loss: 0.00114758
2024-11-06 14:06:50,140 - INFO - Epoch [1/300], Batch [29/43], Training Loss: 0.00149733
2024-11-06 14:06:50,144 - INFO - Epoch [1/300], Batch [30/43], Training Loss: 0.00211747
2024-11-06 14:06:50,147 - INFO - Epoch [1/300], Batch [31/43], Training Loss: 0.00214070
2024-11-06 14:06:50,151 - INFO - Epoch [1/300], Batch [32/43], Training Loss: 0.00191488
2024-11-06 14:06:50,155 - INFO - Epoch [1/300], Batch [33/43], Training Loss: 0.00154856
2024-11-06 14:06:50,158 - INFO - Epoch [1/300], Batch [34/43], Training Loss: 0.00122477
2024-11-06 14:06:50,162 - INFO - Epoch [1/300], Batch [35/43], Training Loss: 0.00086297
2024-11-06 14:06:50,167 - INFO - Epoch [1/300], Batch [36/43], Training Loss: 0.00057655
2024-11-06 14:06:50,171 - INFO - Epoch [1/300], Batch [37/43], Training Loss: 0.00024455
2024-11-06 14:06:50,175 - INFO - Epoch [1/300], Batch [38/43], Training Loss: 0.00013542
2024-11-06 14:06:50,178 - INFO - Epoch [1/300], Batch [39/43], Training Loss: 0.00007442
2024-11-06 14:06:50,182 - INFO - Epoch [1/300], Batch [40/43], Training Loss: 0.00006367
2024-11-06 14:06:50,185 - INFO - Epoch [1/300], Batch [41/43], Training Loss: 0.00013678
2024-11-06 14:06:50,188 - INFO - Epoch [1/300], Batch [42/43], Training Loss: 0.00017420
2024-11-06 14:06:50,193 - INFO - Epoch [1/300], Batch [43/43], Training Loss: 0.00032204
2024-11-06 14:06:50,209 - INFO - Epoch [1/300], Average Training Loss: 0.01532250, Validation Loss: 0.00052080
2024-11-06 14:06:50,214 - INFO - Epoch [2/300], Batch [1/43], Training Loss: 0.00029298
2024-11-06 14:06:50,219 - INFO - Epoch [2/300], Batch [2/43], Training Loss: 0.00043955
2024-11-06 14:06:50,223 - INFO - Epoch [2/300], Batch [3/43], Training Loss: 0.00044194
2024-11-06 14:06:50,226 - INFO - Epoch [2/300], Batch [4/43], Training Loss: 0.00045618
2024-11-06 14:06:50,231 - INFO - Epoch [2/300], Batch [5/43], Training Loss: 0.00034810
2024-11-06 14:06:50,235 - INFO - Epoch [2/300], Batch [6/43], Training Loss: 0.00041872
2024-11-06 14:06:50,239 - INFO - Epoch [2/300], Batch [7/43], Training Loss: 0.00032448
2024-11-06 14:06:50,243 - INFO - Epoch [2/300], Batch [8/43], Training Loss: 0.00013219
2024-11-06 14:06:50,247 - INFO - Epoch [2/300], Batch [9/43], Training Loss: 0.00003994
2024-11-06 14:06:50,251 - INFO - Epoch [2/300], Batch [10/43], Training Loss: 0.00004492
2024-11-06 14:06:50,254 - INFO - Epoch [2/300], Batch [11/43], Training Loss: 0.00005218
2024-11-06 14:06:50,258 - INFO - Epoch [2/300], Batch [12/43], Training Loss: 0.00004920
2024-11-06 14:06:50,261 - INFO - Epoch [2/300], Batch [13/43], Training Loss: 0.00009981
2024-11-06 14:06:50,266 - INFO - Epoch [2/300], Batch [14/43], Training Loss: 0.00007737
2024-11-06 14:06:50,270 - INFO - Epoch [2/300], Batch [15/43], Training Loss: 0.00013354
2024-11-06 14:06:50,274 - INFO - Epoch [2/300], Batch [16/43], Training Loss: 0.00010554
2024-11-06 14:06:50,277 - INFO - Epoch [2/300], Batch [17/43], Training Loss: 0.00012079
2024-11-06 14:06:50,281 - INFO - Epoch [2/300], Batch [18/43], Training Loss: 0.00007467
2024-11-06 14:06:50,284 - INFO - Epoch [2/300], Batch [19/43], Training Loss: 0.00013649
2024-11-06 14:06:50,288 - INFO - Epoch [2/300], Batch [20/43], Training Loss: 0.00012231
2024-11-06 14:06:50,292 - INFO - Epoch [2/300], Batch [21/43], Training Loss: 0.00013536
2024-11-06 14:06:50,296 - INFO - Epoch [2/300], Batch [22/43], Training Loss: 0.00010305
2024-11-06 14:06:50,299 - INFO - Epoch [2/300], Batch [23/43], Training Loss: 0.00008990
2024-11-06 14:06:50,302 - INFO - Epoch [2/300], Batch [24/43], Training Loss: 0.00006871
2024-11-06 14:06:50,306 - INFO - Epoch [2/300], Batch [25/43], Training Loss: 0.00002535
2024-11-06 14:06:50,309 - INFO - Epoch [2/300], Batch [26/43], Training Loss: 0.00004147
2024-11-06 14:06:50,313 - INFO - Epoch [2/300], Batch [27/43], Training Loss: 0.00012110
2024-11-06 14:06:50,316 - INFO - Epoch [2/300], Batch [28/43], Training Loss: 0.00005923
2024-11-06 14:06:50,319 - INFO - Epoch [2/300], Batch [29/43], Training Loss: 0.00002664
2024-11-06 14:06:50,323 - INFO - Epoch [2/300], Batch [30/43], Training Loss: 0.00005871
2024-11-06 14:06:50,326 - INFO - Epoch [2/300], Batch [31/43], Training Loss: 0.00009771
2024-11-06 14:06:50,329 - INFO - Epoch [2/300], Batch [32/43], Training Loss: 0.00005550
2024-11-06 14:06:50,333 - INFO - Epoch [2/300], Batch [33/43], Training Loss: 0.00008838
2024-11-06 14:06:50,336 - INFO - Epoch [2/300], Batch [34/43], Training Loss: 0.00010368
2024-11-06 14:06:50,340 - INFO - Epoch [2/300], Batch [35/43], Training Loss: 0.00006902
2024-11-06 14:06:50,344 - INFO - Epoch [2/300], Batch [36/43], Training Loss: 0.00016039
2024-11-06 14:06:50,348 - INFO - Epoch [2/300], Batch [37/43], Training Loss: 0.00010081
2024-11-06 14:06:50,352 - INFO - Epoch [2/300], Batch [38/43], Training Loss: 0.00003958
2024-11-06 14:06:50,357 - INFO - Epoch [2/300], Batch [39/43], Training Loss: 0.00004914
2024-11-06 14:06:50,360 - INFO - Epoch [2/300], Batch [40/43], Training Loss: 0.00006705
2024-11-06 14:06:50,364 - INFO - Epoch [2/300], Batch [41/43], Training Loss: 0.00007880
2024-11-06 14:06:50,368 - INFO - Epoch [2/300], Batch [42/43], Training Loss: 0.00005648
2024-11-06 14:06:50,372 - INFO - Epoch [2/300], Batch [43/43], Training Loss: 0.00006988
2024-11-06 14:06:50,384 - INFO - Epoch [2/300], Average Training Loss: 0.00013202, Validation Loss: 0.00007380
2024-11-06 14:06:50,388 - INFO - Epoch [3/300], Batch [1/43], Training Loss: 0.00003141
2024-11-06 14:06:50,392 - INFO - Epoch [3/300], Batch [2/43], Training Loss: 0.00006813
2024-11-06 14:06:50,396 - INFO - Epoch [3/300], Batch [3/43], Training Loss: 0.00005500
2024-11-06 14:06:50,400 - INFO - Epoch [3/300], Batch [4/43], Training Loss: 0.00008598
2024-11-06 14:06:50,404 - INFO - Epoch [3/300], Batch [5/43], Training Loss: 0.00004484
2024-11-06 14:06:50,408 - INFO - Epoch [3/300], Batch [6/43], Training Loss: 0.00006075
2024-11-06 14:06:50,411 - INFO - Epoch [3/300], Batch [7/43], Training Loss: 0.00012703
2024-11-06 14:06:50,414 - INFO - Epoch [3/300], Batch [8/43], Training Loss: 0.00007426
2024-11-06 14:06:50,418 - INFO - Epoch [3/300], Batch [9/43], Training Loss: 0.00004975
2024-11-06 14:06:50,422 - INFO - Epoch [3/300], Batch [10/43], Training Loss: 0.00006443
2024-11-06 14:06:50,426 - INFO - Epoch [3/300], Batch [11/43], Training Loss: 0.00007126
2024-11-06 14:06:50,429 - INFO - Epoch [3/300], Batch [12/43], Training Loss: 0.00005240
2024-11-06 14:06:50,433 - INFO - Epoch [3/300], Batch [13/43], Training Loss: 0.00004921
2024-11-06 14:06:50,436 - INFO - Epoch [3/300], Batch [14/43], Training Loss: 0.00005689
2024-11-06 14:06:50,441 - INFO - Epoch [3/300], Batch [15/43], Training Loss: 0.00004671
2024-11-06 14:06:50,445 - INFO - Epoch [3/300], Batch [16/43], Training Loss: 0.00004101
2024-11-06 14:06:50,448 - INFO - Epoch [3/300], Batch [17/43], Training Loss: 0.00002994
2024-11-06 14:06:50,452 - INFO - Epoch [3/300], Batch [18/43], Training Loss: 0.00013230
2024-11-06 14:06:50,456 - INFO - Epoch [3/300], Batch [19/43], Training Loss: 0.00010576
2024-11-06 14:06:50,460 - INFO - Epoch [3/300], Batch [20/43], Training Loss: 0.00001954
2024-11-06 14:06:50,463 - INFO - Epoch [3/300], Batch [21/43], Training Loss: 0.00004289
2024-11-06 14:06:50,467 - INFO - Epoch [3/300], Batch [22/43], Training Loss: 0.00002740
2024-11-06 14:06:50,470 - INFO - Epoch [3/300], Batch [23/43], Training Loss: 0.00005637
2024-11-06 14:06:50,474 - INFO - Epoch [3/300], Batch [24/43], Training Loss: 0.00005904
2024-11-06 14:06:50,478 - INFO - Epoch [3/300], Batch [25/43], Training Loss: 0.00003706
2024-11-06 14:06:50,482 - INFO - Epoch [3/300], Batch [26/43], Training Loss: 0.00007502
2024-11-06 14:06:50,486 - INFO - Epoch [3/300], Batch [27/43], Training Loss: 0.00005416
2024-11-06 14:06:50,489 - INFO - Epoch [3/300], Batch [28/43], Training Loss: 0.00005455
2024-11-06 14:06:50,493 - INFO - Epoch [3/300], Batch [29/43], Training Loss: 0.00005512
2024-11-06 14:06:50,497 - INFO - Epoch [3/300], Batch [30/43], Training Loss: 0.00006314
2024-11-06 14:06:50,501 - INFO - Epoch [3/300], Batch [31/43], Training Loss: 0.00004231
2024-11-06 14:06:50,505 - INFO - Epoch [3/300], Batch [32/43], Training Loss: 0.00003141
2024-11-06 14:06:50,509 - INFO - Epoch [3/300], Batch [33/43], Training Loss: 0.00008961
2024-11-06 14:06:50,513 - INFO - Epoch [3/300], Batch [34/43], Training Loss: 0.00010118
2024-11-06 14:06:50,516 - INFO - Epoch [3/300], Batch [35/43], Training Loss: 0.00008320
2024-11-06 14:06:50,519 - INFO - Epoch [3/300], Batch [36/43], Training Loss: 0.00004049
2024-11-06 14:06:50,522 - INFO - Epoch [3/300], Batch [37/43], Training Loss: 0.00005942
2024-11-06 14:06:50,526 - INFO - Epoch [3/300], Batch [38/43], Training Loss: 0.00004174
2024-11-06 14:06:50,529 - INFO - Epoch [3/300], Batch [39/43], Training Loss: 0.00006266
2024-11-06 14:06:50,533 - INFO - Epoch [3/300], Batch [40/43], Training Loss: 0.00003785
2024-11-06 14:06:50,536 - INFO - Epoch [3/300], Batch [41/43], Training Loss: 0.00008630
2024-11-06 14:06:50,540 - INFO - Epoch [3/300], Batch [42/43], Training Loss: 0.00008986
2024-11-06 14:06:50,544 - INFO - Epoch [3/300], Batch [43/43], Training Loss: 0.00002728
2024-11-06 14:06:50,555 - INFO - Epoch [3/300], Average Training Loss: 0.00006011, Validation Loss: 0.00007508
2024-11-06 14:06:50,559 - INFO - Epoch [4/300], Batch [1/43], Training Loss: 0.00006251
2024-11-06 14:06:50,562 - INFO - Epoch [4/300], Batch [2/43], Training Loss: 0.00006179
2024-11-06 14:06:50,565 - INFO - Epoch [4/300], Batch [3/43], Training Loss: 0.00003174
2024-11-06 14:06:50,567 - INFO - Epoch [4/300], Batch [4/43], Training Loss: 0.00006467
2024-11-06 14:06:50,571 - INFO - Epoch [4/300], Batch [5/43], Training Loss: 0.00007171
2024-11-06 14:06:50,574 - INFO - Epoch [4/300], Batch [6/43], Training Loss: 0.00003454
2024-11-06 14:06:50,577 - INFO - Epoch [4/300], Batch [7/43], Training Loss: 0.00007728
2024-11-06 14:06:50,581 - INFO - Epoch [4/300], Batch [8/43], Training Loss: 0.00009814
2024-11-06 14:06:50,584 - INFO - Epoch [4/300], Batch [9/43], Training Loss: 0.00004306
2024-11-06 14:06:50,587 - INFO - Epoch [4/300], Batch [10/43], Training Loss: 0.00004844
2024-11-06 14:06:50,591 - INFO - Epoch [4/300], Batch [11/43], Training Loss: 0.00003799
2024-11-06 14:06:50,594 - INFO - Epoch [4/300], Batch [12/43], Training Loss: 0.00006681
2024-11-06 14:06:50,597 - INFO - Epoch [4/300], Batch [13/43], Training Loss: 0.00003771
2024-11-06 14:06:50,601 - INFO - Epoch [4/300], Batch [14/43], Training Loss: 0.00004974
2024-11-06 14:06:50,604 - INFO - Epoch [4/300], Batch [15/43], Training Loss: 0.00003992
2024-11-06 14:06:50,607 - INFO - Epoch [4/300], Batch [16/43], Training Loss: 0.00007124
2024-11-06 14:06:50,609 - INFO - Epoch [4/300], Batch [17/43], Training Loss: 0.00004647
2024-11-06 14:06:50,612 - INFO - Epoch [4/300], Batch [18/43], Training Loss: 0.00002123
2024-11-06 14:06:50,615 - INFO - Epoch [4/300], Batch [19/43], Training Loss: 0.00009037
2024-11-06 14:06:50,618 - INFO - Epoch [4/300], Batch [20/43], Training Loss: 0.00008343
2024-11-06 14:06:50,621 - INFO - Epoch [4/300], Batch [21/43], Training Loss: 0.00007068
2024-11-06 14:06:50,624 - INFO - Epoch [4/300], Batch [22/43], Training Loss: 0.00010415
2024-11-06 14:06:50,627 - INFO - Epoch [4/300], Batch [23/43], Training Loss: 0.00001137
2024-11-06 14:06:50,630 - INFO - Epoch [4/300], Batch [24/43], Training Loss: 0.00003190
2024-11-06 14:06:50,633 - INFO - Epoch [4/300], Batch [25/43], Training Loss: 0.00004610
2024-11-06 14:06:50,636 - INFO - Epoch [4/300], Batch [26/43], Training Loss: 0.00005176
2024-11-06 14:06:50,639 - INFO - Epoch [4/300], Batch [27/43], Training Loss: 0.00009183
2024-11-06 14:06:50,643 - INFO - Epoch [4/300], Batch [28/43], Training Loss: 0.00008535
2024-11-06 14:06:50,647 - INFO - Epoch [4/300], Batch [29/43], Training Loss: 0.00007375
2024-11-06 14:06:50,650 - INFO - Epoch [4/300], Batch [30/43], Training Loss: 0.00004410
2024-11-06 14:06:50,653 - INFO - Epoch [4/300], Batch [31/43], Training Loss: 0.00007271
2024-11-06 14:06:50,657 - INFO - Epoch [4/300], Batch [32/43], Training Loss: 0.00008649
2024-11-06 14:06:50,660 - INFO - Epoch [4/300], Batch [33/43], Training Loss: 0.00006234
2024-11-06 14:06:50,664 - INFO - Epoch [4/300], Batch [34/43], Training Loss: 0.00003485
2024-11-06 14:06:50,667 - INFO - Epoch [4/300], Batch [35/43], Training Loss: 0.00004026
2024-11-06 14:06:50,670 - INFO - Epoch [4/300], Batch [36/43], Training Loss: 0.00006124
2024-11-06 14:06:50,674 - INFO - Epoch [4/300], Batch [37/43], Training Loss: 0.00009555
2024-11-06 14:06:50,678 - INFO - Epoch [4/300], Batch [38/43], Training Loss: 0.00005053
2024-11-06 14:06:50,683 - INFO - Epoch [4/300], Batch [39/43], Training Loss: 0.00005606
2024-11-06 14:06:50,688 - INFO - Epoch [4/300], Batch [40/43], Training Loss: 0.00003664
2024-11-06 14:06:50,692 - INFO - Epoch [4/300], Batch [41/43], Training Loss: 0.00005764
2024-11-06 14:06:50,695 - INFO - Epoch [4/300], Batch [42/43], Training Loss: 0.00005083
2024-11-06 14:06:50,699 - INFO - Epoch [4/300], Batch [43/43], Training Loss: 0.00005557
2024-11-06 14:06:50,710 - INFO - Epoch [4/300], Average Training Loss: 0.00005838, Validation Loss: 0.00009431
2024-11-06 14:06:50,714 - INFO - Epoch [5/300], Batch [1/43], Training Loss: 0.00006079
2024-11-06 14:06:50,718 - INFO - Epoch [5/300], Batch [2/43], Training Loss: 0.00007694
2024-11-06 14:06:50,721 - INFO - Epoch [5/300], Batch [3/43], Training Loss: 0.00004964
2024-11-06 14:06:50,725 - INFO - Epoch [5/300], Batch [4/43], Training Loss: 0.00011077
2024-11-06 14:06:50,729 - INFO - Epoch [5/300], Batch [5/43], Training Loss: 0.00001882
2024-11-06 14:06:50,732 - INFO - Epoch [5/300], Batch [6/43], Training Loss: 0.00003591
2024-11-06 14:06:50,736 - INFO - Epoch [5/300], Batch [7/43], Training Loss: 0.00002912
2024-11-06 14:06:50,739 - INFO - Epoch [5/300], Batch [8/43], Training Loss: 0.00007292
2024-11-06 14:06:50,743 - INFO - Epoch [5/300], Batch [9/43], Training Loss: 0.00005728
2024-11-06 14:06:50,747 - INFO - Epoch [5/300], Batch [10/43], Training Loss: 0.00005396
2024-11-06 14:06:50,751 - INFO - Epoch [5/300], Batch [11/43], Training Loss: 0.00004511
2024-11-06 14:06:50,754 - INFO - Epoch [5/300], Batch [12/43], Training Loss: 0.00008186
2024-11-06 14:06:50,757 - INFO - Epoch [5/300], Batch [13/43], Training Loss: 0.00003027
2024-11-06 14:06:50,761 - INFO - Epoch [5/300], Batch [14/43], Training Loss: 0.00003335
2024-11-06 14:06:50,765 - INFO - Epoch [5/300], Batch [15/43], Training Loss: 0.00005918
2024-11-06 14:06:50,769 - INFO - Epoch [5/300], Batch [16/43], Training Loss: 0.00008587
2024-11-06 14:06:50,773 - INFO - Epoch [5/300], Batch [17/43], Training Loss: 0.00007482
2024-11-06 14:06:50,777 - INFO - Epoch [5/300], Batch [18/43], Training Loss: 0.00009989
2024-11-06 14:06:50,780 - INFO - Epoch [5/300], Batch [19/43], Training Loss: 0.00003110
2024-11-06 14:06:50,784 - INFO - Epoch [5/300], Batch [20/43], Training Loss: 0.00003242
2024-11-06 14:06:50,787 - INFO - Epoch [5/300], Batch [21/43], Training Loss: 0.00007785
2024-11-06 14:06:50,790 - INFO - Epoch [5/300], Batch [22/43], Training Loss: 0.00003920
2024-11-06 14:06:50,792 - INFO - Epoch [5/300], Batch [23/43], Training Loss: 0.00003749
2024-11-06 14:06:50,795 - INFO - Epoch [5/300], Batch [24/43], Training Loss: 0.00006262
2024-11-06 14:06:50,799 - INFO - Epoch [5/300], Batch [25/43], Training Loss: 0.00007348
2024-11-06 14:06:50,802 - INFO - Epoch [5/300], Batch [26/43], Training Loss: 0.00003310
2024-11-06 14:06:50,805 - INFO - Epoch [5/300], Batch [27/43], Training Loss: 0.00008783
2024-11-06 14:06:50,809 - INFO - Epoch [5/300], Batch [28/43], Training Loss: 0.00002711
2024-11-06 14:06:50,812 - INFO - Epoch [5/300], Batch [29/43], Training Loss: 0.00004108
2024-11-06 14:06:50,816 - INFO - Epoch [5/300], Batch [30/43], Training Loss: 0.00012415
2024-11-06 14:06:50,820 - INFO - Epoch [5/300], Batch [31/43], Training Loss: 0.00005097
2024-11-06 14:06:50,824 - INFO - Epoch [5/300], Batch [32/43], Training Loss: 0.00006478
2024-11-06 14:06:50,827 - INFO - Epoch [5/300], Batch [33/43], Training Loss: 0.00009103
2024-11-06 14:06:50,831 - INFO - Epoch [5/300], Batch [34/43], Training Loss: 0.00006066
2024-11-06 14:06:50,835 - INFO - Epoch [5/300], Batch [35/43], Training Loss: 0.00004548
2024-11-06 14:06:50,838 - INFO - Epoch [5/300], Batch [36/43], Training Loss: 0.00008038
2024-11-06 14:06:50,842 - INFO - Epoch [5/300], Batch [37/43], Training Loss: 0.00006003
2024-11-06 14:06:50,845 - INFO - Epoch [5/300], Batch [38/43], Training Loss: 0.00004131
2024-11-06 14:06:50,849 - INFO - Epoch [5/300], Batch [39/43], Training Loss: 0.00007290
2024-11-06 14:06:50,851 - INFO - Epoch [5/300], Batch [40/43], Training Loss: 0.00006293
2024-11-06 14:06:50,855 - INFO - Epoch [5/300], Batch [41/43], Training Loss: 0.00006222
2024-11-06 14:06:50,858 - INFO - Epoch [5/300], Batch [42/43], Training Loss: 0.00006576
2024-11-06 14:06:50,862 - INFO - Epoch [5/300], Batch [43/43], Training Loss: 0.00002454
2024-11-06 14:06:50,873 - INFO - Epoch [5/300], Average Training Loss: 0.00005877, Validation Loss: 0.00007411
2024-11-06 14:06:50,877 - INFO - Epoch [6/300], Batch [1/43], Training Loss: 0.00002963
2024-11-06 14:06:50,882 - INFO - Epoch [6/300], Batch [2/43], Training Loss: 0.00007440
2024-11-06 14:06:50,885 - INFO - Epoch [6/300], Batch [3/43], Training Loss: 0.00003911
2024-11-06 14:06:50,889 - INFO - Epoch [6/300], Batch [4/43], Training Loss: 0.00005544
2024-11-06 14:06:50,893 - INFO - Epoch [6/300], Batch [5/43], Training Loss: 0.00013755
2024-11-06 14:06:50,895 - INFO - Epoch [6/300], Batch [6/43], Training Loss: 0.00002658
2024-11-06 14:06:50,898 - INFO - Epoch [6/300], Batch [7/43], Training Loss: 0.00008755
2024-11-06 14:06:50,901 - INFO - Epoch [6/300], Batch [8/43], Training Loss: 0.00005679
2024-11-06 14:06:50,904 - INFO - Epoch [6/300], Batch [9/43], Training Loss: 0.00004304
2024-11-06 14:06:50,907 - INFO - Epoch [6/300], Batch [10/43], Training Loss: 0.00007713
2024-11-06 14:06:50,910 - INFO - Epoch [6/300], Batch [11/43], Training Loss: 0.00004644
2024-11-06 14:06:50,914 - INFO - Epoch [6/300], Batch [12/43], Training Loss: 0.00004566
2024-11-06 14:06:50,917 - INFO - Epoch [6/300], Batch [13/43], Training Loss: 0.00002381
2024-11-06 14:06:50,920 - INFO - Epoch [6/300], Batch [14/43], Training Loss: 0.00002823
2024-11-06 14:06:50,923 - INFO - Epoch [6/300], Batch [15/43], Training Loss: 0.00006346
2024-11-06 14:06:50,926 - INFO - Epoch [6/300], Batch [16/43], Training Loss: 0.00003470
2024-11-06 14:06:50,930 - INFO - Epoch [6/300], Batch [17/43], Training Loss: 0.00004818
2024-11-06 14:06:50,933 - INFO - Epoch [6/300], Batch [18/43], Training Loss: 0.00008189
2024-11-06 14:06:50,936 - INFO - Epoch [6/300], Batch [19/43], Training Loss: 0.00007943
2024-11-06 14:06:50,939 - INFO - Epoch [6/300], Batch [20/43], Training Loss: 0.00003978
2024-11-06 14:06:50,943 - INFO - Epoch [6/300], Batch [21/43], Training Loss: 0.00008848
2024-11-06 14:06:50,946 - INFO - Epoch [6/300], Batch [22/43], Training Loss: 0.00004587
2024-11-06 14:06:50,949 - INFO - Epoch [6/300], Batch [23/43], Training Loss: 0.00004689
2024-11-06 14:06:50,951 - INFO - Epoch [6/300], Batch [24/43], Training Loss: 0.00007986
2024-11-06 14:06:50,954 - INFO - Epoch [6/300], Batch [25/43], Training Loss: 0.00007507
2024-11-06 14:06:50,957 - INFO - Epoch [6/300], Batch [26/43], Training Loss: 0.00005721
2024-11-06 14:06:50,961 - INFO - Epoch [6/300], Batch [27/43], Training Loss: 0.00004717
2024-11-06 14:06:50,964 - INFO - Epoch [6/300], Batch [28/43], Training Loss: 0.00005210
2024-11-06 14:06:50,968 - INFO - Epoch [6/300], Batch [29/43], Training Loss: 0.00010317
2024-11-06 14:06:50,971 - INFO - Epoch [6/300], Batch [30/43], Training Loss: 0.00008006
2024-11-06 14:06:50,974 - INFO - Epoch [6/300], Batch [31/43], Training Loss: 0.00007743
2024-11-06 14:06:50,976 - INFO - Epoch [6/300], Batch [32/43], Training Loss: 0.00003877
2024-11-06 14:06:50,979 - INFO - Epoch [6/300], Batch [33/43], Training Loss: 0.00003613
2024-11-06 14:06:50,982 - INFO - Epoch [6/300], Batch [34/43], Training Loss: 0.00004938
2024-11-06 14:06:50,985 - INFO - Epoch [6/300], Batch [35/43], Training Loss: 0.00001464
2024-11-06 14:06:50,987 - INFO - Epoch [6/300], Batch [36/43], Training Loss: 0.00006425
2024-11-06 14:06:50,990 - INFO - Epoch [6/300], Batch [37/43], Training Loss: 0.00006387
2024-11-06 14:06:50,994 - INFO - Epoch [6/300], Batch [38/43], Training Loss: 0.00009199
2024-11-06 14:06:50,997 - INFO - Epoch [6/300], Batch [39/43], Training Loss: 0.00010568
2024-11-06 14:06:51,000 - INFO - Epoch [6/300], Batch [40/43], Training Loss: 0.00004601
2024-11-06 14:06:51,003 - INFO - Epoch [6/300], Batch [41/43], Training Loss: 0.00004390
2024-11-06 14:06:51,007 - INFO - Epoch [6/300], Batch [42/43], Training Loss: 0.00008198
2024-11-06 14:06:51,011 - INFO - Epoch [6/300], Batch [43/43], Training Loss: 0.00003145
2024-11-06 14:06:51,021 - INFO - Epoch [6/300], Average Training Loss: 0.00005907, Validation Loss: 0.00007546
2024-11-06 14:06:51,024 - INFO - Epoch [7/300], Batch [1/43], Training Loss: 0.00005308
2024-11-06 14:06:51,028 - INFO - Epoch [7/300], Batch [2/43], Training Loss: 0.00009715
2024-11-06 14:06:51,030 - INFO - Epoch [7/300], Batch [3/43], Training Loss: 0.00003883
2024-11-06 14:06:51,034 - INFO - Epoch [7/300], Batch [4/43], Training Loss: 0.00005312
2024-11-06 14:06:51,037 - INFO - Epoch [7/300], Batch [5/43], Training Loss: 0.00006048
2024-11-06 14:06:51,040 - INFO - Epoch [7/300], Batch [6/43], Training Loss: 0.00005742
2024-11-06 14:06:51,042 - INFO - Epoch [7/300], Batch [7/43], Training Loss: 0.00008944
2024-11-06 14:06:51,045 - INFO - Epoch [7/300], Batch [8/43], Training Loss: 0.00006840
2024-11-06 14:06:51,048 - INFO - Epoch [7/300], Batch [9/43], Training Loss: 0.00006613
2024-11-06 14:06:51,052 - INFO - Epoch [7/300], Batch [10/43], Training Loss: 0.00005948
2024-11-06 14:06:51,056 - INFO - Epoch [7/300], Batch [11/43], Training Loss: 0.00006339
2024-11-06 14:06:51,059 - INFO - Epoch [7/300], Batch [12/43], Training Loss: 0.00005179
2024-11-06 14:06:51,062 - INFO - Epoch [7/300], Batch [13/43], Training Loss: 0.00005098
2024-11-06 14:06:51,066 - INFO - Epoch [7/300], Batch [14/43], Training Loss: 0.00003824
2024-11-06 14:06:51,070 - INFO - Epoch [7/300], Batch [15/43], Training Loss: 0.00003230
2024-11-06 14:06:51,073 - INFO - Epoch [7/300], Batch [16/43], Training Loss: 0.00005459
2024-11-06 14:06:51,077 - INFO - Epoch [7/300], Batch [17/43], Training Loss: 0.00004587
2024-11-06 14:06:51,080 - INFO - Epoch [7/300], Batch [18/43], Training Loss: 0.00006903
2024-11-06 14:06:51,084 - INFO - Epoch [7/300], Batch [19/43], Training Loss: 0.00006434
2024-11-06 14:06:51,087 - INFO - Epoch [7/300], Batch [20/43], Training Loss: 0.00003439
2024-11-06 14:06:51,091 - INFO - Epoch [7/300], Batch [21/43], Training Loss: 0.00005640
2024-11-06 14:06:51,095 - INFO - Epoch [7/300], Batch [22/43], Training Loss: 0.00012564
2024-11-06 14:06:51,098 - INFO - Epoch [7/300], Batch [23/43], Training Loss: 0.00004776
2024-11-06 14:06:51,102 - INFO - Epoch [7/300], Batch [24/43], Training Loss: 0.00006188
2024-11-06 14:06:51,105 - INFO - Epoch [7/300], Batch [25/43], Training Loss: 0.00005796
2024-11-06 14:06:51,108 - INFO - Epoch [7/300], Batch [26/43], Training Loss: 0.00005606
2024-11-06 14:06:51,111 - INFO - Epoch [7/300], Batch [27/43], Training Loss: 0.00009132
2024-11-06 14:06:51,114 - INFO - Epoch [7/300], Batch [28/43], Training Loss: 0.00008452
2024-11-06 14:06:51,117 - INFO - Epoch [7/300], Batch [29/43], Training Loss: 0.00004021
2024-11-06 14:06:51,119 - INFO - Epoch [7/300], Batch [30/43], Training Loss: 0.00005692
2024-11-06 14:06:51,122 - INFO - Epoch [7/300], Batch [31/43], Training Loss: 0.00006237
2024-11-06 14:06:51,125 - INFO - Epoch [7/300], Batch [32/43], Training Loss: 0.00004984
2024-11-06 14:06:51,128 - INFO - Epoch [7/300], Batch [33/43], Training Loss: 0.00009421
2024-11-06 14:06:51,131 - INFO - Epoch [7/300], Batch [34/43], Training Loss: 0.00005371
2024-11-06 14:06:51,134 - INFO - Epoch [7/300], Batch [35/43], Training Loss: 0.00003851
2024-11-06 14:06:51,137 - INFO - Epoch [7/300], Batch [36/43], Training Loss: 0.00007358
2024-11-06 14:06:51,139 - INFO - Epoch [7/300], Batch [37/43], Training Loss: 0.00005406
2024-11-06 14:06:51,142 - INFO - Epoch [7/300], Batch [38/43], Training Loss: 0.00006447
2024-11-06 14:06:51,145 - INFO - Epoch [7/300], Batch [39/43], Training Loss: 0.00005668
2024-11-06 14:06:51,149 - INFO - Epoch [7/300], Batch [40/43], Training Loss: 0.00010703
2024-11-06 14:06:51,152 - INFO - Epoch [7/300], Batch [41/43], Training Loss: 0.00005149
2024-11-06 14:06:51,155 - INFO - Epoch [7/300], Batch [42/43], Training Loss: 0.00004676
2024-11-06 14:06:51,159 - INFO - Epoch [7/300], Batch [43/43], Training Loss: 0.00003157
2024-11-06 14:06:51,170 - INFO - Epoch [7/300], Average Training Loss: 0.00006073, Validation Loss: 0.00008036
2024-11-06 14:06:51,173 - INFO - Epoch [8/300], Batch [1/43], Training Loss: 0.00012742
2024-11-06 14:06:51,176 - INFO - Epoch [8/300], Batch [2/43], Training Loss: 0.00002346
2024-11-06 14:06:51,180 - INFO - Epoch [8/300], Batch [3/43], Training Loss: 0.00003276
2024-11-06 14:06:51,184 - INFO - Epoch [8/300], Batch [4/43], Training Loss: 0.00004100
2024-11-06 14:06:51,188 - INFO - Epoch [8/300], Batch [5/43], Training Loss: 0.00006092
2024-11-06 14:06:51,191 - INFO - Epoch [8/300], Batch [6/43], Training Loss: 0.00006112
2024-11-06 14:06:51,195 - INFO - Epoch [8/300], Batch [7/43], Training Loss: 0.00003522
2024-11-06 14:06:51,198 - INFO - Epoch [8/300], Batch [8/43], Training Loss: 0.00007438
2024-11-06 14:06:51,201 - INFO - Epoch [8/300], Batch [9/43], Training Loss: 0.00003635
2024-11-06 14:06:51,205 - INFO - Epoch [8/300], Batch [10/43], Training Loss: 0.00003752
2024-11-06 14:06:51,208 - INFO - Epoch [8/300], Batch [11/43], Training Loss: 0.00004294
2024-11-06 14:06:51,211 - INFO - Epoch [8/300], Batch [12/43], Training Loss: 0.00010449
2024-11-06 14:06:51,214 - INFO - Epoch [8/300], Batch [13/43], Training Loss: 0.00005765
2024-11-06 14:06:51,217 - INFO - Epoch [8/300], Batch [14/43], Training Loss: 0.00005667
2024-11-06 14:06:51,219 - INFO - Epoch [8/300], Batch [15/43], Training Loss: 0.00005832
2024-11-06 14:06:51,222 - INFO - Epoch [8/300], Batch [16/43], Training Loss: 0.00009163
2024-11-06 14:06:51,225 - INFO - Epoch [8/300], Batch [17/43], Training Loss: 0.00005094
2024-11-06 14:06:51,228 - INFO - Epoch [8/300], Batch [18/43], Training Loss: 0.00006965
2024-11-06 14:06:51,231 - INFO - Epoch [8/300], Batch [19/43], Training Loss: 0.00003560
2024-11-06 14:06:51,235 - INFO - Epoch [8/300], Batch [20/43], Training Loss: 0.00008927
2024-11-06 14:06:51,238 - INFO - Epoch [8/300], Batch [21/43], Training Loss: 0.00005433
2024-11-06 14:06:51,241 - INFO - Epoch [8/300], Batch [22/43], Training Loss: 0.00004813
2024-11-06 14:06:51,245 - INFO - Epoch [8/300], Batch [23/43], Training Loss: 0.00004444
2024-11-06 14:06:51,248 - INFO - Epoch [8/300], Batch [24/43], Training Loss: 0.00003019
2024-11-06 14:06:51,251 - INFO - Epoch [8/300], Batch [25/43], Training Loss: 0.00003779
2024-11-06 14:06:51,253 - INFO - Epoch [8/300], Batch [26/43], Training Loss: 0.00006667
2024-11-06 14:06:51,257 - INFO - Epoch [8/300], Batch [27/43], Training Loss: 0.00006616
2024-11-06 14:06:51,261 - INFO - Epoch [8/300], Batch [28/43], Training Loss: 0.00005592
2024-11-06 14:06:51,264 - INFO - Epoch [8/300], Batch [29/43], Training Loss: 0.00002044
2024-11-06 14:06:51,268 - INFO - Epoch [8/300], Batch [30/43], Training Loss: 0.00004447
2024-11-06 14:06:51,271 - INFO - Epoch [8/300], Batch [31/43], Training Loss: 0.00002550
2024-11-06 14:06:51,275 - INFO - Epoch [8/300], Batch [32/43], Training Loss: 0.00005679
2024-11-06 14:06:51,279 - INFO - Epoch [8/300], Batch [33/43], Training Loss: 0.00010011
2024-11-06 14:06:51,283 - INFO - Epoch [8/300], Batch [34/43], Training Loss: 0.00001770
2024-11-06 14:06:51,286 - INFO - Epoch [8/300], Batch [35/43], Training Loss: 0.00005388
2024-11-06 14:06:51,290 - INFO - Epoch [8/300], Batch [36/43], Training Loss: 0.00006609
2024-11-06 14:06:51,294 - INFO - Epoch [8/300], Batch [37/43], Training Loss: 0.00009891
2024-11-06 14:06:51,298 - INFO - Epoch [8/300], Batch [38/43], Training Loss: 0.00004090
2024-11-06 14:06:51,302 - INFO - Epoch [8/300], Batch [39/43], Training Loss: 0.00008008
2024-11-06 14:06:51,306 - INFO - Epoch [8/300], Batch [40/43], Training Loss: 0.00007227
2024-11-06 14:06:51,309 - INFO - Epoch [8/300], Batch [41/43], Training Loss: 0.00007333
2024-11-06 14:06:51,313 - INFO - Epoch [8/300], Batch [42/43], Training Loss: 0.00006024
2024-11-06 14:06:51,317 - INFO - Epoch [8/300], Batch [43/43], Training Loss: 0.00007212
2024-11-06 14:06:51,329 - INFO - Epoch [8/300], Average Training Loss: 0.00005753, Validation Loss: 0.00007449
2024-11-06 14:06:51,333 - INFO - Epoch [9/300], Batch [1/43], Training Loss: 0.00006162
2024-11-06 14:06:51,337 - INFO - Epoch [9/300], Batch [2/43], Training Loss: 0.00003791
2024-11-06 14:06:51,341 - INFO - Epoch [9/300], Batch [3/43], Training Loss: 0.00005629
2024-11-06 14:06:51,345 - INFO - Epoch [9/300], Batch [4/43], Training Loss: 0.00008358
2024-11-06 14:06:51,349 - INFO - Epoch [9/300], Batch [5/43], Training Loss: 0.00005437
2024-11-06 14:06:51,354 - INFO - Epoch [9/300], Batch [6/43], Training Loss: 0.00007406
2024-11-06 14:06:51,357 - INFO - Epoch [9/300], Batch [7/43], Training Loss: 0.00007075
2024-11-06 14:06:51,361 - INFO - Epoch [9/300], Batch [8/43], Training Loss: 0.00003253
2024-11-06 14:06:51,366 - INFO - Epoch [9/300], Batch [9/43], Training Loss: 0.00003802
2024-11-06 14:06:51,369 - INFO - Epoch [9/300], Batch [10/43], Training Loss: 0.00005986
2024-11-06 14:06:51,373 - INFO - Epoch [9/300], Batch [11/43], Training Loss: 0.00007461
2024-11-06 14:06:51,377 - INFO - Epoch [9/300], Batch [12/43], Training Loss: 0.00006140
2024-11-06 14:06:51,381 - INFO - Epoch [9/300], Batch [13/43], Training Loss: 0.00006152
2024-11-06 14:06:51,384 - INFO - Epoch [9/300], Batch [14/43], Training Loss: 0.00005841
2024-11-06 14:06:51,387 - INFO - Epoch [9/300], Batch [15/43], Training Loss: 0.00003787
2024-11-06 14:06:51,391 - INFO - Epoch [9/300], Batch [16/43], Training Loss: 0.00010938
2024-11-06 14:06:51,394 - INFO - Epoch [9/300], Batch [17/43], Training Loss: 0.00003264
2024-11-06 14:06:51,399 - INFO - Epoch [9/300], Batch [18/43], Training Loss: 0.00003202
2024-11-06 14:06:51,404 - INFO - Epoch [9/300], Batch [19/43], Training Loss: 0.00004314
2024-11-06 14:06:51,408 - INFO - Epoch [9/300], Batch [20/43], Training Loss: 0.00007328
2024-11-06 14:06:51,413 - INFO - Epoch [9/300], Batch [21/43], Training Loss: 0.00003813
2024-11-06 14:06:51,417 - INFO - Epoch [9/300], Batch [22/43], Training Loss: 0.00004810
2024-11-06 14:06:51,420 - INFO - Epoch [9/300], Batch [23/43], Training Loss: 0.00004841
2024-11-06 14:06:51,423 - INFO - Epoch [9/300], Batch [24/43], Training Loss: 0.00012394
2024-11-06 14:06:51,427 - INFO - Epoch [9/300], Batch [25/43], Training Loss: 0.00003771
2024-11-06 14:06:51,431 - INFO - Epoch [9/300], Batch [26/43], Training Loss: 0.00007302
2024-11-06 14:06:51,436 - INFO - Epoch [9/300], Batch [27/43], Training Loss: 0.00006153
2024-11-06 14:06:51,440 - INFO - Epoch [9/300], Batch [28/43], Training Loss: 0.00004079
2024-11-06 14:06:51,444 - INFO - Epoch [9/300], Batch [29/43], Training Loss: 0.00004430
2024-11-06 14:06:51,448 - INFO - Epoch [9/300], Batch [30/43], Training Loss: 0.00004918
2024-11-06 14:06:51,452 - INFO - Epoch [9/300], Batch [31/43], Training Loss: 0.00005008
2024-11-06 14:06:51,456 - INFO - Epoch [9/300], Batch [32/43], Training Loss: 0.00005760
2024-11-06 14:06:51,461 - INFO - Epoch [9/300], Batch [33/43], Training Loss: 0.00005800
2024-11-06 14:06:51,465 - INFO - Epoch [9/300], Batch [34/43], Training Loss: 0.00004979
2024-11-06 14:06:51,468 - INFO - Epoch [9/300], Batch [35/43], Training Loss: 0.00008559
2024-11-06 14:06:51,471 - INFO - Epoch [9/300], Batch [36/43], Training Loss: 0.00009238
2024-11-06 14:06:51,475 - INFO - Epoch [9/300], Batch [37/43], Training Loss: 0.00006439
2024-11-06 14:06:51,478 - INFO - Epoch [9/300], Batch [38/43], Training Loss: 0.00005786
2024-11-06 14:06:51,482 - INFO - Epoch [9/300], Batch [39/43], Training Loss: 0.00004894
2024-11-06 14:06:51,485 - INFO - Epoch [9/300], Batch [40/43], Training Loss: 0.00004742
2024-11-06 14:06:51,488 - INFO - Epoch [9/300], Batch [41/43], Training Loss: 0.00008457
2024-11-06 14:06:51,491 - INFO - Epoch [9/300], Batch [42/43], Training Loss: 0.00003874
2024-11-06 14:06:51,494 - INFO - Epoch [9/300], Batch [43/43], Training Loss: 0.00004858
2024-11-06 14:06:51,505 - INFO - Epoch [9/300], Average Training Loss: 0.00005819, Validation Loss: 0.00007347
2024-11-06 14:06:51,508 - INFO - Epoch [10/300], Batch [1/43], Training Loss: 0.00007925
2024-11-06 14:06:51,511 - INFO - Epoch [10/300], Batch [2/43], Training Loss: 0.00007024
2024-11-06 14:06:51,513 - INFO - Epoch [10/300], Batch [3/43], Training Loss: 0.00004001
2024-11-06 14:06:51,516 - INFO - Epoch [10/300], Batch [4/43], Training Loss: 0.00004285
2024-11-06 14:06:51,520 - INFO - Epoch [10/300], Batch [5/43], Training Loss: 0.00007088
2024-11-06 14:06:51,523 - INFO - Epoch [10/300], Batch [6/43], Training Loss: 0.00003137
2024-11-06 14:06:51,527 - INFO - Epoch [10/300], Batch [7/43], Training Loss: 0.00003837
2024-11-06 14:06:51,530 - INFO - Epoch [10/300], Batch [8/43], Training Loss: 0.00005024
2024-11-06 14:06:51,534 - INFO - Epoch [10/300], Batch [9/43], Training Loss: 0.00005673
2024-11-06 14:06:51,538 - INFO - Epoch [10/300], Batch [10/43], Training Loss: 0.00011149
2024-11-06 14:06:51,541 - INFO - Epoch [10/300], Batch [11/43], Training Loss: 0.00005434
2024-11-06 14:06:51,544 - INFO - Epoch [10/300], Batch [12/43], Training Loss: 0.00010099
2024-11-06 14:06:51,547 - INFO - Epoch [10/300], Batch [13/43], Training Loss: 0.00005959
2024-11-06 14:06:51,550 - INFO - Epoch [10/300], Batch [14/43], Training Loss: 0.00008051
2024-11-06 14:06:51,553 - INFO - Epoch [10/300], Batch [15/43], Training Loss: 0.00004219
2024-11-06 14:06:51,557 - INFO - Epoch [10/300], Batch [16/43], Training Loss: 0.00006623
2024-11-06 14:06:51,560 - INFO - Epoch [10/300], Batch [17/43], Training Loss: 0.00004117
2024-11-06 14:06:51,564 - INFO - Epoch [10/300], Batch [18/43], Training Loss: 0.00004304
2024-11-06 14:06:51,567 - INFO - Epoch [10/300], Batch [19/43], Training Loss: 0.00004989
2024-11-06 14:06:51,571 - INFO - Epoch [10/300], Batch [20/43], Training Loss: 0.00006119
2024-11-06 14:06:51,574 - INFO - Epoch [10/300], Batch [21/43], Training Loss: 0.00008777
2024-11-06 14:06:51,577 - INFO - Epoch [10/300], Batch [22/43], Training Loss: 0.00010110
2024-11-06 14:06:51,580 - INFO - Epoch [10/300], Batch [23/43], Training Loss: 0.00004852
2024-11-06 14:06:51,583 - INFO - Epoch [10/300], Batch [24/43], Training Loss: 0.00004581
2024-11-06 14:06:51,586 - INFO - Epoch [10/300], Batch [25/43], Training Loss: 0.00006362
2024-11-06 14:06:51,590 - INFO - Epoch [10/300], Batch [26/43], Training Loss: 0.00006686
2024-11-06 14:06:51,593 - INFO - Epoch [10/300], Batch [27/43], Training Loss: 0.00007723
2024-11-06 14:06:51,597 - INFO - Epoch [10/300], Batch [28/43], Training Loss: 0.00004288
2024-11-06 14:06:51,600 - INFO - Epoch [10/300], Batch [29/43], Training Loss: 0.00006050
2024-11-06 14:06:51,603 - INFO - Epoch [10/300], Batch [30/43], Training Loss: 0.00006236
2024-11-06 14:06:51,606 - INFO - Epoch [10/300], Batch [31/43], Training Loss: 0.00003137
2024-11-06 14:06:51,609 - INFO - Epoch [10/300], Batch [32/43], Training Loss: 0.00007254
2024-11-06 14:06:51,611 - INFO - Epoch [10/300], Batch [33/43], Training Loss: 0.00008346
2024-11-06 14:06:51,614 - INFO - Epoch [10/300], Batch [34/43], Training Loss: 0.00002512
2024-11-06 14:06:51,617 - INFO - Epoch [10/300], Batch [35/43], Training Loss: 0.00013796
2024-11-06 14:06:51,620 - INFO - Epoch [10/300], Batch [36/43], Training Loss: 0.00003864
2024-11-06 14:06:51,623 - INFO - Epoch [10/300], Batch [37/43], Training Loss: 0.00002499
2024-11-06 14:06:51,626 - INFO - Epoch [10/300], Batch [38/43], Training Loss: 0.00006571
2024-11-06 14:06:51,630 - INFO - Epoch [10/300], Batch [39/43], Training Loss: 0.00003521
2024-11-06 14:06:51,633 - INFO - Epoch [10/300], Batch [40/43], Training Loss: 0.00004247
2024-11-06 14:06:51,636 - INFO - Epoch [10/300], Batch [41/43], Training Loss: 0.00005241
2024-11-06 14:06:51,639 - INFO - Epoch [10/300], Batch [42/43], Training Loss: 0.00007947
2024-11-06 14:06:51,643 - INFO - Epoch [10/300], Batch [43/43], Training Loss: 0.00004816
2024-11-06 14:06:51,653 - INFO - Epoch [10/300], Average Training Loss: 0.00006011, Validation Loss: 0.00007501
2024-11-06 14:06:51,656 - INFO - Epoch [11/300], Batch [1/43], Training Loss: 0.00009700
2024-11-06 14:06:51,659 - INFO - Epoch [11/300], Batch [2/43], Training Loss: 0.00003616
2024-11-06 14:06:51,662 - INFO - Epoch [11/300], Batch [3/43], Training Loss: 0.00005381
2024-11-06 14:06:51,666 - INFO - Epoch [11/300], Batch [4/43], Training Loss: 0.00006514
2024-11-06 14:06:51,669 - INFO - Epoch [11/300], Batch [5/43], Training Loss: 0.00006798
2024-11-06 14:06:51,672 - INFO - Epoch [11/300], Batch [6/43], Training Loss: 0.00003169
2024-11-06 14:06:51,675 - INFO - Epoch [11/300], Batch [7/43], Training Loss: 0.00002618
2024-11-06 14:06:51,678 - INFO - Epoch [11/300], Batch [8/43], Training Loss: 0.00003074
2024-11-06 14:06:51,681 - INFO - Epoch [11/300], Batch [9/43], Training Loss: 0.00006310
2024-11-06 14:06:51,684 - INFO - Epoch [11/300], Batch [10/43], Training Loss: 0.00004522
2024-11-06 14:06:51,687 - INFO - Epoch [11/300], Batch [11/43], Training Loss: 0.00003929
2024-11-06 14:06:51,690 - INFO - Epoch [11/300], Batch [12/43], Training Loss: 0.00004958
2024-11-06 14:06:51,695 - INFO - Epoch [11/300], Batch [13/43], Training Loss: 0.00004533
2024-11-06 14:06:51,698 - INFO - Epoch [11/300], Batch [14/43], Training Loss: 0.00008320
2024-11-06 14:06:51,701 - INFO - Epoch [11/300], Batch [15/43], Training Loss: 0.00007955
2024-11-06 14:06:51,705 - INFO - Epoch [11/300], Batch [16/43], Training Loss: 0.00007549
2024-11-06 14:06:51,708 - INFO - Epoch [11/300], Batch [17/43], Training Loss: 0.00004993
2024-11-06 14:06:51,712 - INFO - Epoch [11/300], Batch [18/43], Training Loss: 0.00007478
2024-11-06 14:06:51,716 - INFO - Epoch [11/300], Batch [19/43], Training Loss: 0.00002849
2024-11-06 14:06:51,720 - INFO - Epoch [11/300], Batch [20/43], Training Loss: 0.00006815
2024-11-06 14:06:51,723 - INFO - Epoch [11/300], Batch [21/43], Training Loss: 0.00006921
2024-11-06 14:06:51,726 - INFO - Epoch [11/300], Batch [22/43], Training Loss: 0.00012110
2024-11-06 14:06:51,729 - INFO - Epoch [11/300], Batch [23/43], Training Loss: 0.00008607
2024-11-06 14:06:51,733 - INFO - Epoch [11/300], Batch [24/43], Training Loss: 0.00007638
2024-11-06 14:06:51,737 - INFO - Epoch [11/300], Batch [25/43], Training Loss: 0.00006242
2024-11-06 14:06:51,741 - INFO - Epoch [11/300], Batch [26/43], Training Loss: 0.00007200
2024-11-06 14:06:51,745 - INFO - Epoch [11/300], Batch [27/43], Training Loss: 0.00006561
2024-11-06 14:06:51,749 - INFO - Epoch [11/300], Batch [28/43], Training Loss: 0.00011153
2024-11-06 14:06:51,752 - INFO - Epoch [11/300], Batch [29/43], Training Loss: 0.00004917
2024-11-06 14:06:51,755 - INFO - Epoch [11/300], Batch [30/43], Training Loss: 0.00004872
2024-11-06 14:06:51,760 - INFO - Epoch [11/300], Batch [31/43], Training Loss: 0.00008291
2024-11-06 14:06:51,764 - INFO - Epoch [11/300], Batch [32/43], Training Loss: 0.00008188
2024-11-06 14:06:51,768 - INFO - Epoch [11/300], Batch [33/43], Training Loss: 0.00006418
2024-11-06 14:06:51,772 - INFO - Epoch [11/300], Batch [34/43], Training Loss: 0.00010292
2024-11-06 14:06:51,776 - INFO - Epoch [11/300], Batch [35/43], Training Loss: 0.00005333
2024-11-06 14:06:51,780 - INFO - Epoch [11/300], Batch [36/43], Training Loss: 0.00002545
2024-11-06 14:06:51,783 - INFO - Epoch [11/300], Batch [37/43], Training Loss: 0.00006541
2024-11-06 14:06:51,786 - INFO - Epoch [11/300], Batch [38/43], Training Loss: 0.00002873
2024-11-06 14:06:51,790 - INFO - Epoch [11/300], Batch [39/43], Training Loss: 0.00002187
2024-11-06 14:06:51,794 - INFO - Epoch [11/300], Batch [40/43], Training Loss: 0.00005865
2024-11-06 14:06:51,799 - INFO - Epoch [11/300], Batch [41/43], Training Loss: 0.00005032
2024-11-06 14:06:51,803 - INFO - Epoch [11/300], Batch [42/43], Training Loss: 0.00004601
2024-11-06 14:06:51,807 - INFO - Epoch [11/300], Batch [43/43], Training Loss: 0.00006950
2024-11-06 14:06:51,819 - INFO - Epoch [11/300], Average Training Loss: 0.00006103, Validation Loss: 0.00008420
2024-11-06 14:06:51,823 - INFO - Epoch [12/300], Batch [1/43], Training Loss: 0.00007237
2024-11-06 14:06:51,827 - INFO - Epoch [12/300], Batch [2/43], Training Loss: 0.00007581
2024-11-06 14:06:51,831 - INFO - Epoch [12/300], Batch [3/43], Training Loss: 0.00005777
2024-11-06 14:06:51,836 - INFO - Epoch [12/300], Batch [4/43], Training Loss: 0.00001671
2024-11-06 14:06:51,842 - INFO - Epoch [12/300], Batch [5/43], Training Loss: 0.00004667
2024-11-06 14:06:51,847 - INFO - Epoch [12/300], Batch [6/43], Training Loss: 0.00004592
2024-11-06 14:06:51,852 - INFO - Epoch [12/300], Batch [7/43], Training Loss: 0.00007592
2024-11-06 14:06:51,858 - INFO - Epoch [12/300], Batch [8/43], Training Loss: 0.00004646
2024-11-06 14:06:51,862 - INFO - Epoch [12/300], Batch [9/43], Training Loss: 0.00005200
2024-11-06 14:06:51,867 - INFO - Epoch [12/300], Batch [10/43], Training Loss: 0.00011123
2024-11-06 14:06:51,872 - INFO - Epoch [12/300], Batch [11/43], Training Loss: 0.00005348
2024-11-06 14:06:51,876 - INFO - Epoch [12/300], Batch [12/43], Training Loss: 0.00007642
2024-11-06 14:06:51,881 - INFO - Epoch [12/300], Batch [13/43], Training Loss: 0.00004436
2024-11-06 14:06:51,885 - INFO - Epoch [12/300], Batch [14/43], Training Loss: 0.00003108
2024-11-06 14:06:51,889 - INFO - Epoch [12/300], Batch [15/43], Training Loss: 0.00008040
2024-11-06 14:06:51,892 - INFO - Epoch [12/300], Batch [16/43], Training Loss: 0.00005459
2024-11-06 14:06:51,896 - INFO - Epoch [12/300], Batch [17/43], Training Loss: 0.00006464
2024-11-06 14:06:51,901 - INFO - Epoch [12/300], Batch [18/43], Training Loss: 0.00003701
2024-11-06 14:06:51,906 - INFO - Epoch [12/300], Batch [19/43], Training Loss: 0.00003449
2024-11-06 14:06:51,910 - INFO - Epoch [12/300], Batch [20/43], Training Loss: 0.00002686
2024-11-06 14:06:51,914 - INFO - Epoch [12/300], Batch [21/43], Training Loss: 0.00003452
2024-11-06 14:06:51,918 - INFO - Epoch [12/300], Batch [22/43], Training Loss: 0.00008564
2024-11-06 14:06:51,922 - INFO - Epoch [12/300], Batch [23/43], Training Loss: 0.00008554
2024-11-06 14:06:51,926 - INFO - Epoch [12/300], Batch [24/43], Training Loss: 0.00010202
2024-11-06 14:06:51,930 - INFO - Epoch [12/300], Batch [25/43], Training Loss: 0.00005528
2024-11-06 14:06:51,935 - INFO - Epoch [12/300], Batch [26/43], Training Loss: 0.00004512
2024-11-06 14:06:51,939 - INFO - Epoch [12/300], Batch [27/43], Training Loss: 0.00003723
2024-11-06 14:06:51,943 - INFO - Epoch [12/300], Batch [28/43], Training Loss: 0.00008409
2024-11-06 14:06:51,947 - INFO - Epoch [12/300], Batch [29/43], Training Loss: 0.00006196
2024-11-06 14:06:51,951 - INFO - Epoch [12/300], Batch [30/43], Training Loss: 0.00009055
2024-11-06 14:06:51,956 - INFO - Epoch [12/300], Batch [31/43], Training Loss: 0.00003836
2024-11-06 14:06:51,960 - INFO - Epoch [12/300], Batch [32/43], Training Loss: 0.00004349
2024-11-06 14:06:51,964 - INFO - Epoch [12/300], Batch [33/43], Training Loss: 0.00007794
2024-11-06 14:06:51,969 - INFO - Epoch [12/300], Batch [34/43], Training Loss: 0.00004523
2024-11-06 14:06:51,973 - INFO - Epoch [12/300], Batch [35/43], Training Loss: 0.00003174
2024-11-06 14:06:51,977 - INFO - Epoch [12/300], Batch [36/43], Training Loss: 0.00006560
2024-11-06 14:06:51,981 - INFO - Epoch [12/300], Batch [37/43], Training Loss: 0.00003707
2024-11-06 14:06:51,985 - INFO - Epoch [12/300], Batch [38/43], Training Loss: 0.00005810
2024-11-06 14:06:51,989 - INFO - Epoch [12/300], Batch [39/43], Training Loss: 0.00005633
2024-11-06 14:06:51,994 - INFO - Epoch [12/300], Batch [40/43], Training Loss: 0.00008560
2024-11-06 14:06:51,998 - INFO - Epoch [12/300], Batch [41/43], Training Loss: 0.00006970
2024-11-06 14:06:52,002 - INFO - Epoch [12/300], Batch [42/43], Training Loss: 0.00006270
2024-11-06 14:06:52,006 - INFO - Epoch [12/300], Batch [43/43], Training Loss: 0.00005336
2024-11-06 14:06:52,018 - INFO - Epoch [12/300], Average Training Loss: 0.00005840, Validation Loss: 0.00007442
2024-11-06 14:06:52,022 - INFO - Epoch [13/300], Batch [1/43], Training Loss: 0.00005306
2024-11-06 14:06:52,027 - INFO - Epoch [13/300], Batch [2/43], Training Loss: 0.00005649
2024-11-06 14:06:52,032 - INFO - Epoch [13/300], Batch [3/43], Training Loss: 0.00006692
2024-11-06 14:06:52,036 - INFO - Epoch [13/300], Batch [4/43], Training Loss: 0.00005546
2024-11-06 14:06:52,042 - INFO - Epoch [13/300], Batch [5/43], Training Loss: 0.00006583
2024-11-06 14:06:52,046 - INFO - Epoch [13/300], Batch [6/43], Training Loss: 0.00003841
2024-11-06 14:06:52,049 - INFO - Epoch [13/300], Batch [7/43], Training Loss: 0.00006519
2024-11-06 14:06:52,053 - INFO - Epoch [13/300], Batch [8/43], Training Loss: 0.00009064
2024-11-06 14:06:52,057 - INFO - Epoch [13/300], Batch [9/43], Training Loss: 0.00009150
2024-11-06 14:06:52,060 - INFO - Epoch [13/300], Batch [10/43], Training Loss: 0.00002989
2024-11-06 14:06:52,063 - INFO - Epoch [13/300], Batch [11/43], Training Loss: 0.00002641
2024-11-06 14:06:52,067 - INFO - Epoch [13/300], Batch [12/43], Training Loss: 0.00005569
2024-11-06 14:06:52,071 - INFO - Epoch [13/300], Batch [13/43], Training Loss: 0.00002683
2024-11-06 14:06:52,074 - INFO - Epoch [13/300], Batch [14/43], Training Loss: 0.00004190
2024-11-06 14:06:52,078 - INFO - Epoch [13/300], Batch [15/43], Training Loss: 0.00011170
2024-11-06 14:06:52,081 - INFO - Epoch [13/300], Batch [16/43], Training Loss: 0.00003784
2024-11-06 14:06:52,086 - INFO - Epoch [13/300], Batch [17/43], Training Loss: 0.00005431
2024-11-06 14:06:52,090 - INFO - Epoch [13/300], Batch [18/43], Training Loss: 0.00007916
2024-11-06 14:06:52,094 - INFO - Epoch [13/300], Batch [19/43], Training Loss: 0.00006932
2024-11-06 14:06:52,099 - INFO - Epoch [13/300], Batch [20/43], Training Loss: 0.00002987
2024-11-06 14:06:52,103 - INFO - Epoch [13/300], Batch [21/43], Training Loss: 0.00005175
2024-11-06 14:06:52,107 - INFO - Epoch [13/300], Batch [22/43], Training Loss: 0.00004829
2024-11-06 14:06:52,111 - INFO - Epoch [13/300], Batch [23/43], Training Loss: 0.00003970
2024-11-06 14:06:52,115 - INFO - Epoch [13/300], Batch [24/43], Training Loss: 0.00004029
2024-11-06 14:06:52,119 - INFO - Epoch [13/300], Batch [25/43], Training Loss: 0.00005366
2024-11-06 14:06:52,123 - INFO - Epoch [13/300], Batch [26/43], Training Loss: 0.00003021
2024-11-06 14:06:52,127 - INFO - Epoch [13/300], Batch [27/43], Training Loss: 0.00009428
2024-11-06 14:06:52,131 - INFO - Epoch [13/300], Batch [28/43], Training Loss: 0.00004754
2024-11-06 14:06:52,136 - INFO - Epoch [13/300], Batch [29/43], Training Loss: 0.00005774
2024-11-06 14:06:52,141 - INFO - Epoch [13/300], Batch [30/43], Training Loss: 0.00004995
2024-11-06 14:06:52,145 - INFO - Epoch [13/300], Batch [31/43], Training Loss: 0.00004339
2024-11-06 14:06:52,149 - INFO - Epoch [13/300], Batch [32/43], Training Loss: 0.00006124
2024-11-06 14:06:52,152 - INFO - Epoch [13/300], Batch [33/43], Training Loss: 0.00005007
2024-11-06 14:06:52,158 - INFO - Epoch [13/300], Batch [34/43], Training Loss: 0.00006868
2024-11-06 14:06:52,162 - INFO - Epoch [13/300], Batch [35/43], Training Loss: 0.00007384
2024-11-06 14:06:52,167 - INFO - Epoch [13/300], Batch [36/43], Training Loss: 0.00006015
2024-11-06 14:06:52,171 - INFO - Epoch [13/300], Batch [37/43], Training Loss: 0.00005629
2024-11-06 14:06:52,175 - INFO - Epoch [13/300], Batch [38/43], Training Loss: 0.00006499
2024-11-06 14:06:52,179 - INFO - Epoch [13/300], Batch [39/43], Training Loss: 0.00005989
2024-11-06 14:06:52,183 - INFO - Epoch [13/300], Batch [40/43], Training Loss: 0.00007196
2024-11-06 14:06:52,188 - INFO - Epoch [13/300], Batch [41/43], Training Loss: 0.00004238
2024-11-06 14:06:52,192 - INFO - Epoch [13/300], Batch [42/43], Training Loss: 0.00007761
2024-11-06 14:06:52,196 - INFO - Epoch [13/300], Batch [43/43], Training Loss: 0.00007510
2024-11-06 14:06:52,208 - INFO - Epoch [13/300], Average Training Loss: 0.00005734, Validation Loss: 0.00007286
2024-11-06 14:06:52,213 - INFO - Epoch [14/300], Batch [1/43], Training Loss: 0.00007189
2024-11-06 14:06:52,217 - INFO - Epoch [14/300], Batch [2/43], Training Loss: 0.00006512
2024-11-06 14:06:52,222 - INFO - Epoch [14/300], Batch [3/43], Training Loss: 0.00004835
2024-11-06 14:06:52,226 - INFO - Epoch [14/300], Batch [4/43], Training Loss: 0.00006866
2024-11-06 14:06:52,230 - INFO - Epoch [14/300], Batch [5/43], Training Loss: 0.00013624
2024-11-06 14:06:52,234 - INFO - Epoch [14/300], Batch [6/43], Training Loss: 0.00004220
2024-11-06 14:06:52,239 - INFO - Epoch [14/300], Batch [7/43], Training Loss: 0.00009345
2024-11-06 14:06:52,242 - INFO - Epoch [14/300], Batch [8/43], Training Loss: 0.00001787
2024-11-06 14:06:52,247 - INFO - Epoch [14/300], Batch [9/43], Training Loss: 0.00005346
2024-11-06 14:06:52,250 - INFO - Epoch [14/300], Batch [10/43], Training Loss: 0.00008235
2024-11-06 14:06:52,254 - INFO - Epoch [14/300], Batch [11/43], Training Loss: 0.00003016
2024-11-06 14:06:52,258 - INFO - Epoch [14/300], Batch [12/43], Training Loss: 0.00005124
2024-11-06 14:06:52,262 - INFO - Epoch [14/300], Batch [13/43], Training Loss: 0.00004626
2024-11-06 14:06:52,266 - INFO - Epoch [14/300], Batch [14/43], Training Loss: 0.00004409
2024-11-06 14:06:52,271 - INFO - Epoch [14/300], Batch [15/43], Training Loss: 0.00004620
2024-11-06 14:06:52,275 - INFO - Epoch [14/300], Batch [16/43], Training Loss: 0.00006339
2024-11-06 14:06:52,279 - INFO - Epoch [14/300], Batch [17/43], Training Loss: 0.00005497
2024-11-06 14:06:52,284 - INFO - Epoch [14/300], Batch [18/43], Training Loss: 0.00009657
2024-11-06 14:06:52,288 - INFO - Epoch [14/300], Batch [19/43], Training Loss: 0.00003328
2024-11-06 14:06:52,291 - INFO - Epoch [14/300], Batch [20/43], Training Loss: 0.00003873
2024-11-06 14:06:52,294 - INFO - Epoch [14/300], Batch [21/43], Training Loss: 0.00005448
2024-11-06 14:06:52,299 - INFO - Epoch [14/300], Batch [22/43], Training Loss: 0.00002840
2024-11-06 14:06:52,303 - INFO - Epoch [14/300], Batch [23/43], Training Loss: 0.00005970
2024-11-06 14:06:52,306 - INFO - Epoch [14/300], Batch [24/43], Training Loss: 0.00005635
2024-11-06 14:06:52,310 - INFO - Epoch [14/300], Batch [25/43], Training Loss: 0.00007611
2024-11-06 14:06:52,315 - INFO - Epoch [14/300], Batch [26/43], Training Loss: 0.00011428
2024-11-06 14:06:52,319 - INFO - Epoch [14/300], Batch [27/43], Training Loss: 0.00005976
2024-11-06 14:06:52,323 - INFO - Epoch [14/300], Batch [28/43], Training Loss: 0.00005391
2024-11-06 14:06:52,327 - INFO - Epoch [14/300], Batch [29/43], Training Loss: 0.00010932
2024-11-06 14:06:52,330 - INFO - Epoch [14/300], Batch [30/43], Training Loss: 0.00006761
2024-11-06 14:06:52,334 - INFO - Epoch [14/300], Batch [31/43], Training Loss: 0.00005499
2024-11-06 14:06:52,338 - INFO - Epoch [14/300], Batch [32/43], Training Loss: 0.00003421
2024-11-06 14:06:52,342 - INFO - Epoch [14/300], Batch [33/43], Training Loss: 0.00008210
2024-11-06 14:06:52,346 - INFO - Epoch [14/300], Batch [34/43], Training Loss: 0.00004465
2024-11-06 14:06:52,350 - INFO - Epoch [14/300], Batch [35/43], Training Loss: 0.00005738
2024-11-06 14:06:52,354 - INFO - Epoch [14/300], Batch [36/43], Training Loss: 0.00007265
2024-11-06 14:06:52,357 - INFO - Epoch [14/300], Batch [37/43], Training Loss: 0.00009276
2024-11-06 14:06:52,360 - INFO - Epoch [14/300], Batch [38/43], Training Loss: 0.00004731
2024-11-06 14:06:52,364 - INFO - Epoch [14/300], Batch [39/43], Training Loss: 0.00006966
2024-11-06 14:06:52,368 - INFO - Epoch [14/300], Batch [40/43], Training Loss: 0.00003381
2024-11-06 14:06:52,372 - INFO - Epoch [14/300], Batch [41/43], Training Loss: 0.00004201
2024-11-06 14:06:52,376 - INFO - Epoch [14/300], Batch [42/43], Training Loss: 0.00009454
2024-11-06 14:06:52,380 - INFO - Epoch [14/300], Batch [43/43], Training Loss: 0.00009020
2024-11-06 14:06:52,392 - INFO - Epoch [14/300], Average Training Loss: 0.00006234, Validation Loss: 0.00009423
2024-11-06 14:06:52,395 - INFO - Epoch [15/300], Batch [1/43], Training Loss: 0.00004832
2024-11-06 14:06:52,399 - INFO - Epoch [15/300], Batch [2/43], Training Loss: 0.00012226
2024-11-06 14:06:52,403 - INFO - Epoch [15/300], Batch [3/43], Training Loss: 0.00008474
2024-11-06 14:06:52,407 - INFO - Epoch [15/300], Batch [4/43], Training Loss: 0.00008239
2024-11-06 14:06:52,410 - INFO - Epoch [15/300], Batch [5/43], Training Loss: 0.00007029
2024-11-06 14:06:52,414 - INFO - Epoch [15/300], Batch [6/43], Training Loss: 0.00007080
2024-11-06 14:06:52,419 - INFO - Epoch [15/300], Batch [7/43], Training Loss: 0.00007730
2024-11-06 14:06:52,423 - INFO - Epoch [15/300], Batch [8/43], Training Loss: 0.00004616
2024-11-06 14:06:52,427 - INFO - Epoch [15/300], Batch [9/43], Training Loss: 0.00006797
2024-11-06 14:06:52,430 - INFO - Epoch [15/300], Batch [10/43], Training Loss: 0.00007260
2024-11-06 14:06:52,434 - INFO - Epoch [15/300], Batch [11/43], Training Loss: 0.00003201
2024-11-06 14:06:52,439 - INFO - Epoch [15/300], Batch [12/43], Training Loss: 0.00006136
2024-11-06 14:06:52,443 - INFO - Epoch [15/300], Batch [13/43], Training Loss: 0.00005257
2024-11-06 14:06:52,447 - INFO - Epoch [15/300], Batch [14/43], Training Loss: 0.00002548
2024-11-06 14:06:52,450 - INFO - Epoch [15/300], Batch [15/43], Training Loss: 0.00004722
2024-11-06 14:06:52,454 - INFO - Epoch [15/300], Batch [16/43], Training Loss: 0.00004250
2024-11-06 14:06:52,458 - INFO - Epoch [15/300], Batch [17/43], Training Loss: 0.00007337
2024-11-06 14:06:52,462 - INFO - Epoch [15/300], Batch [18/43], Training Loss: 0.00009391
2024-11-06 14:06:52,466 - INFO - Epoch [15/300], Batch [19/43], Training Loss: 0.00008156
2024-11-06 14:06:52,470 - INFO - Epoch [15/300], Batch [20/43], Training Loss: 0.00009696
2024-11-06 14:06:52,473 - INFO - Epoch [15/300], Batch [21/43], Training Loss: 0.00006018
2024-11-06 14:06:52,477 - INFO - Epoch [15/300], Batch [22/43], Training Loss: 0.00005715
2024-11-06 14:06:52,481 - INFO - Epoch [15/300], Batch [23/43], Training Loss: 0.00007401
2024-11-06 14:06:52,485 - INFO - Epoch [15/300], Batch [24/43], Training Loss: 0.00006030
2024-11-06 14:06:52,488 - INFO - Epoch [15/300], Batch [25/43], Training Loss: 0.00009484
2024-11-06 14:06:52,491 - INFO - Epoch [15/300], Batch [26/43], Training Loss: 0.00007750
2024-11-06 14:06:52,495 - INFO - Epoch [15/300], Batch [27/43], Training Loss: 0.00005420
2024-11-06 14:06:52,498 - INFO - Epoch [15/300], Batch [28/43], Training Loss: 0.00004584
2024-11-06 14:06:52,503 - INFO - Epoch [15/300], Batch [29/43], Training Loss: 0.00007718
2024-11-06 14:06:52,507 - INFO - Epoch [15/300], Batch [30/43], Training Loss: 0.00004850
2024-11-06 14:06:52,511 - INFO - Epoch [15/300], Batch [31/43], Training Loss: 0.00004776
2024-11-06 14:06:52,515 - INFO - Epoch [15/300], Batch [32/43], Training Loss: 0.00003345
2024-11-06 14:06:52,519 - INFO - Epoch [15/300], Batch [33/43], Training Loss: 0.00011356
2024-11-06 14:06:52,524 - INFO - Epoch [15/300], Batch [34/43], Training Loss: 0.00007405
2024-11-06 14:06:52,528 - INFO - Epoch [15/300], Batch [35/43], Training Loss: 0.00011813
2024-11-06 14:06:52,532 - INFO - Epoch [15/300], Batch [36/43], Training Loss: 0.00003948
2024-11-06 14:06:52,536 - INFO - Epoch [15/300], Batch [37/43], Training Loss: 0.00004364
2024-11-06 14:06:52,541 - INFO - Epoch [15/300], Batch [38/43], Training Loss: 0.00007374
2024-11-06 14:06:52,545 - INFO - Epoch [15/300], Batch [39/43], Training Loss: 0.00005832
2024-11-06 14:06:52,549 - INFO - Epoch [15/300], Batch [40/43], Training Loss: 0.00003299
2024-11-06 14:06:52,552 - INFO - Epoch [15/300], Batch [41/43], Training Loss: 0.00006724
2024-11-06 14:06:52,556 - INFO - Epoch [15/300], Batch [42/43], Training Loss: 0.00003222
2024-11-06 14:06:52,561 - INFO - Epoch [15/300], Batch [43/43], Training Loss: 0.00004305
2024-11-06 14:06:52,573 - INFO - Epoch [15/300], Average Training Loss: 0.00006458, Validation Loss: 0.00007529
2024-11-06 14:06:52,577 - INFO - Epoch [16/300], Batch [1/43], Training Loss: 0.00009294
2024-11-06 14:06:52,581 - INFO - Epoch [16/300], Batch [2/43], Training Loss: 0.00007298
2024-11-06 14:06:52,584 - INFO - Epoch [16/300], Batch [3/43], Training Loss: 0.00006737
2024-11-06 14:06:52,588 - INFO - Epoch [16/300], Batch [4/43], Training Loss: 0.00007015
2024-11-06 14:06:52,592 - INFO - Epoch [16/300], Batch [5/43], Training Loss: 0.00008149
2024-11-06 14:06:52,595 - INFO - Epoch [16/300], Batch [6/43], Training Loss: 0.00005942
2024-11-06 14:06:52,599 - INFO - Epoch [16/300], Batch [7/43], Training Loss: 0.00004879
2024-11-06 14:06:52,603 - INFO - Epoch [16/300], Batch [8/43], Training Loss: 0.00003598
2024-11-06 14:06:52,607 - INFO - Epoch [16/300], Batch [9/43], Training Loss: 0.00003323
2024-11-06 14:06:52,611 - INFO - Epoch [16/300], Batch [10/43], Training Loss: 0.00007356
2024-11-06 14:06:52,614 - INFO - Epoch [16/300], Batch [11/43], Training Loss: 0.00009003
2024-11-06 14:06:52,619 - INFO - Epoch [16/300], Batch [12/43], Training Loss: 0.00006284
2024-11-06 14:06:52,622 - INFO - Epoch [16/300], Batch [13/43], Training Loss: 0.00008002
2024-11-06 14:06:52,626 - INFO - Epoch [16/300], Batch [14/43], Training Loss: 0.00006458
2024-11-06 14:06:52,629 - INFO - Epoch [16/300], Batch [15/43], Training Loss: 0.00005359
2024-11-06 14:06:52,633 - INFO - Epoch [16/300], Batch [16/43], Training Loss: 0.00006485
2024-11-06 14:06:52,638 - INFO - Epoch [16/300], Batch [17/43], Training Loss: 0.00004331
2024-11-06 14:06:52,642 - INFO - Epoch [16/300], Batch [18/43], Training Loss: 0.00002912
2024-11-06 14:06:52,646 - INFO - Epoch [16/300], Batch [19/43], Training Loss: 0.00003683
2024-11-06 14:06:52,650 - INFO - Epoch [16/300], Batch [20/43], Training Loss: 0.00004718
2024-11-06 14:06:52,654 - INFO - Epoch [16/300], Batch [21/43], Training Loss: 0.00005715
2024-11-06 14:06:52,658 - INFO - Epoch [16/300], Batch [22/43], Training Loss: 0.00004303
2024-11-06 14:06:52,661 - INFO - Epoch [16/300], Batch [23/43], Training Loss: 0.00006333
2024-11-06 14:06:52,665 - INFO - Epoch [16/300], Batch [24/43], Training Loss: 0.00005351
2024-11-06 14:06:52,670 - INFO - Epoch [16/300], Batch [25/43], Training Loss: 0.00005165
2024-11-06 14:06:52,673 - INFO - Epoch [16/300], Batch [26/43], Training Loss: 0.00006111
2024-11-06 14:06:52,677 - INFO - Epoch [16/300], Batch [27/43], Training Loss: 0.00005914
2024-11-06 14:06:52,679 - INFO - Epoch [16/300], Batch [28/43], Training Loss: 0.00006213
2024-11-06 14:06:52,683 - INFO - Epoch [16/300], Batch [29/43], Training Loss: 0.00005236
2024-11-06 14:06:52,687 - INFO - Epoch [16/300], Batch [30/43], Training Loss: 0.00004325
2024-11-06 14:06:52,691 - INFO - Epoch [16/300], Batch [31/43], Training Loss: 0.00006263
2024-11-06 14:06:52,693 - INFO - Epoch [16/300], Batch [32/43], Training Loss: 0.00009063
2024-11-06 14:06:52,697 - INFO - Epoch [16/300], Batch [33/43], Training Loss: 0.00003414
2024-11-06 14:06:52,700 - INFO - Epoch [16/300], Batch [34/43], Training Loss: 0.00006234
2024-11-06 14:06:52,704 - INFO - Epoch [16/300], Batch [35/43], Training Loss: 0.00008578
2024-11-06 14:06:52,708 - INFO - Epoch [16/300], Batch [36/43], Training Loss: 0.00004578
2024-11-06 14:06:52,711 - INFO - Epoch [16/300], Batch [37/43], Training Loss: 0.00004388
2024-11-06 14:06:52,714 - INFO - Epoch [16/300], Batch [38/43], Training Loss: 0.00004755
2024-11-06 14:06:52,717 - INFO - Epoch [16/300], Batch [39/43], Training Loss: 0.00004952
2024-11-06 14:06:52,720 - INFO - Epoch [16/300], Batch [40/43], Training Loss: 0.00003600
2024-11-06 14:06:52,723 - INFO - Epoch [16/300], Batch [41/43], Training Loss: 0.00007953
2024-11-06 14:06:52,725 - INFO - Epoch [16/300], Batch [42/43], Training Loss: 0.00003972
2024-11-06 14:06:52,728 - INFO - Epoch [16/300], Batch [43/43], Training Loss: 0.00004276
2024-11-06 14:06:52,738 - INFO - Epoch [16/300], Average Training Loss: 0.00005756, Validation Loss: 0.00008290
2024-11-06 14:06:52,741 - INFO - Epoch [17/300], Batch [1/43], Training Loss: 0.00004871
2024-11-06 14:06:52,745 - INFO - Epoch [17/300], Batch [2/43], Training Loss: 0.00004113
2024-11-06 14:06:52,749 - INFO - Epoch [17/300], Batch [3/43], Training Loss: 0.00008136
2024-11-06 14:06:52,752 - INFO - Epoch [17/300], Batch [4/43], Training Loss: 0.00006462
2024-11-06 14:06:52,756 - INFO - Epoch [17/300], Batch [5/43], Training Loss: 0.00002736
2024-11-06 14:06:52,759 - INFO - Epoch [17/300], Batch [6/43], Training Loss: 0.00006968
2024-11-06 14:06:52,762 - INFO - Epoch [17/300], Batch [7/43], Training Loss: 0.00012369
2024-11-06 14:06:52,765 - INFO - Epoch [17/300], Batch [8/43], Training Loss: 0.00008563
2024-11-06 14:06:52,768 - INFO - Epoch [17/300], Batch [9/43], Training Loss: 0.00008378
2024-11-06 14:06:52,771 - INFO - Epoch [17/300], Batch [10/43], Training Loss: 0.00004876
2024-11-06 14:06:52,773 - INFO - Epoch [17/300], Batch [11/43], Training Loss: 0.00004477
2024-11-06 14:06:52,776 - INFO - Epoch [17/300], Batch [12/43], Training Loss: 0.00006011
2024-11-06 14:06:52,780 - INFO - Epoch [17/300], Batch [13/43], Training Loss: 0.00006100
2024-11-06 14:06:52,783 - INFO - Epoch [17/300], Batch [14/43], Training Loss: 0.00007890
2024-11-06 14:06:52,786 - INFO - Epoch [17/300], Batch [15/43], Training Loss: 0.00008135
2024-11-06 14:06:52,789 - INFO - Epoch [17/300], Batch [16/43], Training Loss: 0.00003378
2024-11-06 14:06:52,792 - INFO - Epoch [17/300], Batch [17/43], Training Loss: 0.00009823
2024-11-06 14:06:52,796 - INFO - Epoch [17/300], Batch [18/43], Training Loss: 0.00002250
2024-11-06 14:06:52,800 - INFO - Epoch [17/300], Batch [19/43], Training Loss: 0.00005805
2024-11-06 14:06:52,804 - INFO - Epoch [17/300], Batch [20/43], Training Loss: 0.00003505
2024-11-06 14:06:52,807 - INFO - Epoch [17/300], Batch [21/43], Training Loss: 0.00006358
2024-11-06 14:06:52,811 - INFO - Epoch [17/300], Batch [22/43], Training Loss: 0.00003415
2024-11-06 14:06:52,814 - INFO - Epoch [17/300], Batch [23/43], Training Loss: 0.00008740
2024-11-06 14:06:52,817 - INFO - Epoch [17/300], Batch [24/43], Training Loss: 0.00004257
2024-11-06 14:06:52,821 - INFO - Epoch [17/300], Batch [25/43], Training Loss: 0.00003508
2024-11-06 14:06:52,825 - INFO - Epoch [17/300], Batch [26/43], Training Loss: 0.00007901
2024-11-06 14:06:52,830 - INFO - Epoch [17/300], Batch [27/43], Training Loss: 0.00011550
2024-11-06 14:06:52,834 - INFO - Epoch [17/300], Batch [28/43], Training Loss: 0.00003142
2024-11-06 14:06:52,838 - INFO - Epoch [17/300], Batch [29/43], Training Loss: 0.00007751
2024-11-06 14:06:52,842 - INFO - Epoch [17/300], Batch [30/43], Training Loss: 0.00004648
2024-11-06 14:06:52,846 - INFO - Epoch [17/300], Batch [31/43], Training Loss: 0.00006631
2024-11-06 14:06:52,850 - INFO - Epoch [17/300], Batch [32/43], Training Loss: 0.00007199
2024-11-06 14:06:52,853 - INFO - Epoch [17/300], Batch [33/43], Training Loss: 0.00005915
2024-11-06 14:06:52,857 - INFO - Epoch [17/300], Batch [34/43], Training Loss: 0.00004162
2024-11-06 14:06:52,860 - INFO - Epoch [17/300], Batch [35/43], Training Loss: 0.00006505
2024-11-06 14:06:52,863 - INFO - Epoch [17/300], Batch [36/43], Training Loss: 0.00004423
2024-11-06 14:06:52,866 - INFO - Epoch [17/300], Batch [37/43], Training Loss: 0.00002182
2024-11-06 14:06:52,870 - INFO - Epoch [17/300], Batch [38/43], Training Loss: 0.00004373
2024-11-06 14:06:52,873 - INFO - Epoch [17/300], Batch [39/43], Training Loss: 0.00007844
2024-11-06 14:06:52,876 - INFO - Epoch [17/300], Batch [40/43], Training Loss: 0.00006263
2024-11-06 14:06:52,879 - INFO - Epoch [17/300], Batch [41/43], Training Loss: 0.00009013
2024-11-06 14:06:52,882 - INFO - Epoch [17/300], Batch [42/43], Training Loss: 0.00003259
2024-11-06 14:06:52,886 - INFO - Epoch [17/300], Batch [43/43], Training Loss: 0.00005651
2024-11-06 14:06:52,897 - INFO - Epoch [17/300], Average Training Loss: 0.00006036, Validation Loss: 0.00007645
2024-11-06 14:06:52,901 - INFO - Epoch [18/300], Batch [1/43], Training Loss: 0.00007150
2024-11-06 14:06:52,905 - INFO - Epoch [18/300], Batch [2/43], Training Loss: 0.00007973
2024-11-06 14:06:52,909 - INFO - Epoch [18/300], Batch [3/43], Training Loss: 0.00008057
2024-11-06 14:06:52,912 - INFO - Epoch [18/300], Batch [4/43], Training Loss: 0.00004675
2024-11-06 14:06:52,915 - INFO - Epoch [18/300], Batch [5/43], Training Loss: 0.00005605
2024-11-06 14:06:52,918 - INFO - Epoch [18/300], Batch [6/43], Training Loss: 0.00004773
2024-11-06 14:06:52,921 - INFO - Epoch [18/300], Batch [7/43], Training Loss: 0.00005550
2024-11-06 14:06:52,925 - INFO - Epoch [18/300], Batch [8/43], Training Loss: 0.00004010
2024-11-06 14:06:52,928 - INFO - Epoch [18/300], Batch [9/43], Training Loss: 0.00004770
2024-11-06 14:06:52,933 - INFO - Epoch [18/300], Batch [10/43], Training Loss: 0.00006892
2024-11-06 14:06:52,937 - INFO - Epoch [18/300], Batch [11/43], Training Loss: 0.00004574
2024-11-06 14:06:52,940 - INFO - Epoch [18/300], Batch [12/43], Training Loss: 0.00006935
2024-11-06 14:06:52,943 - INFO - Epoch [18/300], Batch [13/43], Training Loss: 0.00003737
2024-11-06 14:06:52,946 - INFO - Epoch [18/300], Batch [14/43], Training Loss: 0.00004717
2024-11-06 14:06:52,949 - INFO - Epoch [18/300], Batch [15/43], Training Loss: 0.00004163
2024-11-06 14:06:52,952 - INFO - Epoch [18/300], Batch [16/43], Training Loss: 0.00003962
2024-11-06 14:06:52,955 - INFO - Epoch [18/300], Batch [17/43], Training Loss: 0.00004666
2024-11-06 14:06:52,959 - INFO - Epoch [18/300], Batch [18/43], Training Loss: 0.00005310
2024-11-06 14:06:52,962 - INFO - Epoch [18/300], Batch [19/43], Training Loss: 0.00007490
2024-11-06 14:06:52,966 - INFO - Epoch [18/300], Batch [20/43], Training Loss: 0.00003662
2024-11-06 14:06:52,969 - INFO - Epoch [18/300], Batch [21/43], Training Loss: 0.00003024
2024-11-06 14:06:52,973 - INFO - Epoch [18/300], Batch [22/43], Training Loss: 0.00003821
2024-11-06 14:06:52,977 - INFO - Epoch [18/300], Batch [23/43], Training Loss: 0.00005023
2024-11-06 14:06:52,980 - INFO - Epoch [18/300], Batch [24/43], Training Loss: 0.00005710
2024-11-06 14:06:52,982 - INFO - Epoch [18/300], Batch [25/43], Training Loss: 0.00009533
2024-11-06 14:06:52,985 - INFO - Epoch [18/300], Batch [26/43], Training Loss: 0.00006426
2024-11-06 14:06:52,989 - INFO - Epoch [18/300], Batch [27/43], Training Loss: 0.00003806
2024-11-06 14:06:52,993 - INFO - Epoch [18/300], Batch [28/43], Training Loss: 0.00007532
2024-11-06 14:06:52,997 - INFO - Epoch [18/300], Batch [29/43], Training Loss: 0.00004993
2024-11-06 14:06:53,001 - INFO - Epoch [18/300], Batch [30/43], Training Loss: 0.00005615
2024-11-06 14:06:53,004 - INFO - Epoch [18/300], Batch [31/43], Training Loss: 0.00010426
2024-11-06 14:06:53,008 - INFO - Epoch [18/300], Batch [32/43], Training Loss: 0.00004397
2024-11-06 14:06:53,012 - INFO - Epoch [18/300], Batch [33/43], Training Loss: 0.00009643
2024-11-06 14:06:53,016 - INFO - Epoch [18/300], Batch [34/43], Training Loss: 0.00008280
2024-11-06 14:06:53,020 - INFO - Epoch [18/300], Batch [35/43], Training Loss: 0.00007296
2024-11-06 14:06:53,024 - INFO - Epoch [18/300], Batch [36/43], Training Loss: 0.00008161
2024-11-06 14:06:53,027 - INFO - Epoch [18/300], Batch [37/43], Training Loss: 0.00005586
2024-11-06 14:06:53,031 - INFO - Epoch [18/300], Batch [38/43], Training Loss: 0.00008921
2024-11-06 14:06:53,035 - INFO - Epoch [18/300], Batch [39/43], Training Loss: 0.00004697
2024-11-06 14:06:53,038 - INFO - Epoch [18/300], Batch [40/43], Training Loss: 0.00002921
2024-11-06 14:06:53,042 - INFO - Epoch [18/300], Batch [41/43], Training Loss: 0.00005477
2024-11-06 14:06:53,046 - INFO - Epoch [18/300], Batch [42/43], Training Loss: 0.00002639
2024-11-06 14:06:53,050 - INFO - Epoch [18/300], Batch [43/43], Training Loss: 0.00004257
2024-11-06 14:06:53,061 - INFO - Epoch [18/300], Average Training Loss: 0.00005741, Validation Loss: 0.00008263
2024-11-06 14:06:53,065 - INFO - Epoch [19/300], Batch [1/43], Training Loss: 0.00002196
2024-11-06 14:06:53,069 - INFO - Epoch [19/300], Batch [2/43], Training Loss: 0.00006696
2024-11-06 14:06:53,073 - INFO - Epoch [19/300], Batch [3/43], Training Loss: 0.00003081
2024-11-06 14:06:53,077 - INFO - Epoch [19/300], Batch [4/43], Training Loss: 0.00004342
2024-11-06 14:06:53,081 - INFO - Epoch [19/300], Batch [5/43], Training Loss: 0.00005057
2024-11-06 14:06:53,085 - INFO - Epoch [19/300], Batch [6/43], Training Loss: 0.00003341
2024-11-06 14:06:53,088 - INFO - Epoch [19/300], Batch [7/43], Training Loss: 0.00005053
2024-11-06 14:06:53,092 - INFO - Epoch [19/300], Batch [8/43], Training Loss: 0.00004025
2024-11-06 14:06:53,096 - INFO - Epoch [19/300], Batch [9/43], Training Loss: 0.00008073
2024-11-06 14:06:53,100 - INFO - Epoch [19/300], Batch [10/43], Training Loss: 0.00003074
2024-11-06 14:06:53,105 - INFO - Epoch [19/300], Batch [11/43], Training Loss: 0.00006211
2024-11-06 14:06:53,109 - INFO - Epoch [19/300], Batch [12/43], Training Loss: 0.00005708
2024-11-06 14:06:53,113 - INFO - Epoch [19/300], Batch [13/43], Training Loss: 0.00005854
2024-11-06 14:06:53,117 - INFO - Epoch [19/300], Batch [14/43], Training Loss: 0.00007130
2024-11-06 14:06:53,121 - INFO - Epoch [19/300], Batch [15/43], Training Loss: 0.00006981
2024-11-06 14:06:53,125 - INFO - Epoch [19/300], Batch [16/43], Training Loss: 0.00005038
2024-11-06 14:06:53,129 - INFO - Epoch [19/300], Batch [17/43], Training Loss: 0.00007827
2024-11-06 14:06:53,133 - INFO - Epoch [19/300], Batch [18/43], Training Loss: 0.00006763
2024-11-06 14:06:53,138 - INFO - Epoch [19/300], Batch [19/43], Training Loss: 0.00005462
2024-11-06 14:06:53,146 - INFO - Epoch [19/300], Batch [20/43], Training Loss: 0.00008421
2024-11-06 14:06:53,150 - INFO - Epoch [19/300], Batch [21/43], Training Loss: 0.00006887
2024-11-06 14:06:53,155 - INFO - Epoch [19/300], Batch [22/43], Training Loss: 0.00005444
2024-11-06 14:06:53,160 - INFO - Epoch [19/300], Batch [23/43], Training Loss: 0.00007440
2024-11-06 14:06:53,164 - INFO - Epoch [19/300], Batch [24/43], Training Loss: 0.00004303
2024-11-06 14:06:53,169 - INFO - Epoch [19/300], Batch [25/43], Training Loss: 0.00012730
2024-11-06 14:06:53,173 - INFO - Epoch [19/300], Batch [26/43], Training Loss: 0.00004339
2024-11-06 14:06:53,178 - INFO - Epoch [19/300], Batch [27/43], Training Loss: 0.00004991
2024-11-06 14:06:53,182 - INFO - Epoch [19/300], Batch [28/43], Training Loss: 0.00003618
2024-11-06 14:06:53,186 - INFO - Epoch [19/300], Batch [29/43], Training Loss: 0.00006113
2024-11-06 14:06:53,190 - INFO - Epoch [19/300], Batch [30/43], Training Loss: 0.00007919
2024-11-06 14:06:53,194 - INFO - Epoch [19/300], Batch [31/43], Training Loss: 0.00002765
2024-11-06 14:06:53,198 - INFO - Epoch [19/300], Batch [32/43], Training Loss: 0.00006717
2024-11-06 14:06:53,202 - INFO - Epoch [19/300], Batch [33/43], Training Loss: 0.00007620
2024-11-06 14:06:53,206 - INFO - Epoch [19/300], Batch [34/43], Training Loss: 0.00004555
2024-11-06 14:06:53,210 - INFO - Epoch [19/300], Batch [35/43], Training Loss: 0.00005234
2024-11-06 14:06:53,215 - INFO - Epoch [19/300], Batch [36/43], Training Loss: 0.00007349
2024-11-06 14:06:53,219 - INFO - Epoch [19/300], Batch [37/43], Training Loss: 0.00006590
2024-11-06 14:06:53,223 - INFO - Epoch [19/300], Batch [38/43], Training Loss: 0.00007840
2024-11-06 14:06:53,227 - INFO - Epoch [19/300], Batch [39/43], Training Loss: 0.00007766
2024-11-06 14:06:53,231 - INFO - Epoch [19/300], Batch [40/43], Training Loss: 0.00009991
2024-11-06 14:06:53,236 - INFO - Epoch [19/300], Batch [41/43], Training Loss: 0.00003746
2024-11-06 14:06:53,239 - INFO - Epoch [19/300], Batch [42/43], Training Loss: 0.00006851
2024-11-06 14:06:53,243 - INFO - Epoch [19/300], Batch [43/43], Training Loss: 0.00003206
2024-11-06 14:06:53,256 - INFO - Epoch [19/300], Average Training Loss: 0.00005915, Validation Loss: 0.00007282
2024-11-06 14:06:53,260 - INFO - Epoch [20/300], Batch [1/43], Training Loss: 0.00008409
2024-11-06 14:06:53,264 - INFO - Epoch [20/300], Batch [2/43], Training Loss: 0.00007857
2024-11-06 14:06:53,268 - INFO - Epoch [20/300], Batch [3/43], Training Loss: 0.00006093
2024-11-06 14:06:53,272 - INFO - Epoch [20/300], Batch [4/43], Training Loss: 0.00005408
2024-11-06 14:06:53,276 - INFO - Epoch [20/300], Batch [5/43], Training Loss: 0.00009674
2024-11-06 14:06:53,280 - INFO - Epoch [20/300], Batch [6/43], Training Loss: 0.00006402
2024-11-06 14:06:53,284 - INFO - Epoch [20/300], Batch [7/43], Training Loss: 0.00007172
2024-11-06 14:06:53,288 - INFO - Epoch [20/300], Batch [8/43], Training Loss: 0.00005569
2024-11-06 14:06:53,292 - INFO - Epoch [20/300], Batch [9/43], Training Loss: 0.00002228
2024-11-06 14:06:53,296 - INFO - Epoch [20/300], Batch [10/43], Training Loss: 0.00007131
2024-11-06 14:06:53,300 - INFO - Epoch [20/300], Batch [11/43], Training Loss: 0.00004526
2024-11-06 14:06:53,304 - INFO - Epoch [20/300], Batch [12/43], Training Loss: 0.00005194
2024-11-06 14:06:53,307 - INFO - Epoch [20/300], Batch [13/43], Training Loss: 0.00005065
2024-11-06 14:06:53,311 - INFO - Epoch [20/300], Batch [14/43], Training Loss: 0.00003711
2024-11-06 14:06:53,315 - INFO - Epoch [20/300], Batch [15/43], Training Loss: 0.00009288
2024-11-06 14:06:53,319 - INFO - Epoch [20/300], Batch [16/43], Training Loss: 0.00006626
2024-11-06 14:06:53,322 - INFO - Epoch [20/300], Batch [17/43], Training Loss: 0.00008630
2024-11-06 14:06:53,326 - INFO - Epoch [20/300], Batch [18/43], Training Loss: 0.00007327
2024-11-06 14:06:53,329 - INFO - Epoch [20/300], Batch [19/43], Training Loss: 0.00004705
2024-11-06 14:06:53,333 - INFO - Epoch [20/300], Batch [20/43], Training Loss: 0.00006700
2024-11-06 14:06:53,337 - INFO - Epoch [20/300], Batch [21/43], Training Loss: 0.00005663
2024-11-06 14:06:53,341 - INFO - Epoch [20/300], Batch [22/43], Training Loss: 0.00005937
2024-11-06 14:06:53,346 - INFO - Epoch [20/300], Batch [23/43], Training Loss: 0.00007947
2024-11-06 14:06:53,349 - INFO - Epoch [20/300], Batch [24/43], Training Loss: 0.00005713
2024-11-06 14:06:53,353 - INFO - Epoch [20/300], Batch [25/43], Training Loss: 0.00004004
2024-11-06 14:06:53,357 - INFO - Epoch [20/300], Batch [26/43], Training Loss: 0.00004155
2024-11-06 14:06:53,361 - INFO - Epoch [20/300], Batch [27/43], Training Loss: 0.00003309
2024-11-06 14:06:53,365 - INFO - Epoch [20/300], Batch [28/43], Training Loss: 0.00007886
2024-11-06 14:06:53,369 - INFO - Epoch [20/300], Batch [29/43], Training Loss: 0.00002792
2024-11-06 14:06:53,372 - INFO - Epoch [20/300], Batch [30/43], Training Loss: 0.00002981
2024-11-06 14:06:53,377 - INFO - Epoch [20/300], Batch [31/43], Training Loss: 0.00003905
2024-11-06 14:06:53,380 - INFO - Epoch [20/300], Batch [32/43], Training Loss: 0.00007698
2024-11-06 14:06:53,383 - INFO - Epoch [20/300], Batch [33/43], Training Loss: 0.00004644
2024-11-06 14:06:53,387 - INFO - Epoch [20/300], Batch [34/43], Training Loss: 0.00006449
2024-11-06 14:06:53,390 - INFO - Epoch [20/300], Batch [35/43], Training Loss: 0.00006650
2024-11-06 14:06:53,394 - INFO - Epoch [20/300], Batch [36/43], Training Loss: 0.00005455
2024-11-06 14:06:53,398 - INFO - Epoch [20/300], Batch [37/43], Training Loss: 0.00005978
2024-11-06 14:06:53,401 - INFO - Epoch [20/300], Batch [38/43], Training Loss: 0.00003756
2024-11-06 14:06:53,404 - INFO - Epoch [20/300], Batch [39/43], Training Loss: 0.00003833
2024-11-06 14:06:53,408 - INFO - Epoch [20/300], Batch [40/43], Training Loss: 0.00004641
2024-11-06 14:06:53,411 - INFO - Epoch [20/300], Batch [41/43], Training Loss: 0.00004291
2024-11-06 14:06:53,415 - INFO - Epoch [20/300], Batch [42/43], Training Loss: 0.00006237
2024-11-06 14:06:53,419 - INFO - Epoch [20/300], Batch [43/43], Training Loss: 0.00006597
2024-11-06 14:06:53,431 - INFO - Epoch [20/300], Average Training Loss: 0.00005773, Validation Loss: 0.00008306
2024-11-06 14:06:53,434 - INFO - Epoch [21/300], Batch [1/43], Training Loss: 0.00010841
2024-11-06 14:06:53,438 - INFO - Epoch [21/300], Batch [2/43], Training Loss: 0.00006621
2024-11-06 14:06:53,442 - INFO - Epoch [21/300], Batch [3/43], Training Loss: 0.00010718
2024-11-06 14:06:53,445 - INFO - Epoch [21/300], Batch [4/43], Training Loss: 0.00004056
2024-11-06 14:06:53,449 - INFO - Epoch [21/300], Batch [5/43], Training Loss: 0.00004531
2024-11-06 14:06:53,454 - INFO - Epoch [21/300], Batch [6/43], Training Loss: 0.00007214
2024-11-06 14:06:53,457 - INFO - Epoch [21/300], Batch [7/43], Training Loss: 0.00006927
2024-11-06 14:06:53,461 - INFO - Epoch [21/300], Batch [8/43], Training Loss: 0.00008843
2024-11-06 14:06:53,465 - INFO - Epoch [21/300], Batch [9/43], Training Loss: 0.00006666
2024-11-06 14:06:53,468 - INFO - Epoch [21/300], Batch [10/43], Training Loss: 0.00006776
2024-11-06 14:06:53,472 - INFO - Epoch [21/300], Batch [11/43], Training Loss: 0.00007159
2024-11-06 14:06:53,475 - INFO - Epoch [21/300], Batch [12/43], Training Loss: 0.00011658
2024-11-06 14:06:53,479 - INFO - Epoch [21/300], Batch [13/43], Training Loss: 0.00006642
2024-11-06 14:06:53,482 - INFO - Epoch [21/300], Batch [14/43], Training Loss: 0.00003803
2024-11-06 14:06:53,486 - INFO - Epoch [21/300], Batch [15/43], Training Loss: 0.00002051
2024-11-06 14:06:53,489 - INFO - Epoch [21/300], Batch [16/43], Training Loss: 0.00004915
2024-11-06 14:06:53,492 - INFO - Epoch [21/300], Batch [17/43], Training Loss: 0.00006062
2024-11-06 14:06:53,496 - INFO - Epoch [21/300], Batch [18/43], Training Loss: 0.00005450
2024-11-06 14:06:53,499 - INFO - Epoch [21/300], Batch [19/43], Training Loss: 0.00006967
2024-11-06 14:06:53,502 - INFO - Epoch [21/300], Batch [20/43], Training Loss: 0.00006633
2024-11-06 14:06:53,505 - INFO - Epoch [21/300], Batch [21/43], Training Loss: 0.00004351
2024-11-06 14:06:53,509 - INFO - Epoch [21/300], Batch [22/43], Training Loss: 0.00004216
2024-11-06 14:06:53,513 - INFO - Epoch [21/300], Batch [23/43], Training Loss: 0.00003496
2024-11-06 14:06:53,517 - INFO - Epoch [21/300], Batch [24/43], Training Loss: 0.00005566
2024-11-06 14:06:53,521 - INFO - Epoch [21/300], Batch [25/43], Training Loss: 0.00002104
2024-11-06 14:06:53,524 - INFO - Epoch [21/300], Batch [26/43], Training Loss: 0.00002902
2024-11-06 14:06:53,529 - INFO - Epoch [21/300], Batch [27/43], Training Loss: 0.00005171
2024-11-06 14:06:53,533 - INFO - Epoch [21/300], Batch [28/43], Training Loss: 0.00007312
2024-11-06 14:06:53,537 - INFO - Epoch [21/300], Batch [29/43], Training Loss: 0.00003097
2024-11-06 14:06:53,541 - INFO - Epoch [21/300], Batch [30/43], Training Loss: 0.00003592
2024-11-06 14:06:53,545 - INFO - Epoch [21/300], Batch [31/43], Training Loss: 0.00002810
2024-11-06 14:06:53,548 - INFO - Epoch [21/300], Batch [32/43], Training Loss: 0.00003194
2024-11-06 14:06:53,551 - INFO - Epoch [21/300], Batch [33/43], Training Loss: 0.00003702
2024-11-06 14:06:53,555 - INFO - Epoch [21/300], Batch [34/43], Training Loss: 0.00003458
2024-11-06 14:06:53,559 - INFO - Epoch [21/300], Batch [35/43], Training Loss: 0.00005895
2024-11-06 14:06:53,563 - INFO - Epoch [21/300], Batch [36/43], Training Loss: 0.00004334
2024-11-06 14:06:53,567 - INFO - Epoch [21/300], Batch [37/43], Training Loss: 0.00003869
2024-11-06 14:06:53,571 - INFO - Epoch [21/300], Batch [38/43], Training Loss: 0.00013056
2024-11-06 14:06:53,575 - INFO - Epoch [21/300], Batch [39/43], Training Loss: 0.00004719
2024-11-06 14:06:53,579 - INFO - Epoch [21/300], Batch [40/43], Training Loss: 0.00012141
2024-11-06 14:06:53,583 - INFO - Epoch [21/300], Batch [41/43], Training Loss: 0.00005333
2024-11-06 14:06:53,587 - INFO - Epoch [21/300], Batch [42/43], Training Loss: 0.00007138
2024-11-06 14:06:53,591 - INFO - Epoch [21/300], Batch [43/43], Training Loss: 0.00004569
2024-11-06 14:06:53,601 - INFO - Epoch [21/300], Average Training Loss: 0.00005827, Validation Loss: 0.00007231
2024-11-06 14:06:53,605 - INFO - Epoch [22/300], Batch [1/43], Training Loss: 0.00003903
2024-11-06 14:06:53,609 - INFO - Epoch [22/300], Batch [2/43], Training Loss: 0.00005332
2024-11-06 14:06:53,614 - INFO - Epoch [22/300], Batch [3/43], Training Loss: 0.00008959
2024-11-06 14:06:53,618 - INFO - Epoch [22/300], Batch [4/43], Training Loss: 0.00005432
2024-11-06 14:06:53,621 - INFO - Epoch [22/300], Batch [5/43], Training Loss: 0.00004739
2024-11-06 14:06:53,625 - INFO - Epoch [22/300], Batch [6/43], Training Loss: 0.00006210
2024-11-06 14:06:53,629 - INFO - Epoch [22/300], Batch [7/43], Training Loss: 0.00006717
2024-11-06 14:06:53,633 - INFO - Epoch [22/300], Batch [8/43], Training Loss: 0.00006392
2024-11-06 14:06:53,636 - INFO - Epoch [22/300], Batch [9/43], Training Loss: 0.00011361
2024-11-06 14:06:53,640 - INFO - Epoch [22/300], Batch [10/43], Training Loss: 0.00006428
2024-11-06 14:06:53,644 - INFO - Epoch [22/300], Batch [11/43], Training Loss: 0.00003901
2024-11-06 14:06:53,648 - INFO - Epoch [22/300], Batch [12/43], Training Loss: 0.00002822
2024-11-06 14:06:53,652 - INFO - Epoch [22/300], Batch [13/43], Training Loss: 0.00004667
2024-11-06 14:06:53,655 - INFO - Epoch [22/300], Batch [14/43], Training Loss: 0.00004814
2024-11-06 14:06:53,659 - INFO - Epoch [22/300], Batch [15/43], Training Loss: 0.00004753
2024-11-06 14:06:53,663 - INFO - Epoch [22/300], Batch [16/43], Training Loss: 0.00006101
2024-11-06 14:06:53,666 - INFO - Epoch [22/300], Batch [17/43], Training Loss: 0.00006709
2024-11-06 14:06:53,669 - INFO - Epoch [22/300], Batch [18/43], Training Loss: 0.00003421
2024-11-06 14:06:53,673 - INFO - Epoch [22/300], Batch [19/43], Training Loss: 0.00006501
2024-11-06 14:06:53,676 - INFO - Epoch [22/300], Batch [20/43], Training Loss: 0.00002428
2024-11-06 14:06:53,680 - INFO - Epoch [22/300], Batch [21/43], Training Loss: 0.00004723
2024-11-06 14:06:53,683 - INFO - Epoch [22/300], Batch [22/43], Training Loss: 0.00003620
2024-11-06 14:06:53,686 - INFO - Epoch [22/300], Batch [23/43], Training Loss: 0.00007346
2024-11-06 14:06:53,690 - INFO - Epoch [22/300], Batch [24/43], Training Loss: 0.00007009
2024-11-06 14:06:53,693 - INFO - Epoch [22/300], Batch [25/43], Training Loss: 0.00003439
2024-11-06 14:06:53,696 - INFO - Epoch [22/300], Batch [26/43], Training Loss: 0.00006307
2024-11-06 14:06:53,700 - INFO - Epoch [22/300], Batch [27/43], Training Loss: 0.00003419
2024-11-06 14:06:53,703 - INFO - Epoch [22/300], Batch [28/43], Training Loss: 0.00008984
2024-11-06 14:06:53,708 - INFO - Epoch [22/300], Batch [29/43], Training Loss: 0.00007434
2024-11-06 14:06:53,712 - INFO - Epoch [22/300], Batch [30/43], Training Loss: 0.00003451
2024-11-06 14:06:53,716 - INFO - Epoch [22/300], Batch [31/43], Training Loss: 0.00013937
2024-11-06 14:06:53,719 - INFO - Epoch [22/300], Batch [32/43], Training Loss: 0.00007183
2024-11-06 14:06:53,723 - INFO - Epoch [22/300], Batch [33/43], Training Loss: 0.00003873
2024-11-06 14:06:53,726 - INFO - Epoch [22/300], Batch [34/43], Training Loss: 0.00005290
2024-11-06 14:06:53,729 - INFO - Epoch [22/300], Batch [35/43], Training Loss: 0.00012410
2024-11-06 14:06:53,733 - INFO - Epoch [22/300], Batch [36/43], Training Loss: 0.00005577
2024-11-06 14:06:53,736 - INFO - Epoch [22/300], Batch [37/43], Training Loss: 0.00007854
2024-11-06 14:06:53,740 - INFO - Epoch [22/300], Batch [38/43], Training Loss: 0.00004037
2024-11-06 14:06:53,743 - INFO - Epoch [22/300], Batch [39/43], Training Loss: 0.00006847
2024-11-06 14:06:53,747 - INFO - Epoch [22/300], Batch [40/43], Training Loss: 0.00004480
2024-11-06 14:06:53,751 - INFO - Epoch [22/300], Batch [41/43], Training Loss: 0.00004723
2024-11-06 14:06:53,755 - INFO - Epoch [22/300], Batch [42/43], Training Loss: 0.00005768
2024-11-06 14:06:53,759 - INFO - Epoch [22/300], Batch [43/43], Training Loss: 0.00003801
2024-11-06 14:06:53,770 - INFO - Epoch [22/300], Average Training Loss: 0.00005886, Validation Loss: 0.00008715
2024-11-06 14:06:53,774 - INFO - Epoch [23/300], Batch [1/43], Training Loss: 0.00012545
2024-11-06 14:06:53,777 - INFO - Epoch [23/300], Batch [2/43], Training Loss: 0.00008000
2024-11-06 14:06:53,780 - INFO - Epoch [23/300], Batch [3/43], Training Loss: 0.00004065
2024-11-06 14:06:53,784 - INFO - Epoch [23/300], Batch [4/43], Training Loss: 0.00007894
2024-11-06 14:06:53,788 - INFO - Epoch [23/300], Batch [5/43], Training Loss: 0.00003666
2024-11-06 14:06:53,791 - INFO - Epoch [23/300], Batch [6/43], Training Loss: 0.00004729
2024-11-06 14:06:53,795 - INFO - Epoch [23/300], Batch [7/43], Training Loss: 0.00003120
2024-11-06 14:06:53,799 - INFO - Epoch [23/300], Batch [8/43], Training Loss: 0.00009293
2024-11-06 14:06:53,803 - INFO - Epoch [23/300], Batch [9/43], Training Loss: 0.00006110
2024-11-06 14:06:53,807 - INFO - Epoch [23/300], Batch [10/43], Training Loss: 0.00006765
2024-11-06 14:06:53,811 - INFO - Epoch [23/300], Batch [11/43], Training Loss: 0.00002678
2024-11-06 14:06:53,814 - INFO - Epoch [23/300], Batch [12/43], Training Loss: 0.00002285
2024-11-06 14:06:53,819 - INFO - Epoch [23/300], Batch [13/43], Training Loss: 0.00004916
2024-11-06 14:06:53,823 - INFO - Epoch [23/300], Batch [14/43], Training Loss: 0.00007417
2024-11-06 14:06:53,828 - INFO - Epoch [23/300], Batch [15/43], Training Loss: 0.00003834
2024-11-06 14:06:53,832 - INFO - Epoch [23/300], Batch [16/43], Training Loss: 0.00011981
2024-11-06 14:06:53,837 - INFO - Epoch [23/300], Batch [17/43], Training Loss: 0.00006127
2024-11-06 14:06:53,841 - INFO - Epoch [23/300], Batch [18/43], Training Loss: 0.00006479
2024-11-06 14:06:53,845 - INFO - Epoch [23/300], Batch [19/43], Training Loss: 0.00002571
2024-11-06 14:06:53,848 - INFO - Epoch [23/300], Batch [20/43], Training Loss: 0.00005009
2024-11-06 14:06:53,852 - INFO - Epoch [23/300], Batch [21/43], Training Loss: 0.00010392
2024-11-06 14:06:53,856 - INFO - Epoch [23/300], Batch [22/43], Training Loss: 0.00008250
2024-11-06 14:06:53,859 - INFO - Epoch [23/300], Batch [23/43], Training Loss: 0.00008583
2024-11-06 14:06:53,862 - INFO - Epoch [23/300], Batch [24/43], Training Loss: 0.00003643
2024-11-06 14:06:53,865 - INFO - Epoch [23/300], Batch [25/43], Training Loss: 0.00003763
2024-11-06 14:06:53,868 - INFO - Epoch [23/300], Batch [26/43], Training Loss: 0.00003538
2024-11-06 14:06:53,872 - INFO - Epoch [23/300], Batch [27/43], Training Loss: 0.00007014
2024-11-06 14:06:53,876 - INFO - Epoch [23/300], Batch [28/43], Training Loss: 0.00006095
2024-11-06 14:06:53,880 - INFO - Epoch [23/300], Batch [29/43], Training Loss: 0.00004491
2024-11-06 14:06:53,884 - INFO - Epoch [23/300], Batch [30/43], Training Loss: 0.00001728
2024-11-06 14:06:53,888 - INFO - Epoch [23/300], Batch [31/43], Training Loss: 0.00004468
2024-11-06 14:06:53,892 - INFO - Epoch [23/300], Batch [32/43], Training Loss: 0.00007365
2024-11-06 14:06:53,896 - INFO - Epoch [23/300], Batch [33/43], Training Loss: 0.00005544
2024-11-06 14:06:53,900 - INFO - Epoch [23/300], Batch [34/43], Training Loss: 0.00005375
2024-11-06 14:06:53,904 - INFO - Epoch [23/300], Batch [35/43], Training Loss: 0.00004326
2024-11-06 14:06:53,909 - INFO - Epoch [23/300], Batch [36/43], Training Loss: 0.00006846
2024-11-06 14:06:53,912 - INFO - Epoch [23/300], Batch [37/43], Training Loss: 0.00006536
2024-11-06 14:06:53,916 - INFO - Epoch [23/300], Batch [38/43], Training Loss: 0.00005569
2024-11-06 14:06:53,919 - INFO - Epoch [23/300], Batch [39/43], Training Loss: 0.00004126
2024-11-06 14:06:53,923 - INFO - Epoch [23/300], Batch [40/43], Training Loss: 0.00006807
2024-11-06 14:06:53,926 - INFO - Epoch [23/300], Batch [41/43], Training Loss: 0.00003267
2024-11-06 14:06:53,930 - INFO - Epoch [23/300], Batch [42/43], Training Loss: 0.00007321
2024-11-06 14:06:53,933 - INFO - Epoch [23/300], Batch [43/43], Training Loss: 0.00005944
2024-11-06 14:06:53,945 - INFO - Epoch [23/300], Average Training Loss: 0.00005825, Validation Loss: 0.00007864
2024-11-06 14:06:53,949 - INFO - Epoch [24/300], Batch [1/43], Training Loss: 0.00003943
2024-11-06 14:06:53,953 - INFO - Epoch [24/300], Batch [2/43], Training Loss: 0.00002431
2024-11-06 14:06:53,956 - INFO - Epoch [24/300], Batch [3/43], Training Loss: 0.00002977
2024-11-06 14:06:53,959 - INFO - Epoch [24/300], Batch [4/43], Training Loss: 0.00005844
2024-11-06 14:06:53,962 - INFO - Epoch [24/300], Batch [5/43], Training Loss: 0.00005971
2024-11-06 14:06:53,965 - INFO - Epoch [24/300], Batch [6/43], Training Loss: 0.00004918
2024-11-06 14:06:53,968 - INFO - Epoch [24/300], Batch [7/43], Training Loss: 0.00005036
2024-11-06 14:06:53,972 - INFO - Epoch [24/300], Batch [8/43], Training Loss: 0.00006154
2024-11-06 14:06:53,975 - INFO - Epoch [24/300], Batch [9/43], Training Loss: 0.00005124
2024-11-06 14:06:53,977 - INFO - Epoch [24/300], Batch [10/43], Training Loss: 0.00002730
2024-11-06 14:06:53,980 - INFO - Epoch [24/300], Batch [11/43], Training Loss: 0.00004531
2024-11-06 14:06:53,984 - INFO - Epoch [24/300], Batch [12/43], Training Loss: 0.00003510
2024-11-06 14:06:53,987 - INFO - Epoch [24/300], Batch [13/43], Training Loss: 0.00010056
2024-11-06 14:06:53,991 - INFO - Epoch [24/300], Batch [14/43], Training Loss: 0.00007865
2024-11-06 14:06:53,993 - INFO - Epoch [24/300], Batch [15/43], Training Loss: 0.00003523
2024-11-06 14:06:53,996 - INFO - Epoch [24/300], Batch [16/43], Training Loss: 0.00009113
2024-11-06 14:06:54,000 - INFO - Epoch [24/300], Batch [17/43], Training Loss: 0.00006127
2024-11-06 14:06:54,004 - INFO - Epoch [24/300], Batch [18/43], Training Loss: 0.00004580
2024-11-06 14:06:54,007 - INFO - Epoch [24/300], Batch [19/43], Training Loss: 0.00005233
2024-11-06 14:06:54,011 - INFO - Epoch [24/300], Batch [20/43], Training Loss: 0.00006031
2024-11-06 14:06:54,014 - INFO - Epoch [24/300], Batch [21/43], Training Loss: 0.00006529
2024-11-06 14:06:54,018 - INFO - Epoch [24/300], Batch [22/43], Training Loss: 0.00006637
2024-11-06 14:06:54,021 - INFO - Epoch [24/300], Batch [23/43], Training Loss: 0.00005781
2024-11-06 14:06:54,024 - INFO - Epoch [24/300], Batch [24/43], Training Loss: 0.00002691
2024-11-06 14:06:54,027 - INFO - Epoch [24/300], Batch [25/43], Training Loss: 0.00006786
2024-11-06 14:06:54,031 - INFO - Epoch [24/300], Batch [26/43], Training Loss: 0.00014024
2024-11-06 14:06:54,034 - INFO - Epoch [24/300], Batch [27/43], Training Loss: 0.00006840
2024-11-06 14:06:54,037 - INFO - Epoch [24/300], Batch [28/43], Training Loss: 0.00005396
2024-11-06 14:06:54,040 - INFO - Epoch [24/300], Batch [29/43], Training Loss: 0.00007860
2024-11-06 14:06:54,043 - INFO - Epoch [24/300], Batch [30/43], Training Loss: 0.00004972
2024-11-06 14:06:54,046 - INFO - Epoch [24/300], Batch [31/43], Training Loss: 0.00003838
2024-11-06 14:06:54,050 - INFO - Epoch [24/300], Batch [32/43], Training Loss: 0.00006149
2024-11-06 14:06:54,053 - INFO - Epoch [24/300], Batch [33/43], Training Loss: 0.00002904
2024-11-06 14:06:54,056 - INFO - Epoch [24/300], Batch [34/43], Training Loss: 0.00009414
2024-11-06 14:06:54,059 - INFO - Epoch [24/300], Batch [35/43], Training Loss: 0.00006117
2024-11-06 14:06:54,062 - INFO - Epoch [24/300], Batch [36/43], Training Loss: 0.00006055
2024-11-06 14:06:54,065 - INFO - Epoch [24/300], Batch [37/43], Training Loss: 0.00007501
2024-11-06 14:06:54,068 - INFO - Epoch [24/300], Batch [38/43], Training Loss: 0.00005616
2024-11-06 14:06:54,071 - INFO - Epoch [24/300], Batch [39/43], Training Loss: 0.00004538
2024-11-06 14:06:54,074 - INFO - Epoch [24/300], Batch [40/43], Training Loss: 0.00008507
2024-11-06 14:06:54,078 - INFO - Epoch [24/300], Batch [41/43], Training Loss: 0.00002613
2024-11-06 14:06:54,081 - INFO - Epoch [24/300], Batch [42/43], Training Loss: 0.00006100
2024-11-06 14:06:54,085 - INFO - Epoch [24/300], Batch [43/43], Training Loss: 0.00003227
2024-11-06 14:06:54,095 - INFO - Epoch [24/300], Average Training Loss: 0.00005716, Validation Loss: 0.00007813
2024-11-06 14:06:54,099 - INFO - Epoch [25/300], Batch [1/43], Training Loss: 0.00003346
2024-11-06 14:06:54,103 - INFO - Epoch [25/300], Batch [2/43], Training Loss: 0.00006385
2024-11-06 14:06:54,106 - INFO - Epoch [25/300], Batch [3/43], Training Loss: 0.00002658
2024-11-06 14:06:54,110 - INFO - Epoch [25/300], Batch [4/43], Training Loss: 0.00006880
2024-11-06 14:06:54,114 - INFO - Epoch [25/300], Batch [5/43], Training Loss: 0.00004426
2024-11-06 14:06:54,117 - INFO - Epoch [25/300], Batch [6/43], Training Loss: 0.00006286
2024-11-06 14:06:54,121 - INFO - Epoch [25/300], Batch [7/43], Training Loss: 0.00005713
2024-11-06 14:06:54,124 - INFO - Epoch [25/300], Batch [8/43], Training Loss: 0.00003726
2024-11-06 14:06:54,127 - INFO - Epoch [25/300], Batch [9/43], Training Loss: 0.00005555
2024-11-06 14:06:54,130 - INFO - Epoch [25/300], Batch [10/43], Training Loss: 0.00007209
2024-11-06 14:06:54,133 - INFO - Epoch [25/300], Batch [11/43], Training Loss: 0.00006523
2024-11-06 14:06:54,137 - INFO - Epoch [25/300], Batch [12/43], Training Loss: 0.00004496
2024-11-06 14:06:54,140 - INFO - Epoch [25/300], Batch [13/43], Training Loss: 0.00003922
2024-11-06 14:06:54,143 - INFO - Epoch [25/300], Batch [14/43], Training Loss: 0.00006366
2024-11-06 14:06:54,146 - INFO - Epoch [25/300], Batch [15/43], Training Loss: 0.00006523
2024-11-06 14:06:54,149 - INFO - Epoch [25/300], Batch [16/43], Training Loss: 0.00002555
2024-11-06 14:06:54,152 - INFO - Epoch [25/300], Batch [17/43], Training Loss: 0.00009680
2024-11-06 14:06:54,156 - INFO - Epoch [25/300], Batch [18/43], Training Loss: 0.00006185
2024-11-06 14:06:54,159 - INFO - Epoch [25/300], Batch [19/43], Training Loss: 0.00003726
2024-11-06 14:06:54,163 - INFO - Epoch [25/300], Batch [20/43], Training Loss: 0.00006191
2024-11-06 14:06:54,167 - INFO - Epoch [25/300], Batch [21/43], Training Loss: 0.00006424
2024-11-06 14:06:54,170 - INFO - Epoch [25/300], Batch [22/43], Training Loss: 0.00004116
2024-11-06 14:06:54,173 - INFO - Epoch [25/300], Batch [23/43], Training Loss: 0.00007584
2024-11-06 14:06:54,176 - INFO - Epoch [25/300], Batch [24/43], Training Loss: 0.00004504
2024-11-06 14:06:54,179 - INFO - Epoch [25/300], Batch [25/43], Training Loss: 0.00004950
2024-11-06 14:06:54,182 - INFO - Epoch [25/300], Batch [26/43], Training Loss: 0.00008214
2024-11-06 14:06:54,185 - INFO - Epoch [25/300], Batch [27/43], Training Loss: 0.00007620
2024-11-06 14:06:54,188 - INFO - Epoch [25/300], Batch [28/43], Training Loss: 0.00010529
2024-11-06 14:06:54,191 - INFO - Epoch [25/300], Batch [29/43], Training Loss: 0.00002019
2024-11-06 14:06:54,193 - INFO - Epoch [25/300], Batch [30/43], Training Loss: 0.00002937
2024-11-06 14:06:54,196 - INFO - Epoch [25/300], Batch [31/43], Training Loss: 0.00007354
2024-11-06 14:06:54,199 - INFO - Epoch [25/300], Batch [32/43], Training Loss: 0.00003523
2024-11-06 14:06:54,203 - INFO - Epoch [25/300], Batch [33/43], Training Loss: 0.00005113
2024-11-06 14:06:54,207 - INFO - Epoch [25/300], Batch [34/43], Training Loss: 0.00006480
2024-11-06 14:06:54,210 - INFO - Epoch [25/300], Batch [35/43], Training Loss: 0.00008975
2024-11-06 14:06:54,214 - INFO - Epoch [25/300], Batch [36/43], Training Loss: 0.00005705
2024-11-06 14:06:54,218 - INFO - Epoch [25/300], Batch [37/43], Training Loss: 0.00004880
2024-11-06 14:06:54,221 - INFO - Epoch [25/300], Batch [38/43], Training Loss: 0.00007120
2024-11-06 14:06:54,224 - INFO - Epoch [25/300], Batch [39/43], Training Loss: 0.00006348
2024-11-06 14:06:54,227 - INFO - Epoch [25/300], Batch [40/43], Training Loss: 0.00001583
2024-11-06 14:06:54,229 - INFO - Epoch [25/300], Batch [41/43], Training Loss: 0.00008376
2024-11-06 14:06:54,232 - INFO - Epoch [25/300], Batch [42/43], Training Loss: 0.00010865
2024-11-06 14:06:54,236 - INFO - Epoch [25/300], Batch [43/43], Training Loss: 0.00004502
2024-11-06 14:06:54,245 - INFO - Epoch [25/300], Average Training Loss: 0.00005769, Validation Loss: 0.00007359
2024-11-06 14:06:54,249 - INFO - Epoch [26/300], Batch [1/43], Training Loss: 0.00003383
2024-11-06 14:06:54,252 - INFO - Epoch [26/300], Batch [2/43], Training Loss: 0.00005582
2024-11-06 14:06:54,255 - INFO - Epoch [26/300], Batch [3/43], Training Loss: 0.00004787
2024-11-06 14:06:54,258 - INFO - Epoch [26/300], Batch [4/43], Training Loss: 0.00004126
2024-11-06 14:06:54,261 - INFO - Epoch [26/300], Batch [5/43], Training Loss: 0.00007724
2024-11-06 14:06:54,264 - INFO - Epoch [26/300], Batch [6/43], Training Loss: 0.00003751
2024-11-06 14:06:54,267 - INFO - Epoch [26/300], Batch [7/43], Training Loss: 0.00009690
2024-11-06 14:06:54,271 - INFO - Epoch [26/300], Batch [8/43], Training Loss: 0.00007256
2024-11-06 14:06:54,275 - INFO - Epoch [26/300], Batch [9/43], Training Loss: 0.00007761
2024-11-06 14:06:54,278 - INFO - Epoch [26/300], Batch [10/43], Training Loss: 0.00005295
2024-11-06 14:06:54,282 - INFO - Epoch [26/300], Batch [11/43], Training Loss: 0.00004805
2024-11-06 14:06:54,285 - INFO - Epoch [26/300], Batch [12/43], Training Loss: 0.00005043
2024-11-06 14:06:54,289 - INFO - Epoch [26/300], Batch [13/43], Training Loss: 0.00006069
2024-11-06 14:06:54,292 - INFO - Epoch [26/300], Batch [14/43], Training Loss: 0.00005423
2024-11-06 14:06:54,295 - INFO - Epoch [26/300], Batch [15/43], Training Loss: 0.00004624
2024-11-06 14:06:54,298 - INFO - Epoch [26/300], Batch [16/43], Training Loss: 0.00006361
2024-11-06 14:06:54,302 - INFO - Epoch [26/300], Batch [17/43], Training Loss: 0.00007334
2024-11-06 14:06:54,306 - INFO - Epoch [26/300], Batch [18/43], Training Loss: 0.00002653
2024-11-06 14:06:54,309 - INFO - Epoch [26/300], Batch [19/43], Training Loss: 0.00004794
2024-11-06 14:06:54,313 - INFO - Epoch [26/300], Batch [20/43], Training Loss: 0.00006377
2024-11-06 14:06:54,316 - INFO - Epoch [26/300], Batch [21/43], Training Loss: 0.00003521
2024-11-06 14:06:54,319 - INFO - Epoch [26/300], Batch [22/43], Training Loss: 0.00006195
2024-11-06 14:06:54,322 - INFO - Epoch [26/300], Batch [23/43], Training Loss: 0.00003967
2024-11-06 14:06:54,325 - INFO - Epoch [26/300], Batch [24/43], Training Loss: 0.00005556
2024-11-06 14:06:54,328 - INFO - Epoch [26/300], Batch [25/43], Training Loss: 0.00005493
2024-11-06 14:06:54,332 - INFO - Epoch [26/300], Batch [26/43], Training Loss: 0.00007416
2024-11-06 14:06:54,336 - INFO - Epoch [26/300], Batch [27/43], Training Loss: 0.00004461
2024-11-06 14:06:54,339 - INFO - Epoch [26/300], Batch [28/43], Training Loss: 0.00004335
2024-11-06 14:06:54,344 - INFO - Epoch [26/300], Batch [29/43], Training Loss: 0.00007910
2024-11-06 14:06:54,348 - INFO - Epoch [26/300], Batch [30/43], Training Loss: 0.00006301
2024-11-06 14:06:54,352 - INFO - Epoch [26/300], Batch [31/43], Training Loss: 0.00005440
2024-11-06 14:06:54,356 - INFO - Epoch [26/300], Batch [32/43], Training Loss: 0.00005757
2024-11-06 14:06:54,359 - INFO - Epoch [26/300], Batch [33/43], Training Loss: 0.00004514
2024-11-06 14:06:54,361 - INFO - Epoch [26/300], Batch [34/43], Training Loss: 0.00005140
2024-11-06 14:06:54,364 - INFO - Epoch [26/300], Batch [35/43], Training Loss: 0.00005868
2024-11-06 14:06:54,368 - INFO - Epoch [26/300], Batch [36/43], Training Loss: 0.00002722
2024-11-06 14:06:54,372 - INFO - Epoch [26/300], Batch [37/43], Training Loss: 0.00005303
2024-11-06 14:06:54,375 - INFO - Epoch [26/300], Batch [38/43], Training Loss: 0.00007485
2024-11-06 14:06:54,378 - INFO - Epoch [26/300], Batch [39/43], Training Loss: 0.00005948
2024-11-06 14:06:54,383 - INFO - Epoch [26/300], Batch [40/43], Training Loss: 0.00006163
2024-11-06 14:06:54,388 - INFO - Epoch [26/300], Batch [41/43], Training Loss: 0.00008420
2024-11-06 14:06:54,392 - INFO - Epoch [26/300], Batch [42/43], Training Loss: 0.00007596
2024-11-06 14:06:54,394 - INFO - Epoch [26/300], Batch [43/43], Training Loss: 0.00006490
2024-11-06 14:06:54,406 - INFO - Epoch [26/300], Average Training Loss: 0.00005694, Validation Loss: 0.00008084
2024-11-06 14:06:54,409 - INFO - Epoch [27/300], Batch [1/43], Training Loss: 0.00004999
2024-11-06 14:06:54,413 - INFO - Epoch [27/300], Batch [2/43], Training Loss: 0.00006660
2024-11-06 14:06:54,415 - INFO - Epoch [27/300], Batch [3/43], Training Loss: 0.00009107
2024-11-06 14:06:54,419 - INFO - Epoch [27/300], Batch [4/43], Training Loss: 0.00006501
2024-11-06 14:06:54,422 - INFO - Epoch [27/300], Batch [5/43], Training Loss: 0.00004465
2024-11-06 14:06:54,425 - INFO - Epoch [27/300], Batch [6/43], Training Loss: 0.00005376
2024-11-06 14:06:54,429 - INFO - Epoch [27/300], Batch [7/43], Training Loss: 0.00003811
2024-11-06 14:06:54,431 - INFO - Epoch [27/300], Batch [8/43], Training Loss: 0.00004305
2024-11-06 14:06:54,434 - INFO - Epoch [27/300], Batch [9/43], Training Loss: 0.00006828
2024-11-06 14:06:54,437 - INFO - Epoch [27/300], Batch [10/43], Training Loss: 0.00004770
2024-11-06 14:06:54,441 - INFO - Epoch [27/300], Batch [11/43], Training Loss: 0.00003296
2024-11-06 14:06:54,444 - INFO - Epoch [27/300], Batch [12/43], Training Loss: 0.00004410
2024-11-06 14:06:54,448 - INFO - Epoch [27/300], Batch [13/43], Training Loss: 0.00006560
2024-11-06 14:06:54,451 - INFO - Epoch [27/300], Batch [14/43], Training Loss: 0.00005718
2024-11-06 14:06:54,454 - INFO - Epoch [27/300], Batch [15/43], Training Loss: 0.00004184
2024-11-06 14:06:54,457 - INFO - Epoch [27/300], Batch [16/43], Training Loss: 0.00005021
2024-11-06 14:06:54,461 - INFO - Epoch [27/300], Batch [17/43], Training Loss: 0.00009715
2024-11-06 14:06:54,465 - INFO - Epoch [27/300], Batch [18/43], Training Loss: 0.00007394
2024-11-06 14:06:54,469 - INFO - Epoch [27/300], Batch [19/43], Training Loss: 0.00009258
2024-11-06 14:06:54,472 - INFO - Epoch [27/300], Batch [20/43], Training Loss: 0.00005871
2024-11-06 14:06:54,475 - INFO - Epoch [27/300], Batch [21/43], Training Loss: 0.00003433
2024-11-06 14:06:54,479 - INFO - Epoch [27/300], Batch [22/43], Training Loss: 0.00004097
2024-11-06 14:06:54,483 - INFO - Epoch [27/300], Batch [23/43], Training Loss: 0.00003034
2024-11-06 14:06:54,486 - INFO - Epoch [27/300], Batch [24/43], Training Loss: 0.00005810
2024-11-06 14:06:54,489 - INFO - Epoch [27/300], Batch [25/43], Training Loss: 0.00002191
2024-11-06 14:06:54,492 - INFO - Epoch [27/300], Batch [26/43], Training Loss: 0.00004378
2024-11-06 14:06:54,495 - INFO - Epoch [27/300], Batch [27/43], Training Loss: 0.00003119
2024-11-06 14:06:54,498 - INFO - Epoch [27/300], Batch [28/43], Training Loss: 0.00003230
2024-11-06 14:06:54,500 - INFO - Epoch [27/300], Batch [29/43], Training Loss: 0.00007282
2024-11-06 14:06:54,503 - INFO - Epoch [27/300], Batch [30/43], Training Loss: 0.00003293
2024-11-06 14:06:54,506 - INFO - Epoch [27/300], Batch [31/43], Training Loss: 0.00007675
2024-11-06 14:06:54,509 - INFO - Epoch [27/300], Batch [32/43], Training Loss: 0.00006785
2024-11-06 14:06:54,512 - INFO - Epoch [27/300], Batch [33/43], Training Loss: 0.00006484
2024-11-06 14:06:54,515 - INFO - Epoch [27/300], Batch [34/43], Training Loss: 0.00003823
2024-11-06 14:06:54,518 - INFO - Epoch [27/300], Batch [35/43], Training Loss: 0.00006844
2024-11-06 14:06:54,522 - INFO - Epoch [27/300], Batch [36/43], Training Loss: 0.00003268
2024-11-06 14:06:54,526 - INFO - Epoch [27/300], Batch [37/43], Training Loss: 0.00006245
2024-11-06 14:06:54,529 - INFO - Epoch [27/300], Batch [38/43], Training Loss: 0.00010150
2024-11-06 14:06:54,533 - INFO - Epoch [27/300], Batch [39/43], Training Loss: 0.00003655
2024-11-06 14:06:54,536 - INFO - Epoch [27/300], Batch [40/43], Training Loss: 0.00005350
2024-11-06 14:06:54,539 - INFO - Epoch [27/300], Batch [41/43], Training Loss: 0.00007062
2024-11-06 14:06:54,541 - INFO - Epoch [27/300], Batch [42/43], Training Loss: 0.00007221
2024-11-06 14:06:54,544 - INFO - Epoch [27/300], Batch [43/43], Training Loss: 0.00007416
2024-11-06 14:06:54,554 - INFO - Epoch [27/300], Average Training Loss: 0.00005584, Validation Loss: 0.00007053
2024-11-06 14:06:54,557 - INFO - Epoch [28/300], Batch [1/43], Training Loss: 0.00004119
2024-11-06 14:06:54,561 - INFO - Epoch [28/300], Batch [2/43], Training Loss: 0.00004553
2024-11-06 14:06:54,564 - INFO - Epoch [28/300], Batch [3/43], Training Loss: 0.00004080
2024-11-06 14:06:54,568 - INFO - Epoch [28/300], Batch [4/43], Training Loss: 0.00006259
2024-11-06 14:06:54,571 - INFO - Epoch [28/300], Batch [5/43], Training Loss: 0.00005012
2024-11-06 14:06:54,574 - INFO - Epoch [28/300], Batch [6/43], Training Loss: 0.00006097
2024-11-06 14:06:54,576 - INFO - Epoch [28/300], Batch [7/43], Training Loss: 0.00010096
2024-11-06 14:06:54,580 - INFO - Epoch [28/300], Batch [8/43], Training Loss: 0.00006960
2024-11-06 14:06:54,584 - INFO - Epoch [28/300], Batch [9/43], Training Loss: 0.00005734
2024-11-06 14:06:54,587 - INFO - Epoch [28/300], Batch [10/43], Training Loss: 0.00004419
2024-11-06 14:06:54,590 - INFO - Epoch [28/300], Batch [11/43], Training Loss: 0.00003790
2024-11-06 14:06:54,593 - INFO - Epoch [28/300], Batch [12/43], Training Loss: 0.00004860
2024-11-06 14:06:54,596 - INFO - Epoch [28/300], Batch [13/43], Training Loss: 0.00005088
2024-11-06 14:06:54,599 - INFO - Epoch [28/300], Batch [14/43], Training Loss: 0.00006511
2024-11-06 14:06:54,602 - INFO - Epoch [28/300], Batch [15/43], Training Loss: 0.00003166
2024-11-06 14:06:54,605 - INFO - Epoch [28/300], Batch [16/43], Training Loss: 0.00005831
2024-11-06 14:06:54,609 - INFO - Epoch [28/300], Batch [17/43], Training Loss: 0.00006528
2024-11-06 14:06:54,612 - INFO - Epoch [28/300], Batch [18/43], Training Loss: 0.00012898
2024-11-06 14:06:54,617 - INFO - Epoch [28/300], Batch [19/43], Training Loss: 0.00006661
2024-11-06 14:06:54,620 - INFO - Epoch [28/300], Batch [20/43], Training Loss: 0.00001891
2024-11-06 14:06:54,623 - INFO - Epoch [28/300], Batch [21/43], Training Loss: 0.00004113
2024-11-06 14:06:54,626 - INFO - Epoch [28/300], Batch [22/43], Training Loss: 0.00002332
2024-11-06 14:06:54,630 - INFO - Epoch [28/300], Batch [23/43], Training Loss: 0.00006794
2024-11-06 14:06:54,634 - INFO - Epoch [28/300], Batch [24/43], Training Loss: 0.00005738
2024-11-06 14:06:54,637 - INFO - Epoch [28/300], Batch [25/43], Training Loss: 0.00007350
2024-11-06 14:06:54,640 - INFO - Epoch [28/300], Batch [26/43], Training Loss: 0.00008668
2024-11-06 14:06:54,643 - INFO - Epoch [28/300], Batch [27/43], Training Loss: 0.00005168
2024-11-06 14:06:54,647 - INFO - Epoch [28/300], Batch [28/43], Training Loss: 0.00004694
2024-11-06 14:06:54,650 - INFO - Epoch [28/300], Batch [29/43], Training Loss: 0.00005798
2024-11-06 14:06:54,653 - INFO - Epoch [28/300], Batch [30/43], Training Loss: 0.00007078
2024-11-06 14:06:54,655 - INFO - Epoch [28/300], Batch [31/43], Training Loss: 0.00006679
2024-11-06 14:06:54,658 - INFO - Epoch [28/300], Batch [32/43], Training Loss: 0.00004861
2024-11-06 14:06:54,662 - INFO - Epoch [28/300], Batch [33/43], Training Loss: 0.00004583
2024-11-06 14:06:54,666 - INFO - Epoch [28/300], Batch [34/43], Training Loss: 0.00007239
2024-11-06 14:06:54,669 - INFO - Epoch [28/300], Batch [35/43], Training Loss: 0.00002580
2024-11-06 14:06:54,672 - INFO - Epoch [28/300], Batch [36/43], Training Loss: 0.00004974
2024-11-06 14:06:54,676 - INFO - Epoch [28/300], Batch [37/43], Training Loss: 0.00009229
2024-11-06 14:06:54,679 - INFO - Epoch [28/300], Batch [38/43], Training Loss: 0.00005080
2024-11-06 14:06:54,683 - INFO - Epoch [28/300], Batch [39/43], Training Loss: 0.00006025
2024-11-06 14:06:54,686 - INFO - Epoch [28/300], Batch [40/43], Training Loss: 0.00010375
2024-11-06 14:06:54,690 - INFO - Epoch [28/300], Batch [41/43], Training Loss: 0.00008680
2024-11-06 14:06:54,693 - INFO - Epoch [28/300], Batch [42/43], Training Loss: 0.00005294
2024-11-06 14:06:54,697 - INFO - Epoch [28/300], Batch [43/43], Training Loss: 0.00005192
2024-11-06 14:06:54,709 - INFO - Epoch [28/300], Average Training Loss: 0.00005885, Validation Loss: 0.00007020
2024-11-06 14:06:54,713 - INFO - Epoch [29/300], Batch [1/43], Training Loss: 0.00007504
2024-11-06 14:06:54,717 - INFO - Epoch [29/300], Batch [2/43], Training Loss: 0.00004344
2024-11-06 14:06:54,720 - INFO - Epoch [29/300], Batch [3/43], Training Loss: 0.00002044
2024-11-06 14:06:54,724 - INFO - Epoch [29/300], Batch [4/43], Training Loss: 0.00001558
2024-11-06 14:06:54,727 - INFO - Epoch [29/300], Batch [5/43], Training Loss: 0.00004980
2024-11-06 14:06:54,731 - INFO - Epoch [29/300], Batch [6/43], Training Loss: 0.00005237
2024-11-06 14:06:54,734 - INFO - Epoch [29/300], Batch [7/43], Training Loss: 0.00001720
2024-11-06 14:06:54,738 - INFO - Epoch [29/300], Batch [8/43], Training Loss: 0.00014608
2024-11-06 14:06:54,741 - INFO - Epoch [29/300], Batch [9/43], Training Loss: 0.00004733
2024-11-06 14:06:54,745 - INFO - Epoch [29/300], Batch [10/43], Training Loss: 0.00005069
2024-11-06 14:06:54,748 - INFO - Epoch [29/300], Batch [11/43], Training Loss: 0.00003519
2024-11-06 14:06:54,751 - INFO - Epoch [29/300], Batch [12/43], Training Loss: 0.00004912
2024-11-06 14:06:54,755 - INFO - Epoch [29/300], Batch [13/43], Training Loss: 0.00006650
2024-11-06 14:06:54,758 - INFO - Epoch [29/300], Batch [14/43], Training Loss: 0.00006494
2024-11-06 14:06:54,762 - INFO - Epoch [29/300], Batch [15/43], Training Loss: 0.00009023
2024-11-06 14:06:54,766 - INFO - Epoch [29/300], Batch [16/43], Training Loss: 0.00007880
2024-11-06 14:06:54,770 - INFO - Epoch [29/300], Batch [17/43], Training Loss: 0.00005093
2024-11-06 14:06:54,774 - INFO - Epoch [29/300], Batch [18/43], Training Loss: 0.00005829
2024-11-06 14:06:54,778 - INFO - Epoch [29/300], Batch [19/43], Training Loss: 0.00004865
2024-11-06 14:06:54,782 - INFO - Epoch [29/300], Batch [20/43], Training Loss: 0.00004778
2024-11-06 14:06:54,787 - INFO - Epoch [29/300], Batch [21/43], Training Loss: 0.00005727
2024-11-06 14:06:54,792 - INFO - Epoch [29/300], Batch [22/43], Training Loss: 0.00007566
2024-11-06 14:06:54,797 - INFO - Epoch [29/300], Batch [23/43], Training Loss: 0.00011386
2024-11-06 14:06:54,801 - INFO - Epoch [29/300], Batch [24/43], Training Loss: 0.00006258
2024-11-06 14:06:54,805 - INFO - Epoch [29/300], Batch [25/43], Training Loss: 0.00009351
2024-11-06 14:06:54,809 - INFO - Epoch [29/300], Batch [26/43], Training Loss: 0.00007132
2024-11-06 14:06:54,814 - INFO - Epoch [29/300], Batch [27/43], Training Loss: 0.00009702
2024-11-06 14:06:54,817 - INFO - Epoch [29/300], Batch [28/43], Training Loss: 0.00006212
2024-11-06 14:06:54,821 - INFO - Epoch [29/300], Batch [29/43], Training Loss: 0.00006633
2024-11-06 14:06:54,825 - INFO - Epoch [29/300], Batch [30/43], Training Loss: 0.00007244
2024-11-06 14:06:54,829 - INFO - Epoch [29/300], Batch [31/43], Training Loss: 0.00005064
2024-11-06 14:06:54,833 - INFO - Epoch [29/300], Batch [32/43], Training Loss: 0.00005103
2024-11-06 14:06:54,837 - INFO - Epoch [29/300], Batch [33/43], Training Loss: 0.00005139
2024-11-06 14:06:54,841 - INFO - Epoch [29/300], Batch [34/43], Training Loss: 0.00002951
2024-11-06 14:06:54,845 - INFO - Epoch [29/300], Batch [35/43], Training Loss: 0.00006240
2024-11-06 14:06:54,849 - INFO - Epoch [29/300], Batch [36/43], Training Loss: 0.00014736
2024-11-06 14:06:54,853 - INFO - Epoch [29/300], Batch [37/43], Training Loss: 0.00003443
2024-11-06 14:06:54,856 - INFO - Epoch [29/300], Batch [38/43], Training Loss: 0.00005486
2024-11-06 14:06:54,861 - INFO - Epoch [29/300], Batch [39/43], Training Loss: 0.00006788
2024-11-06 14:06:54,864 - INFO - Epoch [29/300], Batch [40/43], Training Loss: 0.00004517
2024-11-06 14:06:54,868 - INFO - Epoch [29/300], Batch [41/43], Training Loss: 0.00007061
2024-11-06 14:06:54,871 - INFO - Epoch [29/300], Batch [42/43], Training Loss: 0.00006065
2024-11-06 14:06:54,875 - INFO - Epoch [29/300], Batch [43/43], Training Loss: 0.00003895
2024-11-06 14:06:54,886 - INFO - Epoch [29/300], Average Training Loss: 0.00006152, Validation Loss: 0.00007405
2024-11-06 14:06:54,890 - INFO - Epoch [30/300], Batch [1/43], Training Loss: 0.00008266
2024-11-06 14:06:54,893 - INFO - Epoch [30/300], Batch [2/43], Training Loss: 0.00006381
2024-11-06 14:06:54,897 - INFO - Epoch [30/300], Batch [3/43], Training Loss: 0.00010366
2024-11-06 14:06:54,901 - INFO - Epoch [30/300], Batch [4/43], Training Loss: 0.00003839
2024-11-06 14:06:54,904 - INFO - Epoch [30/300], Batch [5/43], Training Loss: 0.00002159
2024-11-06 14:06:54,908 - INFO - Epoch [30/300], Batch [6/43], Training Loss: 0.00005156
2024-11-06 14:06:54,911 - INFO - Epoch [30/300], Batch [7/43], Training Loss: 0.00005834
2024-11-06 14:06:54,914 - INFO - Epoch [30/300], Batch [8/43], Training Loss: 0.00005792
2024-11-06 14:06:54,917 - INFO - Epoch [30/300], Batch [9/43], Training Loss: 0.00003705
2024-11-06 14:06:54,920 - INFO - Epoch [30/300], Batch [10/43], Training Loss: 0.00005439
2024-11-06 14:06:54,923 - INFO - Epoch [30/300], Batch [11/43], Training Loss: 0.00004154
2024-11-06 14:06:54,926 - INFO - Epoch [30/300], Batch [12/43], Training Loss: 0.00004336
2024-11-06 14:06:54,929 - INFO - Epoch [30/300], Batch [13/43], Training Loss: 0.00009374
2024-11-06 14:06:54,933 - INFO - Epoch [30/300], Batch [14/43], Training Loss: 0.00002719
2024-11-06 14:06:54,936 - INFO - Epoch [30/300], Batch [15/43], Training Loss: 0.00002292
2024-11-06 14:06:54,939 - INFO - Epoch [30/300], Batch [16/43], Training Loss: 0.00003503
2024-11-06 14:06:54,942 - INFO - Epoch [30/300], Batch [17/43], Training Loss: 0.00004385
2024-11-06 14:06:54,946 - INFO - Epoch [30/300], Batch [18/43], Training Loss: 0.00006010
2024-11-06 14:06:54,949 - INFO - Epoch [30/300], Batch [19/43], Training Loss: 0.00003918
2024-11-06 14:06:54,952 - INFO - Epoch [30/300], Batch [20/43], Training Loss: 0.00002467
2024-11-06 14:06:54,956 - INFO - Epoch [30/300], Batch [21/43], Training Loss: 0.00003682
2024-11-06 14:06:54,959 - INFO - Epoch [30/300], Batch [22/43], Training Loss: 0.00007861
2024-11-06 14:06:54,963 - INFO - Epoch [30/300], Batch [23/43], Training Loss: 0.00007877
2024-11-06 14:06:54,966 - INFO - Epoch [30/300], Batch [24/43], Training Loss: 0.00007836
2024-11-06 14:06:54,968 - INFO - Epoch [30/300], Batch [25/43], Training Loss: 0.00003488
2024-11-06 14:06:54,971 - INFO - Epoch [30/300], Batch [26/43], Training Loss: 0.00002939
2024-11-06 14:06:54,975 - INFO - Epoch [30/300], Batch [27/43], Training Loss: 0.00004602
2024-11-06 14:06:54,978 - INFO - Epoch [30/300], Batch [28/43], Training Loss: 0.00006458
2024-11-06 14:06:54,982 - INFO - Epoch [30/300], Batch [29/43], Training Loss: 0.00007241
2024-11-06 14:06:54,985 - INFO - Epoch [30/300], Batch [30/43], Training Loss: 0.00005449
2024-11-06 14:06:54,989 - INFO - Epoch [30/300], Batch [31/43], Training Loss: 0.00005447
2024-11-06 14:06:54,993 - INFO - Epoch [30/300], Batch [32/43], Training Loss: 0.00008362
2024-11-06 14:06:54,997 - INFO - Epoch [30/300], Batch [33/43], Training Loss: 0.00003745
2024-11-06 14:06:55,000 - INFO - Epoch [30/300], Batch [34/43], Training Loss: 0.00006806
2024-11-06 14:06:55,004 - INFO - Epoch [30/300], Batch [35/43], Training Loss: 0.00009082
2024-11-06 14:06:55,007 - INFO - Epoch [30/300], Batch [36/43], Training Loss: 0.00008070
2024-11-06 14:06:55,010 - INFO - Epoch [30/300], Batch [37/43], Training Loss: 0.00005078
2024-11-06 14:06:55,013 - INFO - Epoch [30/300], Batch [38/43], Training Loss: 0.00008482
2024-11-06 14:06:55,015 - INFO - Epoch [30/300], Batch [39/43], Training Loss: 0.00006248
2024-11-06 14:06:55,018 - INFO - Epoch [30/300], Batch [40/43], Training Loss: 0.00005879
2024-11-06 14:06:55,021 - INFO - Epoch [30/300], Batch [41/43], Training Loss: 0.00007267
2024-11-06 14:06:55,025 - INFO - Epoch [30/300], Batch [42/43], Training Loss: 0.00010266
2024-11-06 14:06:55,029 - INFO - Epoch [30/300], Batch [43/43], Training Loss: 0.00008290
2024-11-06 14:06:55,041 - INFO - Epoch [30/300], Average Training Loss: 0.00005827, Validation Loss: 0.00007509
2024-11-06 14:06:55,045 - INFO - Epoch [31/300], Batch [1/43], Training Loss: 0.00002290
2024-11-06 14:06:55,049 - INFO - Epoch [31/300], Batch [2/43], Training Loss: 0.00006060
2024-11-06 14:06:55,053 - INFO - Epoch [31/300], Batch [3/43], Training Loss: 0.00005428
2024-11-06 14:06:55,056 - INFO - Epoch [31/300], Batch [4/43], Training Loss: 0.00005036
2024-11-06 14:06:55,060 - INFO - Epoch [31/300], Batch [5/43], Training Loss: 0.00003976
2024-11-06 14:06:55,063 - INFO - Epoch [31/300], Batch [6/43], Training Loss: 0.00006486
2024-11-06 14:06:55,066 - INFO - Epoch [31/300], Batch [7/43], Training Loss: 0.00004858
2024-11-06 14:06:55,069 - INFO - Epoch [31/300], Batch [8/43], Training Loss: 0.00006444
2024-11-06 14:06:55,072 - INFO - Epoch [31/300], Batch [9/43], Training Loss: 0.00004438
2024-11-06 14:06:55,075 - INFO - Epoch [31/300], Batch [10/43], Training Loss: 0.00004611
2024-11-06 14:06:55,078 - INFO - Epoch [31/300], Batch [11/43], Training Loss: 0.00005007
2024-11-06 14:06:55,082 - INFO - Epoch [31/300], Batch [12/43], Training Loss: 0.00005210
2024-11-06 14:06:55,085 - INFO - Epoch [31/300], Batch [13/43], Training Loss: 0.00002915
2024-11-06 14:06:55,088 - INFO - Epoch [31/300], Batch [14/43], Training Loss: 0.00004677
2024-11-06 14:06:55,091 - INFO - Epoch [31/300], Batch [15/43], Training Loss: 0.00004107
2024-11-06 14:06:55,095 - INFO - Epoch [31/300], Batch [16/43], Training Loss: 0.00004349
2024-11-06 14:06:55,099 - INFO - Epoch [31/300], Batch [17/43], Training Loss: 0.00005796
2024-11-06 14:06:55,103 - INFO - Epoch [31/300], Batch [18/43], Training Loss: 0.00007185
2024-11-06 14:06:55,107 - INFO - Epoch [31/300], Batch [19/43], Training Loss: 0.00004108
2024-11-06 14:06:55,109 - INFO - Epoch [31/300], Batch [20/43], Training Loss: 0.00005808
2024-11-06 14:06:55,113 - INFO - Epoch [31/300], Batch [21/43], Training Loss: 0.00007045
2024-11-06 14:06:55,115 - INFO - Epoch [31/300], Batch [22/43], Training Loss: 0.00006658
2024-11-06 14:06:55,119 - INFO - Epoch [31/300], Batch [23/43], Training Loss: 0.00003773
2024-11-06 14:06:55,122 - INFO - Epoch [31/300], Batch [24/43], Training Loss: 0.00005081
2024-11-06 14:06:55,125 - INFO - Epoch [31/300], Batch [25/43], Training Loss: 0.00007926
2024-11-06 14:06:55,128 - INFO - Epoch [31/300], Batch [26/43], Training Loss: 0.00005794
2024-11-06 14:06:55,132 - INFO - Epoch [31/300], Batch [27/43], Training Loss: 0.00007723
2024-11-06 14:06:55,136 - INFO - Epoch [31/300], Batch [28/43], Training Loss: 0.00004510
2024-11-06 14:06:55,139 - INFO - Epoch [31/300], Batch [29/43], Training Loss: 0.00004388
2024-11-06 14:06:55,143 - INFO - Epoch [31/300], Batch [30/43], Training Loss: 0.00007825
2024-11-06 14:06:55,146 - INFO - Epoch [31/300], Batch [31/43], Training Loss: 0.00006292
2024-11-06 14:06:55,149 - INFO - Epoch [31/300], Batch [32/43], Training Loss: 0.00005940
2024-11-06 14:06:55,153 - INFO - Epoch [31/300], Batch [33/43], Training Loss: 0.00007174
2024-11-06 14:06:55,156 - INFO - Epoch [31/300], Batch [34/43], Training Loss: 0.00008233
2024-11-06 14:06:55,159 - INFO - Epoch [31/300], Batch [35/43], Training Loss: 0.00004232
2024-11-06 14:06:55,162 - INFO - Epoch [31/300], Batch [36/43], Training Loss: 0.00005211
2024-11-06 14:06:55,165 - INFO - Epoch [31/300], Batch [37/43], Training Loss: 0.00004161
2024-11-06 14:06:55,168 - INFO - Epoch [31/300], Batch [38/43], Training Loss: 0.00008165
2024-11-06 14:06:55,171 - INFO - Epoch [31/300], Batch [39/43], Training Loss: 0.00003552
2024-11-06 14:06:55,174 - INFO - Epoch [31/300], Batch [40/43], Training Loss: 0.00005221
2024-11-06 14:06:55,176 - INFO - Epoch [31/300], Batch [41/43], Training Loss: 0.00005442
2024-11-06 14:06:55,179 - INFO - Epoch [31/300], Batch [42/43], Training Loss: 0.00007164
2024-11-06 14:06:55,183 - INFO - Epoch [31/300], Batch [43/43], Training Loss: 0.00004118
2024-11-06 14:06:55,195 - INFO - Epoch [31/300], Average Training Loss: 0.00005452, Validation Loss: 0.00006958
2024-11-06 14:06:55,198 - INFO - Epoch [32/300], Batch [1/43], Training Loss: 0.00005795
2024-11-06 14:06:55,201 - INFO - Epoch [32/300], Batch [2/43], Training Loss: 0.00007403
2024-11-06 14:06:55,204 - INFO - Epoch [32/300], Batch [3/43], Training Loss: 0.00003237
2024-11-06 14:06:55,207 - INFO - Epoch [32/300], Batch [4/43], Training Loss: 0.00004414
2024-11-06 14:06:55,210 - INFO - Epoch [32/300], Batch [5/43], Training Loss: 0.00006496
2024-11-06 14:06:55,214 - INFO - Epoch [32/300], Batch [6/43], Training Loss: 0.00005746
2024-11-06 14:06:55,217 - INFO - Epoch [32/300], Batch [7/43], Training Loss: 0.00006704
2024-11-06 14:06:55,220 - INFO - Epoch [32/300], Batch [8/43], Training Loss: 0.00003750
2024-11-06 14:06:55,224 - INFO - Epoch [32/300], Batch [9/43], Training Loss: 0.00009160
2024-11-06 14:06:55,227 - INFO - Epoch [32/300], Batch [10/43], Training Loss: 0.00005882
2024-11-06 14:06:55,231 - INFO - Epoch [32/300], Batch [11/43], Training Loss: 0.00003820
2024-11-06 14:06:55,235 - INFO - Epoch [32/300], Batch [12/43], Training Loss: 0.00003748
2024-11-06 14:06:55,238 - INFO - Epoch [32/300], Batch [13/43], Training Loss: 0.00010115
2024-11-06 14:06:55,241 - INFO - Epoch [32/300], Batch [14/43], Training Loss: 0.00005089
2024-11-06 14:06:55,245 - INFO - Epoch [32/300], Batch [15/43], Training Loss: 0.00006035
2024-11-06 14:06:55,249 - INFO - Epoch [32/300], Batch [16/43], Training Loss: 0.00004253
2024-11-06 14:06:55,252 - INFO - Epoch [32/300], Batch [17/43], Training Loss: 0.00007279
2024-11-06 14:06:55,255 - INFO - Epoch [32/300], Batch [18/43], Training Loss: 0.00004027
2024-11-06 14:06:55,258 - INFO - Epoch [32/300], Batch [19/43], Training Loss: 0.00005061
2024-11-06 14:06:55,261 - INFO - Epoch [32/300], Batch [20/43], Training Loss: 0.00002337
2024-11-06 14:06:55,264 - INFO - Epoch [32/300], Batch [21/43], Training Loss: 0.00006809
2024-11-06 14:06:55,267 - INFO - Epoch [32/300], Batch [22/43], Training Loss: 0.00007479
2024-11-06 14:06:55,271 - INFO - Epoch [32/300], Batch [23/43], Training Loss: 0.00005969
2024-11-06 14:06:55,273 - INFO - Epoch [32/300], Batch [24/43], Training Loss: 0.00002626
2024-11-06 14:06:55,278 - INFO - Epoch [32/300], Batch [25/43], Training Loss: 0.00004914
2024-11-06 14:06:55,283 - INFO - Epoch [32/300], Batch [26/43], Training Loss: 0.00006762
2024-11-06 14:06:55,287 - INFO - Epoch [32/300], Batch [27/43], Training Loss: 0.00005534
2024-11-06 14:06:55,291 - INFO - Epoch [32/300], Batch [28/43], Training Loss: 0.00006786
2024-11-06 14:06:55,294 - INFO - Epoch [32/300], Batch [29/43], Training Loss: 0.00007070
2024-11-06 14:06:55,298 - INFO - Epoch [32/300], Batch [30/43], Training Loss: 0.00002388
2024-11-06 14:06:55,301 - INFO - Epoch [32/300], Batch [31/43], Training Loss: 0.00006601
2024-11-06 14:06:55,305 - INFO - Epoch [32/300], Batch [32/43], Training Loss: 0.00005251
2024-11-06 14:06:55,309 - INFO - Epoch [32/300], Batch [33/43], Training Loss: 0.00003208
2024-11-06 14:06:55,313 - INFO - Epoch [32/300], Batch [34/43], Training Loss: 0.00003061
2024-11-06 14:06:55,316 - INFO - Epoch [32/300], Batch [35/43], Training Loss: 0.00004880
2024-11-06 14:06:55,320 - INFO - Epoch [32/300], Batch [36/43], Training Loss: 0.00008522
2024-11-06 14:06:55,323 - INFO - Epoch [32/300], Batch [37/43], Training Loss: 0.00009310
2024-11-06 14:06:55,327 - INFO - Epoch [32/300], Batch [38/43], Training Loss: 0.00005175
2024-11-06 14:06:55,330 - INFO - Epoch [32/300], Batch [39/43], Training Loss: 0.00002736
2024-11-06 14:06:55,334 - INFO - Epoch [32/300], Batch [40/43], Training Loss: 0.00004374
2024-11-06 14:06:55,339 - INFO - Epoch [32/300], Batch [41/43], Training Loss: 0.00005859
2024-11-06 14:06:55,343 - INFO - Epoch [32/300], Batch [42/43], Training Loss: 0.00006674
2024-11-06 14:06:55,347 - INFO - Epoch [32/300], Batch [43/43], Training Loss: 0.00007558
2024-11-06 14:06:55,358 - INFO - Epoch [32/300], Average Training Loss: 0.00005579, Validation Loss: 0.00006904
2024-11-06 14:06:55,361 - INFO - Epoch [33/300], Batch [1/43], Training Loss: 0.00004728
2024-11-06 14:06:55,365 - INFO - Epoch [33/300], Batch [2/43], Training Loss: 0.00003629
2024-11-06 14:06:55,368 - INFO - Epoch [33/300], Batch [3/43], Training Loss: 0.00008922
2024-11-06 14:06:55,372 - INFO - Epoch [33/300], Batch [4/43], Training Loss: 0.00002824
2024-11-06 14:06:55,376 - INFO - Epoch [33/300], Batch [5/43], Training Loss: 0.00005499
2024-11-06 14:06:55,380 - INFO - Epoch [33/300], Batch [6/43], Training Loss: 0.00008231
2024-11-06 14:06:55,383 - INFO - Epoch [33/300], Batch [7/43], Training Loss: 0.00005711
2024-11-06 14:06:55,388 - INFO - Epoch [33/300], Batch [8/43], Training Loss: 0.00004511
2024-11-06 14:06:55,392 - INFO - Epoch [33/300], Batch [9/43], Training Loss: 0.00004495
2024-11-06 14:06:55,395 - INFO - Epoch [33/300], Batch [10/43], Training Loss: 0.00005600
2024-11-06 14:06:55,398 - INFO - Epoch [33/300], Batch [11/43], Training Loss: 0.00003511
2024-11-06 14:06:55,402 - INFO - Epoch [33/300], Batch [12/43], Training Loss: 0.00006040
2024-11-06 14:06:55,406 - INFO - Epoch [33/300], Batch [13/43], Training Loss: 0.00006734
2024-11-06 14:06:55,410 - INFO - Epoch [33/300], Batch [14/43], Training Loss: 0.00007421
2024-11-06 14:06:55,413 - INFO - Epoch [33/300], Batch [15/43], Training Loss: 0.00006477
2024-11-06 14:06:55,417 - INFO - Epoch [33/300], Batch [16/43], Training Loss: 0.00007922
2024-11-06 14:06:55,420 - INFO - Epoch [33/300], Batch [17/43], Training Loss: 0.00004296
2024-11-06 14:06:55,425 - INFO - Epoch [33/300], Batch [18/43], Training Loss: 0.00003944
2024-11-06 14:06:55,428 - INFO - Epoch [33/300], Batch [19/43], Training Loss: 0.00005592
2024-11-06 14:06:55,432 - INFO - Epoch [33/300], Batch [20/43], Training Loss: 0.00005572
2024-11-06 14:06:55,436 - INFO - Epoch [33/300], Batch [21/43], Training Loss: 0.00006392
2024-11-06 14:06:55,440 - INFO - Epoch [33/300], Batch [22/43], Training Loss: 0.00006526
2024-11-06 14:06:55,444 - INFO - Epoch [33/300], Batch [23/43], Training Loss: 0.00007706
2024-11-06 14:06:55,449 - INFO - Epoch [33/300], Batch [24/43], Training Loss: 0.00004576
2024-11-06 14:06:55,453 - INFO - Epoch [33/300], Batch [25/43], Training Loss: 0.00005003
2024-11-06 14:06:55,457 - INFO - Epoch [33/300], Batch [26/43], Training Loss: 0.00005289
2024-11-06 14:06:55,461 - INFO - Epoch [33/300], Batch [27/43], Training Loss: 0.00005933
2024-11-06 14:06:55,465 - INFO - Epoch [33/300], Batch [28/43], Training Loss: 0.00009944
2024-11-06 14:06:55,469 - INFO - Epoch [33/300], Batch [29/43], Training Loss: 0.00006628
2024-11-06 14:06:55,472 - INFO - Epoch [33/300], Batch [30/43], Training Loss: 0.00010009
2024-11-06 14:06:55,475 - INFO - Epoch [33/300], Batch [31/43], Training Loss: 0.00004245
2024-11-06 14:06:55,477 - INFO - Epoch [33/300], Batch [32/43], Training Loss: 0.00005268
2024-11-06 14:06:55,480 - INFO - Epoch [33/300], Batch [33/43], Training Loss: 0.00006238
2024-11-06 14:06:55,483 - INFO - Epoch [33/300], Batch [34/43], Training Loss: 0.00003271
2024-11-06 14:06:55,486 - INFO - Epoch [33/300], Batch [35/43], Training Loss: 0.00004002
2024-11-06 14:06:55,490 - INFO - Epoch [33/300], Batch [36/43], Training Loss: 0.00005255
2024-11-06 14:06:55,494 - INFO - Epoch [33/300], Batch [37/43], Training Loss: 0.00005346
2024-11-06 14:06:55,497 - INFO - Epoch [33/300], Batch [38/43], Training Loss: 0.00004791
2024-11-06 14:06:55,500 - INFO - Epoch [33/300], Batch [39/43], Training Loss: 0.00008185
2024-11-06 14:06:55,503 - INFO - Epoch [33/300], Batch [40/43], Training Loss: 0.00002856
2024-11-06 14:06:55,507 - INFO - Epoch [33/300], Batch [41/43], Training Loss: 0.00003053
2024-11-06 14:06:55,510 - INFO - Epoch [33/300], Batch [42/43], Training Loss: 0.00002983
2024-11-06 14:06:55,515 - INFO - Epoch [33/300], Batch [43/43], Training Loss: 0.00003175
2024-11-06 14:06:55,524 - INFO - Epoch [33/300], Average Training Loss: 0.00005543, Validation Loss: 0.00008372
2024-11-06 14:06:55,528 - INFO - Epoch [34/300], Batch [1/43], Training Loss: 0.00005922
2024-11-06 14:06:55,532 - INFO - Epoch [34/300], Batch [2/43], Training Loss: 0.00006144
2024-11-06 14:06:55,536 - INFO - Epoch [34/300], Batch [3/43], Training Loss: 0.00005912
2024-11-06 14:06:55,539 - INFO - Epoch [34/300], Batch [4/43], Training Loss: 0.00011444
2024-11-06 14:06:55,542 - INFO - Epoch [34/300], Batch [5/43], Training Loss: 0.00006614
2024-11-06 14:06:55,545 - INFO - Epoch [34/300], Batch [6/43], Training Loss: 0.00008322
2024-11-06 14:06:55,548 - INFO - Epoch [34/300], Batch [7/43], Training Loss: 0.00006039
2024-11-06 14:06:55,550 - INFO - Epoch [34/300], Batch [8/43], Training Loss: 0.00005816
2024-11-06 14:06:55,553 - INFO - Epoch [34/300], Batch [9/43], Training Loss: 0.00004246
2024-11-06 14:06:55,556 - INFO - Epoch [34/300], Batch [10/43], Training Loss: 0.00006684
2024-11-06 14:06:55,560 - INFO - Epoch [34/300], Batch [11/43], Training Loss: 0.00009071
2024-11-06 14:06:55,564 - INFO - Epoch [34/300], Batch [12/43], Training Loss: 0.00002354
2024-11-06 14:06:55,567 - INFO - Epoch [34/300], Batch [13/43], Training Loss: 0.00007500
2024-11-06 14:06:55,570 - INFO - Epoch [34/300], Batch [14/43], Training Loss: 0.00001903
2024-11-06 14:06:55,573 - INFO - Epoch [34/300], Batch [15/43], Training Loss: 0.00005538
2024-11-06 14:06:55,576 - INFO - Epoch [34/300], Batch [16/43], Training Loss: 0.00006955
2024-11-06 14:06:55,580 - INFO - Epoch [34/300], Batch [17/43], Training Loss: 0.00001627
2024-11-06 14:06:55,583 - INFO - Epoch [34/300], Batch [18/43], Training Loss: 0.00006049
2024-11-06 14:06:55,586 - INFO - Epoch [34/300], Batch [19/43], Training Loss: 0.00005904
2024-11-06 14:06:55,589 - INFO - Epoch [34/300], Batch [20/43], Training Loss: 0.00003343
2024-11-06 14:06:55,593 - INFO - Epoch [34/300], Batch [21/43], Training Loss: 0.00008875
2024-11-06 14:06:55,596 - INFO - Epoch [34/300], Batch [22/43], Training Loss: 0.00006017
2024-11-06 14:06:55,600 - INFO - Epoch [34/300], Batch [23/43], Training Loss: 0.00004008
2024-11-06 14:06:55,603 - INFO - Epoch [34/300], Batch [24/43], Training Loss: 0.00004819
2024-11-06 14:06:55,607 - INFO - Epoch [34/300], Batch [25/43], Training Loss: 0.00003364
2024-11-06 14:06:55,610 - INFO - Epoch [34/300], Batch [26/43], Training Loss: 0.00005831
2024-11-06 14:06:55,614 - INFO - Epoch [34/300], Batch [27/43], Training Loss: 0.00006902
2024-11-06 14:06:55,617 - INFO - Epoch [34/300], Batch [28/43], Training Loss: 0.00005332
2024-11-06 14:06:55,620 - INFO - Epoch [34/300], Batch [29/43], Training Loss: 0.00004076
2024-11-06 14:06:55,623 - INFO - Epoch [34/300], Batch [30/43], Training Loss: 0.00005975
2024-11-06 14:06:55,626 - INFO - Epoch [34/300], Batch [31/43], Training Loss: 0.00003929
2024-11-06 14:06:55,629 - INFO - Epoch [34/300], Batch [32/43], Training Loss: 0.00004172
2024-11-06 14:06:55,631 - INFO - Epoch [34/300], Batch [33/43], Training Loss: 0.00007473
2024-11-06 14:06:55,634 - INFO - Epoch [34/300], Batch [34/43], Training Loss: 0.00007647
2024-11-06 14:06:55,638 - INFO - Epoch [34/300], Batch [35/43], Training Loss: 0.00004456
2024-11-06 14:06:55,641 - INFO - Epoch [34/300], Batch [36/43], Training Loss: 0.00008041
2024-11-06 14:06:55,644 - INFO - Epoch [34/300], Batch [37/43], Training Loss: 0.00005150
2024-11-06 14:06:55,647 - INFO - Epoch [34/300], Batch [38/43], Training Loss: 0.00006217
2024-11-06 14:06:55,649 - INFO - Epoch [34/300], Batch [39/43], Training Loss: 0.00001731
2024-11-06 14:06:55,653 - INFO - Epoch [34/300], Batch [40/43], Training Loss: 0.00004982
2024-11-06 14:06:55,657 - INFO - Epoch [34/300], Batch [41/43], Training Loss: 0.00005613
2024-11-06 14:06:55,660 - INFO - Epoch [34/300], Batch [42/43], Training Loss: 0.00003280
2024-11-06 14:06:55,664 - INFO - Epoch [34/300], Batch [43/43], Training Loss: 0.00005983
2024-11-06 14:06:55,674 - INFO - Epoch [34/300], Average Training Loss: 0.00005611, Validation Loss: 0.00006697
2024-11-06 14:06:55,677 - INFO - Epoch [35/300], Batch [1/43], Training Loss: 0.00006086
2024-11-06 14:06:55,680 - INFO - Epoch [35/300], Batch [2/43], Training Loss: 0.00003784
2024-11-06 14:06:55,684 - INFO - Epoch [35/300], Batch [3/43], Training Loss: 0.00007326
2024-11-06 14:06:55,687 - INFO - Epoch [35/300], Batch [4/43], Training Loss: 0.00012242
2024-11-06 14:06:55,691 - INFO - Epoch [35/300], Batch [5/43], Training Loss: 0.00005206
2024-11-06 14:06:55,694 - INFO - Epoch [35/300], Batch [6/43], Training Loss: 0.00004371
2024-11-06 14:06:55,697 - INFO - Epoch [35/300], Batch [7/43], Training Loss: 0.00003541
2024-11-06 14:06:55,701 - INFO - Epoch [35/300], Batch [8/43], Training Loss: 0.00002899
2024-11-06 14:06:55,705 - INFO - Epoch [35/300], Batch [9/43], Training Loss: 0.00007960
2024-11-06 14:06:55,708 - INFO - Epoch [35/300], Batch [10/43], Training Loss: 0.00010314
2024-11-06 14:06:55,711 - INFO - Epoch [35/300], Batch [11/43], Training Loss: 0.00003333
2024-11-06 14:06:55,714 - INFO - Epoch [35/300], Batch [12/43], Training Loss: 0.00008294
2024-11-06 14:06:55,717 - INFO - Epoch [35/300], Batch [13/43], Training Loss: 0.00003390
2024-11-06 14:06:55,720 - INFO - Epoch [35/300], Batch [14/43], Training Loss: 0.00001930
2024-11-06 14:06:55,723 - INFO - Epoch [35/300], Batch [15/43], Training Loss: 0.00004184
2024-11-06 14:06:55,727 - INFO - Epoch [35/300], Batch [16/43], Training Loss: 0.00002606
2024-11-06 14:06:55,730 - INFO - Epoch [35/300], Batch [17/43], Training Loss: 0.00007636
2024-11-06 14:06:55,733 - INFO - Epoch [35/300], Batch [18/43], Training Loss: 0.00002160
2024-11-06 14:06:55,735 - INFO - Epoch [35/300], Batch [19/43], Training Loss: 0.00006229
2024-11-06 14:06:55,738 - INFO - Epoch [35/300], Batch [20/43], Training Loss: 0.00003977
2024-11-06 14:06:55,741 - INFO - Epoch [35/300], Batch [21/43], Training Loss: 0.00004353
2024-11-06 14:06:55,744 - INFO - Epoch [35/300], Batch [22/43], Training Loss: 0.00005106
2024-11-06 14:06:55,747 - INFO - Epoch [35/300], Batch [23/43], Training Loss: 0.00003514
2024-11-06 14:06:55,751 - INFO - Epoch [35/300], Batch [24/43], Training Loss: 0.00005720
2024-11-06 14:06:55,754 - INFO - Epoch [35/300], Batch [25/43], Training Loss: 0.00004826
2024-11-06 14:06:55,756 - INFO - Epoch [35/300], Batch [26/43], Training Loss: 0.00006359
2024-11-06 14:06:55,759 - INFO - Epoch [35/300], Batch [27/43], Training Loss: 0.00006100
2024-11-06 14:06:55,762 - INFO - Epoch [35/300], Batch [28/43], Training Loss: 0.00006160
2024-11-06 14:06:55,766 - INFO - Epoch [35/300], Batch [29/43], Training Loss: 0.00004169
2024-11-06 14:06:55,769 - INFO - Epoch [35/300], Batch [30/43], Training Loss: 0.00005722
2024-11-06 14:06:55,772 - INFO - Epoch [35/300], Batch [31/43], Training Loss: 0.00001843
2024-11-06 14:06:55,774 - INFO - Epoch [35/300], Batch [32/43], Training Loss: 0.00007259
2024-11-06 14:06:55,778 - INFO - Epoch [35/300], Batch [33/43], Training Loss: 0.00007370
2024-11-06 14:06:55,781 - INFO - Epoch [35/300], Batch [34/43], Training Loss: 0.00003451
2024-11-06 14:06:55,785 - INFO - Epoch [35/300], Batch [35/43], Training Loss: 0.00004086
2024-11-06 14:06:55,788 - INFO - Epoch [35/300], Batch [36/43], Training Loss: 0.00005589
2024-11-06 14:06:55,792 - INFO - Epoch [35/300], Batch [37/43], Training Loss: 0.00004421
2024-11-06 14:06:55,796 - INFO - Epoch [35/300], Batch [38/43], Training Loss: 0.00005304
2024-11-06 14:06:55,800 - INFO - Epoch [35/300], Batch [39/43], Training Loss: 0.00006176
2024-11-06 14:06:55,805 - INFO - Epoch [35/300], Batch [40/43], Training Loss: 0.00006401
2024-11-06 14:06:55,808 - INFO - Epoch [35/300], Batch [41/43], Training Loss: 0.00003307
2024-11-06 14:06:55,811 - INFO - Epoch [35/300], Batch [42/43], Training Loss: 0.00006664
2024-11-06 14:06:55,814 - INFO - Epoch [35/300], Batch [43/43], Training Loss: 0.00006366
2024-11-06 14:06:55,824 - INFO - Epoch [35/300], Average Training Loss: 0.00005296, Validation Loss: 0.00006749
2024-11-06 14:06:55,828 - INFO - Epoch [36/300], Batch [1/43], Training Loss: 0.00006385
2024-11-06 14:06:55,832 - INFO - Epoch [36/300], Batch [2/43], Training Loss: 0.00003143
2024-11-06 14:06:55,836 - INFO - Epoch [36/300], Batch [3/43], Training Loss: 0.00004308
2024-11-06 14:06:55,841 - INFO - Epoch [36/300], Batch [4/43], Training Loss: 0.00001949
2024-11-06 14:06:55,846 - INFO - Epoch [36/300], Batch [5/43], Training Loss: 0.00004489
2024-11-06 14:06:55,850 - INFO - Epoch [36/300], Batch [6/43], Training Loss: 0.00002772
2024-11-06 14:06:55,855 - INFO - Epoch [36/300], Batch [7/43], Training Loss: 0.00007490
2024-11-06 14:06:55,859 - INFO - Epoch [36/300], Batch [8/43], Training Loss: 0.00004186
2024-11-06 14:06:55,862 - INFO - Epoch [36/300], Batch [9/43], Training Loss: 0.00003664
2024-11-06 14:06:55,866 - INFO - Epoch [36/300], Batch [10/43], Training Loss: 0.00004745
2024-11-06 14:06:55,870 - INFO - Epoch [36/300], Batch [11/43], Training Loss: 0.00003199
2024-11-06 14:06:55,874 - INFO - Epoch [36/300], Batch [12/43], Training Loss: 0.00004807
2024-11-06 14:06:55,878 - INFO - Epoch [36/300], Batch [13/43], Training Loss: 0.00005138
2024-11-06 14:06:55,882 - INFO - Epoch [36/300], Batch [14/43], Training Loss: 0.00008766
2024-11-06 14:06:55,885 - INFO - Epoch [36/300], Batch [15/43], Training Loss: 0.00006236
2024-11-06 14:06:55,890 - INFO - Epoch [36/300], Batch [16/43], Training Loss: 0.00002996
2024-11-06 14:06:55,933 - INFO - Epoch [36/300], Batch [17/43], Training Loss: 0.00006646
2024-11-06 14:06:55,950 - INFO - Epoch [36/300], Batch [18/43], Training Loss: 0.00004051
2024-11-06 14:06:55,954 - INFO - Epoch [36/300], Batch [19/43], Training Loss: 0.00002746
2024-11-06 14:06:55,958 - INFO - Epoch [36/300], Batch [20/43], Training Loss: 0.00005486
2024-11-06 14:06:55,964 - INFO - Epoch [36/300], Batch [21/43], Training Loss: 0.00002393
2024-11-06 14:06:55,976 - INFO - Epoch [36/300], Batch [22/43], Training Loss: 0.00005541
2024-11-06 14:06:55,981 - INFO - Epoch [36/300], Batch [23/43], Training Loss: 0.00006264
2024-11-06 14:06:55,986 - INFO - Epoch [36/300], Batch [24/43], Training Loss: 0.00008600
2024-11-06 14:06:55,989 - INFO - Epoch [36/300], Batch [25/43], Training Loss: 0.00003694
2024-11-06 14:06:55,993 - INFO - Epoch [36/300], Batch [26/43], Training Loss: 0.00011598
2024-11-06 14:06:55,997 - INFO - Epoch [36/300], Batch [27/43], Training Loss: 0.00004286
2024-11-06 14:06:56,001 - INFO - Epoch [36/300], Batch [28/43], Training Loss: 0.00006651
2024-11-06 14:06:56,005 - INFO - Epoch [36/300], Batch [29/43], Training Loss: 0.00006753
2024-11-06 14:06:56,008 - INFO - Epoch [36/300], Batch [30/43], Training Loss: 0.00003688
2024-11-06 14:06:56,011 - INFO - Epoch [36/300], Batch [31/43], Training Loss: 0.00005543
2024-11-06 14:06:56,014 - INFO - Epoch [36/300], Batch [32/43], Training Loss: 0.00008078
2024-11-06 14:06:56,018 - INFO - Epoch [36/300], Batch [33/43], Training Loss: 0.00009918
2024-11-06 14:06:56,021 - INFO - Epoch [36/300], Batch [34/43], Training Loss: 0.00005147
2024-11-06 14:06:56,025 - INFO - Epoch [36/300], Batch [35/43], Training Loss: 0.00006664
2024-11-06 14:06:56,029 - INFO - Epoch [36/300], Batch [36/43], Training Loss: 0.00007213
2024-11-06 14:06:56,032 - INFO - Epoch [36/300], Batch [37/43], Training Loss: 0.00004381
2024-11-06 14:06:56,036 - INFO - Epoch [36/300], Batch [38/43], Training Loss: 0.00003685
2024-11-06 14:06:56,039 - INFO - Epoch [36/300], Batch [39/43], Training Loss: 0.00002618
2024-11-06 14:06:56,043 - INFO - Epoch [36/300], Batch [40/43], Training Loss: 0.00005180
2024-11-06 14:06:56,048 - INFO - Epoch [36/300], Batch [41/43], Training Loss: 0.00002043
2024-11-06 14:06:56,052 - INFO - Epoch [36/300], Batch [42/43], Training Loss: 0.00003721
2024-11-06 14:06:56,056 - INFO - Epoch [36/300], Batch [43/43], Training Loss: 0.00007862
2024-11-06 14:06:56,070 - INFO - Epoch [36/300], Average Training Loss: 0.00005226, Validation Loss: 0.00006776
2024-11-06 14:06:56,074 - INFO - Epoch [37/300], Batch [1/43], Training Loss: 0.00009260
2024-11-06 14:06:56,077 - INFO - Epoch [37/300], Batch [2/43], Training Loss: 0.00008308
2024-11-06 14:06:56,081 - INFO - Epoch [37/300], Batch [3/43], Training Loss: 0.00003235
2024-11-06 14:06:56,084 - INFO - Epoch [37/300], Batch [4/43], Training Loss: 0.00006619
2024-11-06 14:06:56,087 - INFO - Epoch [37/300], Batch [5/43], Training Loss: 0.00006451
2024-11-06 14:06:56,091 - INFO - Epoch [37/300], Batch [6/43], Training Loss: 0.00002188
2024-11-06 14:06:56,095 - INFO - Epoch [37/300], Batch [7/43], Training Loss: 0.00006408
2024-11-06 14:06:56,099 - INFO - Epoch [37/300], Batch [8/43], Training Loss: 0.00003847
2024-11-06 14:06:56,103 - INFO - Epoch [37/300], Batch [9/43], Training Loss: 0.00006967
2024-11-06 14:06:56,106 - INFO - Epoch [37/300], Batch [10/43], Training Loss: 0.00003088
2024-11-06 14:06:56,110 - INFO - Epoch [37/300], Batch [11/43], Training Loss: 0.00004178
2024-11-06 14:06:56,113 - INFO - Epoch [37/300], Batch [12/43], Training Loss: 0.00006919
2024-11-06 14:06:56,117 - INFO - Epoch [37/300], Batch [13/43], Training Loss: 0.00007338
2024-11-06 14:06:56,122 - INFO - Epoch [37/300], Batch [14/43], Training Loss: 0.00003191
2024-11-06 14:06:56,125 - INFO - Epoch [37/300], Batch [15/43], Training Loss: 0.00004880
2024-11-06 14:06:56,128 - INFO - Epoch [37/300], Batch [16/43], Training Loss: 0.00003164
2024-11-06 14:06:56,132 - INFO - Epoch [37/300], Batch [17/43], Training Loss: 0.00004613
2024-11-06 14:06:56,135 - INFO - Epoch [37/300], Batch [18/43], Training Loss: 0.00006424
2024-11-06 14:06:56,140 - INFO - Epoch [37/300], Batch [19/43], Training Loss: 0.00005597
2024-11-06 14:06:56,144 - INFO - Epoch [37/300], Batch [20/43], Training Loss: 0.00005582
2024-11-06 14:06:56,148 - INFO - Epoch [37/300], Batch [21/43], Training Loss: 0.00002261
2024-11-06 14:06:56,152 - INFO - Epoch [37/300], Batch [22/43], Training Loss: 0.00003696
2024-11-06 14:06:56,155 - INFO - Epoch [37/300], Batch [23/43], Training Loss: 0.00005064
2024-11-06 14:06:56,159 - INFO - Epoch [37/300], Batch [24/43], Training Loss: 0.00006640
2024-11-06 14:06:56,163 - INFO - Epoch [37/300], Batch [25/43], Training Loss: 0.00002696
2024-11-06 14:06:56,167 - INFO - Epoch [37/300], Batch [26/43], Training Loss: 0.00002605
2024-11-06 14:06:56,171 - INFO - Epoch [37/300], Batch [27/43], Training Loss: 0.00008184
2024-11-06 14:06:56,175 - INFO - Epoch [37/300], Batch [28/43], Training Loss: 0.00004273
2024-11-06 14:06:56,178 - INFO - Epoch [37/300], Batch [29/43], Training Loss: 0.00005182
2024-11-06 14:06:56,182 - INFO - Epoch [37/300], Batch [30/43], Training Loss: 0.00003890
2024-11-06 14:06:56,186 - INFO - Epoch [37/300], Batch [31/43], Training Loss: 0.00003003
2024-11-06 14:06:56,190 - INFO - Epoch [37/300], Batch [32/43], Training Loss: 0.00003024
2024-11-06 14:06:56,193 - INFO - Epoch [37/300], Batch [33/43], Training Loss: 0.00003649
2024-11-06 14:06:56,196 - INFO - Epoch [37/300], Batch [34/43], Training Loss: 0.00003965
2024-11-06 14:06:56,201 - INFO - Epoch [37/300], Batch [35/43], Training Loss: 0.00003313
2024-11-06 14:06:56,205 - INFO - Epoch [37/300], Batch [36/43], Training Loss: 0.00004657
2024-11-06 14:06:56,208 - INFO - Epoch [37/300], Batch [37/43], Training Loss: 0.00005808
2024-11-06 14:06:56,212 - INFO - Epoch [37/300], Batch [38/43], Training Loss: 0.00004211
2024-11-06 14:06:56,216 - INFO - Epoch [37/300], Batch [39/43], Training Loss: 0.00005511
2024-11-06 14:06:56,219 - INFO - Epoch [37/300], Batch [40/43], Training Loss: 0.00005316
2024-11-06 14:06:56,222 - INFO - Epoch [37/300], Batch [41/43], Training Loss: 0.00009239
2024-11-06 14:06:56,226 - INFO - Epoch [37/300], Batch [42/43], Training Loss: 0.00003870
2024-11-06 14:06:56,230 - INFO - Epoch [37/300], Batch [43/43], Training Loss: 0.00005920
2024-11-06 14:06:56,243 - INFO - Epoch [37/300], Average Training Loss: 0.00004982, Validation Loss: 0.00006377
2024-11-06 14:06:56,247 - INFO - Epoch [38/300], Batch [1/43], Training Loss: 0.00007275
2024-11-06 14:06:56,250 - INFO - Epoch [38/300], Batch [2/43], Training Loss: 0.00003538
2024-11-06 14:06:56,253 - INFO - Epoch [38/300], Batch [3/43], Training Loss: 0.00003138
2024-11-06 14:06:56,256 - INFO - Epoch [38/300], Batch [4/43], Training Loss: 0.00001632
2024-11-06 14:06:56,259 - INFO - Epoch [38/300], Batch [5/43], Training Loss: 0.00007074
2024-11-06 14:06:56,262 - INFO - Epoch [38/300], Batch [6/43], Training Loss: 0.00003271
2024-11-06 14:06:56,266 - INFO - Epoch [38/300], Batch [7/43], Training Loss: 0.00004353
2024-11-06 14:06:56,270 - INFO - Epoch [38/300], Batch [8/43], Training Loss: 0.00002209
2024-11-06 14:06:56,273 - INFO - Epoch [38/300], Batch [9/43], Training Loss: 0.00005994
2024-11-06 14:06:56,278 - INFO - Epoch [38/300], Batch [10/43], Training Loss: 0.00006438
2024-11-06 14:06:56,283 - INFO - Epoch [38/300], Batch [11/43], Training Loss: 0.00003013
2024-11-06 14:06:56,287 - INFO - Epoch [38/300], Batch [12/43], Training Loss: 0.00008198
2024-11-06 14:06:56,290 - INFO - Epoch [38/300], Batch [13/43], Training Loss: 0.00003552
2024-11-06 14:06:56,294 - INFO - Epoch [38/300], Batch [14/43], Training Loss: 0.00006844
2024-11-06 14:06:56,298 - INFO - Epoch [38/300], Batch [15/43], Training Loss: 0.00002057
2024-11-06 14:06:56,302 - INFO - Epoch [38/300], Batch [16/43], Training Loss: 0.00004845
2024-11-06 14:06:56,306 - INFO - Epoch [38/300], Batch [17/43], Training Loss: 0.00006016
2024-11-06 14:06:56,309 - INFO - Epoch [38/300], Batch [18/43], Training Loss: 0.00007839
2024-11-06 14:06:56,312 - INFO - Epoch [38/300], Batch [19/43], Training Loss: 0.00005580
2024-11-06 14:06:56,315 - INFO - Epoch [38/300], Batch [20/43], Training Loss: 0.00008594
2024-11-06 14:06:56,319 - INFO - Epoch [38/300], Batch [21/43], Training Loss: 0.00005399
2024-11-06 14:06:56,322 - INFO - Epoch [38/300], Batch [22/43], Training Loss: 0.00005105
2024-11-06 14:06:56,327 - INFO - Epoch [38/300], Batch [23/43], Training Loss: 0.00004982
2024-11-06 14:06:56,332 - INFO - Epoch [38/300], Batch [24/43], Training Loss: 0.00003587
2024-11-06 14:06:56,335 - INFO - Epoch [38/300], Batch [25/43], Training Loss: 0.00006669
2024-11-06 14:06:56,339 - INFO - Epoch [38/300], Batch [26/43], Training Loss: 0.00004514
2024-11-06 14:06:56,343 - INFO - Epoch [38/300], Batch [27/43], Training Loss: 0.00003746
2024-11-06 14:06:56,347 - INFO - Epoch [38/300], Batch [28/43], Training Loss: 0.00004361
2024-11-06 14:06:56,352 - INFO - Epoch [38/300], Batch [29/43], Training Loss: 0.00004567
2024-11-06 14:06:56,355 - INFO - Epoch [38/300], Batch [30/43], Training Loss: 0.00008427
2024-11-06 14:06:56,359 - INFO - Epoch [38/300], Batch [31/43], Training Loss: 0.00005008
2024-11-06 14:06:56,362 - INFO - Epoch [38/300], Batch [32/43], Training Loss: 0.00006351
2024-11-06 14:06:56,365 - INFO - Epoch [38/300], Batch [33/43], Training Loss: 0.00004008
2024-11-06 14:06:56,369 - INFO - Epoch [38/300], Batch [34/43], Training Loss: 0.00007944
2024-11-06 14:06:56,373 - INFO - Epoch [38/300], Batch [35/43], Training Loss: 0.00002780
2024-11-06 14:06:56,377 - INFO - Epoch [38/300], Batch [36/43], Training Loss: 0.00005106
2024-11-06 14:06:56,381 - INFO - Epoch [38/300], Batch [37/43], Training Loss: 0.00008258
2024-11-06 14:06:56,385 - INFO - Epoch [38/300], Batch [38/43], Training Loss: 0.00006002
2024-11-06 14:06:56,389 - INFO - Epoch [38/300], Batch [39/43], Training Loss: 0.00003945
2024-11-06 14:06:56,393 - INFO - Epoch [38/300], Batch [40/43], Training Loss: 0.00006782
2024-11-06 14:06:56,397 - INFO - Epoch [38/300], Batch [41/43], Training Loss: 0.00006941
2024-11-06 14:06:56,401 - INFO - Epoch [38/300], Batch [42/43], Training Loss: 0.00007051
2024-11-06 14:06:56,405 - INFO - Epoch [38/300], Batch [43/43], Training Loss: 0.00006168
2024-11-06 14:06:56,416 - INFO - Epoch [38/300], Average Training Loss: 0.00005329, Validation Loss: 0.00005536
2024-11-06 14:06:56,420 - INFO - Epoch [39/300], Batch [1/43], Training Loss: 0.00006218
2024-11-06 14:06:56,425 - INFO - Epoch [39/300], Batch [2/43], Training Loss: 0.00007107
2024-11-06 14:06:56,429 - INFO - Epoch [39/300], Batch [3/43], Training Loss: 0.00004310
2024-11-06 14:06:56,433 - INFO - Epoch [39/300], Batch [4/43], Training Loss: 0.00004431
2024-11-06 14:06:56,436 - INFO - Epoch [39/300], Batch [5/43], Training Loss: 0.00006149
2024-11-06 14:06:56,440 - INFO - Epoch [39/300], Batch [6/43], Training Loss: 0.00002419
2024-11-06 14:06:56,444 - INFO - Epoch [39/300], Batch [7/43], Training Loss: 0.00004039
2024-11-06 14:06:56,447 - INFO - Epoch [39/300], Batch [8/43], Training Loss: 0.00007803
2024-11-06 14:06:56,451 - INFO - Epoch [39/300], Batch [9/43], Training Loss: 0.00007598
2024-11-06 14:06:56,454 - INFO - Epoch [39/300], Batch [10/43], Training Loss: 0.00005156
2024-11-06 14:06:56,458 - INFO - Epoch [39/300], Batch [11/43], Training Loss: 0.00002194
2024-11-06 14:06:56,462 - INFO - Epoch [39/300], Batch [12/43], Training Loss: 0.00002569
2024-11-06 14:06:56,466 - INFO - Epoch [39/300], Batch [13/43], Training Loss: 0.00021533
2024-11-06 14:06:56,470 - INFO - Epoch [39/300], Batch [14/43], Training Loss: 0.00002514
2024-11-06 14:06:56,474 - INFO - Epoch [39/300], Batch [15/43], Training Loss: 0.00004416
2024-11-06 14:06:56,478 - INFO - Epoch [39/300], Batch [16/43], Training Loss: 0.00005150
2024-11-06 14:06:56,482 - INFO - Epoch [39/300], Batch [17/43], Training Loss: 0.00006259
2024-11-06 14:06:56,486 - INFO - Epoch [39/300], Batch [18/43], Training Loss: 0.00005353
2024-11-06 14:06:56,489 - INFO - Epoch [39/300], Batch [19/43], Training Loss: 0.00007399
2024-11-06 14:06:56,493 - INFO - Epoch [39/300], Batch [20/43], Training Loss: 0.00004930
2024-11-06 14:06:56,496 - INFO - Epoch [39/300], Batch [21/43], Training Loss: 0.00003862
2024-11-06 14:06:56,500 - INFO - Epoch [39/300], Batch [22/43], Training Loss: 0.00006307
2024-11-06 14:06:56,504 - INFO - Epoch [39/300], Batch [23/43], Training Loss: 0.00008445
2024-11-06 14:06:56,508 - INFO - Epoch [39/300], Batch [24/43], Training Loss: 0.00006176
2024-11-06 14:06:56,512 - INFO - Epoch [39/300], Batch [25/43], Training Loss: 0.00001732
2024-11-06 14:06:56,517 - INFO - Epoch [39/300], Batch [26/43], Training Loss: 0.00004114
2024-11-06 14:06:56,521 - INFO - Epoch [39/300], Batch [27/43], Training Loss: 0.00003898
2024-11-06 14:06:56,524 - INFO - Epoch [39/300], Batch [28/43], Training Loss: 0.00002326
2024-11-06 14:06:56,527 - INFO - Epoch [39/300], Batch [29/43], Training Loss: 0.00007885
2024-11-06 14:06:56,530 - INFO - Epoch [39/300], Batch [30/43], Training Loss: 0.00003906
2024-11-06 14:06:56,535 - INFO - Epoch [39/300], Batch [31/43], Training Loss: 0.00006538
2024-11-06 14:06:56,538 - INFO - Epoch [39/300], Batch [32/43], Training Loss: 0.00003354
2024-11-06 14:06:56,541 - INFO - Epoch [39/300], Batch [33/43], Training Loss: 0.00005911
2024-11-06 14:06:56,544 - INFO - Epoch [39/300], Batch [34/43], Training Loss: 0.00005120
2024-11-06 14:06:56,547 - INFO - Epoch [39/300], Batch [35/43], Training Loss: 0.00000995
2024-11-06 14:06:56,551 - INFO - Epoch [39/300], Batch [36/43], Training Loss: 0.00003923
2024-11-06 14:06:56,554 - INFO - Epoch [39/300], Batch [37/43], Training Loss: 0.00001941
2024-11-06 14:06:56,557 - INFO - Epoch [39/300], Batch [38/43], Training Loss: 0.00002307
2024-11-06 14:06:56,562 - INFO - Epoch [39/300], Batch [39/43], Training Loss: 0.00002440
2024-11-06 14:06:56,565 - INFO - Epoch [39/300], Batch [40/43], Training Loss: 0.00003259
2024-11-06 14:06:56,569 - INFO - Epoch [39/300], Batch [41/43], Training Loss: 0.00004250
2024-11-06 14:06:56,572 - INFO - Epoch [39/300], Batch [42/43], Training Loss: 0.00002762
2024-11-06 14:06:56,576 - INFO - Epoch [39/300], Batch [43/43], Training Loss: 0.00003975
2024-11-06 14:06:56,587 - INFO - Epoch [39/300], Average Training Loss: 0.00004953, Validation Loss: 0.00005402
2024-11-06 14:06:56,591 - INFO - Epoch [40/300], Batch [1/43], Training Loss: 0.00004531
2024-11-06 14:06:56,595 - INFO - Epoch [40/300], Batch [2/43], Training Loss: 0.00003322
2024-11-06 14:06:56,599 - INFO - Epoch [40/300], Batch [3/43], Training Loss: 0.00004469
2024-11-06 14:06:56,603 - INFO - Epoch [40/300], Batch [4/43], Training Loss: 0.00005362
2024-11-06 14:06:56,606 - INFO - Epoch [40/300], Batch [5/43], Training Loss: 0.00004159
2024-11-06 14:06:56,611 - INFO - Epoch [40/300], Batch [6/43], Training Loss: 0.00003571
2024-11-06 14:06:56,614 - INFO - Epoch [40/300], Batch [7/43], Training Loss: 0.00003505
2024-11-06 14:06:56,618 - INFO - Epoch [40/300], Batch [8/43], Training Loss: 0.00001798
2024-11-06 14:06:56,622 - INFO - Epoch [40/300], Batch [9/43], Training Loss: 0.00008364
2024-11-06 14:06:56,625 - INFO - Epoch [40/300], Batch [10/43], Training Loss: 0.00005319
2024-11-06 14:06:56,629 - INFO - Epoch [40/300], Batch [11/43], Training Loss: 0.00006512
2024-11-06 14:06:56,632 - INFO - Epoch [40/300], Batch [12/43], Training Loss: 0.00006787
2024-11-06 14:06:56,636 - INFO - Epoch [40/300], Batch [13/43], Training Loss: 0.00004151
2024-11-06 14:06:56,640 - INFO - Epoch [40/300], Batch [14/43], Training Loss: 0.00003964
2024-11-06 14:06:56,644 - INFO - Epoch [40/300], Batch [15/43], Training Loss: 0.00001129
2024-11-06 14:06:56,648 - INFO - Epoch [40/300], Batch [16/43], Training Loss: 0.00005255
2024-11-06 14:06:56,651 - INFO - Epoch [40/300], Batch [17/43], Training Loss: 0.00004776
2024-11-06 14:06:56,655 - INFO - Epoch [40/300], Batch [18/43], Training Loss: 0.00002475
2024-11-06 14:06:56,659 - INFO - Epoch [40/300], Batch [19/43], Training Loss: 0.00001995
2024-11-06 14:06:56,662 - INFO - Epoch [40/300], Batch [20/43], Training Loss: 0.00003059
2024-11-06 14:06:56,666 - INFO - Epoch [40/300], Batch [21/43], Training Loss: 0.00004066
2024-11-06 14:06:56,670 - INFO - Epoch [40/300], Batch [22/43], Training Loss: 0.00005865
2024-11-06 14:06:56,673 - INFO - Epoch [40/300], Batch [23/43], Training Loss: 0.00004104
2024-11-06 14:06:56,676 - INFO - Epoch [40/300], Batch [24/43], Training Loss: 0.00002440
2024-11-06 14:06:56,679 - INFO - Epoch [40/300], Batch [25/43], Training Loss: 0.00003636
2024-11-06 14:06:56,681 - INFO - Epoch [40/300], Batch [26/43], Training Loss: 0.00004519
2024-11-06 14:06:56,684 - INFO - Epoch [40/300], Batch [27/43], Training Loss: 0.00002725
2024-11-06 14:06:56,687 - INFO - Epoch [40/300], Batch [28/43], Training Loss: 0.00005760
2024-11-06 14:06:56,690 - INFO - Epoch [40/300], Batch [29/43], Training Loss: 0.00003817
2024-11-06 14:06:56,694 - INFO - Epoch [40/300], Batch [30/43], Training Loss: 0.00004554
2024-11-06 14:06:56,696 - INFO - Epoch [40/300], Batch [31/43], Training Loss: 0.00003448
2024-11-06 14:06:56,701 - INFO - Epoch [40/300], Batch [32/43], Training Loss: 0.00004587
2024-11-06 14:06:56,705 - INFO - Epoch [40/300], Batch [33/43], Training Loss: 0.00002032
2024-11-06 14:06:56,709 - INFO - Epoch [40/300], Batch [34/43], Training Loss: 0.00002182
2024-11-06 14:06:56,713 - INFO - Epoch [40/300], Batch [35/43], Training Loss: 0.00009105
2024-11-06 14:06:56,717 - INFO - Epoch [40/300], Batch [36/43], Training Loss: 0.00005694
2024-11-06 14:06:56,721 - INFO - Epoch [40/300], Batch [37/43], Training Loss: 0.00002730
2024-11-06 14:06:56,725 - INFO - Epoch [40/300], Batch [38/43], Training Loss: 0.00004699
2024-11-06 14:06:56,729 - INFO - Epoch [40/300], Batch [39/43], Training Loss: 0.00005561
2024-11-06 14:06:56,733 - INFO - Epoch [40/300], Batch [40/43], Training Loss: 0.00002676
2024-11-06 14:06:56,737 - INFO - Epoch [40/300], Batch [41/43], Training Loss: 0.00003660
2024-11-06 14:06:56,741 - INFO - Epoch [40/300], Batch [42/43], Training Loss: 0.00004861
2024-11-06 14:06:56,745 - INFO - Epoch [40/300], Batch [43/43], Training Loss: 0.00001701
2024-11-06 14:06:56,756 - INFO - Epoch [40/300], Average Training Loss: 0.00004161, Validation Loss: 0.00004639
2024-11-06 14:06:56,760 - INFO - Epoch [41/300], Batch [1/43], Training Loss: 0.00001892
2024-11-06 14:06:56,763 - INFO - Epoch [41/300], Batch [2/43], Training Loss: 0.00002596
2024-11-06 14:06:56,767 - INFO - Epoch [41/300], Batch [3/43], Training Loss: 0.00003461
2024-11-06 14:06:56,771 - INFO - Epoch [41/300], Batch [4/43], Training Loss: 0.00002090
2024-11-06 14:06:56,774 - INFO - Epoch [41/300], Batch [5/43], Training Loss: 0.00001119
2024-11-06 14:06:56,778 - INFO - Epoch [41/300], Batch [6/43], Training Loss: 0.00001088
2024-11-06 14:06:56,781 - INFO - Epoch [41/300], Batch [7/43], Training Loss: 0.00007918
2024-11-06 14:06:56,786 - INFO - Epoch [41/300], Batch [8/43], Training Loss: 0.00002466
2024-11-06 14:06:56,789 - INFO - Epoch [41/300], Batch [9/43], Training Loss: 0.00001599
2024-11-06 14:06:56,793 - INFO - Epoch [41/300], Batch [10/43], Training Loss: 0.00004037
2024-11-06 14:06:56,798 - INFO - Epoch [41/300], Batch [11/43], Training Loss: 0.00004001
2024-11-06 14:06:56,801 - INFO - Epoch [41/300], Batch [12/43], Training Loss: 0.00002846
2024-11-06 14:06:56,805 - INFO - Epoch [41/300], Batch [13/43], Training Loss: 0.00003742
2024-11-06 14:06:56,809 - INFO - Epoch [41/300], Batch [14/43], Training Loss: 0.00004049
2024-11-06 14:06:56,812 - INFO - Epoch [41/300], Batch [15/43], Training Loss: 0.00002902
2024-11-06 14:06:56,816 - INFO - Epoch [41/300], Batch [16/43], Training Loss: 0.00004102
2024-11-06 14:06:56,819 - INFO - Epoch [41/300], Batch [17/43], Training Loss: 0.00005649
2024-11-06 14:06:56,822 - INFO - Epoch [41/300], Batch [18/43], Training Loss: 0.00004632
2024-11-06 14:06:56,826 - INFO - Epoch [41/300], Batch [19/43], Training Loss: 0.00002123
2024-11-06 14:06:56,829 - INFO - Epoch [41/300], Batch [20/43], Training Loss: 0.00003577
2024-11-06 14:06:56,834 - INFO - Epoch [41/300], Batch [21/43], Training Loss: 0.00002537
2024-11-06 14:06:56,839 - INFO - Epoch [41/300], Batch [22/43], Training Loss: 0.00004989
2024-11-06 14:06:56,843 - INFO - Epoch [41/300], Batch [23/43], Training Loss: 0.00006797
2024-11-06 14:06:56,848 - INFO - Epoch [41/300], Batch [24/43], Training Loss: 0.00002719
2024-11-06 14:06:56,852 - INFO - Epoch [41/300], Batch [25/43], Training Loss: 0.00007501
2024-11-06 14:06:56,856 - INFO - Epoch [41/300], Batch [26/43], Training Loss: 0.00002950
2024-11-06 14:06:56,859 - INFO - Epoch [41/300], Batch [27/43], Training Loss: 0.00002668
2024-11-06 14:06:56,863 - INFO - Epoch [41/300], Batch [28/43], Training Loss: 0.00002664
2024-11-06 14:06:56,866 - INFO - Epoch [41/300], Batch [29/43], Training Loss: 0.00007813
2024-11-06 14:06:56,869 - INFO - Epoch [41/300], Batch [30/43], Training Loss: 0.00005148
2024-11-06 14:06:56,873 - INFO - Epoch [41/300], Batch [31/43], Training Loss: 0.00004214
2024-11-06 14:06:56,877 - INFO - Epoch [41/300], Batch [32/43], Training Loss: 0.00006302
2024-11-06 14:06:56,881 - INFO - Epoch [41/300], Batch [33/43], Training Loss: 0.00006228
2024-11-06 14:06:56,885 - INFO - Epoch [41/300], Batch [34/43], Training Loss: 0.00003935
2024-11-06 14:06:56,889 - INFO - Epoch [41/300], Batch [35/43], Training Loss: 0.00002059
2024-11-06 14:06:56,893 - INFO - Epoch [41/300], Batch [36/43], Training Loss: 0.00003507
2024-11-06 14:06:56,896 - INFO - Epoch [41/300], Batch [37/43], Training Loss: 0.00013872
2024-11-06 14:06:56,899 - INFO - Epoch [41/300], Batch [38/43], Training Loss: 0.00007457
2024-11-06 14:06:56,903 - INFO - Epoch [41/300], Batch [39/43], Training Loss: 0.00003950
2024-11-06 14:06:56,908 - INFO - Epoch [41/300], Batch [40/43], Training Loss: 0.00005527
2024-11-06 14:06:56,912 - INFO - Epoch [41/300], Batch [41/43], Training Loss: 0.00007662
2024-11-06 14:06:56,916 - INFO - Epoch [41/300], Batch [42/43], Training Loss: 0.00002976
2024-11-06 14:06:56,920 - INFO - Epoch [41/300], Batch [43/43], Training Loss: 0.00001929
2024-11-06 14:06:56,931 - INFO - Epoch [41/300], Average Training Loss: 0.00004216, Validation Loss: 0.00006608
2024-11-06 14:06:56,935 - INFO - Epoch [42/300], Batch [1/43], Training Loss: 0.00002397
2024-11-06 14:06:56,939 - INFO - Epoch [42/300], Batch [2/43], Training Loss: 0.00005665
2024-11-06 14:06:56,943 - INFO - Epoch [42/300], Batch [3/43], Training Loss: 0.00004747
2024-11-06 14:06:56,946 - INFO - Epoch [42/300], Batch [4/43], Training Loss: 0.00002812
2024-11-06 14:06:56,950 - INFO - Epoch [42/300], Batch [5/43], Training Loss: 0.00003666
2024-11-06 14:06:56,953 - INFO - Epoch [42/300], Batch [6/43], Training Loss: 0.00004868
2024-11-06 14:06:56,956 - INFO - Epoch [42/300], Batch [7/43], Training Loss: 0.00005022
2024-11-06 14:06:56,959 - INFO - Epoch [42/300], Batch [8/43], Training Loss: 0.00002032
2024-11-06 14:06:56,963 - INFO - Epoch [42/300], Batch [9/43], Training Loss: 0.00010287
2024-11-06 14:06:56,966 - INFO - Epoch [42/300], Batch [10/43], Training Loss: 0.00002254
2024-11-06 14:06:56,969 - INFO - Epoch [42/300], Batch [11/43], Training Loss: 0.00006772
2024-11-06 14:06:56,972 - INFO - Epoch [42/300], Batch [12/43], Training Loss: 0.00003736
2024-11-06 14:06:56,975 - INFO - Epoch [42/300], Batch [13/43], Training Loss: 0.00001012
2024-11-06 14:06:56,977 - INFO - Epoch [42/300], Batch [14/43], Training Loss: 0.00007361
2024-11-06 14:06:56,980 - INFO - Epoch [42/300], Batch [15/43], Training Loss: 0.00004123
2024-11-06 14:06:56,984 - INFO - Epoch [42/300], Batch [16/43], Training Loss: 0.00002218
2024-11-06 14:06:56,988 - INFO - Epoch [42/300], Batch [17/43], Training Loss: 0.00001855
2024-11-06 14:06:56,992 - INFO - Epoch [42/300], Batch [18/43], Training Loss: 0.00004072
2024-11-06 14:06:56,996 - INFO - Epoch [42/300], Batch [19/43], Training Loss: 0.00003224
2024-11-06 14:06:57,000 - INFO - Epoch [42/300], Batch [20/43], Training Loss: 0.00005050
2024-11-06 14:06:57,005 - INFO - Epoch [42/300], Batch [21/43], Training Loss: 0.00004135
2024-11-06 14:06:57,009 - INFO - Epoch [42/300], Batch [22/43], Training Loss: 0.00003828
2024-11-06 14:06:57,013 - INFO - Epoch [42/300], Batch [23/43], Training Loss: 0.00003011
2024-11-06 14:06:57,017 - INFO - Epoch [42/300], Batch [24/43], Training Loss: 0.00003542
2024-11-06 14:06:57,020 - INFO - Epoch [42/300], Batch [25/43], Training Loss: 0.00001614
2024-11-06 14:06:57,024 - INFO - Epoch [42/300], Batch [26/43], Training Loss: 0.00004573
2024-11-06 14:06:57,027 - INFO - Epoch [42/300], Batch [27/43], Training Loss: 0.00004355
2024-11-06 14:06:57,030 - INFO - Epoch [42/300], Batch [28/43], Training Loss: 0.00005694
2024-11-06 14:06:57,034 - INFO - Epoch [42/300], Batch [29/43], Training Loss: 0.00002737
2024-11-06 14:06:57,038 - INFO - Epoch [42/300], Batch [30/43], Training Loss: 0.00003646
2024-11-06 14:06:57,042 - INFO - Epoch [42/300], Batch [31/43], Training Loss: 0.00006456
2024-11-06 14:06:57,045 - INFO - Epoch [42/300], Batch [32/43], Training Loss: 0.00002555
2024-11-06 14:06:57,048 - INFO - Epoch [42/300], Batch [33/43], Training Loss: 0.00003705
2024-11-06 14:06:57,051 - INFO - Epoch [42/300], Batch [34/43], Training Loss: 0.00003155
2024-11-06 14:06:57,054 - INFO - Epoch [42/300], Batch [35/43], Training Loss: 0.00002720
2024-11-06 14:06:57,056 - INFO - Epoch [42/300], Batch [36/43], Training Loss: 0.00004890
2024-11-06 14:06:57,059 - INFO - Epoch [42/300], Batch [37/43], Training Loss: 0.00002789
2024-11-06 14:06:57,063 - INFO - Epoch [42/300], Batch [38/43], Training Loss: 0.00003079
2024-11-06 14:06:57,066 - INFO - Epoch [42/300], Batch [39/43], Training Loss: 0.00002317
2024-11-06 14:06:57,069 - INFO - Epoch [42/300], Batch [40/43], Training Loss: 0.00003049
2024-11-06 14:06:57,072 - INFO - Epoch [42/300], Batch [41/43], Training Loss: 0.00002796
2024-11-06 14:06:57,075 - INFO - Epoch [42/300], Batch [42/43], Training Loss: 0.00004867
2024-11-06 14:06:57,078 - INFO - Epoch [42/300], Batch [43/43], Training Loss: 0.00002214
2024-11-06 14:06:57,088 - INFO - Epoch [42/300], Average Training Loss: 0.00003835, Validation Loss: 0.00004201
2024-11-06 14:06:57,092 - INFO - Epoch [43/300], Batch [1/43], Training Loss: 0.00002116
2024-11-06 14:06:57,096 - INFO - Epoch [43/300], Batch [2/43], Training Loss: 0.00007099
2024-11-06 14:06:57,100 - INFO - Epoch [43/300], Batch [3/43], Training Loss: 0.00002349
2024-11-06 14:06:57,103 - INFO - Epoch [43/300], Batch [4/43], Training Loss: 0.00002789
2024-11-06 14:06:57,107 - INFO - Epoch [43/300], Batch [5/43], Training Loss: 0.00004236
2024-11-06 14:06:57,110 - INFO - Epoch [43/300], Batch [6/43], Training Loss: 0.00002176
2024-11-06 14:06:57,113 - INFO - Epoch [43/300], Batch [7/43], Training Loss: 0.00003579
2024-11-06 14:06:57,116 - INFO - Epoch [43/300], Batch [8/43], Training Loss: 0.00003012
2024-11-06 14:06:57,119 - INFO - Epoch [43/300], Batch [9/43], Training Loss: 0.00001611
2024-11-06 14:06:57,122 - INFO - Epoch [43/300], Batch [10/43], Training Loss: 0.00005554
2024-11-06 14:06:57,125 - INFO - Epoch [43/300], Batch [11/43], Training Loss: 0.00003489
2024-11-06 14:06:57,128 - INFO - Epoch [43/300], Batch [12/43], Training Loss: 0.00003104
2024-11-06 14:06:57,131 - INFO - Epoch [43/300], Batch [13/43], Training Loss: 0.00001864
2024-11-06 14:06:57,134 - INFO - Epoch [43/300], Batch [14/43], Training Loss: 0.00002783
2024-11-06 14:06:57,138 - INFO - Epoch [43/300], Batch [15/43], Training Loss: 0.00003270
2024-11-06 14:06:57,141 - INFO - Epoch [43/300], Batch [16/43], Training Loss: 0.00003665
2024-11-06 14:06:57,144 - INFO - Epoch [43/300], Batch [17/43], Training Loss: 0.00001760
2024-11-06 14:06:57,147 - INFO - Epoch [43/300], Batch [18/43], Training Loss: 0.00005296
2024-11-06 14:06:57,151 - INFO - Epoch [43/300], Batch [19/43], Training Loss: 0.00002481
2024-11-06 14:06:57,154 - INFO - Epoch [43/300], Batch [20/43], Training Loss: 0.00004538
2024-11-06 14:06:57,158 - INFO - Epoch [43/300], Batch [21/43], Training Loss: 0.00003204
2024-11-06 14:06:57,160 - INFO - Epoch [43/300], Batch [22/43], Training Loss: 0.00003101
2024-11-06 14:06:57,163 - INFO - Epoch [43/300], Batch [23/43], Training Loss: 0.00001645
2024-11-06 14:06:57,165 - INFO - Epoch [43/300], Batch [24/43], Training Loss: 0.00002619
2024-11-06 14:06:57,168 - INFO - Epoch [43/300], Batch [25/43], Training Loss: 0.00003769
2024-11-06 14:06:57,172 - INFO - Epoch [43/300], Batch [26/43], Training Loss: 0.00002331
2024-11-06 14:06:57,175 - INFO - Epoch [43/300], Batch [27/43], Training Loss: 0.00000630
2024-11-06 14:06:57,179 - INFO - Epoch [43/300], Batch [28/43], Training Loss: 0.00003677
2024-11-06 14:06:57,182 - INFO - Epoch [43/300], Batch [29/43], Training Loss: 0.00002761
2024-11-06 14:06:57,185 - INFO - Epoch [43/300], Batch [30/43], Training Loss: 0.00000880
2024-11-06 14:06:57,189 - INFO - Epoch [43/300], Batch [31/43], Training Loss: 0.00001007
2024-11-06 14:06:57,192 - INFO - Epoch [43/300], Batch [32/43], Training Loss: 0.00002311
2024-11-06 14:06:57,195 - INFO - Epoch [43/300], Batch [33/43], Training Loss: 0.00004500
2024-11-06 14:06:57,198 - INFO - Epoch [43/300], Batch [34/43], Training Loss: 0.00002526
2024-11-06 14:06:57,201 - INFO - Epoch [43/300], Batch [35/43], Training Loss: 0.00003147
2024-11-06 14:06:57,204 - INFO - Epoch [43/300], Batch [36/43], Training Loss: 0.00002070
2024-11-06 14:06:57,208 - INFO - Epoch [43/300], Batch [37/43], Training Loss: 0.00002789
2024-11-06 14:06:57,212 - INFO - Epoch [43/300], Batch [38/43], Training Loss: 0.00004956
2024-11-06 14:06:57,216 - INFO - Epoch [43/300], Batch [39/43], Training Loss: 0.00003279
2024-11-06 14:06:57,219 - INFO - Epoch [43/300], Batch [40/43], Training Loss: 0.00002193
2024-11-06 14:06:57,223 - INFO - Epoch [43/300], Batch [41/43], Training Loss: 0.00003174
2024-11-06 14:06:57,228 - INFO - Epoch [43/300], Batch [42/43], Training Loss: 0.00004571
2024-11-06 14:06:57,231 - INFO - Epoch [43/300], Batch [43/43], Training Loss: 0.00003371
2024-11-06 14:06:57,242 - INFO - Epoch [43/300], Average Training Loss: 0.00003053, Validation Loss: 0.00003506
2024-11-06 14:06:57,246 - INFO - Epoch [44/300], Batch [1/43], Training Loss: 0.00002494
2024-11-06 14:06:57,249 - INFO - Epoch [44/300], Batch [2/43], Training Loss: 0.00002619
2024-11-06 14:06:57,252 - INFO - Epoch [44/300], Batch [3/43], Training Loss: 0.00002117
2024-11-06 14:06:57,256 - INFO - Epoch [44/300], Batch [4/43], Training Loss: 0.00001910
2024-11-06 14:06:57,259 - INFO - Epoch [44/300], Batch [5/43], Training Loss: 0.00002421
2024-11-06 14:06:57,262 - INFO - Epoch [44/300], Batch [6/43], Training Loss: 0.00002634
2024-11-06 14:06:57,265 - INFO - Epoch [44/300], Batch [7/43], Training Loss: 0.00001266
2024-11-06 14:06:57,267 - INFO - Epoch [44/300], Batch [8/43], Training Loss: 0.00003140
2024-11-06 14:06:57,271 - INFO - Epoch [44/300], Batch [9/43], Training Loss: 0.00006822
2024-11-06 14:06:57,276 - INFO - Epoch [44/300], Batch [10/43], Training Loss: 0.00001928
2024-11-06 14:06:57,280 - INFO - Epoch [44/300], Batch [11/43], Training Loss: 0.00004964
2024-11-06 14:06:57,284 - INFO - Epoch [44/300], Batch [12/43], Training Loss: 0.00004382
2024-11-06 14:06:57,288 - INFO - Epoch [44/300], Batch [13/43], Training Loss: 0.00002947
2024-11-06 14:06:57,291 - INFO - Epoch [44/300], Batch [14/43], Training Loss: 0.00003133
2024-11-06 14:06:57,294 - INFO - Epoch [44/300], Batch [15/43], Training Loss: 0.00003610
2024-11-06 14:06:57,298 - INFO - Epoch [44/300], Batch [16/43], Training Loss: 0.00001480
2024-11-06 14:06:57,302 - INFO - Epoch [44/300], Batch [17/43], Training Loss: 0.00002155
2024-11-06 14:06:57,305 - INFO - Epoch [44/300], Batch [18/43], Training Loss: 0.00001022
2024-11-06 14:06:57,309 - INFO - Epoch [44/300], Batch [19/43], Training Loss: 0.00002639
2024-11-06 14:06:57,313 - INFO - Epoch [44/300], Batch [20/43], Training Loss: 0.00004103
2024-11-06 14:06:57,317 - INFO - Epoch [44/300], Batch [21/43], Training Loss: 0.00004323
2024-11-06 14:06:57,321 - INFO - Epoch [44/300], Batch [22/43], Training Loss: 0.00003946
2024-11-06 14:06:57,325 - INFO - Epoch [44/300], Batch [23/43], Training Loss: 0.00003705
2024-11-06 14:06:57,329 - INFO - Epoch [44/300], Batch [24/43], Training Loss: 0.00003274
2024-11-06 14:06:57,332 - INFO - Epoch [44/300], Batch [25/43], Training Loss: 0.00002420
2024-11-06 14:06:57,336 - INFO - Epoch [44/300], Batch [26/43], Training Loss: 0.00002218
2024-11-06 14:06:57,340 - INFO - Epoch [44/300], Batch [27/43], Training Loss: 0.00002417
2024-11-06 14:06:57,346 - INFO - Epoch [44/300], Batch [28/43], Training Loss: 0.00002311
2024-11-06 14:06:57,352 - INFO - Epoch [44/300], Batch [29/43], Training Loss: 0.00001643
2024-11-06 14:06:57,357 - INFO - Epoch [44/300], Batch [30/43], Training Loss: 0.00001903
2024-11-06 14:06:57,361 - INFO - Epoch [44/300], Batch [31/43], Training Loss: 0.00004290
2024-11-06 14:06:57,365 - INFO - Epoch [44/300], Batch [32/43], Training Loss: 0.00001879
2024-11-06 14:06:57,370 - INFO - Epoch [44/300], Batch [33/43], Training Loss: 0.00001610
2024-11-06 14:06:57,375 - INFO - Epoch [44/300], Batch [34/43], Training Loss: 0.00004578
2024-11-06 14:06:57,379 - INFO - Epoch [44/300], Batch [35/43], Training Loss: 0.00002330
2024-11-06 14:06:57,383 - INFO - Epoch [44/300], Batch [36/43], Training Loss: 0.00002077
2024-11-06 14:06:57,387 - INFO - Epoch [44/300], Batch [37/43], Training Loss: 0.00001318
2024-11-06 14:06:57,391 - INFO - Epoch [44/300], Batch [38/43], Training Loss: 0.00003631
2024-11-06 14:06:57,396 - INFO - Epoch [44/300], Batch [39/43], Training Loss: 0.00002156
2024-11-06 14:06:57,400 - INFO - Epoch [44/300], Batch [40/43], Training Loss: 0.00001969
2024-11-06 14:06:57,405 - INFO - Epoch [44/300], Batch [41/43], Training Loss: 0.00001902
2024-11-06 14:06:57,409 - INFO - Epoch [44/300], Batch [42/43], Training Loss: 0.00002732
2024-11-06 14:06:57,414 - INFO - Epoch [44/300], Batch [43/43], Training Loss: 0.00003369
2024-11-06 14:06:57,428 - INFO - Epoch [44/300], Average Training Loss: 0.00002786, Validation Loss: 0.00004047
2024-11-06 14:06:57,432 - INFO - Epoch [45/300], Batch [1/43], Training Loss: 0.00004930
2024-11-06 14:06:57,437 - INFO - Epoch [45/300], Batch [2/43], Training Loss: 0.00001255
2024-11-06 14:06:57,442 - INFO - Epoch [45/300], Batch [3/43], Training Loss: 0.00003351
2024-11-06 14:06:57,447 - INFO - Epoch [45/300], Batch [4/43], Training Loss: 0.00002367
2024-11-06 14:06:57,451 - INFO - Epoch [45/300], Batch [5/43], Training Loss: 0.00004792
2024-11-06 14:06:57,455 - INFO - Epoch [45/300], Batch [6/43], Training Loss: 0.00001642
2024-11-06 14:06:57,459 - INFO - Epoch [45/300], Batch [7/43], Training Loss: 0.00000879
2024-11-06 14:06:57,464 - INFO - Epoch [45/300], Batch [8/43], Training Loss: 0.00003718
2024-11-06 14:06:57,469 - INFO - Epoch [45/300], Batch [9/43], Training Loss: 0.00002529
2024-11-06 14:06:57,473 - INFO - Epoch [45/300], Batch [10/43], Training Loss: 0.00001537
2024-11-06 14:06:57,478 - INFO - Epoch [45/300], Batch [11/43], Training Loss: 0.00001806
2024-11-06 14:06:57,481 - INFO - Epoch [45/300], Batch [12/43], Training Loss: 0.00002487
2024-11-06 14:06:57,486 - INFO - Epoch [45/300], Batch [13/43], Training Loss: 0.00003448
2024-11-06 14:06:57,490 - INFO - Epoch [45/300], Batch [14/43], Training Loss: 0.00003896
2024-11-06 14:06:57,494 - INFO - Epoch [45/300], Batch [15/43], Training Loss: 0.00002221
2024-11-06 14:06:57,498 - INFO - Epoch [45/300], Batch [16/43], Training Loss: 0.00002079
2024-11-06 14:06:57,502 - INFO - Epoch [45/300], Batch [17/43], Training Loss: 0.00001360
2024-11-06 14:06:57,507 - INFO - Epoch [45/300], Batch [18/43], Training Loss: 0.00002910
2024-11-06 14:06:57,511 - INFO - Epoch [45/300], Batch [19/43], Training Loss: 0.00004087
2024-11-06 14:06:57,515 - INFO - Epoch [45/300], Batch [20/43], Training Loss: 0.00005056
2024-11-06 14:06:57,519 - INFO - Epoch [45/300], Batch [21/43], Training Loss: 0.00002270
2024-11-06 14:06:57,523 - INFO - Epoch [45/300], Batch [22/43], Training Loss: 0.00001984
2024-11-06 14:06:57,526 - INFO - Epoch [45/300], Batch [23/43], Training Loss: 0.00003027
2024-11-06 14:06:57,530 - INFO - Epoch [45/300], Batch [24/43], Training Loss: 0.00001987
2024-11-06 14:06:57,535 - INFO - Epoch [45/300], Batch [25/43], Training Loss: 0.00002329
2024-11-06 14:06:57,539 - INFO - Epoch [45/300], Batch [26/43], Training Loss: 0.00002240
2024-11-06 14:06:57,544 - INFO - Epoch [45/300], Batch [27/43], Training Loss: 0.00003479
2024-11-06 14:06:57,548 - INFO - Epoch [45/300], Batch [28/43], Training Loss: 0.00001215
2024-11-06 14:06:57,553 - INFO - Epoch [45/300], Batch [29/43], Training Loss: 0.00004965
2024-11-06 14:06:57,557 - INFO - Epoch [45/300], Batch [30/43], Training Loss: 0.00002522
2024-11-06 14:06:57,561 - INFO - Epoch [45/300], Batch [31/43], Training Loss: 0.00001936
2024-11-06 14:06:57,565 - INFO - Epoch [45/300], Batch [32/43], Training Loss: 0.00004226
2024-11-06 14:06:57,570 - INFO - Epoch [45/300], Batch [33/43], Training Loss: 0.00003276
2024-11-06 14:06:57,574 - INFO - Epoch [45/300], Batch [34/43], Training Loss: 0.00002941
2024-11-06 14:06:57,578 - INFO - Epoch [45/300], Batch [35/43], Training Loss: 0.00003448
2024-11-06 14:06:57,583 - INFO - Epoch [45/300], Batch [36/43], Training Loss: 0.00002149
2024-11-06 14:06:57,587 - INFO - Epoch [45/300], Batch [37/43], Training Loss: 0.00003366
2024-11-06 14:06:57,591 - INFO - Epoch [45/300], Batch [38/43], Training Loss: 0.00003027
2024-11-06 14:06:57,595 - INFO - Epoch [45/300], Batch [39/43], Training Loss: 0.00001770
2024-11-06 14:06:57,599 - INFO - Epoch [45/300], Batch [40/43], Training Loss: 0.00001955
2024-11-06 14:06:57,603 - INFO - Epoch [45/300], Batch [41/43], Training Loss: 0.00002054
2024-11-06 14:06:57,607 - INFO - Epoch [45/300], Batch [42/43], Training Loss: 0.00002643
2024-11-06 14:06:57,611 - INFO - Epoch [45/300], Batch [43/43], Training Loss: 0.00002844
2024-11-06 14:06:57,624 - INFO - Epoch [45/300], Average Training Loss: 0.00002744, Validation Loss: 0.00003018
2024-11-06 14:06:57,628 - INFO - Epoch [46/300], Batch [1/43], Training Loss: 0.00001923
2024-11-06 14:06:57,632 - INFO - Epoch [46/300], Batch [2/43], Training Loss: 0.00003016
2024-11-06 14:06:57,637 - INFO - Epoch [46/300], Batch [3/43], Training Loss: 0.00005721
2024-11-06 14:06:57,640 - INFO - Epoch [46/300], Batch [4/43], Training Loss: 0.00002093
2024-11-06 14:06:57,643 - INFO - Epoch [46/300], Batch [5/43], Training Loss: 0.00001399
2024-11-06 14:06:57,648 - INFO - Epoch [46/300], Batch [6/43], Training Loss: 0.00002604
2024-11-06 14:06:57,652 - INFO - Epoch [46/300], Batch [7/43], Training Loss: 0.00003664
2024-11-06 14:06:57,656 - INFO - Epoch [46/300], Batch [8/43], Training Loss: 0.00004021
2024-11-06 14:06:57,660 - INFO - Epoch [46/300], Batch [9/43], Training Loss: 0.00004136
2024-11-06 14:06:57,664 - INFO - Epoch [46/300], Batch [10/43], Training Loss: 0.00003362
2024-11-06 14:06:57,668 - INFO - Epoch [46/300], Batch [11/43], Training Loss: 0.00001876
2024-11-06 14:06:57,672 - INFO - Epoch [46/300], Batch [12/43], Training Loss: 0.00004904
2024-11-06 14:06:57,676 - INFO - Epoch [46/300], Batch [13/43], Training Loss: 0.00003426
2024-11-06 14:06:57,680 - INFO - Epoch [46/300], Batch [14/43], Training Loss: 0.00004831
2024-11-06 14:06:57,684 - INFO - Epoch [46/300], Batch [15/43], Training Loss: 0.00001999
2024-11-06 14:06:57,688 - INFO - Epoch [46/300], Batch [16/43], Training Loss: 0.00003975
2024-11-06 14:06:57,693 - INFO - Epoch [46/300], Batch [17/43], Training Loss: 0.00002931
2024-11-06 14:06:57,697 - INFO - Epoch [46/300], Batch [18/43], Training Loss: 0.00001368
2024-11-06 14:06:57,701 - INFO - Epoch [46/300], Batch [19/43], Training Loss: 0.00003266
2024-11-06 14:06:57,704 - INFO - Epoch [46/300], Batch [20/43], Training Loss: 0.00002000
2024-11-06 14:06:57,708 - INFO - Epoch [46/300], Batch [21/43], Training Loss: 0.00001508
2024-11-06 14:06:57,714 - INFO - Epoch [46/300], Batch [22/43], Training Loss: 0.00002418
2024-11-06 14:06:57,717 - INFO - Epoch [46/300], Batch [23/43], Training Loss: 0.00005881
2024-11-06 14:06:57,722 - INFO - Epoch [46/300], Batch [24/43], Training Loss: 0.00001688
2024-11-06 14:06:57,726 - INFO - Epoch [46/300], Batch [25/43], Training Loss: 0.00002243
2024-11-06 14:06:57,730 - INFO - Epoch [46/300], Batch [26/43], Training Loss: 0.00002108
2024-11-06 14:06:57,734 - INFO - Epoch [46/300], Batch [27/43], Training Loss: 0.00000835
2024-11-06 14:06:57,739 - INFO - Epoch [46/300], Batch [28/43], Training Loss: 0.00002736
2024-11-06 14:06:57,743 - INFO - Epoch [46/300], Batch [29/43], Training Loss: 0.00004527
2024-11-06 14:06:57,748 - INFO - Epoch [46/300], Batch [30/43], Training Loss: 0.00001649
2024-11-06 14:06:57,752 - INFO - Epoch [46/300], Batch [31/43], Training Loss: 0.00002222
2024-11-06 14:06:57,755 - INFO - Epoch [46/300], Batch [32/43], Training Loss: 0.00003259
2024-11-06 14:06:57,759 - INFO - Epoch [46/300], Batch [33/43], Training Loss: 0.00003800
2024-11-06 14:06:57,763 - INFO - Epoch [46/300], Batch [34/43], Training Loss: 0.00003036
2024-11-06 14:06:57,766 - INFO - Epoch [46/300], Batch [35/43], Training Loss: 0.00003530
2024-11-06 14:06:57,771 - INFO - Epoch [46/300], Batch [36/43], Training Loss: 0.00004028
2024-11-06 14:06:57,775 - INFO - Epoch [46/300], Batch [37/43], Training Loss: 0.00002837
2024-11-06 14:06:57,779 - INFO - Epoch [46/300], Batch [38/43], Training Loss: 0.00001503
2024-11-06 14:06:57,783 - INFO - Epoch [46/300], Batch [39/43], Training Loss: 0.00004651
2024-11-06 14:06:57,787 - INFO - Epoch [46/300], Batch [40/43], Training Loss: 0.00003373
2024-11-06 14:06:57,790 - INFO - Epoch [46/300], Batch [41/43], Training Loss: 0.00001106
2024-11-06 14:06:57,794 - INFO - Epoch [46/300], Batch [42/43], Training Loss: 0.00001866
2024-11-06 14:06:57,798 - INFO - Epoch [46/300], Batch [43/43], Training Loss: 0.00002400
2024-11-06 14:06:57,811 - INFO - Epoch [46/300], Average Training Loss: 0.00002924, Validation Loss: 0.00004831
2024-11-06 14:06:57,816 - INFO - Epoch [47/300], Batch [1/43], Training Loss: 0.00002990
2024-11-06 14:06:57,819 - INFO - Epoch [47/300], Batch [2/43], Training Loss: 0.00003872
2024-11-06 14:06:57,823 - INFO - Epoch [47/300], Batch [3/43], Training Loss: 0.00002820
2024-11-06 14:06:57,828 - INFO - Epoch [47/300], Batch [4/43], Training Loss: 0.00004393
2024-11-06 14:06:57,832 - INFO - Epoch [47/300], Batch [5/43], Training Loss: 0.00005028
2024-11-06 14:06:57,836 - INFO - Epoch [47/300], Batch [6/43], Training Loss: 0.00003702
2024-11-06 14:06:57,840 - INFO - Epoch [47/300], Batch [7/43], Training Loss: 0.00003351
2024-11-06 14:06:57,845 - INFO - Epoch [47/300], Batch [8/43], Training Loss: 0.00006905
2024-11-06 14:06:57,849 - INFO - Epoch [47/300], Batch [9/43], Training Loss: 0.00004372
2024-11-06 14:06:57,853 - INFO - Epoch [47/300], Batch [10/43], Training Loss: 0.00001487
2024-11-06 14:06:57,857 - INFO - Epoch [47/300], Batch [11/43], Training Loss: 0.00002690
2024-11-06 14:06:57,860 - INFO - Epoch [47/300], Batch [12/43], Training Loss: 0.00003092
2024-11-06 14:06:57,864 - INFO - Epoch [47/300], Batch [13/43], Training Loss: 0.00002031
2024-11-06 14:06:57,869 - INFO - Epoch [47/300], Batch [14/43], Training Loss: 0.00002369
2024-11-06 14:06:57,873 - INFO - Epoch [47/300], Batch [15/43], Training Loss: 0.00000716
2024-11-06 14:06:57,877 - INFO - Epoch [47/300], Batch [16/43], Training Loss: 0.00001007
2024-11-06 14:06:57,882 - INFO - Epoch [47/300], Batch [17/43], Training Loss: 0.00002504
2024-11-06 14:06:57,885 - INFO - Epoch [47/300], Batch [18/43], Training Loss: 0.00002176
2024-11-06 14:06:57,889 - INFO - Epoch [47/300], Batch [19/43], Training Loss: 0.00004402
2024-11-06 14:06:57,893 - INFO - Epoch [47/300], Batch [20/43], Training Loss: 0.00001276
2024-11-06 14:06:57,897 - INFO - Epoch [47/300], Batch [21/43], Training Loss: 0.00005399
2024-11-06 14:06:57,900 - INFO - Epoch [47/300], Batch [22/43], Training Loss: 0.00003249
2024-11-06 14:06:57,905 - INFO - Epoch [47/300], Batch [23/43], Training Loss: 0.00001923
2024-11-06 14:06:57,909 - INFO - Epoch [47/300], Batch [24/43], Training Loss: 0.00001504
2024-11-06 14:06:57,913 - INFO - Epoch [47/300], Batch [25/43], Training Loss: 0.00003471
2024-11-06 14:06:57,916 - INFO - Epoch [47/300], Batch [26/43], Training Loss: 0.00001620
2024-11-06 14:06:57,920 - INFO - Epoch [47/300], Batch [27/43], Training Loss: 0.00001928
2024-11-06 14:06:57,925 - INFO - Epoch [47/300], Batch [28/43], Training Loss: 0.00002380
2024-11-06 14:06:57,929 - INFO - Epoch [47/300], Batch [29/43], Training Loss: 0.00005924
2024-11-06 14:06:57,933 - INFO - Epoch [47/300], Batch [30/43], Training Loss: 0.00001545
2024-11-06 14:06:57,938 - INFO - Epoch [47/300], Batch [31/43], Training Loss: 0.00004123
2024-11-06 14:06:57,942 - INFO - Epoch [47/300], Batch [32/43], Training Loss: 0.00003240
2024-11-06 14:06:57,946 - INFO - Epoch [47/300], Batch [33/43], Training Loss: 0.00003409
2024-11-06 14:06:57,950 - INFO - Epoch [47/300], Batch [34/43], Training Loss: 0.00000961
2024-11-06 14:06:57,954 - INFO - Epoch [47/300], Batch [35/43], Training Loss: 0.00002400
2024-11-06 14:06:57,958 - INFO - Epoch [47/300], Batch [36/43], Training Loss: 0.00005787
2024-11-06 14:06:57,962 - INFO - Epoch [47/300], Batch [37/43], Training Loss: 0.00001836
2024-11-06 14:06:57,966 - INFO - Epoch [47/300], Batch [38/43], Training Loss: 0.00002810
2024-11-06 14:06:57,970 - INFO - Epoch [47/300], Batch [39/43], Training Loss: 0.00001974
2024-11-06 14:06:57,974 - INFO - Epoch [47/300], Batch [40/43], Training Loss: 0.00002056
2024-11-06 14:06:57,977 - INFO - Epoch [47/300], Batch [41/43], Training Loss: 0.00002790
2024-11-06 14:06:57,981 - INFO - Epoch [47/300], Batch [42/43], Training Loss: 0.00001138
2024-11-06 14:06:57,985 - INFO - Epoch [47/300], Batch [43/43], Training Loss: 0.00002244
2024-11-06 14:06:57,997 - INFO - Epoch [47/300], Average Training Loss: 0.00002905, Validation Loss: 0.00003006
2024-11-06 14:06:58,001 - INFO - Epoch [48/300], Batch [1/43], Training Loss: 0.00003570
2024-11-06 14:06:58,006 - INFO - Epoch [48/300], Batch [2/43], Training Loss: 0.00003331
2024-11-06 14:06:58,012 - INFO - Epoch [48/300], Batch [3/43], Training Loss: 0.00001380
2024-11-06 14:06:58,016 - INFO - Epoch [48/300], Batch [4/43], Training Loss: 0.00001906
2024-11-06 14:06:58,020 - INFO - Epoch [48/300], Batch [5/43], Training Loss: 0.00002772
2024-11-06 14:06:58,026 - INFO - Epoch [48/300], Batch [6/43], Training Loss: 0.00001955
2024-11-06 14:06:58,031 - INFO - Epoch [48/300], Batch [7/43], Training Loss: 0.00002189
2024-11-06 14:06:58,036 - INFO - Epoch [48/300], Batch [8/43], Training Loss: 0.00002031
2024-11-06 14:06:58,040 - INFO - Epoch [48/300], Batch [9/43], Training Loss: 0.00001192
2024-11-06 14:06:58,044 - INFO - Epoch [48/300], Batch [10/43], Training Loss: 0.00001866
2024-11-06 14:06:58,048 - INFO - Epoch [48/300], Batch [11/43], Training Loss: 0.00003331
2024-11-06 14:06:58,052 - INFO - Epoch [48/300], Batch [12/43], Training Loss: 0.00001995
2024-11-06 14:06:58,055 - INFO - Epoch [48/300], Batch [13/43], Training Loss: 0.00002785
2024-11-06 14:06:58,058 - INFO - Epoch [48/300], Batch [14/43], Training Loss: 0.00000975
2024-11-06 14:06:58,062 - INFO - Epoch [48/300], Batch [15/43], Training Loss: 0.00006607
2024-11-06 14:06:58,069 - INFO - Epoch [48/300], Batch [16/43], Training Loss: 0.00004046
2024-11-06 14:06:58,073 - INFO - Epoch [48/300], Batch [17/43], Training Loss: 0.00003439
2024-11-06 14:06:58,076 - INFO - Epoch [48/300], Batch [18/43], Training Loss: 0.00003645
2024-11-06 14:06:58,081 - INFO - Epoch [48/300], Batch [19/43], Training Loss: 0.00004413
2024-11-06 14:06:58,085 - INFO - Epoch [48/300], Batch [20/43], Training Loss: 0.00002047
2024-11-06 14:06:58,089 - INFO - Epoch [48/300], Batch [21/43], Training Loss: 0.00002051
2024-11-06 14:06:58,092 - INFO - Epoch [48/300], Batch [22/43], Training Loss: 0.00007020
2024-11-06 14:06:58,096 - INFO - Epoch [48/300], Batch [23/43], Training Loss: 0.00002879
2024-11-06 14:06:58,100 - INFO - Epoch [48/300], Batch [24/43], Training Loss: 0.00001029
2024-11-06 14:06:58,103 - INFO - Epoch [48/300], Batch [25/43], Training Loss: 0.00004665
2024-11-06 14:06:58,107 - INFO - Epoch [48/300], Batch [26/43], Training Loss: 0.00004082
2024-11-06 14:06:58,110 - INFO - Epoch [48/300], Batch [27/43], Training Loss: 0.00003121
2024-11-06 14:06:58,114 - INFO - Epoch [48/300], Batch [28/43], Training Loss: 0.00002861
2024-11-06 14:06:58,119 - INFO - Epoch [48/300], Batch [29/43], Training Loss: 0.00004940
2024-11-06 14:06:58,122 - INFO - Epoch [48/300], Batch [30/43], Training Loss: 0.00007912
2024-11-06 14:06:58,126 - INFO - Epoch [48/300], Batch [31/43], Training Loss: 0.00001990
2024-11-06 14:06:58,129 - INFO - Epoch [48/300], Batch [32/43], Training Loss: 0.00002349
2024-11-06 14:06:58,133 - INFO - Epoch [48/300], Batch [33/43], Training Loss: 0.00004872
2024-11-06 14:06:58,137 - INFO - Epoch [48/300], Batch [34/43], Training Loss: 0.00003785
2024-11-06 14:06:58,140 - INFO - Epoch [48/300], Batch [35/43], Training Loss: 0.00002039
2024-11-06 14:06:58,145 - INFO - Epoch [48/300], Batch [36/43], Training Loss: 0.00006341
2024-11-06 14:06:58,149 - INFO - Epoch [48/300], Batch [37/43], Training Loss: 0.00005916
2024-11-06 14:06:58,152 - INFO - Epoch [48/300], Batch [38/43], Training Loss: 0.00002791
2024-11-06 14:06:58,156 - INFO - Epoch [48/300], Batch [39/43], Training Loss: 0.00003420
2024-11-06 14:06:58,160 - INFO - Epoch [48/300], Batch [40/43], Training Loss: 0.00006039
2024-11-06 14:06:58,164 - INFO - Epoch [48/300], Batch [41/43], Training Loss: 0.00004669
2024-11-06 14:06:58,168 - INFO - Epoch [48/300], Batch [42/43], Training Loss: 0.00002560
2024-11-06 14:06:58,172 - INFO - Epoch [48/300], Batch [43/43], Training Loss: 0.00002036
2024-11-06 14:06:58,183 - INFO - Epoch [48/300], Average Training Loss: 0.00003368, Validation Loss: 0.00008077
2024-11-06 14:06:58,187 - INFO - Epoch [49/300], Batch [1/43], Training Loss: 0.00003691
2024-11-06 14:06:58,190 - INFO - Epoch [49/300], Batch [2/43], Training Loss: 0.00002851
2024-11-06 14:06:58,194 - INFO - Epoch [49/300], Batch [3/43], Training Loss: 0.00003719
2024-11-06 14:06:58,198 - INFO - Epoch [49/300], Batch [4/43], Training Loss: 0.00003318
2024-11-06 14:06:58,201 - INFO - Epoch [49/300], Batch [5/43], Training Loss: 0.00004535
2024-11-06 14:06:58,205 - INFO - Epoch [49/300], Batch [6/43], Training Loss: 0.00007975
2024-11-06 14:06:58,210 - INFO - Epoch [49/300], Batch [7/43], Training Loss: 0.00005432
2024-11-06 14:06:58,214 - INFO - Epoch [49/300], Batch [8/43], Training Loss: 0.00002526
2024-11-06 14:06:58,217 - INFO - Epoch [49/300], Batch [9/43], Training Loss: 0.00005389
2024-11-06 14:06:58,221 - INFO - Epoch [49/300], Batch [10/43], Training Loss: 0.00001745
2024-11-06 14:06:58,224 - INFO - Epoch [49/300], Batch [11/43], Training Loss: 0.00003790
2024-11-06 14:06:58,227 - INFO - Epoch [49/300], Batch [12/43], Training Loss: 0.00003566
2024-11-06 14:06:58,230 - INFO - Epoch [49/300], Batch [13/43], Training Loss: 0.00005003
2024-11-06 14:06:58,234 - INFO - Epoch [49/300], Batch [14/43], Training Loss: 0.00002146
2024-11-06 14:06:58,238 - INFO - Epoch [49/300], Batch [15/43], Training Loss: 0.00002392
2024-11-06 14:06:58,242 - INFO - Epoch [49/300], Batch [16/43], Training Loss: 0.00003298
2024-11-06 14:06:58,245 - INFO - Epoch [49/300], Batch [17/43], Training Loss: 0.00001637
2024-11-06 14:06:58,249 - INFO - Epoch [49/300], Batch [18/43], Training Loss: 0.00003799
2024-11-06 14:06:58,252 - INFO - Epoch [49/300], Batch [19/43], Training Loss: 0.00002144
2024-11-06 14:06:58,255 - INFO - Epoch [49/300], Batch [20/43], Training Loss: 0.00002482
2024-11-06 14:06:58,259 - INFO - Epoch [49/300], Batch [21/43], Training Loss: 0.00002665
2024-11-06 14:06:58,262 - INFO - Epoch [49/300], Batch [22/43], Training Loss: 0.00003055
2024-11-06 14:06:58,267 - INFO - Epoch [49/300], Batch [23/43], Training Loss: 0.00001008
2024-11-06 14:06:58,270 - INFO - Epoch [49/300], Batch [24/43], Training Loss: 0.00001598
2024-11-06 14:06:58,275 - INFO - Epoch [49/300], Batch [25/43], Training Loss: 0.00002668
2024-11-06 14:06:58,278 - INFO - Epoch [49/300], Batch [26/43], Training Loss: 0.00002243
2024-11-06 14:06:58,281 - INFO - Epoch [49/300], Batch [27/43], Training Loss: 0.00000743
2024-11-06 14:06:58,285 - INFO - Epoch [49/300], Batch [28/43], Training Loss: 0.00002262
2024-11-06 14:06:58,288 - INFO - Epoch [49/300], Batch [29/43], Training Loss: 0.00002459
2024-11-06 14:06:58,292 - INFO - Epoch [49/300], Batch [30/43], Training Loss: 0.00001721
2024-11-06 14:06:58,296 - INFO - Epoch [49/300], Batch [31/43], Training Loss: 0.00002044
2024-11-06 14:06:58,301 - INFO - Epoch [49/300], Batch [32/43], Training Loss: 0.00003322
2024-11-06 14:06:58,305 - INFO - Epoch [49/300], Batch [33/43], Training Loss: 0.00003419
2024-11-06 14:06:58,309 - INFO - Epoch [49/300], Batch [34/43], Training Loss: 0.00001250
2024-11-06 14:06:58,312 - INFO - Epoch [49/300], Batch [35/43], Training Loss: 0.00003187
2024-11-06 14:06:58,316 - INFO - Epoch [49/300], Batch [36/43], Training Loss: 0.00001581
2024-11-06 14:06:58,319 - INFO - Epoch [49/300], Batch [37/43], Training Loss: 0.00002036
2024-11-06 14:06:58,322 - INFO - Epoch [49/300], Batch [38/43], Training Loss: 0.00001200
2024-11-06 14:06:58,326 - INFO - Epoch [49/300], Batch [39/43], Training Loss: 0.00002045
2024-11-06 14:06:58,329 - INFO - Epoch [49/300], Batch [40/43], Training Loss: 0.00003368
2024-11-06 14:06:58,332 - INFO - Epoch [49/300], Batch [41/43], Training Loss: 0.00001234
2024-11-06 14:06:58,335 - INFO - Epoch [49/300], Batch [42/43], Training Loss: 0.00002336
2024-11-06 14:06:58,338 - INFO - Epoch [49/300], Batch [43/43], Training Loss: 0.00002833
2024-11-06 14:06:58,347 - INFO - Epoch [49/300], Average Training Loss: 0.00002831, Validation Loss: 0.00002885
2024-11-06 14:06:58,350 - INFO - Epoch [50/300], Batch [1/43], Training Loss: 0.00002722
2024-11-06 14:06:58,354 - INFO - Epoch [50/300], Batch [2/43], Training Loss: 0.00002693
2024-11-06 14:06:58,357 - INFO - Epoch [50/300], Batch [3/43], Training Loss: 0.00002415
2024-11-06 14:06:58,360 - INFO - Epoch [50/300], Batch [4/43], Training Loss: 0.00001854
2024-11-06 14:06:58,365 - INFO - Epoch [50/300], Batch [5/43], Training Loss: 0.00004794
2024-11-06 14:06:58,369 - INFO - Epoch [50/300], Batch [6/43], Training Loss: 0.00003813
2024-11-06 14:06:58,373 - INFO - Epoch [50/300], Batch [7/43], Training Loss: 0.00001756
2024-11-06 14:06:58,377 - INFO - Epoch [50/300], Batch [8/43], Training Loss: 0.00001911
2024-11-06 14:06:58,381 - INFO - Epoch [50/300], Batch [9/43], Training Loss: 0.00004128
2024-11-06 14:06:58,385 - INFO - Epoch [50/300], Batch [10/43], Training Loss: 0.00001383
2024-11-06 14:06:58,389 - INFO - Epoch [50/300], Batch [11/43], Training Loss: 0.00001665
2024-11-06 14:06:58,393 - INFO - Epoch [50/300], Batch [12/43], Training Loss: 0.00006672
2024-11-06 14:06:58,396 - INFO - Epoch [50/300], Batch [13/43], Training Loss: 0.00001405
2024-11-06 14:06:58,401 - INFO - Epoch [50/300], Batch [14/43], Training Loss: 0.00001998
2024-11-06 14:06:58,404 - INFO - Epoch [50/300], Batch [15/43], Training Loss: 0.00002989
2024-11-06 14:06:58,408 - INFO - Epoch [50/300], Batch [16/43], Training Loss: 0.00002298
2024-11-06 14:06:58,411 - INFO - Epoch [50/300], Batch [17/43], Training Loss: 0.00002414
2024-11-06 14:06:58,415 - INFO - Epoch [50/300], Batch [18/43], Training Loss: 0.00000945
2024-11-06 14:06:58,418 - INFO - Epoch [50/300], Batch [19/43], Training Loss: 0.00003565
2024-11-06 14:06:58,422 - INFO - Epoch [50/300], Batch [20/43], Training Loss: 0.00002358
2024-11-06 14:06:58,426 - INFO - Epoch [50/300], Batch [21/43], Training Loss: 0.00002290
2024-11-06 14:06:58,429 - INFO - Epoch [50/300], Batch [22/43], Training Loss: 0.00003304
2024-11-06 14:06:58,431 - INFO - Epoch [50/300], Batch [23/43], Training Loss: 0.00003622
2024-11-06 14:06:58,434 - INFO - Epoch [50/300], Batch [24/43], Training Loss: 0.00003816
2024-11-06 14:06:58,438 - INFO - Epoch [50/300], Batch [25/43], Training Loss: 0.00002315
2024-11-06 14:06:58,441 - INFO - Epoch [50/300], Batch [26/43], Training Loss: 0.00004409
2024-11-06 14:06:58,444 - INFO - Epoch [50/300], Batch [27/43], Training Loss: 0.00001932
2024-11-06 14:06:58,447 - INFO - Epoch [50/300], Batch [28/43], Training Loss: 0.00005262
2024-11-06 14:06:58,450 - INFO - Epoch [50/300], Batch [29/43], Training Loss: 0.00005244
2024-11-06 14:06:58,454 - INFO - Epoch [50/300], Batch [30/43], Training Loss: 0.00002090
2024-11-06 14:06:58,457 - INFO - Epoch [50/300], Batch [31/43], Training Loss: 0.00006510
2024-11-06 14:06:58,460 - INFO - Epoch [50/300], Batch [32/43], Training Loss: 0.00003248
2024-11-06 14:06:58,464 - INFO - Epoch [50/300], Batch [33/43], Training Loss: 0.00002103
2024-11-06 14:06:58,467 - INFO - Epoch [50/300], Batch [34/43], Training Loss: 0.00001147
2024-11-06 14:06:58,470 - INFO - Epoch [50/300], Batch [35/43], Training Loss: 0.00003111
2024-11-06 14:06:58,473 - INFO - Epoch [50/300], Batch [36/43], Training Loss: 0.00002452
2024-11-06 14:06:58,476 - INFO - Epoch [50/300], Batch [37/43], Training Loss: 0.00001073
2024-11-06 14:06:58,479 - INFO - Epoch [50/300], Batch [38/43], Training Loss: 0.00002635
2024-11-06 14:06:58,482 - INFO - Epoch [50/300], Batch [39/43], Training Loss: 0.00001635
2024-11-06 14:06:58,485 - INFO - Epoch [50/300], Batch [40/43], Training Loss: 0.00003044
2024-11-06 14:06:58,489 - INFO - Epoch [50/300], Batch [41/43], Training Loss: 0.00004627
2024-11-06 14:06:58,492 - INFO - Epoch [50/300], Batch [42/43], Training Loss: 0.00003909
2024-11-06 14:06:58,495 - INFO - Epoch [50/300], Batch [43/43], Training Loss: 0.00003329
2024-11-06 14:06:58,507 - INFO - Epoch [50/300], Average Training Loss: 0.00002951, Validation Loss: 0.00005699
2024-11-06 14:06:58,510 - INFO - Epoch [51/300], Batch [1/43], Training Loss: 0.00003422
2024-11-06 14:06:58,514 - INFO - Epoch [51/300], Batch [2/43], Training Loss: 0.00003145
2024-11-06 14:06:58,517 - INFO - Epoch [51/300], Batch [3/43], Training Loss: 0.00002499
2024-11-06 14:06:58,521 - INFO - Epoch [51/300], Batch [4/43], Training Loss: 0.00001877
2024-11-06 14:06:58,525 - INFO - Epoch [51/300], Batch [5/43], Training Loss: 0.00003853
2024-11-06 14:06:58,528 - INFO - Epoch [51/300], Batch [6/43], Training Loss: 0.00004434
2024-11-06 14:06:58,532 - INFO - Epoch [51/300], Batch [7/43], Training Loss: 0.00002996
2024-11-06 14:06:58,535 - INFO - Epoch [51/300], Batch [8/43], Training Loss: 0.00001219
2024-11-06 14:06:58,538 - INFO - Epoch [51/300], Batch [9/43], Training Loss: 0.00005153
2024-11-06 14:06:58,541 - INFO - Epoch [51/300], Batch [10/43], Training Loss: 0.00002870
2024-11-06 14:06:58,544 - INFO - Epoch [51/300], Batch [11/43], Training Loss: 0.00003129
2024-11-06 14:06:58,547 - INFO - Epoch [51/300], Batch [12/43], Training Loss: 0.00003752
2024-11-06 14:06:58,550 - INFO - Epoch [51/300], Batch [13/43], Training Loss: 0.00001383
2024-11-06 14:06:58,554 - INFO - Epoch [51/300], Batch [14/43], Training Loss: 0.00001447
2024-11-06 14:06:58,556 - INFO - Epoch [51/300], Batch [15/43], Training Loss: 0.00002812
2024-11-06 14:06:58,559 - INFO - Epoch [51/300], Batch [16/43], Training Loss: 0.00002714
2024-11-06 14:06:58,562 - INFO - Epoch [51/300], Batch [17/43], Training Loss: 0.00001922
2024-11-06 14:06:58,565 - INFO - Epoch [51/300], Batch [18/43], Training Loss: 0.00002018
2024-11-06 14:06:58,568 - INFO - Epoch [51/300], Batch [19/43], Training Loss: 0.00002362
2024-11-06 14:06:58,572 - INFO - Epoch [51/300], Batch [20/43], Training Loss: 0.00004279
2024-11-06 14:06:58,576 - INFO - Epoch [51/300], Batch [21/43], Training Loss: 0.00001971
2024-11-06 14:06:58,579 - INFO - Epoch [51/300], Batch [22/43], Training Loss: 0.00001926
2024-11-06 14:06:58,582 - INFO - Epoch [51/300], Batch [23/43], Training Loss: 0.00002105
2024-11-06 14:06:58,587 - INFO - Epoch [51/300], Batch [24/43], Training Loss: 0.00001759
2024-11-06 14:06:58,592 - INFO - Epoch [51/300], Batch [25/43], Training Loss: 0.00001301
2024-11-06 14:06:58,595 - INFO - Epoch [51/300], Batch [26/43], Training Loss: 0.00003626
2024-11-06 14:06:58,598 - INFO - Epoch [51/300], Batch [27/43], Training Loss: 0.00001196
2024-11-06 14:06:58,601 - INFO - Epoch [51/300], Batch [28/43], Training Loss: 0.00003213
2024-11-06 14:06:58,605 - INFO - Epoch [51/300], Batch [29/43], Training Loss: 0.00002184
2024-11-06 14:06:58,608 - INFO - Epoch [51/300], Batch [30/43], Training Loss: 0.00002385
2024-11-06 14:06:58,611 - INFO - Epoch [51/300], Batch [31/43], Training Loss: 0.00002181
2024-11-06 14:06:58,614 - INFO - Epoch [51/300], Batch [32/43], Training Loss: 0.00001722
2024-11-06 14:06:58,617 - INFO - Epoch [51/300], Batch [33/43], Training Loss: 0.00005087
2024-11-06 14:06:58,620 - INFO - Epoch [51/300], Batch [34/43], Training Loss: 0.00002417
2024-11-06 14:06:58,623 - INFO - Epoch [51/300], Batch [35/43], Training Loss: 0.00002186
2024-11-06 14:06:58,627 - INFO - Epoch [51/300], Batch [36/43], Training Loss: 0.00002296
2024-11-06 14:06:58,631 - INFO - Epoch [51/300], Batch [37/43], Training Loss: 0.00002288
2024-11-06 14:06:58,636 - INFO - Epoch [51/300], Batch [38/43], Training Loss: 0.00003960
2024-11-06 14:06:58,640 - INFO - Epoch [51/300], Batch [39/43], Training Loss: 0.00001934
2024-11-06 14:06:58,644 - INFO - Epoch [51/300], Batch [40/43], Training Loss: 0.00001238
2024-11-06 14:06:58,648 - INFO - Epoch [51/300], Batch [41/43], Training Loss: 0.00003482
2024-11-06 14:06:58,652 - INFO - Epoch [51/300], Batch [42/43], Training Loss: 0.00003942
2024-11-06 14:06:58,656 - INFO - Epoch [51/300], Batch [43/43], Training Loss: 0.00001162
2024-11-06 14:06:58,668 - INFO - Epoch [51/300], Average Training Loss: 0.00002624, Validation Loss: 0.00003135
2024-11-06 14:06:58,672 - INFO - Epoch [52/300], Batch [1/43], Training Loss: 0.00002175
2024-11-06 14:06:58,675 - INFO - Epoch [52/300], Batch [2/43], Training Loss: 0.00001754
2024-11-06 14:06:58,679 - INFO - Epoch [52/300], Batch [3/43], Training Loss: 0.00002548
2024-11-06 14:06:58,683 - INFO - Epoch [52/300], Batch [4/43], Training Loss: 0.00003087
2024-11-06 14:06:58,687 - INFO - Epoch [52/300], Batch [5/43], Training Loss: 0.00002615
2024-11-06 14:06:58,690 - INFO - Epoch [52/300], Batch [6/43], Training Loss: 0.00002633
2024-11-06 14:06:58,694 - INFO - Epoch [52/300], Batch [7/43], Training Loss: 0.00004047
2024-11-06 14:06:58,698 - INFO - Epoch [52/300], Batch [8/43], Training Loss: 0.00002386
2024-11-06 14:06:58,702 - INFO - Epoch [52/300], Batch [9/43], Training Loss: 0.00002381
2024-11-06 14:06:58,705 - INFO - Epoch [52/300], Batch [10/43], Training Loss: 0.00003887
2024-11-06 14:06:58,708 - INFO - Epoch [52/300], Batch [11/43], Training Loss: 0.00004607
2024-11-06 14:06:58,711 - INFO - Epoch [52/300], Batch [12/43], Training Loss: 0.00001566
2024-11-06 14:06:58,714 - INFO - Epoch [52/300], Batch [13/43], Training Loss: 0.00002738
2024-11-06 14:06:58,717 - INFO - Epoch [52/300], Batch [14/43], Training Loss: 0.00001827
2024-11-06 14:06:58,720 - INFO - Epoch [52/300], Batch [15/43], Training Loss: 0.00002277
2024-11-06 14:06:58,723 - INFO - Epoch [52/300], Batch [16/43], Training Loss: 0.00003187
2024-11-06 14:06:58,726 - INFO - Epoch [52/300], Batch [17/43], Training Loss: 0.00002622
2024-11-06 14:06:58,730 - INFO - Epoch [52/300], Batch [18/43], Training Loss: 0.00003757
2024-11-06 14:06:58,733 - INFO - Epoch [52/300], Batch [19/43], Training Loss: 0.00002351
2024-11-06 14:06:58,736 - INFO - Epoch [52/300], Batch [20/43], Training Loss: 0.00001773
2024-11-06 14:06:58,739 - INFO - Epoch [52/300], Batch [21/43], Training Loss: 0.00002722
2024-11-06 14:06:58,742 - INFO - Epoch [52/300], Batch [22/43], Training Loss: 0.00001737
2024-11-06 14:06:58,745 - INFO - Epoch [52/300], Batch [23/43], Training Loss: 0.00002395
2024-11-06 14:06:58,748 - INFO - Epoch [52/300], Batch [24/43], Training Loss: 0.00002655
2024-11-06 14:06:58,752 - INFO - Epoch [52/300], Batch [25/43], Training Loss: 0.00001617
2024-11-06 14:06:58,755 - INFO - Epoch [52/300], Batch [26/43], Training Loss: 0.00003643
2024-11-06 14:06:58,759 - INFO - Epoch [52/300], Batch [27/43], Training Loss: 0.00002341
2024-11-06 14:06:58,762 - INFO - Epoch [52/300], Batch [28/43], Training Loss: 0.00000944
2024-11-06 14:06:58,765 - INFO - Epoch [52/300], Batch [29/43], Training Loss: 0.00002285
2024-11-06 14:06:58,768 - INFO - Epoch [52/300], Batch [30/43], Training Loss: 0.00003325
2024-11-06 14:06:58,772 - INFO - Epoch [52/300], Batch [31/43], Training Loss: 0.00003901
2024-11-06 14:06:58,775 - INFO - Epoch [52/300], Batch [32/43], Training Loss: 0.00001778
2024-11-06 14:06:58,778 - INFO - Epoch [52/300], Batch [33/43], Training Loss: 0.00002752
2024-11-06 14:06:58,783 - INFO - Epoch [52/300], Batch [34/43], Training Loss: 0.00003709
2024-11-06 14:06:58,789 - INFO - Epoch [52/300], Batch [35/43], Training Loss: 0.00003468
2024-11-06 14:06:58,794 - INFO - Epoch [52/300], Batch [36/43], Training Loss: 0.00002056
2024-11-06 14:06:58,798 - INFO - Epoch [52/300], Batch [37/43], Training Loss: 0.00002515
2024-11-06 14:06:58,801 - INFO - Epoch [52/300], Batch [38/43], Training Loss: 0.00002727
2024-11-06 14:06:58,804 - INFO - Epoch [52/300], Batch [39/43], Training Loss: 0.00002697
2024-11-06 14:06:58,808 - INFO - Epoch [52/300], Batch [40/43], Training Loss: 0.00002447
2024-11-06 14:06:58,812 - INFO - Epoch [52/300], Batch [41/43], Training Loss: 0.00000923
2024-11-06 14:06:58,815 - INFO - Epoch [52/300], Batch [42/43], Training Loss: 0.00002483
2024-11-06 14:06:58,819 - INFO - Epoch [52/300], Batch [43/43], Training Loss: 0.00001304
2024-11-06 14:06:58,832 - INFO - Epoch [52/300], Average Training Loss: 0.00002573, Validation Loss: 0.00002938
2024-11-06 14:06:58,837 - INFO - Epoch [53/300], Batch [1/43], Training Loss: 0.00001430
2024-11-06 14:06:58,841 - INFO - Epoch [53/300], Batch [2/43], Training Loss: 0.00001007
2024-11-06 14:06:58,845 - INFO - Epoch [53/300], Batch [3/43], Training Loss: 0.00001824
2024-11-06 14:06:58,848 - INFO - Epoch [53/300], Batch [4/43], Training Loss: 0.00001739
2024-11-06 14:06:58,852 - INFO - Epoch [53/300], Batch [5/43], Training Loss: 0.00003228
2024-11-06 14:06:58,855 - INFO - Epoch [53/300], Batch [6/43], Training Loss: 0.00002087
2024-11-06 14:06:58,858 - INFO - Epoch [53/300], Batch [7/43], Training Loss: 0.00002003
2024-11-06 14:06:58,862 - INFO - Epoch [53/300], Batch [8/43], Training Loss: 0.00000855
2024-11-06 14:06:58,866 - INFO - Epoch [53/300], Batch [9/43], Training Loss: 0.00001946
2024-11-06 14:06:58,870 - INFO - Epoch [53/300], Batch [10/43], Training Loss: 0.00001849
2024-11-06 14:06:58,874 - INFO - Epoch [53/300], Batch [11/43], Training Loss: 0.00001748
2024-11-06 14:06:58,878 - INFO - Epoch [53/300], Batch [12/43], Training Loss: 0.00001356
2024-11-06 14:06:58,881 - INFO - Epoch [53/300], Batch [13/43], Training Loss: 0.00003062
2024-11-06 14:06:58,885 - INFO - Epoch [53/300], Batch [14/43], Training Loss: 0.00001431
2024-11-06 14:06:58,888 - INFO - Epoch [53/300], Batch [15/43], Training Loss: 0.00003043
2024-11-06 14:06:58,892 - INFO - Epoch [53/300], Batch [16/43], Training Loss: 0.00003082
2024-11-06 14:06:58,896 - INFO - Epoch [53/300], Batch [17/43], Training Loss: 0.00003415
2024-11-06 14:06:58,900 - INFO - Epoch [53/300], Batch [18/43], Training Loss: 0.00003015
2024-11-06 14:06:58,904 - INFO - Epoch [53/300], Batch [19/43], Training Loss: 0.00002741
2024-11-06 14:06:58,908 - INFO - Epoch [53/300], Batch [20/43], Training Loss: 0.00001386
2024-11-06 14:06:58,912 - INFO - Epoch [53/300], Batch [21/43], Training Loss: 0.00001962
2024-11-06 14:06:58,915 - INFO - Epoch [53/300], Batch [22/43], Training Loss: 0.00002529
2024-11-06 14:06:58,919 - INFO - Epoch [53/300], Batch [23/43], Training Loss: 0.00003431
2024-11-06 14:06:58,922 - INFO - Epoch [53/300], Batch [24/43], Training Loss: 0.00005618
2024-11-06 14:06:58,925 - INFO - Epoch [53/300], Batch [25/43], Training Loss: 0.00001145
2024-11-06 14:06:58,928 - INFO - Epoch [53/300], Batch [26/43], Training Loss: 0.00003363
2024-11-06 14:06:58,931 - INFO - Epoch [53/300], Batch [27/43], Training Loss: 0.00005128
2024-11-06 14:06:58,935 - INFO - Epoch [53/300], Batch [28/43], Training Loss: 0.00002184
2024-11-06 14:06:58,938 - INFO - Epoch [53/300], Batch [29/43], Training Loss: 0.00001859
2024-11-06 14:06:58,940 - INFO - Epoch [53/300], Batch [30/43], Training Loss: 0.00001100
2024-11-06 14:06:58,944 - INFO - Epoch [53/300], Batch [31/43], Training Loss: 0.00002891
2024-11-06 14:06:58,947 - INFO - Epoch [53/300], Batch [32/43], Training Loss: 0.00002320
2024-11-06 14:06:58,950 - INFO - Epoch [53/300], Batch [33/43], Training Loss: 0.00002409
2024-11-06 14:06:58,953 - INFO - Epoch [53/300], Batch [34/43], Training Loss: 0.00000784
2024-11-06 14:06:58,956 - INFO - Epoch [53/300], Batch [35/43], Training Loss: 0.00002552
2024-11-06 14:06:58,958 - INFO - Epoch [53/300], Batch [36/43], Training Loss: 0.00002493
2024-11-06 14:06:58,962 - INFO - Epoch [53/300], Batch [37/43], Training Loss: 0.00005699
2024-11-06 14:06:58,966 - INFO - Epoch [53/300], Batch [38/43], Training Loss: 0.00003430
2024-11-06 14:06:58,970 - INFO - Epoch [53/300], Batch [39/43], Training Loss: 0.00003064
2024-11-06 14:06:58,974 - INFO - Epoch [53/300], Batch [40/43], Training Loss: 0.00001058
2024-11-06 14:06:58,977 - INFO - Epoch [53/300], Batch [41/43], Training Loss: 0.00002540
2024-11-06 14:06:58,981 - INFO - Epoch [53/300], Batch [42/43], Training Loss: 0.00002078
2024-11-06 14:06:58,985 - INFO - Epoch [53/300], Batch [43/43], Training Loss: 0.00002504
2024-11-06 14:06:58,995 - INFO - Epoch [53/300], Average Training Loss: 0.00002428, Validation Loss: 0.00003246
2024-11-06 14:06:58,999 - INFO - Epoch [54/300], Batch [1/43], Training Loss: 0.00001273
2024-11-06 14:06:59,002 - INFO - Epoch [54/300], Batch [2/43], Training Loss: 0.00001766
2024-11-06 14:06:59,005 - INFO - Epoch [54/300], Batch [3/43], Training Loss: 0.00000721
2024-11-06 14:06:59,008 - INFO - Epoch [54/300], Batch [4/43], Training Loss: 0.00003570
2024-11-06 14:06:59,012 - INFO - Epoch [54/300], Batch [5/43], Training Loss: 0.00002373
2024-11-06 14:06:59,015 - INFO - Epoch [54/300], Batch [6/43], Training Loss: 0.00002375
2024-11-06 14:06:59,019 - INFO - Epoch [54/300], Batch [7/43], Training Loss: 0.00002318
2024-11-06 14:06:59,023 - INFO - Epoch [54/300], Batch [8/43], Training Loss: 0.00001697
2024-11-06 14:06:59,026 - INFO - Epoch [54/300], Batch [9/43], Training Loss: 0.00002298
2024-11-06 14:06:59,029 - INFO - Epoch [54/300], Batch [10/43], Training Loss: 0.00002232
2024-11-06 14:06:59,032 - INFO - Epoch [54/300], Batch [11/43], Training Loss: 0.00003623
2024-11-06 14:06:59,035 - INFO - Epoch [54/300], Batch [12/43], Training Loss: 0.00001249
2024-11-06 14:06:59,038 - INFO - Epoch [54/300], Batch [13/43], Training Loss: 0.00003518
2024-11-06 14:06:59,041 - INFO - Epoch [54/300], Batch [14/43], Training Loss: 0.00002658
2024-11-06 14:06:59,045 - INFO - Epoch [54/300], Batch [15/43], Training Loss: 0.00002623
2024-11-06 14:06:59,048 - INFO - Epoch [54/300], Batch [16/43], Training Loss: 0.00002011
2024-11-06 14:06:59,052 - INFO - Epoch [54/300], Batch [17/43], Training Loss: 0.00002619
2024-11-06 14:06:59,055 - INFO - Epoch [54/300], Batch [18/43], Training Loss: 0.00001920
2024-11-06 14:06:59,059 - INFO - Epoch [54/300], Batch [19/43], Training Loss: 0.00001786
2024-11-06 14:06:59,063 - INFO - Epoch [54/300], Batch [20/43], Training Loss: 0.00002963
2024-11-06 14:06:59,066 - INFO - Epoch [54/300], Batch [21/43], Training Loss: 0.00001685
2024-11-06 14:06:59,070 - INFO - Epoch [54/300], Batch [22/43], Training Loss: 0.00003074
2024-11-06 14:06:59,073 - INFO - Epoch [54/300], Batch [23/43], Training Loss: 0.00002285
2024-11-06 14:06:59,076 - INFO - Epoch [54/300], Batch [24/43], Training Loss: 0.00003243
2024-11-06 14:06:59,079 - INFO - Epoch [54/300], Batch [25/43], Training Loss: 0.00001278
2024-11-06 14:06:59,083 - INFO - Epoch [54/300], Batch [26/43], Training Loss: 0.00002908
2024-11-06 14:06:59,086 - INFO - Epoch [54/300], Batch [27/43], Training Loss: 0.00001376
2024-11-06 14:06:59,089 - INFO - Epoch [54/300], Batch [28/43], Training Loss: 0.00002288
2024-11-06 14:06:59,092 - INFO - Epoch [54/300], Batch [29/43], Training Loss: 0.00001734
2024-11-06 14:06:59,096 - INFO - Epoch [54/300], Batch [30/43], Training Loss: 0.00002093
2024-11-06 14:06:59,100 - INFO - Epoch [54/300], Batch [31/43], Training Loss: 0.00001395
2024-11-06 14:06:59,103 - INFO - Epoch [54/300], Batch [32/43], Training Loss: 0.00002818
2024-11-06 14:06:59,107 - INFO - Epoch [54/300], Batch [33/43], Training Loss: 0.00001847
2024-11-06 14:06:59,111 - INFO - Epoch [54/300], Batch [34/43], Training Loss: 0.00001443
2024-11-06 14:06:59,115 - INFO - Epoch [54/300], Batch [35/43], Training Loss: 0.00003856
2024-11-06 14:06:59,118 - INFO - Epoch [54/300], Batch [36/43], Training Loss: 0.00002116
2024-11-06 14:06:59,121 - INFO - Epoch [54/300], Batch [37/43], Training Loss: 0.00003519
2024-11-06 14:06:59,124 - INFO - Epoch [54/300], Batch [38/43], Training Loss: 0.00002553
2024-11-06 14:06:59,126 - INFO - Epoch [54/300], Batch [39/43], Training Loss: 0.00001785
2024-11-06 14:06:59,129 - INFO - Epoch [54/300], Batch [40/43], Training Loss: 0.00001764
2024-11-06 14:06:59,132 - INFO - Epoch [54/300], Batch [41/43], Training Loss: 0.00003534
2024-11-06 14:06:59,135 - INFO - Epoch [54/300], Batch [42/43], Training Loss: 0.00003135
2024-11-06 14:06:59,138 - INFO - Epoch [54/300], Batch [43/43], Training Loss: 0.00001900
2024-11-06 14:06:59,148 - INFO - Epoch [54/300], Average Training Loss: 0.00002308, Validation Loss: 0.00002790
2024-11-06 14:06:59,151 - INFO - Epoch [55/300], Batch [1/43], Training Loss: 0.00000890
2024-11-06 14:06:59,155 - INFO - Epoch [55/300], Batch [2/43], Training Loss: 0.00001248
2024-11-06 14:06:59,159 - INFO - Epoch [55/300], Batch [3/43], Training Loss: 0.00002239
2024-11-06 14:06:59,163 - INFO - Epoch [55/300], Batch [4/43], Training Loss: 0.00002744
2024-11-06 14:06:59,167 - INFO - Epoch [55/300], Batch [5/43], Training Loss: 0.00001375
2024-11-06 14:06:59,172 - INFO - Epoch [55/300], Batch [6/43], Training Loss: 0.00001722
2024-11-06 14:06:59,176 - INFO - Epoch [55/300], Batch [7/43], Training Loss: 0.00002717
2024-11-06 14:06:59,179 - INFO - Epoch [55/300], Batch [8/43], Training Loss: 0.00001611
2024-11-06 14:06:59,183 - INFO - Epoch [55/300], Batch [9/43], Training Loss: 0.00002097
2024-11-06 14:06:59,187 - INFO - Epoch [55/300], Batch [10/43], Training Loss: 0.00002151
2024-11-06 14:06:59,191 - INFO - Epoch [55/300], Batch [11/43], Training Loss: 0.00004037
2024-11-06 14:06:59,194 - INFO - Epoch [55/300], Batch [12/43], Training Loss: 0.00003413
2024-11-06 14:06:59,198 - INFO - Epoch [55/300], Batch [13/43], Training Loss: 0.00003036
2024-11-06 14:06:59,203 - INFO - Epoch [55/300], Batch [14/43], Training Loss: 0.00002368
2024-11-06 14:06:59,207 - INFO - Epoch [55/300], Batch [15/43], Training Loss: 0.00001084
2024-11-06 14:06:59,210 - INFO - Epoch [55/300], Batch [16/43], Training Loss: 0.00001666
2024-11-06 14:06:59,213 - INFO - Epoch [55/300], Batch [17/43], Training Loss: 0.00002830
2024-11-06 14:06:59,217 - INFO - Epoch [55/300], Batch [18/43], Training Loss: 0.00002080
2024-11-06 14:06:59,220 - INFO - Epoch [55/300], Batch [19/43], Training Loss: 0.00002957
2024-11-06 14:06:59,224 - INFO - Epoch [55/300], Batch [20/43], Training Loss: 0.00001834
2024-11-06 14:06:59,227 - INFO - Epoch [55/300], Batch [21/43], Training Loss: 0.00001404
2024-11-06 14:06:59,230 - INFO - Epoch [55/300], Batch [22/43], Training Loss: 0.00002887
2024-11-06 14:06:59,233 - INFO - Epoch [55/300], Batch [23/43], Training Loss: 0.00001456
2024-11-06 14:06:59,236 - INFO - Epoch [55/300], Batch [24/43], Training Loss: 0.00001701
2024-11-06 14:06:59,240 - INFO - Epoch [55/300], Batch [25/43], Training Loss: 0.00000934
2024-11-06 14:06:59,243 - INFO - Epoch [55/300], Batch [26/43], Training Loss: 0.00003196
2024-11-06 14:06:59,246 - INFO - Epoch [55/300], Batch [27/43], Training Loss: 0.00001837
2024-11-06 14:06:59,250 - INFO - Epoch [55/300], Batch [28/43], Training Loss: 0.00002790
2024-11-06 14:06:59,254 - INFO - Epoch [55/300], Batch [29/43], Training Loss: 0.00001221
2024-11-06 14:06:59,257 - INFO - Epoch [55/300], Batch [30/43], Training Loss: 0.00004299
2024-11-06 14:06:59,261 - INFO - Epoch [55/300], Batch [31/43], Training Loss: 0.00002260
2024-11-06 14:06:59,264 - INFO - Epoch [55/300], Batch [32/43], Training Loss: 0.00002202
2024-11-06 14:06:59,268 - INFO - Epoch [55/300], Batch [33/43], Training Loss: 0.00004198
2024-11-06 14:06:59,271 - INFO - Epoch [55/300], Batch [34/43], Training Loss: 0.00001834
2024-11-06 14:06:59,274 - INFO - Epoch [55/300], Batch [35/43], Training Loss: 0.00005375
2024-11-06 14:06:59,276 - INFO - Epoch [55/300], Batch [36/43], Training Loss: 0.00002828
2024-11-06 14:06:59,279 - INFO - Epoch [55/300], Batch [37/43], Training Loss: 0.00005294
2024-11-06 14:06:59,282 - INFO - Epoch [55/300], Batch [38/43], Training Loss: 0.00003851
2024-11-06 14:06:59,285 - INFO - Epoch [55/300], Batch [39/43], Training Loss: 0.00001964
2024-11-06 14:06:59,288 - INFO - Epoch [55/300], Batch [40/43], Training Loss: 0.00002137
2024-11-06 14:06:59,291 - INFO - Epoch [55/300], Batch [41/43], Training Loss: 0.00003242
2024-11-06 14:06:59,294 - INFO - Epoch [55/300], Batch [42/43], Training Loss: 0.00002128
2024-11-06 14:06:59,297 - INFO - Epoch [55/300], Batch [43/43], Training Loss: 0.00002801
2024-11-06 14:06:59,307 - INFO - Epoch [55/300], Average Training Loss: 0.00002464, Validation Loss: 0.00003171
2024-11-06 14:06:59,310 - INFO - Epoch [56/300], Batch [1/43], Training Loss: 0.00001017
2024-11-06 14:06:59,313 - INFO - Epoch [56/300], Batch [2/43], Training Loss: 0.00002357
2024-11-06 14:06:59,316 - INFO - Epoch [56/300], Batch [3/43], Training Loss: 0.00002277
2024-11-06 14:06:59,319 - INFO - Epoch [56/300], Batch [4/43], Training Loss: 0.00003374
2024-11-06 14:06:59,321 - INFO - Epoch [56/300], Batch [5/43], Training Loss: 0.00002513
2024-11-06 14:06:59,324 - INFO - Epoch [56/300], Batch [6/43], Training Loss: 0.00004120
2024-11-06 14:06:59,328 - INFO - Epoch [56/300], Batch [7/43], Training Loss: 0.00003340
2024-11-06 14:06:59,331 - INFO - Epoch [56/300], Batch [8/43], Training Loss: 0.00001999
2024-11-06 14:06:59,333 - INFO - Epoch [56/300], Batch [9/43], Training Loss: 0.00002791
2024-11-06 14:06:59,336 - INFO - Epoch [56/300], Batch [10/43], Training Loss: 0.00004326
2024-11-06 14:06:59,339 - INFO - Epoch [56/300], Batch [11/43], Training Loss: 0.00002096
2024-11-06 14:06:59,342 - INFO - Epoch [56/300], Batch [12/43], Training Loss: 0.00002094
2024-11-06 14:06:59,346 - INFO - Epoch [56/300], Batch [13/43], Training Loss: 0.00001150
2024-11-06 14:06:59,350 - INFO - Epoch [56/300], Batch [14/43], Training Loss: 0.00002752
2024-11-06 14:06:59,354 - INFO - Epoch [56/300], Batch [15/43], Training Loss: 0.00001318
2024-11-06 14:06:59,359 - INFO - Epoch [56/300], Batch [16/43], Training Loss: 0.00002524
2024-11-06 14:06:59,362 - INFO - Epoch [56/300], Batch [17/43], Training Loss: 0.00001592
2024-11-06 14:06:59,366 - INFO - Epoch [56/300], Batch [18/43], Training Loss: 0.00001711
2024-11-06 14:06:59,370 - INFO - Epoch [56/300], Batch [19/43], Training Loss: 0.00004517
2024-11-06 14:06:59,373 - INFO - Epoch [56/300], Batch [20/43], Training Loss: 0.00004397
2024-11-06 14:06:59,377 - INFO - Epoch [56/300], Batch [21/43], Training Loss: 0.00002448
2024-11-06 14:06:59,380 - INFO - Epoch [56/300], Batch [22/43], Training Loss: 0.00003778
2024-11-06 14:06:59,383 - INFO - Epoch [56/300], Batch [23/43], Training Loss: 0.00003613
2024-11-06 14:06:59,386 - INFO - Epoch [56/300], Batch [24/43], Training Loss: 0.00003874
2024-11-06 14:06:59,389 - INFO - Epoch [56/300], Batch [25/43], Training Loss: 0.00002389
2024-11-06 14:06:59,393 - INFO - Epoch [56/300], Batch [26/43], Training Loss: 0.00005822
2024-11-06 14:06:59,397 - INFO - Epoch [56/300], Batch [27/43], Training Loss: 0.00002099
2024-11-06 14:06:59,400 - INFO - Epoch [56/300], Batch [28/43], Training Loss: 0.00001089
2024-11-06 14:06:59,404 - INFO - Epoch [56/300], Batch [29/43], Training Loss: 0.00001434
2024-11-06 14:06:59,407 - INFO - Epoch [56/300], Batch [30/43], Training Loss: 0.00001287
2024-11-06 14:06:59,410 - INFO - Epoch [56/300], Batch [31/43], Training Loss: 0.00002517
2024-11-06 14:06:59,414 - INFO - Epoch [56/300], Batch [32/43], Training Loss: 0.00002546
2024-11-06 14:06:59,418 - INFO - Epoch [56/300], Batch [33/43], Training Loss: 0.00001093
2024-11-06 14:06:59,421 - INFO - Epoch [56/300], Batch [34/43], Training Loss: 0.00002919
2024-11-06 14:06:59,425 - INFO - Epoch [56/300], Batch [35/43], Training Loss: 0.00001794
2024-11-06 14:06:59,428 - INFO - Epoch [56/300], Batch [36/43], Training Loss: 0.00002276
2024-11-06 14:06:59,432 - INFO - Epoch [56/300], Batch [37/43], Training Loss: 0.00002318
2024-11-06 14:06:59,435 - INFO - Epoch [56/300], Batch [38/43], Training Loss: 0.00002064
2024-11-06 14:06:59,439 - INFO - Epoch [56/300], Batch [39/43], Training Loss: 0.00002159
2024-11-06 14:06:59,443 - INFO - Epoch [56/300], Batch [40/43], Training Loss: 0.00003633
2024-11-06 14:06:59,447 - INFO - Epoch [56/300], Batch [41/43], Training Loss: 0.00002690
2024-11-06 14:06:59,451 - INFO - Epoch [56/300], Batch [42/43], Training Loss: 0.00000696
2024-11-06 14:06:59,454 - INFO - Epoch [56/300], Batch [43/43], Training Loss: 0.00002643
2024-11-06 14:06:59,464 - INFO - Epoch [56/300], Average Training Loss: 0.00002545, Validation Loss: 0.00003027
2024-11-06 14:06:59,468 - INFO - Epoch [57/300], Batch [1/43], Training Loss: 0.00002118
2024-11-06 14:06:59,471 - INFO - Epoch [57/300], Batch [2/43], Training Loss: 0.00001038
2024-11-06 14:06:59,474 - INFO - Epoch [57/300], Batch [3/43], Training Loss: 0.00003446
2024-11-06 14:06:59,477 - INFO - Epoch [57/300], Batch [4/43], Training Loss: 0.00002447
2024-11-06 14:06:59,479 - INFO - Epoch [57/300], Batch [5/43], Training Loss: 0.00003374
2024-11-06 14:06:59,482 - INFO - Epoch [57/300], Batch [6/43], Training Loss: 0.00001852
2024-11-06 14:06:59,485 - INFO - Epoch [57/300], Batch [7/43], Training Loss: 0.00001526
2024-11-06 14:06:59,489 - INFO - Epoch [57/300], Batch [8/43], Training Loss: 0.00003129
2024-11-06 14:06:59,492 - INFO - Epoch [57/300], Batch [9/43], Training Loss: 0.00002002
2024-11-06 14:06:59,495 - INFO - Epoch [57/300], Batch [10/43], Training Loss: 0.00004452
2024-11-06 14:06:59,498 - INFO - Epoch [57/300], Batch [11/43], Training Loss: 0.00002540
2024-11-06 14:06:59,502 - INFO - Epoch [57/300], Batch [12/43], Training Loss: 0.00003483
2024-11-06 14:06:59,506 - INFO - Epoch [57/300], Batch [13/43], Training Loss: 0.00001661
2024-11-06 14:06:59,510 - INFO - Epoch [57/300], Batch [14/43], Training Loss: 0.00001638
2024-11-06 14:06:59,514 - INFO - Epoch [57/300], Batch [15/43], Training Loss: 0.00001739
2024-11-06 14:06:59,519 - INFO - Epoch [57/300], Batch [16/43], Training Loss: 0.00002474
2024-11-06 14:06:59,523 - INFO - Epoch [57/300], Batch [17/43], Training Loss: 0.00003028
2024-11-06 14:06:59,527 - INFO - Epoch [57/300], Batch [18/43], Training Loss: 0.00001735
2024-11-06 14:06:59,530 - INFO - Epoch [57/300], Batch [19/43], Training Loss: 0.00000771
2024-11-06 14:06:59,535 - INFO - Epoch [57/300], Batch [20/43], Training Loss: 0.00002250
2024-11-06 14:06:59,540 - INFO - Epoch [57/300], Batch [21/43], Training Loss: 0.00002314
2024-11-06 14:06:59,544 - INFO - Epoch [57/300], Batch [22/43], Training Loss: 0.00005139
2024-11-06 14:06:59,548 - INFO - Epoch [57/300], Batch [23/43], Training Loss: 0.00001779
2024-11-06 14:06:59,553 - INFO - Epoch [57/300], Batch [24/43], Training Loss: 0.00002637
2024-11-06 14:06:59,557 - INFO - Epoch [57/300], Batch [25/43], Training Loss: 0.00005578
2024-11-06 14:06:59,561 - INFO - Epoch [57/300], Batch [26/43], Training Loss: 0.00001970
2024-11-06 14:06:59,566 - INFO - Epoch [57/300], Batch [27/43], Training Loss: 0.00001938
2024-11-06 14:06:59,571 - INFO - Epoch [57/300], Batch [28/43], Training Loss: 0.00003054
2024-11-06 14:06:59,574 - INFO - Epoch [57/300], Batch [29/43], Training Loss: 0.00002976
2024-11-06 14:06:59,578 - INFO - Epoch [57/300], Batch [30/43], Training Loss: 0.00001074
2024-11-06 14:06:59,582 - INFO - Epoch [57/300], Batch [31/43], Training Loss: 0.00002248
2024-11-06 14:06:59,585 - INFO - Epoch [57/300], Batch [32/43], Training Loss: 0.00001919
2024-11-06 14:06:59,588 - INFO - Epoch [57/300], Batch [33/43], Training Loss: 0.00002938
2024-11-06 14:06:59,591 - INFO - Epoch [57/300], Batch [34/43], Training Loss: 0.00001610
2024-11-06 14:06:59,594 - INFO - Epoch [57/300], Batch [35/43], Training Loss: 0.00001601
2024-11-06 14:06:59,596 - INFO - Epoch [57/300], Batch [36/43], Training Loss: 0.00000984
2024-11-06 14:06:59,599 - INFO - Epoch [57/300], Batch [37/43], Training Loss: 0.00001163
2024-11-06 14:06:59,602 - INFO - Epoch [57/300], Batch [38/43], Training Loss: 0.00002917
2024-11-06 14:06:59,606 - INFO - Epoch [57/300], Batch [39/43], Training Loss: 0.00003116
2024-11-06 14:06:59,609 - INFO - Epoch [57/300], Batch [40/43], Training Loss: 0.00002996
2024-11-06 14:06:59,612 - INFO - Epoch [57/300], Batch [41/43], Training Loss: 0.00001384
2024-11-06 14:06:59,614 - INFO - Epoch [57/300], Batch [42/43], Training Loss: 0.00005190
2024-11-06 14:06:59,617 - INFO - Epoch [57/300], Batch [43/43], Training Loss: 0.00002728
2024-11-06 14:06:59,628 - INFO - Epoch [57/300], Average Training Loss: 0.00002464, Validation Loss: 0.00003001
2024-11-06 14:06:59,632 - INFO - Epoch [58/300], Batch [1/43], Training Loss: 0.00002333
2024-11-06 14:06:59,635 - INFO - Epoch [58/300], Batch [2/43], Training Loss: 0.00001547
2024-11-06 14:06:59,638 - INFO - Epoch [58/300], Batch [3/43], Training Loss: 0.00006094
2024-11-06 14:06:59,643 - INFO - Epoch [58/300], Batch [4/43], Training Loss: 0.00003141
2024-11-06 14:06:59,646 - INFO - Epoch [58/300], Batch [5/43], Training Loss: 0.00004019
2024-11-06 14:06:59,650 - INFO - Epoch [58/300], Batch [6/43], Training Loss: 0.00005254
2024-11-06 14:06:59,654 - INFO - Epoch [58/300], Batch [7/43], Training Loss: 0.00001626
2024-11-06 14:06:59,658 - INFO - Epoch [58/300], Batch [8/43], Training Loss: 0.00001553
2024-11-06 14:06:59,661 - INFO - Epoch [58/300], Batch [9/43], Training Loss: 0.00004468
2024-11-06 14:06:59,665 - INFO - Epoch [58/300], Batch [10/43], Training Loss: 0.00001889
2024-11-06 14:06:59,669 - INFO - Epoch [58/300], Batch [11/43], Training Loss: 0.00007098
2024-11-06 14:06:59,672 - INFO - Epoch [58/300], Batch [12/43], Training Loss: 0.00003288
2024-11-06 14:06:59,675 - INFO - Epoch [58/300], Batch [13/43], Training Loss: 0.00001631
2024-11-06 14:06:59,678 - INFO - Epoch [58/300], Batch [14/43], Training Loss: 0.00002013
2024-11-06 14:06:59,682 - INFO - Epoch [58/300], Batch [15/43], Training Loss: 0.00003748
2024-11-06 14:06:59,685 - INFO - Epoch [58/300], Batch [16/43], Training Loss: 0.00006922
2024-11-06 14:06:59,689 - INFO - Epoch [58/300], Batch [17/43], Training Loss: 0.00002178
2024-11-06 14:06:59,692 - INFO - Epoch [58/300], Batch [18/43], Training Loss: 0.00002907
2024-11-06 14:06:59,696 - INFO - Epoch [58/300], Batch [19/43], Training Loss: 0.00002244
2024-11-06 14:06:59,700 - INFO - Epoch [58/300], Batch [20/43], Training Loss: 0.00004831
2024-11-06 14:06:59,703 - INFO - Epoch [58/300], Batch [21/43], Training Loss: 0.00002552
2024-11-06 14:06:59,707 - INFO - Epoch [58/300], Batch [22/43], Training Loss: 0.00002490
2024-11-06 14:06:59,710 - INFO - Epoch [58/300], Batch [23/43], Training Loss: 0.00001928
2024-11-06 14:06:59,713 - INFO - Epoch [58/300], Batch [24/43], Training Loss: 0.00001544
2024-11-06 14:06:59,716 - INFO - Epoch [58/300], Batch [25/43], Training Loss: 0.00004423
2024-11-06 14:06:59,721 - INFO - Epoch [58/300], Batch [26/43], Training Loss: 0.00001495
2024-11-06 14:06:59,725 - INFO - Epoch [58/300], Batch [27/43], Training Loss: 0.00002595
2024-11-06 14:06:59,728 - INFO - Epoch [58/300], Batch [28/43], Training Loss: 0.00003993
2024-11-06 14:06:59,732 - INFO - Epoch [58/300], Batch [29/43], Training Loss: 0.00002084
2024-11-06 14:06:59,735 - INFO - Epoch [58/300], Batch [30/43], Training Loss: 0.00004096
2024-11-06 14:06:59,739 - INFO - Epoch [58/300], Batch [31/43], Training Loss: 0.00002200
2024-11-06 14:06:59,743 - INFO - Epoch [58/300], Batch [32/43], Training Loss: 0.00001924
2024-11-06 14:06:59,746 - INFO - Epoch [58/300], Batch [33/43], Training Loss: 0.00004348
2024-11-06 14:06:59,749 - INFO - Epoch [58/300], Batch [34/43], Training Loss: 0.00002034
2024-11-06 14:06:59,753 - INFO - Epoch [58/300], Batch [35/43], Training Loss: 0.00001973
2024-11-06 14:06:59,757 - INFO - Epoch [58/300], Batch [36/43], Training Loss: 0.00002558
2024-11-06 14:06:59,760 - INFO - Epoch [58/300], Batch [37/43], Training Loss: 0.00002523
2024-11-06 14:06:59,763 - INFO - Epoch [58/300], Batch [38/43], Training Loss: 0.00001656
2024-11-06 14:06:59,766 - INFO - Epoch [58/300], Batch [39/43], Training Loss: 0.00002029
2024-11-06 14:06:59,769 - INFO - Epoch [58/300], Batch [40/43], Training Loss: 0.00002963
2024-11-06 14:06:59,772 - INFO - Epoch [58/300], Batch [41/43], Training Loss: 0.00002304
2024-11-06 14:06:59,776 - INFO - Epoch [58/300], Batch [42/43], Training Loss: 0.00002322
2024-11-06 14:06:59,780 - INFO - Epoch [58/300], Batch [43/43], Training Loss: 0.00001553
2024-11-06 14:06:59,792 - INFO - Epoch [58/300], Average Training Loss: 0.00002939, Validation Loss: 0.00003156
2024-11-06 14:06:59,796 - INFO - Epoch [59/300], Batch [1/43], Training Loss: 0.00002908
2024-11-06 14:06:59,800 - INFO - Epoch [59/300], Batch [2/43], Training Loss: 0.00001717
2024-11-06 14:06:59,804 - INFO - Epoch [59/300], Batch [3/43], Training Loss: 0.00003750
2024-11-06 14:06:59,808 - INFO - Epoch [59/300], Batch [4/43], Training Loss: 0.00002146
2024-11-06 14:06:59,812 - INFO - Epoch [59/300], Batch [5/43], Training Loss: 0.00002416
2024-11-06 14:06:59,816 - INFO - Epoch [59/300], Batch [6/43], Training Loss: 0.00000584
2024-11-06 14:06:59,820 - INFO - Epoch [59/300], Batch [7/43], Training Loss: 0.00000594
2024-11-06 14:06:59,823 - INFO - Epoch [59/300], Batch [8/43], Training Loss: 0.00002016
2024-11-06 14:06:59,827 - INFO - Epoch [59/300], Batch [9/43], Training Loss: 0.00003633
2024-11-06 14:06:59,831 - INFO - Epoch [59/300], Batch [10/43], Training Loss: 0.00003266
2024-11-06 14:06:59,834 - INFO - Epoch [59/300], Batch [11/43], Training Loss: 0.00001605
2024-11-06 14:06:59,837 - INFO - Epoch [59/300], Batch [12/43], Training Loss: 0.00003285
2024-11-06 14:06:59,841 - INFO - Epoch [59/300], Batch [13/43], Training Loss: 0.00002626
2024-11-06 14:06:59,847 - INFO - Epoch [59/300], Batch [14/43], Training Loss: 0.00004599
2024-11-06 14:06:59,851 - INFO - Epoch [59/300], Batch [15/43], Training Loss: 0.00005550
2024-11-06 14:06:59,855 - INFO - Epoch [59/300], Batch [16/43], Training Loss: 0.00004570
2024-11-06 14:06:59,859 - INFO - Epoch [59/300], Batch [17/43], Training Loss: 0.00002573
2024-11-06 14:06:59,863 - INFO - Epoch [59/300], Batch [18/43], Training Loss: 0.00002172
2024-11-06 14:06:59,867 - INFO - Epoch [59/300], Batch [19/43], Training Loss: 0.00002687
2024-11-06 14:06:59,870 - INFO - Epoch [59/300], Batch [20/43], Training Loss: 0.00002955
2024-11-06 14:06:59,873 - INFO - Epoch [59/300], Batch [21/43], Training Loss: 0.00001741
2024-11-06 14:06:59,876 - INFO - Epoch [59/300], Batch [22/43], Training Loss: 0.00001827
2024-11-06 14:06:59,880 - INFO - Epoch [59/300], Batch [23/43], Training Loss: 0.00002139
2024-11-06 14:06:59,884 - INFO - Epoch [59/300], Batch [24/43], Training Loss: 0.00001111
2024-11-06 14:06:59,889 - INFO - Epoch [59/300], Batch [25/43], Training Loss: 0.00002144
2024-11-06 14:06:59,893 - INFO - Epoch [59/300], Batch [26/43], Training Loss: 0.00001840
2024-11-06 14:06:59,897 - INFO - Epoch [59/300], Batch [27/43], Training Loss: 0.00002621
2024-11-06 14:06:59,901 - INFO - Epoch [59/300], Batch [28/43], Training Loss: 0.00001633
2024-11-06 14:06:59,904 - INFO - Epoch [59/300], Batch [29/43], Training Loss: 0.00002629
2024-11-06 14:06:59,908 - INFO - Epoch [59/300], Batch [30/43], Training Loss: 0.00002280
2024-11-06 14:06:59,912 - INFO - Epoch [59/300], Batch [31/43], Training Loss: 0.00004899
2024-11-06 14:06:59,915 - INFO - Epoch [59/300], Batch [32/43], Training Loss: 0.00003018
2024-11-06 14:06:59,917 - INFO - Epoch [59/300], Batch [33/43], Training Loss: 0.00002841
2024-11-06 14:06:59,922 - INFO - Epoch [59/300], Batch [34/43], Training Loss: 0.00003030
2024-11-06 14:06:59,926 - INFO - Epoch [59/300], Batch [35/43], Training Loss: 0.00002116
2024-11-06 14:06:59,930 - INFO - Epoch [59/300], Batch [36/43], Training Loss: 0.00002971
2024-11-06 14:06:59,934 - INFO - Epoch [59/300], Batch [37/43], Training Loss: 0.00001966
2024-11-06 14:06:59,938 - INFO - Epoch [59/300], Batch [38/43], Training Loss: 0.00003663
2024-11-06 14:06:59,942 - INFO - Epoch [59/300], Batch [39/43], Training Loss: 0.00002559
2024-11-06 14:06:59,945 - INFO - Epoch [59/300], Batch [40/43], Training Loss: 0.00002904
2024-11-06 14:06:59,948 - INFO - Epoch [59/300], Batch [41/43], Training Loss: 0.00002725
2024-11-06 14:06:59,951 - INFO - Epoch [59/300], Batch [42/43], Training Loss: 0.00002392
2024-11-06 14:06:59,956 - INFO - Epoch [59/300], Batch [43/43], Training Loss: 0.00002689
2024-11-06 14:06:59,968 - INFO - Epoch [59/300], Average Training Loss: 0.00002637, Validation Loss: 0.00006592
2024-11-06 14:06:59,972 - INFO - Epoch [60/300], Batch [1/43], Training Loss: 0.00008287
2024-11-06 14:06:59,977 - INFO - Epoch [60/300], Batch [2/43], Training Loss: 0.00001860
2024-11-06 14:06:59,982 - INFO - Epoch [60/300], Batch [3/43], Training Loss: 0.00002447
2024-11-06 14:06:59,985 - INFO - Epoch [60/300], Batch [4/43], Training Loss: 0.00005087
2024-11-06 14:06:59,988 - INFO - Epoch [60/300], Batch [5/43], Training Loss: 0.00008425
2024-11-06 14:06:59,991 - INFO - Epoch [60/300], Batch [6/43], Training Loss: 0.00001648
2024-11-06 14:06:59,995 - INFO - Epoch [60/300], Batch [7/43], Training Loss: 0.00005488
2024-11-06 14:06:59,998 - INFO - Epoch [60/300], Batch [8/43], Training Loss: 0.00002957
2024-11-06 14:07:00,002 - INFO - Epoch [60/300], Batch [9/43], Training Loss: 0.00008637
2024-11-06 14:07:00,006 - INFO - Epoch [60/300], Batch [10/43], Training Loss: 0.00003747
2024-11-06 14:07:00,010 - INFO - Epoch [60/300], Batch [11/43], Training Loss: 0.00003316
2024-11-06 14:07:00,013 - INFO - Epoch [60/300], Batch [12/43], Training Loss: 0.00005300
2024-11-06 14:07:00,016 - INFO - Epoch [60/300], Batch [13/43], Training Loss: 0.00004599
2024-11-06 14:07:00,020 - INFO - Epoch [60/300], Batch [14/43], Training Loss: 0.00003127
2024-11-06 14:07:00,023 - INFO - Epoch [60/300], Batch [15/43], Training Loss: 0.00002446
2024-11-06 14:07:00,026 - INFO - Epoch [60/300], Batch [16/43], Training Loss: 0.00003173
2024-11-06 14:07:00,029 - INFO - Epoch [60/300], Batch [17/43], Training Loss: 0.00005957
2024-11-06 14:07:00,032 - INFO - Epoch [60/300], Batch [18/43], Training Loss: 0.00001639
2024-11-06 14:07:00,036 - INFO - Epoch [60/300], Batch [19/43], Training Loss: 0.00000938
2024-11-06 14:07:00,040 - INFO - Epoch [60/300], Batch [20/43], Training Loss: 0.00004963
2024-11-06 14:07:00,043 - INFO - Epoch [60/300], Batch [21/43], Training Loss: 0.00001820
2024-11-06 14:07:00,047 - INFO - Epoch [60/300], Batch [22/43], Training Loss: 0.00002956
2024-11-06 14:07:00,050 - INFO - Epoch [60/300], Batch [23/43], Training Loss: 0.00001331
2024-11-06 14:07:00,053 - INFO - Epoch [60/300], Batch [24/43], Training Loss: 0.00003582
2024-11-06 14:07:00,057 - INFO - Epoch [60/300], Batch [25/43], Training Loss: 0.00003974
2024-11-06 14:07:00,060 - INFO - Epoch [60/300], Batch [26/43], Training Loss: 0.00003323
2024-11-06 14:07:00,064 - INFO - Epoch [60/300], Batch [27/43], Training Loss: 0.00003354
2024-11-06 14:07:00,067 - INFO - Epoch [60/300], Batch [28/43], Training Loss: 0.00002275
2024-11-06 14:07:00,072 - INFO - Epoch [60/300], Batch [29/43], Training Loss: 0.00002155
2024-11-06 14:07:00,075 - INFO - Epoch [60/300], Batch [30/43], Training Loss: 0.00001691
2024-11-06 14:07:00,079 - INFO - Epoch [60/300], Batch [31/43], Training Loss: 0.00001976
2024-11-06 14:07:00,081 - INFO - Epoch [60/300], Batch [32/43], Training Loss: 0.00001471
2024-11-06 14:07:00,085 - INFO - Epoch [60/300], Batch [33/43], Training Loss: 0.00002026
2024-11-06 14:07:00,088 - INFO - Epoch [60/300], Batch [34/43], Training Loss: 0.00002652
2024-11-06 14:07:00,092 - INFO - Epoch [60/300], Batch [35/43], Training Loss: 0.00001868
2024-11-06 14:07:00,096 - INFO - Epoch [60/300], Batch [36/43], Training Loss: 0.00003070
2024-11-06 14:07:00,099 - INFO - Epoch [60/300], Batch [37/43], Training Loss: 0.00002159
2024-11-06 14:07:00,103 - INFO - Epoch [60/300], Batch [38/43], Training Loss: 0.00001156
2024-11-06 14:07:00,107 - INFO - Epoch [60/300], Batch [39/43], Training Loss: 0.00002119
2024-11-06 14:07:00,112 - INFO - Epoch [60/300], Batch [40/43], Training Loss: 0.00002153
2024-11-06 14:07:00,116 - INFO - Epoch [60/300], Batch [41/43], Training Loss: 0.00002551
2024-11-06 14:07:00,121 - INFO - Epoch [60/300], Batch [42/43], Training Loss: 0.00002324
2024-11-06 14:07:00,127 - INFO - Epoch [60/300], Batch [43/43], Training Loss: 0.00001567
2024-11-06 14:07:00,142 - INFO - Epoch [60/300], Average Training Loss: 0.00003200, Validation Loss: 0.00002741
2024-11-06 14:07:00,146 - INFO - Epoch [61/300], Batch [1/43], Training Loss: 0.00001816
2024-11-06 14:07:00,151 - INFO - Epoch [61/300], Batch [2/43], Training Loss: 0.00001829
2024-11-06 14:07:00,155 - INFO - Epoch [61/300], Batch [3/43], Training Loss: 0.00001996
2024-11-06 14:07:00,159 - INFO - Epoch [61/300], Batch [4/43], Training Loss: 0.00002863
2024-11-06 14:07:00,164 - INFO - Epoch [61/300], Batch [5/43], Training Loss: 0.00004505
2024-11-06 14:07:00,168 - INFO - Epoch [61/300], Batch [6/43], Training Loss: 0.00002100
2024-11-06 14:07:00,172 - INFO - Epoch [61/300], Batch [7/43], Training Loss: 0.00003404
2024-11-06 14:07:00,176 - INFO - Epoch [61/300], Batch [8/43], Training Loss: 0.00002468
2024-11-06 14:07:00,180 - INFO - Epoch [61/300], Batch [9/43], Training Loss: 0.00001890
2024-11-06 14:07:00,185 - INFO - Epoch [61/300], Batch [10/43], Training Loss: 0.00001727
2024-11-06 14:07:00,190 - INFO - Epoch [61/300], Batch [11/43], Training Loss: 0.00002517
2024-11-06 14:07:00,196 - INFO - Epoch [61/300], Batch [12/43], Training Loss: 0.00004184
2024-11-06 14:07:00,201 - INFO - Epoch [61/300], Batch [13/43], Training Loss: 0.00000818
2024-11-06 14:07:00,207 - INFO - Epoch [61/300], Batch [14/43], Training Loss: 0.00002197
2024-11-06 14:07:00,215 - INFO - Epoch [61/300], Batch [15/43], Training Loss: 0.00001942
2024-11-06 14:07:00,222 - INFO - Epoch [61/300], Batch [16/43], Training Loss: 0.00001629
2024-11-06 14:07:00,227 - INFO - Epoch [61/300], Batch [17/43], Training Loss: 0.00003032
2024-11-06 14:07:00,233 - INFO - Epoch [61/300], Batch [18/43], Training Loss: 0.00001410
2024-11-06 14:07:00,238 - INFO - Epoch [61/300], Batch [19/43], Training Loss: 0.00002570
2024-11-06 14:07:00,243 - INFO - Epoch [61/300], Batch [20/43], Training Loss: 0.00002943
2024-11-06 14:07:00,249 - INFO - Epoch [61/300], Batch [21/43], Training Loss: 0.00002345
2024-11-06 14:07:00,254 - INFO - Epoch [61/300], Batch [22/43], Training Loss: 0.00003309
2024-11-06 14:07:00,258 - INFO - Epoch [61/300], Batch [23/43], Training Loss: 0.00000515
2024-11-06 14:07:00,262 - INFO - Epoch [61/300], Batch [24/43], Training Loss: 0.00002011
2024-11-06 14:07:00,266 - INFO - Epoch [61/300], Batch [25/43], Training Loss: 0.00001841
2024-11-06 14:07:00,270 - INFO - Epoch [61/300], Batch [26/43], Training Loss: 0.00002079
2024-11-06 14:07:00,274 - INFO - Epoch [61/300], Batch [27/43], Training Loss: 0.00001525
2024-11-06 14:07:00,279 - INFO - Epoch [61/300], Batch [28/43], Training Loss: 0.00003084
2024-11-06 14:07:00,283 - INFO - Epoch [61/300], Batch [29/43], Training Loss: 0.00002259
2024-11-06 14:07:00,287 - INFO - Epoch [61/300], Batch [30/43], Training Loss: 0.00002845
2024-11-06 14:07:00,291 - INFO - Epoch [61/300], Batch [31/43], Training Loss: 0.00003439
2024-11-06 14:07:00,296 - INFO - Epoch [61/300], Batch [32/43], Training Loss: 0.00002203
2024-11-06 14:07:00,301 - INFO - Epoch [61/300], Batch [33/43], Training Loss: 0.00001749
2024-11-06 14:07:00,307 - INFO - Epoch [61/300], Batch [34/43], Training Loss: 0.00004545
2024-11-06 14:07:00,312 - INFO - Epoch [61/300], Batch [35/43], Training Loss: 0.00001139
2024-11-06 14:07:00,317 - INFO - Epoch [61/300], Batch [36/43], Training Loss: 0.00002059
2024-11-06 14:07:00,322 - INFO - Epoch [61/300], Batch [37/43], Training Loss: 0.00002248
2024-11-06 14:07:00,327 - INFO - Epoch [61/300], Batch [38/43], Training Loss: 0.00002011
2024-11-06 14:07:00,332 - INFO - Epoch [61/300], Batch [39/43], Training Loss: 0.00002090
2024-11-06 14:07:00,336 - INFO - Epoch [61/300], Batch [40/43], Training Loss: 0.00001323
2024-11-06 14:07:00,341 - INFO - Epoch [61/300], Batch [41/43], Training Loss: 0.00002645
2024-11-06 14:07:00,346 - INFO - Epoch [61/300], Batch [42/43], Training Loss: 0.00001463
2024-11-06 14:07:00,350 - INFO - Epoch [61/300], Batch [43/43], Training Loss: 0.00001896
2024-11-06 14:07:00,361 - INFO - Epoch [61/300], Average Training Loss: 0.00002290, Validation Loss: 0.00002729
2024-11-06 14:07:00,364 - INFO - Epoch [62/300], Batch [1/43], Training Loss: 0.00000739
2024-11-06 14:07:00,370 - INFO - Epoch [62/300], Batch [2/43], Training Loss: 0.00001377
2024-11-06 14:07:00,375 - INFO - Epoch [62/300], Batch [3/43], Training Loss: 0.00005312
2024-11-06 14:07:00,379 - INFO - Epoch [62/300], Batch [4/43], Training Loss: 0.00001698
2024-11-06 14:07:00,384 - INFO - Epoch [62/300], Batch [5/43], Training Loss: 0.00001737
2024-11-06 14:07:00,388 - INFO - Epoch [62/300], Batch [6/43], Training Loss: 0.00004083
2024-11-06 14:07:00,393 - INFO - Epoch [62/300], Batch [7/43], Training Loss: 0.00003470
2024-11-06 14:07:00,397 - INFO - Epoch [62/300], Batch [8/43], Training Loss: 0.00004375
2024-11-06 14:07:00,402 - INFO - Epoch [62/300], Batch [9/43], Training Loss: 0.00001831
2024-11-06 14:07:00,406 - INFO - Epoch [62/300], Batch [10/43], Training Loss: 0.00001505
2024-11-06 14:07:00,411 - INFO - Epoch [62/300], Batch [11/43], Training Loss: 0.00002184
2024-11-06 14:07:00,416 - INFO - Epoch [62/300], Batch [12/43], Training Loss: 0.00001652
2024-11-06 14:07:00,420 - INFO - Epoch [62/300], Batch [13/43], Training Loss: 0.00001327
2024-11-06 14:07:00,425 - INFO - Epoch [62/300], Batch [14/43], Training Loss: 0.00002518
2024-11-06 14:07:00,429 - INFO - Epoch [62/300], Batch [15/43], Training Loss: 0.00001921
2024-11-06 14:07:00,433 - INFO - Epoch [62/300], Batch [16/43], Training Loss: 0.00004363
2024-11-06 14:07:00,437 - INFO - Epoch [62/300], Batch [17/43], Training Loss: 0.00003658
2024-11-06 14:07:00,440 - INFO - Epoch [62/300], Batch [18/43], Training Loss: 0.00002295
2024-11-06 14:07:00,445 - INFO - Epoch [62/300], Batch [19/43], Training Loss: 0.00001487
2024-11-06 14:07:00,449 - INFO - Epoch [62/300], Batch [20/43], Training Loss: 0.00003079
2024-11-06 14:07:00,454 - INFO - Epoch [62/300], Batch [21/43], Training Loss: 0.00003334
2024-11-06 14:07:00,457 - INFO - Epoch [62/300], Batch [22/43], Training Loss: 0.00000919
2024-11-06 14:07:00,460 - INFO - Epoch [62/300], Batch [23/43], Training Loss: 0.00002120
2024-11-06 14:07:00,465 - INFO - Epoch [62/300], Batch [24/43], Training Loss: 0.00003782
2024-11-06 14:07:00,469 - INFO - Epoch [62/300], Batch [25/43], Training Loss: 0.00002608
2024-11-06 14:07:00,473 - INFO - Epoch [62/300], Batch [26/43], Training Loss: 0.00002183
2024-11-06 14:07:00,478 - INFO - Epoch [62/300], Batch [27/43], Training Loss: 0.00002316
2024-11-06 14:07:00,483 - INFO - Epoch [62/300], Batch [28/43], Training Loss: 0.00002608
2024-11-06 14:07:00,489 - INFO - Epoch [62/300], Batch [29/43], Training Loss: 0.00001951
2024-11-06 14:07:00,493 - INFO - Epoch [62/300], Batch [30/43], Training Loss: 0.00002399
2024-11-06 14:07:00,498 - INFO - Epoch [62/300], Batch [31/43], Training Loss: 0.00001986
2024-11-06 14:07:00,504 - INFO - Epoch [62/300], Batch [32/43], Training Loss: 0.00002632
2024-11-06 14:07:00,508 - INFO - Epoch [62/300], Batch [33/43], Training Loss: 0.00001710
2024-11-06 14:07:00,513 - INFO - Epoch [62/300], Batch [34/43], Training Loss: 0.00002708
2024-11-06 14:07:00,517 - INFO - Epoch [62/300], Batch [35/43], Training Loss: 0.00002351
2024-11-06 14:07:00,521 - INFO - Epoch [62/300], Batch [36/43], Training Loss: 0.00003242
2024-11-06 14:07:00,525 - INFO - Epoch [62/300], Batch [37/43], Training Loss: 0.00003771
2024-11-06 14:07:00,528 - INFO - Epoch [62/300], Batch [38/43], Training Loss: 0.00002118
2024-11-06 14:07:00,533 - INFO - Epoch [62/300], Batch [39/43], Training Loss: 0.00001454
2024-11-06 14:07:00,537 - INFO - Epoch [62/300], Batch [40/43], Training Loss: 0.00001408
2024-11-06 14:07:00,542 - INFO - Epoch [62/300], Batch [41/43], Training Loss: 0.00002296
2024-11-06 14:07:00,546 - INFO - Epoch [62/300], Batch [42/43], Training Loss: 0.00001558
2024-11-06 14:07:00,550 - INFO - Epoch [62/300], Batch [43/43], Training Loss: 0.00001757
2024-11-06 14:07:00,564 - INFO - Epoch [62/300], Average Training Loss: 0.00002414, Validation Loss: 0.00003159
2024-11-06 14:07:00,568 - INFO - Epoch [63/300], Batch [1/43], Training Loss: 0.00001152
2024-11-06 14:07:00,572 - INFO - Epoch [63/300], Batch [2/43], Training Loss: 0.00002328
2024-11-06 14:07:00,581 - INFO - Epoch [63/300], Batch [3/43], Training Loss: 0.00001579
2024-11-06 14:07:00,587 - INFO - Epoch [63/300], Batch [4/43], Training Loss: 0.00002485
2024-11-06 14:07:00,593 - INFO - Epoch [63/300], Batch [5/43], Training Loss: 0.00003152
2024-11-06 14:07:00,597 - INFO - Epoch [63/300], Batch [6/43], Training Loss: 0.00000989
2024-11-06 14:07:00,602 - INFO - Epoch [63/300], Batch [7/43], Training Loss: 0.00001928
2024-11-06 14:07:00,606 - INFO - Epoch [63/300], Batch [8/43], Training Loss: 0.00002328
2024-11-06 14:07:00,610 - INFO - Epoch [63/300], Batch [9/43], Training Loss: 0.00003395
2024-11-06 14:07:00,615 - INFO - Epoch [63/300], Batch [10/43], Training Loss: 0.00001977
2024-11-06 14:07:00,619 - INFO - Epoch [63/300], Batch [11/43], Training Loss: 0.00001484
2024-11-06 14:07:00,628 - INFO - Epoch [63/300], Batch [12/43], Training Loss: 0.00002124
2024-11-06 14:07:00,633 - INFO - Epoch [63/300], Batch [13/43], Training Loss: 0.00001365
2024-11-06 14:07:00,639 - INFO - Epoch [63/300], Batch [14/43], Training Loss: 0.00002022
2024-11-06 14:07:00,644 - INFO - Epoch [63/300], Batch [15/43], Training Loss: 0.00001016
2024-11-06 14:07:00,650 - INFO - Epoch [63/300], Batch [16/43], Training Loss: 0.00001526
2024-11-06 14:07:00,656 - INFO - Epoch [63/300], Batch [17/43], Training Loss: 0.00001544
2024-11-06 14:07:00,662 - INFO - Epoch [63/300], Batch [18/43], Training Loss: 0.00001887
2024-11-06 14:07:00,669 - INFO - Epoch [63/300], Batch [19/43], Training Loss: 0.00001797
2024-11-06 14:07:00,677 - INFO - Epoch [63/300], Batch [20/43], Training Loss: 0.00001092
2024-11-06 14:07:00,686 - INFO - Epoch [63/300], Batch [21/43], Training Loss: 0.00001076
2024-11-06 14:07:00,691 - INFO - Epoch [63/300], Batch [22/43], Training Loss: 0.00002413
2024-11-06 14:07:00,705 - INFO - Epoch [63/300], Batch [23/43], Training Loss: 0.00000836
2024-11-06 14:07:00,713 - INFO - Epoch [63/300], Batch [24/43], Training Loss: 0.00002340
2024-11-06 14:07:00,719 - INFO - Epoch [63/300], Batch [25/43], Training Loss: 0.00003044
2024-11-06 14:07:00,727 - INFO - Epoch [63/300], Batch [26/43], Training Loss: 0.00002881
2024-11-06 14:07:00,733 - INFO - Epoch [63/300], Batch [27/43], Training Loss: 0.00004809
2024-11-06 14:07:00,738 - INFO - Epoch [63/300], Batch [28/43], Training Loss: 0.00003320
2024-11-06 14:07:00,747 - INFO - Epoch [63/300], Batch [29/43], Training Loss: 0.00002171
2024-11-06 14:07:00,760 - INFO - Epoch [63/300], Batch [30/43], Training Loss: 0.00002933
2024-11-06 14:07:00,767 - INFO - Epoch [63/300], Batch [31/43], Training Loss: 0.00001730
2024-11-06 14:07:00,773 - INFO - Epoch [63/300], Batch [32/43], Training Loss: 0.00003434
2024-11-06 14:07:00,778 - INFO - Epoch [63/300], Batch [33/43], Training Loss: 0.00003063
2024-11-06 14:07:00,785 - INFO - Epoch [63/300], Batch [34/43], Training Loss: 0.00005025
2024-11-06 14:07:00,792 - INFO - Epoch [63/300], Batch [35/43], Training Loss: 0.00005635
2024-11-06 14:07:00,797 - INFO - Epoch [63/300], Batch [36/43], Training Loss: 0.00002203
2024-11-06 14:07:00,801 - INFO - Epoch [63/300], Batch [37/43], Training Loss: 0.00001245
2024-11-06 14:07:00,808 - INFO - Epoch [63/300], Batch [38/43], Training Loss: 0.00001535
2024-11-06 14:07:00,813 - INFO - Epoch [63/300], Batch [39/43], Training Loss: 0.00002027
2024-11-06 14:07:00,818 - INFO - Epoch [63/300], Batch [40/43], Training Loss: 0.00001284
2024-11-06 14:07:00,823 - INFO - Epoch [63/300], Batch [41/43], Training Loss: 0.00001564
2024-11-06 14:07:00,827 - INFO - Epoch [63/300], Batch [42/43], Training Loss: 0.00002802
2024-11-06 14:07:00,833 - INFO - Epoch [63/300], Batch [43/43], Training Loss: 0.00003666
2024-11-06 14:07:00,862 - INFO - Epoch [63/300], Average Training Loss: 0.00002284, Validation Loss: 0.00002789
2024-11-06 14:07:00,869 - INFO - Epoch [64/300], Batch [1/43], Training Loss: 0.00003092
2024-11-06 14:07:00,876 - INFO - Epoch [64/300], Batch [2/43], Training Loss: 0.00001887
2024-11-06 14:07:00,884 - INFO - Epoch [64/300], Batch [3/43], Training Loss: 0.00003446
2024-11-06 14:07:00,891 - INFO - Epoch [64/300], Batch [4/43], Training Loss: 0.00005017
2024-11-06 14:07:00,897 - INFO - Epoch [64/300], Batch [5/43], Training Loss: 0.00000964
2024-11-06 14:07:00,902 - INFO - Epoch [64/300], Batch [6/43], Training Loss: 0.00001948
2024-11-06 14:07:00,906 - INFO - Epoch [64/300], Batch [7/43], Training Loss: 0.00004062
2024-11-06 14:07:00,909 - INFO - Epoch [64/300], Batch [8/43], Training Loss: 0.00005973
2024-11-06 14:07:00,948 - INFO - Epoch [64/300], Batch [9/43], Training Loss: 0.00001797
2024-11-06 14:07:00,972 - INFO - Epoch [64/300], Batch [10/43], Training Loss: 0.00006518
2024-11-06 14:07:00,985 - INFO - Epoch [64/300], Batch [11/43], Training Loss: 0.00005649
2024-11-06 14:07:00,989 - INFO - Epoch [64/300], Batch [12/43], Training Loss: 0.00004682
2024-11-06 14:07:00,993 - INFO - Epoch [64/300], Batch [13/43], Training Loss: 0.00000869
2024-11-06 14:07:00,998 - INFO - Epoch [64/300], Batch [14/43], Training Loss: 0.00005947
2024-11-06 14:07:01,003 - INFO - Epoch [64/300], Batch [15/43], Training Loss: 0.00003962
2024-11-06 14:07:01,008 - INFO - Epoch [64/300], Batch [16/43], Training Loss: 0.00003905
2024-11-06 14:07:01,013 - INFO - Epoch [64/300], Batch [17/43], Training Loss: 0.00001332
2024-11-06 14:07:01,017 - INFO - Epoch [64/300], Batch [18/43], Training Loss: 0.00004052
2024-11-06 14:07:01,022 - INFO - Epoch [64/300], Batch [19/43], Training Loss: 0.00002745
2024-11-06 14:07:01,027 - INFO - Epoch [64/300], Batch [20/43], Training Loss: 0.00003764
2024-11-06 14:07:01,031 - INFO - Epoch [64/300], Batch [21/43], Training Loss: 0.00002599
2024-11-06 14:07:01,036 - INFO - Epoch [64/300], Batch [22/43], Training Loss: 0.00002790
2024-11-06 14:07:01,041 - INFO - Epoch [64/300], Batch [23/43], Training Loss: 0.00001515
2024-11-06 14:07:01,044 - INFO - Epoch [64/300], Batch [24/43], Training Loss: 0.00003725
2024-11-06 14:07:01,049 - INFO - Epoch [64/300], Batch [25/43], Training Loss: 0.00002387
2024-11-06 14:07:01,053 - INFO - Epoch [64/300], Batch [26/43], Training Loss: 0.00002085
2024-11-06 14:07:01,058 - INFO - Epoch [64/300], Batch [27/43], Training Loss: 0.00002944
2024-11-06 14:07:01,072 - INFO - Epoch [64/300], Batch [28/43], Training Loss: 0.00001549
2024-11-06 14:07:01,076 - INFO - Epoch [64/300], Batch [29/43], Training Loss: 0.00002977
2024-11-06 14:07:01,080 - INFO - Epoch [64/300], Batch [30/43], Training Loss: 0.00002268
2024-11-06 14:07:01,084 - INFO - Epoch [64/300], Batch [31/43], Training Loss: 0.00002355
2024-11-06 14:07:01,089 - INFO - Epoch [64/300], Batch [32/43], Training Loss: 0.00002446
2024-11-06 14:07:01,093 - INFO - Epoch [64/300], Batch [33/43], Training Loss: 0.00001545
2024-11-06 14:07:01,098 - INFO - Epoch [64/300], Batch [34/43], Training Loss: 0.00001324
2024-11-06 14:07:01,102 - INFO - Epoch [64/300], Batch [35/43], Training Loss: 0.00002588
2024-11-06 14:07:01,105 - INFO - Epoch [64/300], Batch [36/43], Training Loss: 0.00002517
2024-11-06 14:07:01,109 - INFO - Epoch [64/300], Batch [37/43], Training Loss: 0.00005830
2024-11-06 14:07:01,112 - INFO - Epoch [64/300], Batch [38/43], Training Loss: 0.00001853
2024-11-06 14:07:01,116 - INFO - Epoch [64/300], Batch [39/43], Training Loss: 0.00003837
2024-11-06 14:07:01,120 - INFO - Epoch [64/300], Batch [40/43], Training Loss: 0.00001684
2024-11-06 14:07:01,124 - INFO - Epoch [64/300], Batch [41/43], Training Loss: 0.00001212
2024-11-06 14:07:01,128 - INFO - Epoch [64/300], Batch [42/43], Training Loss: 0.00004437
2024-11-06 14:07:01,133 - INFO - Epoch [64/300], Batch [43/43], Training Loss: 0.00000808
2024-11-06 14:07:01,149 - INFO - Epoch [64/300], Average Training Loss: 0.00002997, Validation Loss: 0.00002749
2024-11-06 14:07:01,153 - INFO - Epoch [65/300], Batch [1/43], Training Loss: 0.00002324
2024-11-06 14:07:01,157 - INFO - Epoch [65/300], Batch [2/43], Training Loss: 0.00003022
2024-11-06 14:07:01,163 - INFO - Epoch [65/300], Batch [3/43], Training Loss: 0.00003577
2024-11-06 14:07:01,167 - INFO - Epoch [65/300], Batch [4/43], Training Loss: 0.00000996
2024-11-06 14:07:01,172 - INFO - Epoch [65/300], Batch [5/43], Training Loss: 0.00000985
2024-11-06 14:07:01,177 - INFO - Epoch [65/300], Batch [6/43], Training Loss: 0.00002588
2024-11-06 14:07:01,181 - INFO - Epoch [65/300], Batch [7/43], Training Loss: 0.00001401
2024-11-06 14:07:01,185 - INFO - Epoch [65/300], Batch [8/43], Training Loss: 0.00002341
2024-11-06 14:07:01,189 - INFO - Epoch [65/300], Batch [9/43], Training Loss: 0.00001723
2024-11-06 14:07:01,193 - INFO - Epoch [65/300], Batch [10/43], Training Loss: 0.00002495
2024-11-06 14:07:01,197 - INFO - Epoch [65/300], Batch [11/43], Training Loss: 0.00002449
2024-11-06 14:07:01,201 - INFO - Epoch [65/300], Batch [12/43], Training Loss: 0.00002237
2024-11-06 14:07:01,205 - INFO - Epoch [65/300], Batch [13/43], Training Loss: 0.00003950
2024-11-06 14:07:01,210 - INFO - Epoch [65/300], Batch [14/43], Training Loss: 0.00000899
2024-11-06 14:07:01,215 - INFO - Epoch [65/300], Batch [15/43], Training Loss: 0.00002327
2024-11-06 14:07:01,219 - INFO - Epoch [65/300], Batch [16/43], Training Loss: 0.00001843
2024-11-06 14:07:01,224 - INFO - Epoch [65/300], Batch [17/43], Training Loss: 0.00002378
2024-11-06 14:07:01,228 - INFO - Epoch [65/300], Batch [18/43], Training Loss: 0.00002356
2024-11-06 14:07:01,233 - INFO - Epoch [65/300], Batch [19/43], Training Loss: 0.00002875
2024-11-06 14:07:01,237 - INFO - Epoch [65/300], Batch [20/43], Training Loss: 0.00002095
2024-11-06 14:07:01,242 - INFO - Epoch [65/300], Batch [21/43], Training Loss: 0.00001835
2024-11-06 14:07:01,247 - INFO - Epoch [65/300], Batch [22/43], Training Loss: 0.00002382
2024-11-06 14:07:01,251 - INFO - Epoch [65/300], Batch [23/43], Training Loss: 0.00004229
2024-11-06 14:07:01,255 - INFO - Epoch [65/300], Batch [24/43], Training Loss: 0.00002698
2024-11-06 14:07:01,260 - INFO - Epoch [65/300], Batch [25/43], Training Loss: 0.00002459
2024-11-06 14:07:01,264 - INFO - Epoch [65/300], Batch [26/43], Training Loss: 0.00004440
2024-11-06 14:07:01,268 - INFO - Epoch [65/300], Batch [27/43], Training Loss: 0.00002430
2024-11-06 14:07:01,273 - INFO - Epoch [65/300], Batch [28/43], Training Loss: 0.00002005
2024-11-06 14:07:01,277 - INFO - Epoch [65/300], Batch [29/43], Training Loss: 0.00003119
2024-11-06 14:07:01,281 - INFO - Epoch [65/300], Batch [30/43], Training Loss: 0.00002377
2024-11-06 14:07:01,285 - INFO - Epoch [65/300], Batch [31/43], Training Loss: 0.00002181
2024-11-06 14:07:01,290 - INFO - Epoch [65/300], Batch [32/43], Training Loss: 0.00002062
2024-11-06 14:07:01,294 - INFO - Epoch [65/300], Batch [33/43], Training Loss: 0.00004108
2024-11-06 14:07:01,298 - INFO - Epoch [65/300], Batch [34/43], Training Loss: 0.00002073
2024-11-06 14:07:01,302 - INFO - Epoch [65/300], Batch [35/43], Training Loss: 0.00003245
2024-11-06 14:07:01,307 - INFO - Epoch [65/300], Batch [36/43], Training Loss: 0.00002186
2024-11-06 14:07:01,312 - INFO - Epoch [65/300], Batch [37/43], Training Loss: 0.00001211
2024-11-06 14:07:01,316 - INFO - Epoch [65/300], Batch [38/43], Training Loss: 0.00001453
2024-11-06 14:07:01,320 - INFO - Epoch [65/300], Batch [39/43], Training Loss: 0.00003427
2024-11-06 14:07:01,324 - INFO - Epoch [65/300], Batch [40/43], Training Loss: 0.00002817
2024-11-06 14:07:01,328 - INFO - Epoch [65/300], Batch [41/43], Training Loss: 0.00002563
2024-11-06 14:07:01,333 - INFO - Epoch [65/300], Batch [42/43], Training Loss: 0.00001238
2024-11-06 14:07:01,340 - INFO - Epoch [65/300], Batch [43/43], Training Loss: 0.00001240
2024-11-06 14:07:01,353 - INFO - Epoch [65/300], Average Training Loss: 0.00002387, Validation Loss: 0.00002625
2024-11-06 14:07:01,358 - INFO - Epoch [66/300], Batch [1/43], Training Loss: 0.00001156
2024-11-06 14:07:01,362 - INFO - Epoch [66/300], Batch [2/43], Training Loss: 0.00000457
2024-11-06 14:07:01,367 - INFO - Epoch [66/300], Batch [3/43], Training Loss: 0.00000785
2024-11-06 14:07:01,370 - INFO - Epoch [66/300], Batch [4/43], Training Loss: 0.00001700
2024-11-06 14:07:01,374 - INFO - Epoch [66/300], Batch [5/43], Training Loss: 0.00001688
2024-11-06 14:07:01,378 - INFO - Epoch [66/300], Batch [6/43], Training Loss: 0.00003046
2024-11-06 14:07:01,382 - INFO - Epoch [66/300], Batch [7/43], Training Loss: 0.00001366
2024-11-06 14:07:01,386 - INFO - Epoch [66/300], Batch [8/43], Training Loss: 0.00002257
2024-11-06 14:07:01,390 - INFO - Epoch [66/300], Batch [9/43], Training Loss: 0.00003294
2024-11-06 14:07:01,394 - INFO - Epoch [66/300], Batch [10/43], Training Loss: 0.00001910
2024-11-06 14:07:01,400 - INFO - Epoch [66/300], Batch [11/43], Training Loss: 0.00001620
2024-11-06 14:07:01,404 - INFO - Epoch [66/300], Batch [12/43], Training Loss: 0.00002759
2024-11-06 14:07:01,408 - INFO - Epoch [66/300], Batch [13/43], Training Loss: 0.00003025
2024-11-06 14:07:01,413 - INFO - Epoch [66/300], Batch [14/43], Training Loss: 0.00001398
2024-11-06 14:07:01,417 - INFO - Epoch [66/300], Batch [15/43], Training Loss: 0.00003228
2024-11-06 14:07:01,421 - INFO - Epoch [66/300], Batch [16/43], Training Loss: 0.00003579
2024-11-06 14:07:01,426 - INFO - Epoch [66/300], Batch [17/43], Training Loss: 0.00004217
2024-11-06 14:07:01,430 - INFO - Epoch [66/300], Batch [18/43], Training Loss: 0.00001580
2024-11-06 14:07:01,434 - INFO - Epoch [66/300], Batch [19/43], Training Loss: 0.00003456
2024-11-06 14:07:01,439 - INFO - Epoch [66/300], Batch [20/43], Training Loss: 0.00003024
2024-11-06 14:07:01,444 - INFO - Epoch [66/300], Batch [21/43], Training Loss: 0.00002793
2024-11-06 14:07:01,450 - INFO - Epoch [66/300], Batch [22/43], Training Loss: 0.00002530
2024-11-06 14:07:01,454 - INFO - Epoch [66/300], Batch [23/43], Training Loss: 0.00002613
2024-11-06 14:07:01,458 - INFO - Epoch [66/300], Batch [24/43], Training Loss: 0.00001868
2024-11-06 14:07:01,463 - INFO - Epoch [66/300], Batch [25/43], Training Loss: 0.00000825
2024-11-06 14:07:01,468 - INFO - Epoch [66/300], Batch [26/43], Training Loss: 0.00001232
2024-11-06 14:07:01,472 - INFO - Epoch [66/300], Batch [27/43], Training Loss: 0.00004603
2024-11-06 14:07:01,476 - INFO - Epoch [66/300], Batch [28/43], Training Loss: 0.00001388
2024-11-06 14:07:01,480 - INFO - Epoch [66/300], Batch [29/43], Training Loss: 0.00003323
2024-11-06 14:07:01,484 - INFO - Epoch [66/300], Batch [30/43], Training Loss: 0.00001288
2024-11-06 14:07:01,487 - INFO - Epoch [66/300], Batch [31/43], Training Loss: 0.00001819
2024-11-06 14:07:01,492 - INFO - Epoch [66/300], Batch [32/43], Training Loss: 0.00003519
2024-11-06 14:07:01,496 - INFO - Epoch [66/300], Batch [33/43], Training Loss: 0.00003231
2024-11-06 14:07:01,499 - INFO - Epoch [66/300], Batch [34/43], Training Loss: 0.00002209
2024-11-06 14:07:01,503 - INFO - Epoch [66/300], Batch [35/43], Training Loss: 0.00002828
2024-11-06 14:07:01,506 - INFO - Epoch [66/300], Batch [36/43], Training Loss: 0.00002750
2024-11-06 14:07:01,510 - INFO - Epoch [66/300], Batch [37/43], Training Loss: 0.00004103
2024-11-06 14:07:01,515 - INFO - Epoch [66/300], Batch [38/43], Training Loss: 0.00002127
2024-11-06 14:07:01,519 - INFO - Epoch [66/300], Batch [39/43], Training Loss: 0.00002959
2024-11-06 14:07:01,523 - INFO - Epoch [66/300], Batch [40/43], Training Loss: 0.00002430
2024-11-06 14:07:01,527 - INFO - Epoch [66/300], Batch [41/43], Training Loss: 0.00004126
2024-11-06 14:07:01,531 - INFO - Epoch [66/300], Batch [42/43], Training Loss: 0.00001040
2024-11-06 14:07:01,535 - INFO - Epoch [66/300], Batch [43/43], Training Loss: 0.00002714
2024-11-06 14:07:01,548 - INFO - Epoch [66/300], Average Training Loss: 0.00002415, Validation Loss: 0.00002584
2024-11-06 14:07:01,552 - INFO - Epoch [67/300], Batch [1/43], Training Loss: 0.00001891
2024-11-06 14:07:01,556 - INFO - Epoch [67/300], Batch [2/43], Training Loss: 0.00001741
2024-11-06 14:07:01,559 - INFO - Epoch [67/300], Batch [3/43], Training Loss: 0.00003452
2024-11-06 14:07:01,563 - INFO - Epoch [67/300], Batch [4/43], Training Loss: 0.00002444
2024-11-06 14:07:01,567 - INFO - Epoch [67/300], Batch [5/43], Training Loss: 0.00001521
2024-11-06 14:07:01,571 - INFO - Epoch [67/300], Batch [6/43], Training Loss: 0.00002685
2024-11-06 14:07:01,577 - INFO - Epoch [67/300], Batch [7/43], Training Loss: 0.00003139
2024-11-06 14:07:01,581 - INFO - Epoch [67/300], Batch [8/43], Training Loss: 0.00002772
2024-11-06 14:07:01,586 - INFO - Epoch [67/300], Batch [9/43], Training Loss: 0.00003931
2024-11-06 14:07:01,591 - INFO - Epoch [67/300], Batch [10/43], Training Loss: 0.00001572
2024-11-06 14:07:01,595 - INFO - Epoch [67/300], Batch [11/43], Training Loss: 0.00001676
2024-11-06 14:07:01,599 - INFO - Epoch [67/300], Batch [12/43], Training Loss: 0.00001494
2024-11-06 14:07:01,603 - INFO - Epoch [67/300], Batch [13/43], Training Loss: 0.00003167
2024-11-06 14:07:01,608 - INFO - Epoch [67/300], Batch [14/43], Training Loss: 0.00008022
2024-11-06 14:07:01,612 - INFO - Epoch [67/300], Batch [15/43], Training Loss: 0.00002147
2024-11-06 14:07:01,616 - INFO - Epoch [67/300], Batch [16/43], Training Loss: 0.00002263
2024-11-06 14:07:01,620 - INFO - Epoch [67/300], Batch [17/43], Training Loss: 0.00002013
2024-11-06 14:07:01,624 - INFO - Epoch [67/300], Batch [18/43], Training Loss: 0.00002780
2024-11-06 14:07:01,629 - INFO - Epoch [67/300], Batch [19/43], Training Loss: 0.00001412
2024-11-06 14:07:01,634 - INFO - Epoch [67/300], Batch [20/43], Training Loss: 0.00003593
2024-11-06 14:07:01,638 - INFO - Epoch [67/300], Batch [21/43], Training Loss: 0.00002775
2024-11-06 14:07:01,642 - INFO - Epoch [67/300], Batch [22/43], Training Loss: 0.00001713
2024-11-06 14:07:01,648 - INFO - Epoch [67/300], Batch [23/43], Training Loss: 0.00001321
2024-11-06 14:07:01,652 - INFO - Epoch [67/300], Batch [24/43], Training Loss: 0.00002156
2024-11-06 14:07:01,657 - INFO - Epoch [67/300], Batch [25/43], Training Loss: 0.00001512
2024-11-06 14:07:01,661 - INFO - Epoch [67/300], Batch [26/43], Training Loss: 0.00001662
2024-11-06 14:07:01,665 - INFO - Epoch [67/300], Batch [27/43], Training Loss: 0.00003049
2024-11-06 14:07:01,669 - INFO - Epoch [67/300], Batch [28/43], Training Loss: 0.00001411
2024-11-06 14:07:01,673 - INFO - Epoch [67/300], Batch [29/43], Training Loss: 0.00001325
2024-11-06 14:07:01,677 - INFO - Epoch [67/300], Batch [30/43], Training Loss: 0.00004597
2024-11-06 14:07:01,681 - INFO - Epoch [67/300], Batch [31/43], Training Loss: 0.00001925
2024-11-06 14:07:01,685 - INFO - Epoch [67/300], Batch [32/43], Training Loss: 0.00002598
2024-11-06 14:07:01,689 - INFO - Epoch [67/300], Batch [33/43], Training Loss: 0.00003154
2024-11-06 14:07:01,693 - INFO - Epoch [67/300], Batch [34/43], Training Loss: 0.00001634
2024-11-06 14:07:01,698 - INFO - Epoch [67/300], Batch [35/43], Training Loss: 0.00002030
2024-11-06 14:07:01,702 - INFO - Epoch [67/300], Batch [36/43], Training Loss: 0.00001465
2024-11-06 14:07:01,707 - INFO - Epoch [67/300], Batch [37/43], Training Loss: 0.00003835
2024-11-06 14:07:01,711 - INFO - Epoch [67/300], Batch [38/43], Training Loss: 0.00003213
2024-11-06 14:07:01,717 - INFO - Epoch [67/300], Batch [39/43], Training Loss: 0.00001402
2024-11-06 14:07:01,721 - INFO - Epoch [67/300], Batch [40/43], Training Loss: 0.00002432
2024-11-06 14:07:01,726 - INFO - Epoch [67/300], Batch [41/43], Training Loss: 0.00001721
2024-11-06 14:07:01,732 - INFO - Epoch [67/300], Batch [42/43], Training Loss: 0.00002349
2024-11-06 14:07:01,737 - INFO - Epoch [67/300], Batch [43/43], Training Loss: 0.00003181
2024-11-06 14:07:01,750 - INFO - Epoch [67/300], Average Training Loss: 0.00002469, Validation Loss: 0.00002597
2024-11-06 14:07:01,754 - INFO - Epoch [68/300], Batch [1/43], Training Loss: 0.00002429
2024-11-06 14:07:01,759 - INFO - Epoch [68/300], Batch [2/43], Training Loss: 0.00002254
2024-11-06 14:07:01,764 - INFO - Epoch [68/300], Batch [3/43], Training Loss: 0.00001977
2024-11-06 14:07:01,769 - INFO - Epoch [68/300], Batch [4/43], Training Loss: 0.00001327
2024-11-06 14:07:01,773 - INFO - Epoch [68/300], Batch [5/43], Training Loss: 0.00002634
2024-11-06 14:07:01,778 - INFO - Epoch [68/300], Batch [6/43], Training Loss: 0.00001186
2024-11-06 14:07:01,783 - INFO - Epoch [68/300], Batch [7/43], Training Loss: 0.00001129
2024-11-06 14:07:01,787 - INFO - Epoch [68/300], Batch [8/43], Training Loss: 0.00002289
2024-11-06 14:07:01,791 - INFO - Epoch [68/300], Batch [9/43], Training Loss: 0.00002882
2024-11-06 14:07:01,795 - INFO - Epoch [68/300], Batch [10/43], Training Loss: 0.00002244
2024-11-06 14:07:01,799 - INFO - Epoch [68/300], Batch [11/43], Training Loss: 0.00006249
2024-11-06 14:07:01,803 - INFO - Epoch [68/300], Batch [12/43], Training Loss: 0.00001551
2024-11-06 14:07:01,807 - INFO - Epoch [68/300], Batch [13/43], Training Loss: 0.00002453
2024-11-06 14:07:01,810 - INFO - Epoch [68/300], Batch [14/43], Training Loss: 0.00003572
2024-11-06 14:07:01,815 - INFO - Epoch [68/300], Batch [15/43], Training Loss: 0.00004861
2024-11-06 14:07:01,819 - INFO - Epoch [68/300], Batch [16/43], Training Loss: 0.00001927
2024-11-06 14:07:01,823 - INFO - Epoch [68/300], Batch [17/43], Training Loss: 0.00001689
2024-11-06 14:07:01,826 - INFO - Epoch [68/300], Batch [18/43], Training Loss: 0.00000974
2024-11-06 14:07:01,832 - INFO - Epoch [68/300], Batch [19/43], Training Loss: 0.00002188
2024-11-06 14:07:01,837 - INFO - Epoch [68/300], Batch [20/43], Training Loss: 0.00001420
2024-11-06 14:07:01,842 - INFO - Epoch [68/300], Batch [21/43], Training Loss: 0.00001676
2024-11-06 14:07:01,845 - INFO - Epoch [68/300], Batch [22/43], Training Loss: 0.00002836
2024-11-06 14:07:01,849 - INFO - Epoch [68/300], Batch [23/43], Training Loss: 0.00001505
2024-11-06 14:07:01,853 - INFO - Epoch [68/300], Batch [24/43], Training Loss: 0.00003741
2024-11-06 14:07:01,857 - INFO - Epoch [68/300], Batch [25/43], Training Loss: 0.00001766
2024-11-06 14:07:01,861 - INFO - Epoch [68/300], Batch [26/43], Training Loss: 0.00001033
2024-11-06 14:07:01,867 - INFO - Epoch [68/300], Batch [27/43], Training Loss: 0.00002712
2024-11-06 14:07:01,872 - INFO - Epoch [68/300], Batch [28/43], Training Loss: 0.00002058
2024-11-06 14:07:01,875 - INFO - Epoch [68/300], Batch [29/43], Training Loss: 0.00002935
2024-11-06 14:07:01,879 - INFO - Epoch [68/300], Batch [30/43], Training Loss: 0.00001701
2024-11-06 14:07:01,882 - INFO - Epoch [68/300], Batch [31/43], Training Loss: 0.00002949
2024-11-06 14:07:01,886 - INFO - Epoch [68/300], Batch [32/43], Training Loss: 0.00001743
2024-11-06 14:07:01,889 - INFO - Epoch [68/300], Batch [33/43], Training Loss: 0.00003195
2024-11-06 14:07:01,892 - INFO - Epoch [68/300], Batch [34/43], Training Loss: 0.00001888
2024-11-06 14:07:01,896 - INFO - Epoch [68/300], Batch [35/43], Training Loss: 0.00001763
2024-11-06 14:07:01,901 - INFO - Epoch [68/300], Batch [36/43], Training Loss: 0.00002270
2024-11-06 14:07:01,905 - INFO - Epoch [68/300], Batch [37/43], Training Loss: 0.00001672
2024-11-06 14:07:01,910 - INFO - Epoch [68/300], Batch [38/43], Training Loss: 0.00003267
2024-11-06 14:07:01,915 - INFO - Epoch [68/300], Batch [39/43], Training Loss: 0.00002205
2024-11-06 14:07:01,921 - INFO - Epoch [68/300], Batch [40/43], Training Loss: 0.00001274
2024-11-06 14:07:01,925 - INFO - Epoch [68/300], Batch [41/43], Training Loss: 0.00001378
2024-11-06 14:07:01,930 - INFO - Epoch [68/300], Batch [42/43], Training Loss: 0.00002361
2024-11-06 14:07:01,934 - INFO - Epoch [68/300], Batch [43/43], Training Loss: 0.00001318
2024-11-06 14:07:01,947 - INFO - Epoch [68/300], Average Training Loss: 0.00002244, Validation Loss: 0.00002599
2024-11-06 14:07:01,951 - INFO - Epoch [69/300], Batch [1/43], Training Loss: 0.00002917
2024-11-06 14:07:01,955 - INFO - Epoch [69/300], Batch [2/43], Training Loss: 0.00001109
2024-11-06 14:07:01,959 - INFO - Epoch [69/300], Batch [3/43], Training Loss: 0.00003211
2024-11-06 14:07:01,963 - INFO - Epoch [69/300], Batch [4/43], Training Loss: 0.00003159
2024-11-06 14:07:01,966 - INFO - Epoch [69/300], Batch [5/43], Training Loss: 0.00001587
2024-11-06 14:07:01,970 - INFO - Epoch [69/300], Batch [6/43], Training Loss: 0.00001735
2024-11-06 14:07:01,973 - INFO - Epoch [69/300], Batch [7/43], Training Loss: 0.00002145
2024-11-06 14:07:01,977 - INFO - Epoch [69/300], Batch [8/43], Training Loss: 0.00002490
2024-11-06 14:07:01,980 - INFO - Epoch [69/300], Batch [9/43], Training Loss: 0.00001459
2024-11-06 14:07:01,983 - INFO - Epoch [69/300], Batch [10/43], Training Loss: 0.00004459
2024-11-06 14:07:01,987 - INFO - Epoch [69/300], Batch [11/43], Training Loss: 0.00002168
2024-11-06 14:07:01,990 - INFO - Epoch [69/300], Batch [12/43], Training Loss: 0.00002728
2024-11-06 14:07:01,995 - INFO - Epoch [69/300], Batch [13/43], Training Loss: 0.00001178
2024-11-06 14:07:01,999 - INFO - Epoch [69/300], Batch [14/43], Training Loss: 0.00001419
2024-11-06 14:07:02,003 - INFO - Epoch [69/300], Batch [15/43], Training Loss: 0.00004096
2024-11-06 14:07:02,007 - INFO - Epoch [69/300], Batch [16/43], Training Loss: 0.00003789
2024-11-06 14:07:02,011 - INFO - Epoch [69/300], Batch [17/43], Training Loss: 0.00002246
2024-11-06 14:07:02,014 - INFO - Epoch [69/300], Batch [18/43], Training Loss: 0.00001209
2024-11-06 14:07:02,017 - INFO - Epoch [69/300], Batch [19/43], Training Loss: 0.00001355
2024-11-06 14:07:02,021 - INFO - Epoch [69/300], Batch [20/43], Training Loss: 0.00001366
2024-11-06 14:07:02,025 - INFO - Epoch [69/300], Batch [21/43], Training Loss: 0.00003575
2024-11-06 14:07:02,029 - INFO - Epoch [69/300], Batch [22/43], Training Loss: 0.00000629
2024-11-06 14:07:02,033 - INFO - Epoch [69/300], Batch [23/43], Training Loss: 0.00000710
2024-11-06 14:07:02,037 - INFO - Epoch [69/300], Batch [24/43], Training Loss: 0.00002896
2024-11-06 14:07:02,041 - INFO - Epoch [69/300], Batch [25/43], Training Loss: 0.00002557
2024-11-06 14:07:02,044 - INFO - Epoch [69/300], Batch [26/43], Training Loss: 0.00002620
2024-11-06 14:07:02,048 - INFO - Epoch [69/300], Batch [27/43], Training Loss: 0.00001378
2024-11-06 14:07:02,053 - INFO - Epoch [69/300], Batch [28/43], Training Loss: 0.00002539
2024-11-06 14:07:02,056 - INFO - Epoch [69/300], Batch [29/43], Training Loss: 0.00001210
2024-11-06 14:07:02,060 - INFO - Epoch [69/300], Batch [30/43], Training Loss: 0.00002643
2024-11-06 14:07:02,063 - INFO - Epoch [69/300], Batch [31/43], Training Loss: 0.00001240
2024-11-06 14:07:02,067 - INFO - Epoch [69/300], Batch [32/43], Training Loss: 0.00001009
2024-11-06 14:07:02,071 - INFO - Epoch [69/300], Batch [33/43], Training Loss: 0.00002037
2024-11-06 14:07:02,075 - INFO - Epoch [69/300], Batch [34/43], Training Loss: 0.00001522
2024-11-06 14:07:02,080 - INFO - Epoch [69/300], Batch [35/43], Training Loss: 0.00001454
2024-11-06 14:07:02,084 - INFO - Epoch [69/300], Batch [36/43], Training Loss: 0.00001371
2024-11-06 14:07:02,089 - INFO - Epoch [69/300], Batch [37/43], Training Loss: 0.00001964
2024-11-06 14:07:02,093 - INFO - Epoch [69/300], Batch [38/43], Training Loss: 0.00001788
2024-11-06 14:07:02,098 - INFO - Epoch [69/300], Batch [39/43], Training Loss: 0.00002231
2024-11-06 14:07:02,103 - INFO - Epoch [69/300], Batch [40/43], Training Loss: 0.00001348
2024-11-06 14:07:02,107 - INFO - Epoch [69/300], Batch [41/43], Training Loss: 0.00002325
2024-11-06 14:07:02,111 - INFO - Epoch [69/300], Batch [42/43], Training Loss: 0.00002119
2024-11-06 14:07:02,115 - INFO - Epoch [69/300], Batch [43/43], Training Loss: 0.00005465
2024-11-06 14:07:02,125 - INFO - Epoch [69/300], Average Training Loss: 0.00002150, Validation Loss: 0.00002562
2024-11-06 14:07:02,129 - INFO - Epoch [70/300], Batch [1/43], Training Loss: 0.00000985
2024-11-06 14:07:02,134 - INFO - Epoch [70/300], Batch [2/43], Training Loss: 0.00001326
2024-11-06 14:07:02,138 - INFO - Epoch [70/300], Batch [3/43], Training Loss: 0.00001529
2024-11-06 14:07:02,141 - INFO - Epoch [70/300], Batch [4/43], Training Loss: 0.00002762
2024-11-06 14:07:02,146 - INFO - Epoch [70/300], Batch [5/43], Training Loss: 0.00002808
2024-11-06 14:07:02,151 - INFO - Epoch [70/300], Batch [6/43], Training Loss: 0.00004258
2024-11-06 14:07:02,154 - INFO - Epoch [70/300], Batch [7/43], Training Loss: 0.00001033
2024-11-06 14:07:02,158 - INFO - Epoch [70/300], Batch [8/43], Training Loss: 0.00001359
2024-11-06 14:07:02,161 - INFO - Epoch [70/300], Batch [9/43], Training Loss: 0.00001770
2024-11-06 14:07:02,165 - INFO - Epoch [70/300], Batch [10/43], Training Loss: 0.00003198
2024-11-06 14:07:02,170 - INFO - Epoch [70/300], Batch [11/43], Training Loss: 0.00003449
2024-11-06 14:07:02,173 - INFO - Epoch [70/300], Batch [12/43], Training Loss: 0.00002868
2024-11-06 14:07:02,176 - INFO - Epoch [70/300], Batch [13/43], Training Loss: 0.00002254
2024-11-06 14:07:02,180 - INFO - Epoch [70/300], Batch [14/43], Training Loss: 0.00002517
2024-11-06 14:07:02,184 - INFO - Epoch [70/300], Batch [15/43], Training Loss: 0.00003061
2024-11-06 14:07:02,188 - INFO - Epoch [70/300], Batch [16/43], Training Loss: 0.00001247
2024-11-06 14:07:02,192 - INFO - Epoch [70/300], Batch [17/43], Training Loss: 0.00003064
2024-11-06 14:07:02,196 - INFO - Epoch [70/300], Batch [18/43], Training Loss: 0.00003533
2024-11-06 14:07:02,200 - INFO - Epoch [70/300], Batch [19/43], Training Loss: 0.00001708
2024-11-06 14:07:02,204 - INFO - Epoch [70/300], Batch [20/43], Training Loss: 0.00001576
2024-11-06 14:07:02,207 - INFO - Epoch [70/300], Batch [21/43], Training Loss: 0.00002323
2024-11-06 14:07:02,211 - INFO - Epoch [70/300], Batch [22/43], Training Loss: 0.00002892
2024-11-06 14:07:02,215 - INFO - Epoch [70/300], Batch [23/43], Training Loss: 0.00002942
2024-11-06 14:07:02,219 - INFO - Epoch [70/300], Batch [24/43], Training Loss: 0.00003337
2024-11-06 14:07:02,223 - INFO - Epoch [70/300], Batch [25/43], Training Loss: 0.00001123
2024-11-06 14:07:02,226 - INFO - Epoch [70/300], Batch [26/43], Training Loss: 0.00001596
2024-11-06 14:07:02,230 - INFO - Epoch [70/300], Batch [27/43], Training Loss: 0.00002196
2024-11-06 14:07:02,234 - INFO - Epoch [70/300], Batch [28/43], Training Loss: 0.00002103
2024-11-06 14:07:02,237 - INFO - Epoch [70/300], Batch [29/43], Training Loss: 0.00001161
2024-11-06 14:07:02,242 - INFO - Epoch [70/300], Batch [30/43], Training Loss: 0.00003640
2024-11-06 14:07:02,247 - INFO - Epoch [70/300], Batch [31/43], Training Loss: 0.00002939
2024-11-06 14:07:02,251 - INFO - Epoch [70/300], Batch [32/43], Training Loss: 0.00001864
2024-11-06 14:07:02,255 - INFO - Epoch [70/300], Batch [33/43], Training Loss: 0.00001403
2024-11-06 14:07:02,259 - INFO - Epoch [70/300], Batch [34/43], Training Loss: 0.00002727
2024-11-06 14:07:02,264 - INFO - Epoch [70/300], Batch [35/43], Training Loss: 0.00001564
2024-11-06 14:07:02,268 - INFO - Epoch [70/300], Batch [36/43], Training Loss: 0.00001634
2024-11-06 14:07:02,272 - INFO - Epoch [70/300], Batch [37/43], Training Loss: 0.00001709
2024-11-06 14:07:02,276 - INFO - Epoch [70/300], Batch [38/43], Training Loss: 0.00001750
2024-11-06 14:07:02,280 - INFO - Epoch [70/300], Batch [39/43], Training Loss: 0.00000702
2024-11-06 14:07:02,284 - INFO - Epoch [70/300], Batch [40/43], Training Loss: 0.00001801
2024-11-06 14:07:02,290 - INFO - Epoch [70/300], Batch [41/43], Training Loss: 0.00001575
2024-11-06 14:07:02,294 - INFO - Epoch [70/300], Batch [42/43], Training Loss: 0.00002259
2024-11-06 14:07:02,298 - INFO - Epoch [70/300], Batch [43/43], Training Loss: 0.00002791
2024-11-06 14:07:02,311 - INFO - Epoch [70/300], Average Training Loss: 0.00002194, Validation Loss: 0.00002541
2024-11-06 14:07:02,315 - INFO - Epoch [71/300], Batch [1/43], Training Loss: 0.00001248
2024-11-06 14:07:02,319 - INFO - Epoch [71/300], Batch [2/43], Training Loss: 0.00004611
2024-11-06 14:07:02,322 - INFO - Epoch [71/300], Batch [3/43], Training Loss: 0.00003614
2024-11-06 14:07:02,325 - INFO - Epoch [71/300], Batch [4/43], Training Loss: 0.00003095
2024-11-06 14:07:02,329 - INFO - Epoch [71/300], Batch [5/43], Training Loss: 0.00002622
2024-11-06 14:07:02,333 - INFO - Epoch [71/300], Batch [6/43], Training Loss: 0.00002824
2024-11-06 14:07:02,338 - INFO - Epoch [71/300], Batch [7/43], Training Loss: 0.00001251
2024-11-06 14:07:02,341 - INFO - Epoch [71/300], Batch [8/43], Training Loss: 0.00000999
2024-11-06 14:07:02,344 - INFO - Epoch [71/300], Batch [9/43], Training Loss: 0.00001418
2024-11-06 14:07:02,347 - INFO - Epoch [71/300], Batch [10/43], Training Loss: 0.00001954
2024-11-06 14:07:02,352 - INFO - Epoch [71/300], Batch [11/43], Training Loss: 0.00001410
2024-11-06 14:07:02,355 - INFO - Epoch [71/300], Batch [12/43], Training Loss: 0.00002128
2024-11-06 14:07:02,359 - INFO - Epoch [71/300], Batch [13/43], Training Loss: 0.00003435
2024-11-06 14:07:02,362 - INFO - Epoch [71/300], Batch [14/43], Training Loss: 0.00002336
2024-11-06 14:07:02,367 - INFO - Epoch [71/300], Batch [15/43], Training Loss: 0.00003471
2024-11-06 14:07:02,371 - INFO - Epoch [71/300], Batch [16/43], Training Loss: 0.00001947
2024-11-06 14:07:02,374 - INFO - Epoch [71/300], Batch [17/43], Training Loss: 0.00002338
2024-11-06 14:07:02,377 - INFO - Epoch [71/300], Batch [18/43], Training Loss: 0.00001248
2024-11-06 14:07:02,380 - INFO - Epoch [71/300], Batch [19/43], Training Loss: 0.00001863
2024-11-06 14:07:02,384 - INFO - Epoch [71/300], Batch [20/43], Training Loss: 0.00003241
2024-11-06 14:07:02,388 - INFO - Epoch [71/300], Batch [21/43], Training Loss: 0.00002206
2024-11-06 14:07:02,392 - INFO - Epoch [71/300], Batch [22/43], Training Loss: 0.00002132
2024-11-06 14:07:02,396 - INFO - Epoch [71/300], Batch [23/43], Training Loss: 0.00001024
2024-11-06 14:07:02,400 - INFO - Epoch [71/300], Batch [24/43], Training Loss: 0.00002569
2024-11-06 14:07:02,403 - INFO - Epoch [71/300], Batch [25/43], Training Loss: 0.00002863
2024-11-06 14:07:02,406 - INFO - Epoch [71/300], Batch [26/43], Training Loss: 0.00004139
2024-11-06 14:07:02,410 - INFO - Epoch [71/300], Batch [27/43], Training Loss: 0.00002031
2024-11-06 14:07:02,414 - INFO - Epoch [71/300], Batch [28/43], Training Loss: 0.00004204
2024-11-06 14:07:02,417 - INFO - Epoch [71/300], Batch [29/43], Training Loss: 0.00008095
2024-11-06 14:07:02,420 - INFO - Epoch [71/300], Batch [30/43], Training Loss: 0.00002252
2024-11-06 14:07:02,424 - INFO - Epoch [71/300], Batch [31/43], Training Loss: 0.00002293
2024-11-06 14:07:02,429 - INFO - Epoch [71/300], Batch [32/43], Training Loss: 0.00005012
2024-11-06 14:07:02,433 - INFO - Epoch [71/300], Batch [33/43], Training Loss: 0.00002755
2024-11-06 14:07:02,436 - INFO - Epoch [71/300], Batch [34/43], Training Loss: 0.00001631
2024-11-06 14:07:02,440 - INFO - Epoch [71/300], Batch [35/43], Training Loss: 0.00001511
2024-11-06 14:07:02,444 - INFO - Epoch [71/300], Batch [36/43], Training Loss: 0.00001804
2024-11-06 14:07:02,448 - INFO - Epoch [71/300], Batch [37/43], Training Loss: 0.00001197
2024-11-06 14:07:02,452 - INFO - Epoch [71/300], Batch [38/43], Training Loss: 0.00001748
2024-11-06 14:07:02,456 - INFO - Epoch [71/300], Batch [39/43], Training Loss: 0.00001560
2024-11-06 14:07:02,459 - INFO - Epoch [71/300], Batch [40/43], Training Loss: 0.00001906
2024-11-06 14:07:02,463 - INFO - Epoch [71/300], Batch [41/43], Training Loss: 0.00001038
2024-11-06 14:07:02,467 - INFO - Epoch [71/300], Batch [42/43], Training Loss: 0.00003853
2024-11-06 14:07:02,471 - INFO - Epoch [71/300], Batch [43/43], Training Loss: 0.00002136
2024-11-06 14:07:02,484 - INFO - Epoch [71/300], Average Training Loss: 0.00002489, Validation Loss: 0.00002584
2024-11-06 14:07:02,488 - INFO - Epoch [72/300], Batch [1/43], Training Loss: 0.00001088
2024-11-06 14:07:02,492 - INFO - Epoch [72/300], Batch [2/43], Training Loss: 0.00003589
2024-11-06 14:07:02,496 - INFO - Epoch [72/300], Batch [3/43], Training Loss: 0.00001905
2024-11-06 14:07:02,499 - INFO - Epoch [72/300], Batch [4/43], Training Loss: 0.00002491
2024-11-06 14:07:02,503 - INFO - Epoch [72/300], Batch [5/43], Training Loss: 0.00001257
2024-11-06 14:07:02,507 - INFO - Epoch [72/300], Batch [6/43], Training Loss: 0.00001694
2024-11-06 14:07:02,511 - INFO - Epoch [72/300], Batch [7/43], Training Loss: 0.00002227
2024-11-06 14:07:02,514 - INFO - Epoch [72/300], Batch [8/43], Training Loss: 0.00002560
2024-11-06 14:07:02,517 - INFO - Epoch [72/300], Batch [9/43], Training Loss: 0.00001882
2024-11-06 14:07:02,521 - INFO - Epoch [72/300], Batch [10/43], Training Loss: 0.00005953
2024-11-06 14:07:02,526 - INFO - Epoch [72/300], Batch [11/43], Training Loss: 0.00001120
2024-11-06 14:07:02,529 - INFO - Epoch [72/300], Batch [12/43], Training Loss: 0.00002914
2024-11-06 14:07:02,533 - INFO - Epoch [72/300], Batch [13/43], Training Loss: 0.00002690
2024-11-06 14:07:02,536 - INFO - Epoch [72/300], Batch [14/43], Training Loss: 0.00001677
2024-11-06 14:07:02,539 - INFO - Epoch [72/300], Batch [15/43], Training Loss: 0.00001116
2024-11-06 14:07:02,542 - INFO - Epoch [72/300], Batch [16/43], Training Loss: 0.00001705
2024-11-06 14:07:02,546 - INFO - Epoch [72/300], Batch [17/43], Training Loss: 0.00002281
2024-11-06 14:07:02,549 - INFO - Epoch [72/300], Batch [18/43], Training Loss: 0.00003470
2024-11-06 14:07:02,552 - INFO - Epoch [72/300], Batch [19/43], Training Loss: 0.00001991
2024-11-06 14:07:02,556 - INFO - Epoch [72/300], Batch [20/43], Training Loss: 0.00002022
2024-11-06 14:07:02,559 - INFO - Epoch [72/300], Batch [21/43], Training Loss: 0.00001144
2024-11-06 14:07:02,562 - INFO - Epoch [72/300], Batch [22/43], Training Loss: 0.00002575
2024-11-06 14:07:02,565 - INFO - Epoch [72/300], Batch [23/43], Training Loss: 0.00002244
2024-11-06 14:07:02,568 - INFO - Epoch [72/300], Batch [24/43], Training Loss: 0.00001867
2024-11-06 14:07:02,571 - INFO - Epoch [72/300], Batch [25/43], Training Loss: 0.00001738
2024-11-06 14:07:02,574 - INFO - Epoch [72/300], Batch [26/43], Training Loss: 0.00003699
2024-11-06 14:07:02,577 - INFO - Epoch [72/300], Batch [27/43], Training Loss: 0.00002823
2024-11-06 14:07:02,579 - INFO - Epoch [72/300], Batch [28/43], Training Loss: 0.00002729
2024-11-06 14:07:02,583 - INFO - Epoch [72/300], Batch [29/43], Training Loss: 0.00004060
2024-11-06 14:07:02,587 - INFO - Epoch [72/300], Batch [30/43], Training Loss: 0.00005963
2024-11-06 14:07:02,591 - INFO - Epoch [72/300], Batch [31/43], Training Loss: 0.00002002
2024-11-06 14:07:02,594 - INFO - Epoch [72/300], Batch [32/43], Training Loss: 0.00001863
2024-11-06 14:07:02,598 - INFO - Epoch [72/300], Batch [33/43], Training Loss: 0.00005682
2024-11-06 14:07:02,601 - INFO - Epoch [72/300], Batch [34/43], Training Loss: 0.00008533
2024-11-06 14:07:02,604 - INFO - Epoch [72/300], Batch [35/43], Training Loss: 0.00002441
2024-11-06 14:07:02,607 - INFO - Epoch [72/300], Batch [36/43], Training Loss: 0.00003870
2024-11-06 14:07:02,611 - INFO - Epoch [72/300], Batch [37/43], Training Loss: 0.00009549
2024-11-06 14:07:02,614 - INFO - Epoch [72/300], Batch [38/43], Training Loss: 0.00006652
2024-11-06 14:07:02,618 - INFO - Epoch [72/300], Batch [39/43], Training Loss: 0.00002632
2024-11-06 14:07:02,622 - INFO - Epoch [72/300], Batch [40/43], Training Loss: 0.00008795
2024-11-06 14:07:02,625 - INFO - Epoch [72/300], Batch [41/43], Training Loss: 0.00003910
2024-11-06 14:07:02,629 - INFO - Epoch [72/300], Batch [42/43], Training Loss: 0.00001460
2024-11-06 14:07:02,633 - INFO - Epoch [72/300], Batch [43/43], Training Loss: 0.00001559
2024-11-06 14:07:02,647 - INFO - Epoch [72/300], Average Training Loss: 0.00003103, Validation Loss: 0.00003949
2024-11-06 14:07:02,651 - INFO - Epoch [73/300], Batch [1/43], Training Loss: 0.00004947
2024-11-06 14:07:02,655 - INFO - Epoch [73/300], Batch [2/43], Training Loss: 0.00004470
2024-11-06 14:07:02,659 - INFO - Epoch [73/300], Batch [3/43], Training Loss: 0.00001412
2024-11-06 14:07:02,662 - INFO - Epoch [73/300], Batch [4/43], Training Loss: 0.00002189
2024-11-06 14:07:02,666 - INFO - Epoch [73/300], Batch [5/43], Training Loss: 0.00003241
2024-11-06 14:07:02,669 - INFO - Epoch [73/300], Batch [6/43], Training Loss: 0.00002586
2024-11-06 14:07:02,672 - INFO - Epoch [73/300], Batch [7/43], Training Loss: 0.00000912
2024-11-06 14:07:02,675 - INFO - Epoch [73/300], Batch [8/43], Training Loss: 0.00002672
2024-11-06 14:07:02,679 - INFO - Epoch [73/300], Batch [9/43], Training Loss: 0.00003485
2024-11-06 14:07:02,682 - INFO - Epoch [73/300], Batch [10/43], Training Loss: 0.00001815
2024-11-06 14:07:02,685 - INFO - Epoch [73/300], Batch [11/43], Training Loss: 0.00001760
2024-11-06 14:07:02,688 - INFO - Epoch [73/300], Batch [12/43], Training Loss: 0.00001330
2024-11-06 14:07:02,691 - INFO - Epoch [73/300], Batch [13/43], Training Loss: 0.00003074
2024-11-06 14:07:02,694 - INFO - Epoch [73/300], Batch [14/43], Training Loss: 0.00001868
2024-11-06 14:07:02,697 - INFO - Epoch [73/300], Batch [15/43], Training Loss: 0.00001207
2024-11-06 14:07:02,700 - INFO - Epoch [73/300], Batch [16/43], Training Loss: 0.00001699
2024-11-06 14:07:02,702 - INFO - Epoch [73/300], Batch [17/43], Training Loss: 0.00002921
2024-11-06 14:07:02,705 - INFO - Epoch [73/300], Batch [18/43], Training Loss: 0.00003439
2024-11-06 14:07:02,709 - INFO - Epoch [73/300], Batch [19/43], Training Loss: 0.00001663
2024-11-06 14:07:02,712 - INFO - Epoch [73/300], Batch [20/43], Training Loss: 0.00003117
2024-11-06 14:07:02,715 - INFO - Epoch [73/300], Batch [21/43], Training Loss: 0.00003319
2024-11-06 14:07:02,717 - INFO - Epoch [73/300], Batch [22/43], Training Loss: 0.00001898
2024-11-06 14:07:02,720 - INFO - Epoch [73/300], Batch [23/43], Training Loss: 0.00001705
2024-11-06 14:07:02,723 - INFO - Epoch [73/300], Batch [24/43], Training Loss: 0.00002514
2024-11-06 14:07:02,726 - INFO - Epoch [73/300], Batch [25/43], Training Loss: 0.00001458
2024-11-06 14:07:02,729 - INFO - Epoch [73/300], Batch [26/43], Training Loss: 0.00003468
2024-11-06 14:07:02,731 - INFO - Epoch [73/300], Batch [27/43], Training Loss: 0.00001059
2024-11-06 14:07:02,734 - INFO - Epoch [73/300], Batch [28/43], Training Loss: 0.00001941
2024-11-06 14:07:02,737 - INFO - Epoch [73/300], Batch [29/43], Training Loss: 0.00001444
2024-11-06 14:07:02,741 - INFO - Epoch [73/300], Batch [30/43], Training Loss: 0.00004217
2024-11-06 14:07:02,744 - INFO - Epoch [73/300], Batch [31/43], Training Loss: 0.00002068
2024-11-06 14:07:02,747 - INFO - Epoch [73/300], Batch [32/43], Training Loss: 0.00002203
2024-11-06 14:07:02,750 - INFO - Epoch [73/300], Batch [33/43], Training Loss: 0.00003150
2024-11-06 14:07:02,753 - INFO - Epoch [73/300], Batch [34/43], Training Loss: 0.00001865
2024-11-06 14:07:02,758 - INFO - Epoch [73/300], Batch [35/43], Training Loss: 0.00001816
2024-11-06 14:07:02,761 - INFO - Epoch [73/300], Batch [36/43], Training Loss: 0.00002868
2024-11-06 14:07:02,765 - INFO - Epoch [73/300], Batch [37/43], Training Loss: 0.00002497
2024-11-06 14:07:02,769 - INFO - Epoch [73/300], Batch [38/43], Training Loss: 0.00001714
2024-11-06 14:07:02,772 - INFO - Epoch [73/300], Batch [39/43], Training Loss: 0.00002464
2024-11-06 14:07:02,776 - INFO - Epoch [73/300], Batch [40/43], Training Loss: 0.00002345
2024-11-06 14:07:02,780 - INFO - Epoch [73/300], Batch [41/43], Training Loss: 0.00004453
2024-11-06 14:07:02,783 - INFO - Epoch [73/300], Batch [42/43], Training Loss: 0.00001505
2024-11-06 14:07:02,787 - INFO - Epoch [73/300], Batch [43/43], Training Loss: 0.00003037
2024-11-06 14:07:02,799 - INFO - Epoch [73/300], Average Training Loss: 0.00002438, Validation Loss: 0.00003664
2024-11-06 14:07:02,804 - INFO - Epoch [74/300], Batch [1/43], Training Loss: 0.00005816
2024-11-06 14:07:02,808 - INFO - Epoch [74/300], Batch [2/43], Training Loss: 0.00002747
2024-11-06 14:07:02,812 - INFO - Epoch [74/300], Batch [3/43], Training Loss: 0.00003126
2024-11-06 14:07:02,816 - INFO - Epoch [74/300], Batch [4/43], Training Loss: 0.00001709
2024-11-06 14:07:02,820 - INFO - Epoch [74/300], Batch [5/43], Training Loss: 0.00002354
2024-11-06 14:07:02,823 - INFO - Epoch [74/300], Batch [6/43], Training Loss: 0.00000796
2024-11-06 14:07:02,827 - INFO - Epoch [74/300], Batch [7/43], Training Loss: 0.00001232
2024-11-06 14:07:02,831 - INFO - Epoch [74/300], Batch [8/43], Training Loss: 0.00001699
2024-11-06 14:07:02,834 - INFO - Epoch [74/300], Batch [9/43], Training Loss: 0.00001089
2024-11-06 14:07:02,837 - INFO - Epoch [74/300], Batch [10/43], Training Loss: 0.00001155
2024-11-06 14:07:02,841 - INFO - Epoch [74/300], Batch [11/43], Training Loss: 0.00000614
2024-11-06 14:07:02,844 - INFO - Epoch [74/300], Batch [12/43], Training Loss: 0.00001724
2024-11-06 14:07:02,848 - INFO - Epoch [74/300], Batch [13/43], Training Loss: 0.00001954
2024-11-06 14:07:02,852 - INFO - Epoch [74/300], Batch [14/43], Training Loss: 0.00003223
2024-11-06 14:07:02,857 - INFO - Epoch [74/300], Batch [15/43], Training Loss: 0.00001944
2024-11-06 14:07:02,861 - INFO - Epoch [74/300], Batch [16/43], Training Loss: 0.00002215
2024-11-06 14:07:02,865 - INFO - Epoch [74/300], Batch [17/43], Training Loss: 0.00002265
2024-11-06 14:07:02,869 - INFO - Epoch [74/300], Batch [18/43], Training Loss: 0.00003735
2024-11-06 14:07:02,872 - INFO - Epoch [74/300], Batch [19/43], Training Loss: 0.00002362
2024-11-06 14:07:02,876 - INFO - Epoch [74/300], Batch [20/43], Training Loss: 0.00001707
2024-11-06 14:07:02,879 - INFO - Epoch [74/300], Batch [21/43], Training Loss: 0.00002290
2024-11-06 14:07:02,882 - INFO - Epoch [74/300], Batch [22/43], Training Loss: 0.00002744
2024-11-06 14:07:02,885 - INFO - Epoch [74/300], Batch [23/43], Training Loss: 0.00002147
2024-11-06 14:07:02,888 - INFO - Epoch [74/300], Batch [24/43], Training Loss: 0.00001640
2024-11-06 14:07:02,892 - INFO - Epoch [74/300], Batch [25/43], Training Loss: 0.00001893
2024-11-06 14:07:02,896 - INFO - Epoch [74/300], Batch [26/43], Training Loss: 0.00001076
2024-11-06 14:07:02,899 - INFO - Epoch [74/300], Batch [27/43], Training Loss: 0.00002714
2024-11-06 14:07:02,902 - INFO - Epoch [74/300], Batch [28/43], Training Loss: 0.00000948
2024-11-06 14:07:02,906 - INFO - Epoch [74/300], Batch [29/43], Training Loss: 0.00003151
2024-11-06 14:07:02,910 - INFO - Epoch [74/300], Batch [30/43], Training Loss: 0.00002543
2024-11-06 14:07:02,914 - INFO - Epoch [74/300], Batch [31/43], Training Loss: 0.00001971
2024-11-06 14:07:02,918 - INFO - Epoch [74/300], Batch [32/43], Training Loss: 0.00003710
2024-11-06 14:07:02,922 - INFO - Epoch [74/300], Batch [33/43], Training Loss: 0.00002890
2024-11-06 14:07:02,926 - INFO - Epoch [74/300], Batch [34/43], Training Loss: 0.00001574
2024-11-06 14:07:02,931 - INFO - Epoch [74/300], Batch [35/43], Training Loss: 0.00002116
2024-11-06 14:07:02,935 - INFO - Epoch [74/300], Batch [36/43], Training Loss: 0.00000616
2024-11-06 14:07:02,940 - INFO - Epoch [74/300], Batch [37/43], Training Loss: 0.00001288
2024-11-06 14:07:02,943 - INFO - Epoch [74/300], Batch [38/43], Training Loss: 0.00001569
2024-11-06 14:07:02,947 - INFO - Epoch [74/300], Batch [39/43], Training Loss: 0.00000851
2024-11-06 14:07:02,950 - INFO - Epoch [74/300], Batch [40/43], Training Loss: 0.00002322
2024-11-06 14:07:02,954 - INFO - Epoch [74/300], Batch [41/43], Training Loss: 0.00001410
2024-11-06 14:07:02,959 - INFO - Epoch [74/300], Batch [42/43], Training Loss: 0.00002431
2024-11-06 14:07:02,964 - INFO - Epoch [74/300], Batch [43/43], Training Loss: 0.00003507
2024-11-06 14:07:02,978 - INFO - Epoch [74/300], Average Training Loss: 0.00002113, Validation Loss: 0.00002470
2024-11-06 14:07:02,983 - INFO - Epoch [75/300], Batch [1/43], Training Loss: 0.00001932
2024-11-06 14:07:02,987 - INFO - Epoch [75/300], Batch [2/43], Training Loss: 0.00002590
2024-11-06 14:07:02,991 - INFO - Epoch [75/300], Batch [3/43], Training Loss: 0.00001335
2024-11-06 14:07:02,996 - INFO - Epoch [75/300], Batch [4/43], Training Loss: 0.00002910
2024-11-06 14:07:03,000 - INFO - Epoch [75/300], Batch [5/43], Training Loss: 0.00003049
2024-11-06 14:07:03,004 - INFO - Epoch [75/300], Batch [6/43], Training Loss: 0.00003250
2024-11-06 14:07:03,008 - INFO - Epoch [75/300], Batch [7/43], Training Loss: 0.00001384
2024-11-06 14:07:03,011 - INFO - Epoch [75/300], Batch [8/43], Training Loss: 0.00003324
2024-11-06 14:07:03,015 - INFO - Epoch [75/300], Batch [9/43], Training Loss: 0.00000692
2024-11-06 14:07:03,019 - INFO - Epoch [75/300], Batch [10/43], Training Loss: 0.00002044
2024-11-06 14:07:03,022 - INFO - Epoch [75/300], Batch [11/43], Training Loss: 0.00002680
2024-11-06 14:07:03,026 - INFO - Epoch [75/300], Batch [12/43], Training Loss: 0.00001767
2024-11-06 14:07:03,030 - INFO - Epoch [75/300], Batch [13/43], Training Loss: 0.00001271
2024-11-06 14:07:03,033 - INFO - Epoch [75/300], Batch [14/43], Training Loss: 0.00000851
2024-11-06 14:07:03,037 - INFO - Epoch [75/300], Batch [15/43], Training Loss: 0.00002710
2024-11-06 14:07:03,041 - INFO - Epoch [75/300], Batch [16/43], Training Loss: 0.00001492
2024-11-06 14:07:03,044 - INFO - Epoch [75/300], Batch [17/43], Training Loss: 0.00003170
2024-11-06 14:07:03,049 - INFO - Epoch [75/300], Batch [18/43], Training Loss: 0.00001220
2024-11-06 14:07:03,052 - INFO - Epoch [75/300], Batch [19/43], Training Loss: 0.00003248
2024-11-06 14:07:03,056 - INFO - Epoch [75/300], Batch [20/43], Training Loss: 0.00002131
2024-11-06 14:07:03,060 - INFO - Epoch [75/300], Batch [21/43], Training Loss: 0.00002079
2024-11-06 14:07:03,062 - INFO - Epoch [75/300], Batch [22/43], Training Loss: 0.00001593
2024-11-06 14:07:03,065 - INFO - Epoch [75/300], Batch [23/43], Training Loss: 0.00001811
2024-11-06 14:07:03,068 - INFO - Epoch [75/300], Batch [24/43], Training Loss: 0.00001818
2024-11-06 14:07:03,071 - INFO - Epoch [75/300], Batch [25/43], Training Loss: 0.00001008
2024-11-06 14:07:03,075 - INFO - Epoch [75/300], Batch [26/43], Training Loss: 0.00001924
2024-11-06 14:07:03,078 - INFO - Epoch [75/300], Batch [27/43], Training Loss: 0.00000605
2024-11-06 14:07:03,082 - INFO - Epoch [75/300], Batch [28/43], Training Loss: 0.00002151
2024-11-06 14:07:03,086 - INFO - Epoch [75/300], Batch [29/43], Training Loss: 0.00001889
2024-11-06 14:07:03,090 - INFO - Epoch [75/300], Batch [30/43], Training Loss: 0.00002656
2024-11-06 14:07:03,094 - INFO - Epoch [75/300], Batch [31/43], Training Loss: 0.00000743
2024-11-06 14:07:03,098 - INFO - Epoch [75/300], Batch [32/43], Training Loss: 0.00002134
2024-11-06 14:07:03,101 - INFO - Epoch [75/300], Batch [33/43], Training Loss: 0.00001544
2024-11-06 14:07:03,106 - INFO - Epoch [75/300], Batch [34/43], Training Loss: 0.00002703
2024-11-06 14:07:03,109 - INFO - Epoch [75/300], Batch [35/43], Training Loss: 0.00001915
2024-11-06 14:07:03,112 - INFO - Epoch [75/300], Batch [36/43], Training Loss: 0.00003100
2024-11-06 14:07:03,116 - INFO - Epoch [75/300], Batch [37/43], Training Loss: 0.00003361
2024-11-06 14:07:03,119 - INFO - Epoch [75/300], Batch [38/43], Training Loss: 0.00001531
2024-11-06 14:07:03,123 - INFO - Epoch [75/300], Batch [39/43], Training Loss: 0.00002910
2024-11-06 14:07:03,126 - INFO - Epoch [75/300], Batch [40/43], Training Loss: 0.00003774
2024-11-06 14:07:03,129 - INFO - Epoch [75/300], Batch [41/43], Training Loss: 0.00002084
2024-11-06 14:07:03,133 - INFO - Epoch [75/300], Batch [42/43], Training Loss: 0.00002535
2024-11-06 14:07:03,136 - INFO - Epoch [75/300], Batch [43/43], Training Loss: 0.00004181
2024-11-06 14:07:03,148 - INFO - Epoch [75/300], Average Training Loss: 0.00002165, Validation Loss: 0.00002728
2024-11-06 14:07:03,152 - INFO - Epoch [76/300], Batch [1/43], Training Loss: 0.00002771
2024-11-06 14:07:03,156 - INFO - Epoch [76/300], Batch [2/43], Training Loss: 0.00002233
2024-11-06 14:07:03,159 - INFO - Epoch [76/300], Batch [3/43], Training Loss: 0.00001941
2024-11-06 14:07:03,162 - INFO - Epoch [76/300], Batch [4/43], Training Loss: 0.00003472
2024-11-06 14:07:03,166 - INFO - Epoch [76/300], Batch [5/43], Training Loss: 0.00003550
2024-11-06 14:07:03,170 - INFO - Epoch [76/300], Batch [6/43], Training Loss: 0.00001115
2024-11-06 14:07:03,174 - INFO - Epoch [76/300], Batch [7/43], Training Loss: 0.00003445
2024-11-06 14:07:03,177 - INFO - Epoch [76/300], Batch [8/43], Training Loss: 0.00000962
2024-11-06 14:07:03,181 - INFO - Epoch [76/300], Batch [9/43], Training Loss: 0.00003494
2024-11-06 14:07:03,185 - INFO - Epoch [76/300], Batch [10/43], Training Loss: 0.00002780
2024-11-06 14:07:03,189 - INFO - Epoch [76/300], Batch [11/43], Training Loss: 0.00001845
2024-11-06 14:07:03,192 - INFO - Epoch [76/300], Batch [12/43], Training Loss: 0.00004398
2024-11-06 14:07:03,195 - INFO - Epoch [76/300], Batch [13/43], Training Loss: 0.00001325
2024-11-06 14:07:03,199 - INFO - Epoch [76/300], Batch [14/43], Training Loss: 0.00003563
2024-11-06 14:07:03,202 - INFO - Epoch [76/300], Batch [15/43], Training Loss: 0.00002273
2024-11-06 14:07:03,205 - INFO - Epoch [76/300], Batch [16/43], Training Loss: 0.00001650
2024-11-06 14:07:03,209 - INFO - Epoch [76/300], Batch [17/43], Training Loss: 0.00002596
2024-11-06 14:07:03,212 - INFO - Epoch [76/300], Batch [18/43], Training Loss: 0.00002157
2024-11-06 14:07:03,216 - INFO - Epoch [76/300], Batch [19/43], Training Loss: 0.00001669
2024-11-06 14:07:03,219 - INFO - Epoch [76/300], Batch [20/43], Training Loss: 0.00001360
2024-11-06 14:07:03,222 - INFO - Epoch [76/300], Batch [21/43], Training Loss: 0.00003065
2024-11-06 14:07:03,225 - INFO - Epoch [76/300], Batch [22/43], Training Loss: 0.00001308
2024-11-06 14:07:03,229 - INFO - Epoch [76/300], Batch [23/43], Training Loss: 0.00004458
2024-11-06 14:07:03,232 - INFO - Epoch [76/300], Batch [24/43], Training Loss: 0.00002996
2024-11-06 14:07:03,235 - INFO - Epoch [76/300], Batch [25/43], Training Loss: 0.00003176
2024-11-06 14:07:03,238 - INFO - Epoch [76/300], Batch [26/43], Training Loss: 0.00002379
2024-11-06 14:07:03,241 - INFO - Epoch [76/300], Batch [27/43], Training Loss: 0.00002266
2024-11-06 14:07:03,244 - INFO - Epoch [76/300], Batch [28/43], Training Loss: 0.00002011
2024-11-06 14:07:03,246 - INFO - Epoch [76/300], Batch [29/43], Training Loss: 0.00003903
2024-11-06 14:07:03,249 - INFO - Epoch [76/300], Batch [30/43], Training Loss: 0.00001792
2024-11-06 14:07:03,252 - INFO - Epoch [76/300], Batch [31/43], Training Loss: 0.00001835
2024-11-06 14:07:03,255 - INFO - Epoch [76/300], Batch [32/43], Training Loss: 0.00003170
2024-11-06 14:07:03,260 - INFO - Epoch [76/300], Batch [33/43], Training Loss: 0.00002034
2024-11-06 14:07:03,264 - INFO - Epoch [76/300], Batch [34/43], Training Loss: 0.00001346
2024-11-06 14:07:03,267 - INFO - Epoch [76/300], Batch [35/43], Training Loss: 0.00001026
2024-11-06 14:07:03,270 - INFO - Epoch [76/300], Batch [36/43], Training Loss: 0.00000795
2024-11-06 14:07:03,274 - INFO - Epoch [76/300], Batch [37/43], Training Loss: 0.00001721
2024-11-06 14:07:03,278 - INFO - Epoch [76/300], Batch [38/43], Training Loss: 0.00002446
2024-11-06 14:07:03,282 - INFO - Epoch [76/300], Batch [39/43], Training Loss: 0.00001846
2024-11-06 14:07:03,286 - INFO - Epoch [76/300], Batch [40/43], Training Loss: 0.00001861
2024-11-06 14:07:03,289 - INFO - Epoch [76/300], Batch [41/43], Training Loss: 0.00001104
2024-11-06 14:07:03,292 - INFO - Epoch [76/300], Batch [42/43], Training Loss: 0.00000767
2024-11-06 14:07:03,296 - INFO - Epoch [76/300], Batch [43/43], Training Loss: 0.00001350
2024-11-06 14:07:03,307 - INFO - Epoch [76/300], Average Training Loss: 0.00002262, Validation Loss: 0.00003680
2024-11-06 14:07:03,312 - INFO - Epoch [77/300], Batch [1/43], Training Loss: 0.00001816
2024-11-06 14:07:03,317 - INFO - Epoch [77/300], Batch [2/43], Training Loss: 0.00001921
2024-11-06 14:07:03,321 - INFO - Epoch [77/300], Batch [3/43], Training Loss: 0.00002472
2024-11-06 14:07:03,325 - INFO - Epoch [77/300], Batch [4/43], Training Loss: 0.00003440
2024-11-06 14:07:03,329 - INFO - Epoch [77/300], Batch [5/43], Training Loss: 0.00001540
2024-11-06 14:07:03,333 - INFO - Epoch [77/300], Batch [6/43], Training Loss: 0.00001826
2024-11-06 14:07:03,337 - INFO - Epoch [77/300], Batch [7/43], Training Loss: 0.00005148
2024-11-06 14:07:03,341 - INFO - Epoch [77/300], Batch [8/43], Training Loss: 0.00001532
2024-11-06 14:07:03,345 - INFO - Epoch [77/300], Batch [9/43], Training Loss: 0.00001929
2024-11-06 14:07:03,349 - INFO - Epoch [77/300], Batch [10/43], Training Loss: 0.00002916
2024-11-06 14:07:03,353 - INFO - Epoch [77/300], Batch [11/43], Training Loss: 0.00002436
2024-11-06 14:07:03,357 - INFO - Epoch [77/300], Batch [12/43], Training Loss: 0.00002914
2024-11-06 14:07:03,361 - INFO - Epoch [77/300], Batch [13/43], Training Loss: 0.00002362
2024-11-06 14:07:03,364 - INFO - Epoch [77/300], Batch [14/43], Training Loss: 0.00001041
2024-11-06 14:07:03,368 - INFO - Epoch [77/300], Batch [15/43], Training Loss: 0.00002022
2024-11-06 14:07:03,373 - INFO - Epoch [77/300], Batch [16/43], Training Loss: 0.00002394
2024-11-06 14:07:03,378 - INFO - Epoch [77/300], Batch [17/43], Training Loss: 0.00001635
2024-11-06 14:07:03,383 - INFO - Epoch [77/300], Batch [18/43], Training Loss: 0.00002046
2024-11-06 14:07:03,387 - INFO - Epoch [77/300], Batch [19/43], Training Loss: 0.00003276
2024-11-06 14:07:03,390 - INFO - Epoch [77/300], Batch [20/43], Training Loss: 0.00002577
2024-11-06 14:07:03,395 - INFO - Epoch [77/300], Batch [21/43], Training Loss: 0.00002380
2024-11-06 14:07:03,399 - INFO - Epoch [77/300], Batch [22/43], Training Loss: 0.00001960
2024-11-06 14:07:03,403 - INFO - Epoch [77/300], Batch [23/43], Training Loss: 0.00002581
2024-11-06 14:07:03,407 - INFO - Epoch [77/300], Batch [24/43], Training Loss: 0.00001155
2024-11-06 14:07:03,412 - INFO - Epoch [77/300], Batch [25/43], Training Loss: 0.00001273
2024-11-06 14:07:03,416 - INFO - Epoch [77/300], Batch [26/43], Training Loss: 0.00003190
2024-11-06 14:07:03,420 - INFO - Epoch [77/300], Batch [27/43], Training Loss: 0.00001913
2024-11-06 14:07:03,423 - INFO - Epoch [77/300], Batch [28/43], Training Loss: 0.00001855
2024-11-06 14:07:03,426 - INFO - Epoch [77/300], Batch [29/43], Training Loss: 0.00002363
2024-11-06 14:07:03,430 - INFO - Epoch [77/300], Batch [30/43], Training Loss: 0.00004090
2024-11-06 14:07:03,434 - INFO - Epoch [77/300], Batch [31/43], Training Loss: 0.00001419
2024-11-06 14:07:03,438 - INFO - Epoch [77/300], Batch [32/43], Training Loss: 0.00001772
2024-11-06 14:07:03,442 - INFO - Epoch [77/300], Batch [33/43], Training Loss: 0.00001564
2024-11-06 14:07:03,446 - INFO - Epoch [77/300], Batch [34/43], Training Loss: 0.00002077
2024-11-06 14:07:03,451 - INFO - Epoch [77/300], Batch [35/43], Training Loss: 0.00003788
2024-11-06 14:07:03,455 - INFO - Epoch [77/300], Batch [36/43], Training Loss: 0.00003283
2024-11-06 14:07:03,459 - INFO - Epoch [77/300], Batch [37/43], Training Loss: 0.00005469
2024-11-06 14:07:03,463 - INFO - Epoch [77/300], Batch [38/43], Training Loss: 0.00003074
2024-11-06 14:07:03,468 - INFO - Epoch [77/300], Batch [39/43], Training Loss: 0.00001453
2024-11-06 14:07:03,471 - INFO - Epoch [77/300], Batch [40/43], Training Loss: 0.00001985
2024-11-06 14:07:03,475 - INFO - Epoch [77/300], Batch [41/43], Training Loss: 0.00001826
2024-11-06 14:07:03,479 - INFO - Epoch [77/300], Batch [42/43], Training Loss: 0.00002802
2024-11-06 14:07:03,483 - INFO - Epoch [77/300], Batch [43/43], Training Loss: 0.00003023
2024-11-06 14:07:03,497 - INFO - Epoch [77/300], Average Training Loss: 0.00002408, Validation Loss: 0.00002576
2024-11-06 14:07:03,501 - INFO - Epoch [78/300], Batch [1/43], Training Loss: 0.00000693
2024-11-06 14:07:03,505 - INFO - Epoch [78/300], Batch [2/43], Training Loss: 0.00005338
2024-11-06 14:07:03,510 - INFO - Epoch [78/300], Batch [3/43], Training Loss: 0.00001762
2024-11-06 14:07:03,514 - INFO - Epoch [78/300], Batch [4/43], Training Loss: 0.00000589
2024-11-06 14:07:03,517 - INFO - Epoch [78/300], Batch [5/43], Training Loss: 0.00001460
2024-11-06 14:07:03,521 - INFO - Epoch [78/300], Batch [6/43], Training Loss: 0.00002073
2024-11-06 14:07:03,525 - INFO - Epoch [78/300], Batch [7/43], Training Loss: 0.00003911
2024-11-06 14:07:03,528 - INFO - Epoch [78/300], Batch [8/43], Training Loss: 0.00001612
2024-11-06 14:07:03,532 - INFO - Epoch [78/300], Batch [9/43], Training Loss: 0.00000970
2024-11-06 14:07:03,536 - INFO - Epoch [78/300], Batch [10/43], Training Loss: 0.00003333
2024-11-06 14:07:03,539 - INFO - Epoch [78/300], Batch [11/43], Training Loss: 0.00002395
2024-11-06 14:07:03,542 - INFO - Epoch [78/300], Batch [12/43], Training Loss: 0.00000985
2024-11-06 14:07:03,545 - INFO - Epoch [78/300], Batch [13/43], Training Loss: 0.00003283
2024-11-06 14:07:03,548 - INFO - Epoch [78/300], Batch [14/43], Training Loss: 0.00000784
2024-11-06 14:07:03,550 - INFO - Epoch [78/300], Batch [15/43], Training Loss: 0.00001693
2024-11-06 14:07:03,553 - INFO - Epoch [78/300], Batch [16/43], Training Loss: 0.00001897
2024-11-06 14:07:03,556 - INFO - Epoch [78/300], Batch [17/43], Training Loss: 0.00002931
2024-11-06 14:07:03,559 - INFO - Epoch [78/300], Batch [18/43], Training Loss: 0.00001114
2024-11-06 14:07:03,562 - INFO - Epoch [78/300], Batch [19/43], Training Loss: 0.00002943
2024-11-06 14:07:03,565 - INFO - Epoch [78/300], Batch [20/43], Training Loss: 0.00001853
2024-11-06 14:07:03,568 - INFO - Epoch [78/300], Batch [21/43], Training Loss: 0.00001864
2024-11-06 14:07:03,571 - INFO - Epoch [78/300], Batch [22/43], Training Loss: 0.00002703
2024-11-06 14:07:03,575 - INFO - Epoch [78/300], Batch [23/43], Training Loss: 0.00007084
2024-11-06 14:07:03,577 - INFO - Epoch [78/300], Batch [24/43], Training Loss: 0.00001867
2024-11-06 14:07:03,580 - INFO - Epoch [78/300], Batch [25/43], Training Loss: 0.00005233
2024-11-06 14:07:03,583 - INFO - Epoch [78/300], Batch [26/43], Training Loss: 0.00004637
2024-11-06 14:07:03,586 - INFO - Epoch [78/300], Batch [27/43], Training Loss: 0.00001744
2024-11-06 14:07:03,590 - INFO - Epoch [78/300], Batch [28/43], Training Loss: 0.00003450
2024-11-06 14:07:03,594 - INFO - Epoch [78/300], Batch [29/43], Training Loss: 0.00003430
2024-11-06 14:07:03,599 - INFO - Epoch [78/300], Batch [30/43], Training Loss: 0.00003228
2024-11-06 14:07:03,603 - INFO - Epoch [78/300], Batch [31/43], Training Loss: 0.00001483
2024-11-06 14:07:03,607 - INFO - Epoch [78/300], Batch [32/43], Training Loss: 0.00002971
2024-11-06 14:07:03,610 - INFO - Epoch [78/300], Batch [33/43], Training Loss: 0.00004177
2024-11-06 14:07:03,613 - INFO - Epoch [78/300], Batch [34/43], Training Loss: 0.00002697
2024-11-06 14:07:03,616 - INFO - Epoch [78/300], Batch [35/43], Training Loss: 0.00002020
2024-11-06 14:07:03,619 - INFO - Epoch [78/300], Batch [36/43], Training Loss: 0.00002460
2024-11-06 14:07:03,622 - INFO - Epoch [78/300], Batch [37/43], Training Loss: 0.00003708
2024-11-06 14:07:03,626 - INFO - Epoch [78/300], Batch [38/43], Training Loss: 0.00001505
2024-11-06 14:07:03,629 - INFO - Epoch [78/300], Batch [39/43], Training Loss: 0.00001095
2024-11-06 14:07:03,632 - INFO - Epoch [78/300], Batch [40/43], Training Loss: 0.00003850
2024-11-06 14:07:03,636 - INFO - Epoch [78/300], Batch [41/43], Training Loss: 0.00002729
2024-11-06 14:07:03,640 - INFO - Epoch [78/300], Batch [42/43], Training Loss: 0.00001738
2024-11-06 14:07:03,644 - INFO - Epoch [78/300], Batch [43/43], Training Loss: 0.00002282
2024-11-06 14:07:03,657 - INFO - Epoch [78/300], Average Training Loss: 0.00002548, Validation Loss: 0.00004617
2024-11-06 14:07:03,661 - INFO - Epoch [79/300], Batch [1/43], Training Loss: 0.00001418
2024-11-06 14:07:03,667 - INFO - Epoch [79/300], Batch [2/43], Training Loss: 0.00004404
2024-11-06 14:07:03,672 - INFO - Epoch [79/300], Batch [3/43], Training Loss: 0.00001832
2024-11-06 14:07:03,676 - INFO - Epoch [79/300], Batch [4/43], Training Loss: 0.00001964
2024-11-06 14:07:03,680 - INFO - Epoch [79/300], Batch [5/43], Training Loss: 0.00003682
2024-11-06 14:07:03,712 - INFO - Epoch [79/300], Batch [6/43], Training Loss: 0.00002288
2024-11-06 14:07:03,718 - INFO - Epoch [79/300], Batch [7/43], Training Loss: 0.00001955
2024-11-06 14:07:03,724 - INFO - Epoch [79/300], Batch [8/43], Training Loss: 0.00003190
2024-11-06 14:07:03,729 - INFO - Epoch [79/300], Batch [9/43], Training Loss: 0.00002406
2024-11-06 14:07:03,736 - INFO - Epoch [79/300], Batch [10/43], Training Loss: 0.00001539
2024-11-06 14:07:03,750 - INFO - Epoch [79/300], Batch [11/43], Training Loss: 0.00001118
2024-11-06 14:07:03,753 - INFO - Epoch [79/300], Batch [12/43], Training Loss: 0.00001575
2024-11-06 14:07:03,757 - INFO - Epoch [79/300], Batch [13/43], Training Loss: 0.00002559
2024-11-06 14:07:03,762 - INFO - Epoch [79/300], Batch [14/43], Training Loss: 0.00003399
2024-11-06 14:07:03,768 - INFO - Epoch [79/300], Batch [15/43], Training Loss: 0.00002782
2024-11-06 14:07:03,772 - INFO - Epoch [79/300], Batch [16/43], Training Loss: 0.00001446
2024-11-06 14:07:03,777 - INFO - Epoch [79/300], Batch [17/43], Training Loss: 0.00004850
2024-11-06 14:07:03,781 - INFO - Epoch [79/300], Batch [18/43], Training Loss: 0.00002463
2024-11-06 14:07:03,784 - INFO - Epoch [79/300], Batch [19/43], Training Loss: 0.00001845
2024-11-06 14:07:03,787 - INFO - Epoch [79/300], Batch [20/43], Training Loss: 0.00001225
2024-11-06 14:07:03,791 - INFO - Epoch [79/300], Batch [21/43], Training Loss: 0.00003807
2024-11-06 14:07:03,795 - INFO - Epoch [79/300], Batch [22/43], Training Loss: 0.00002169
2024-11-06 14:07:03,798 - INFO - Epoch [79/300], Batch [23/43], Training Loss: 0.00002191
2024-11-06 14:07:03,802 - INFO - Epoch [79/300], Batch [24/43], Training Loss: 0.00002511
2024-11-06 14:07:03,805 - INFO - Epoch [79/300], Batch [25/43], Training Loss: 0.00001650
2024-11-06 14:07:03,809 - INFO - Epoch [79/300], Batch [26/43], Training Loss: 0.00001187
2024-11-06 14:07:03,812 - INFO - Epoch [79/300], Batch [27/43], Training Loss: 0.00003391
2024-11-06 14:07:03,815 - INFO - Epoch [79/300], Batch [28/43], Training Loss: 0.00002354
2024-11-06 14:07:03,819 - INFO - Epoch [79/300], Batch [29/43], Training Loss: 0.00001596
2024-11-06 14:07:03,822 - INFO - Epoch [79/300], Batch [30/43], Training Loss: 0.00002021
2024-11-06 14:07:03,826 - INFO - Epoch [79/300], Batch [31/43], Training Loss: 0.00001573
2024-11-06 14:07:03,829 - INFO - Epoch [79/300], Batch [32/43], Training Loss: 0.00000885
2024-11-06 14:07:03,833 - INFO - Epoch [79/300], Batch [33/43], Training Loss: 0.00004364
2024-11-06 14:07:03,838 - INFO - Epoch [79/300], Batch [34/43], Training Loss: 0.00002770
2024-11-06 14:07:03,842 - INFO - Epoch [79/300], Batch [35/43], Training Loss: 0.00001975
2024-11-06 14:07:03,846 - INFO - Epoch [79/300], Batch [36/43], Training Loss: 0.00002141
2024-11-06 14:07:03,850 - INFO - Epoch [79/300], Batch [37/43], Training Loss: 0.00004035
2024-11-06 14:07:03,855 - INFO - Epoch [79/300], Batch [38/43], Training Loss: 0.00002842
2024-11-06 14:07:03,859 - INFO - Epoch [79/300], Batch [39/43], Training Loss: 0.00001300
2024-11-06 14:07:03,863 - INFO - Epoch [79/300], Batch [40/43], Training Loss: 0.00001568
2024-11-06 14:07:03,866 - INFO - Epoch [79/300], Batch [41/43], Training Loss: 0.00003329
2024-11-06 14:07:03,871 - INFO - Epoch [79/300], Batch [42/43], Training Loss: 0.00002351
2024-11-06 14:07:03,876 - INFO - Epoch [79/300], Batch [43/43], Training Loss: 0.00001332
2024-11-06 14:07:03,888 - INFO - Epoch [79/300], Average Training Loss: 0.00002355, Validation Loss: 0.00002479
2024-11-06 14:07:03,892 - INFO - Epoch [80/300], Batch [1/43], Training Loss: 0.00001822
2024-11-06 14:07:03,895 - INFO - Epoch [80/300], Batch [2/43], Training Loss: 0.00003457
2024-11-06 14:07:03,898 - INFO - Epoch [80/300], Batch [3/43], Training Loss: 0.00002594
2024-11-06 14:07:03,902 - INFO - Epoch [80/300], Batch [4/43], Training Loss: 0.00001905
2024-11-06 14:07:03,905 - INFO - Epoch [80/300], Batch [5/43], Training Loss: 0.00001728
2024-11-06 14:07:03,909 - INFO - Epoch [80/300], Batch [6/43], Training Loss: 0.00001184
2024-11-06 14:07:03,912 - INFO - Epoch [80/300], Batch [7/43], Training Loss: 0.00001542
2024-11-06 14:07:03,915 - INFO - Epoch [80/300], Batch [8/43], Training Loss: 0.00002148
2024-11-06 14:07:03,919 - INFO - Epoch [80/300], Batch [9/43], Training Loss: 0.00001379
2024-11-06 14:07:03,923 - INFO - Epoch [80/300], Batch [10/43], Training Loss: 0.00002219
2024-11-06 14:07:03,926 - INFO - Epoch [80/300], Batch [11/43], Training Loss: 0.00001311
2024-11-06 14:07:03,930 - INFO - Epoch [80/300], Batch [12/43], Training Loss: 0.00001530
2024-11-06 14:07:03,934 - INFO - Epoch [80/300], Batch [13/43], Training Loss: 0.00001922
2024-11-06 14:07:03,938 - INFO - Epoch [80/300], Batch [14/43], Training Loss: 0.00000762
2024-11-06 14:07:03,942 - INFO - Epoch [80/300], Batch [15/43], Training Loss: 0.00001204
2024-11-06 14:07:03,946 - INFO - Epoch [80/300], Batch [16/43], Training Loss: 0.00002195
2024-11-06 14:07:03,949 - INFO - Epoch [80/300], Batch [17/43], Training Loss: 0.00001457
2024-11-06 14:07:03,953 - INFO - Epoch [80/300], Batch [18/43], Training Loss: 0.00002380
2024-11-06 14:07:03,957 - INFO - Epoch [80/300], Batch [19/43], Training Loss: 0.00002633
2024-11-06 14:07:03,961 - INFO - Epoch [80/300], Batch [20/43], Training Loss: 0.00001088
2024-11-06 14:07:03,965 - INFO - Epoch [80/300], Batch [21/43], Training Loss: 0.00001355
2024-11-06 14:07:03,970 - INFO - Epoch [80/300], Batch [22/43], Training Loss: 0.00003209
2024-11-06 14:07:03,973 - INFO - Epoch [80/300], Batch [23/43], Training Loss: 0.00005536
2024-11-06 14:07:03,976 - INFO - Epoch [80/300], Batch [24/43], Training Loss: 0.00001264
2024-11-06 14:07:03,981 - INFO - Epoch [80/300], Batch [25/43], Training Loss: 0.00005017
2024-11-06 14:07:03,985 - INFO - Epoch [80/300], Batch [26/43], Training Loss: 0.00004478
2024-11-06 14:07:03,990 - INFO - Epoch [80/300], Batch [27/43], Training Loss: 0.00001750
2024-11-06 14:07:03,994 - INFO - Epoch [80/300], Batch [28/43], Training Loss: 0.00001736
2024-11-06 14:07:03,999 - INFO - Epoch [80/300], Batch [29/43], Training Loss: 0.00001196
2024-11-06 14:07:04,002 - INFO - Epoch [80/300], Batch [30/43], Training Loss: 0.00003019
2024-11-06 14:07:04,006 - INFO - Epoch [80/300], Batch [31/43], Training Loss: 0.00001808
2024-11-06 14:07:04,014 - INFO - Epoch [80/300], Batch [32/43], Training Loss: 0.00001998
2024-11-06 14:07:04,020 - INFO - Epoch [80/300], Batch [33/43], Training Loss: 0.00003075
2024-11-06 14:07:04,024 - INFO - Epoch [80/300], Batch [34/43], Training Loss: 0.00001764
2024-11-06 14:07:04,030 - INFO - Epoch [80/300], Batch [35/43], Training Loss: 0.00001122
2024-11-06 14:07:04,038 - INFO - Epoch [80/300], Batch [36/43], Training Loss: 0.00003438
2024-11-06 14:07:04,043 - INFO - Epoch [80/300], Batch [37/43], Training Loss: 0.00002702
2024-11-06 14:07:04,048 - INFO - Epoch [80/300], Batch [38/43], Training Loss: 0.00003003
2024-11-06 14:07:04,052 - INFO - Epoch [80/300], Batch [39/43], Training Loss: 0.00001110
2024-11-06 14:07:04,056 - INFO - Epoch [80/300], Batch [40/43], Training Loss: 0.00003098
2024-11-06 14:07:04,062 - INFO - Epoch [80/300], Batch [41/43], Training Loss: 0.00001974
2024-11-06 14:07:04,066 - INFO - Epoch [80/300], Batch [42/43], Training Loss: 0.00003293
2024-11-06 14:07:04,070 - INFO - Epoch [80/300], Batch [43/43], Training Loss: 0.00000913
2024-11-06 14:07:04,084 - INFO - Epoch [80/300], Average Training Loss: 0.00002193, Validation Loss: 0.00002414
2024-11-06 14:07:04,088 - INFO - Epoch [81/300], Batch [1/43], Training Loss: 0.00000887
2024-11-06 14:07:04,093 - INFO - Epoch [81/300], Batch [2/43], Training Loss: 0.00003240
2024-11-06 14:07:04,098 - INFO - Epoch [81/300], Batch [3/43], Training Loss: 0.00000723
2024-11-06 14:07:04,102 - INFO - Epoch [81/300], Batch [4/43], Training Loss: 0.00002579
2024-11-06 14:07:04,106 - INFO - Epoch [81/300], Batch [5/43], Training Loss: 0.00002748
2024-11-06 14:07:04,110 - INFO - Epoch [81/300], Batch [6/43], Training Loss: 0.00002881
2024-11-06 14:07:04,115 - INFO - Epoch [81/300], Batch [7/43], Training Loss: 0.00001918
2024-11-06 14:07:04,119 - INFO - Epoch [81/300], Batch [8/43], Training Loss: 0.00001679
2024-11-06 14:07:04,124 - INFO - Epoch [81/300], Batch [9/43], Training Loss: 0.00002259
2024-11-06 14:07:04,128 - INFO - Epoch [81/300], Batch [10/43], Training Loss: 0.00002592
2024-11-06 14:07:04,133 - INFO - Epoch [81/300], Batch [11/43], Training Loss: 0.00002616
2024-11-06 14:07:04,137 - INFO - Epoch [81/300], Batch [12/43], Training Loss: 0.00001274
2024-11-06 14:07:04,142 - INFO - Epoch [81/300], Batch [13/43], Training Loss: 0.00001703
2024-11-06 14:07:04,147 - INFO - Epoch [81/300], Batch [14/43], Training Loss: 0.00002085
2024-11-06 14:07:04,152 - INFO - Epoch [81/300], Batch [15/43], Training Loss: 0.00004479
2024-11-06 14:07:04,157 - INFO - Epoch [81/300], Batch [16/43], Training Loss: 0.00002371
2024-11-06 14:07:04,161 - INFO - Epoch [81/300], Batch [17/43], Training Loss: 0.00003249
2024-11-06 14:07:04,165 - INFO - Epoch [81/300], Batch [18/43], Training Loss: 0.00002015
2024-11-06 14:07:04,169 - INFO - Epoch [81/300], Batch [19/43], Training Loss: 0.00001832
2024-11-06 14:07:04,172 - INFO - Epoch [81/300], Batch [20/43], Training Loss: 0.00001635
2024-11-06 14:07:04,176 - INFO - Epoch [81/300], Batch [21/43], Training Loss: 0.00002087
2024-11-06 14:07:04,179 - INFO - Epoch [81/300], Batch [22/43], Training Loss: 0.00001466
2024-11-06 14:07:04,184 - INFO - Epoch [81/300], Batch [23/43], Training Loss: 0.00001050
2024-11-06 14:07:04,188 - INFO - Epoch [81/300], Batch [24/43], Training Loss: 0.00002043
2024-11-06 14:07:04,192 - INFO - Epoch [81/300], Batch [25/43], Training Loss: 0.00002022
2024-11-06 14:07:04,195 - INFO - Epoch [81/300], Batch [26/43], Training Loss: 0.00000697
2024-11-06 14:07:04,199 - INFO - Epoch [81/300], Batch [27/43], Training Loss: 0.00002172
2024-11-06 14:07:04,202 - INFO - Epoch [81/300], Batch [28/43], Training Loss: 0.00002969
2024-11-06 14:07:04,206 - INFO - Epoch [81/300], Batch [29/43], Training Loss: 0.00002291
2024-11-06 14:07:04,209 - INFO - Epoch [81/300], Batch [30/43], Training Loss: 0.00002544
2024-11-06 14:07:04,213 - INFO - Epoch [81/300], Batch [31/43], Training Loss: 0.00001012
2024-11-06 14:07:04,218 - INFO - Epoch [81/300], Batch [32/43], Training Loss: 0.00003075
2024-11-06 14:07:04,222 - INFO - Epoch [81/300], Batch [33/43], Training Loss: 0.00003078
2024-11-06 14:07:04,226 - INFO - Epoch [81/300], Batch [34/43], Training Loss: 0.00001884
2024-11-06 14:07:04,232 - INFO - Epoch [81/300], Batch [35/43], Training Loss: 0.00004975
2024-11-06 14:07:04,235 - INFO - Epoch [81/300], Batch [36/43], Training Loss: 0.00000824
2024-11-06 14:07:04,239 - INFO - Epoch [81/300], Batch [37/43], Training Loss: 0.00002971
2024-11-06 14:07:04,243 - INFO - Epoch [81/300], Batch [38/43], Training Loss: 0.00002613
2024-11-06 14:07:04,247 - INFO - Epoch [81/300], Batch [39/43], Training Loss: 0.00001161
2024-11-06 14:07:04,251 - INFO - Epoch [81/300], Batch [40/43], Training Loss: 0.00001076
2024-11-06 14:07:04,255 - INFO - Epoch [81/300], Batch [41/43], Training Loss: 0.00001605
2024-11-06 14:07:04,260 - INFO - Epoch [81/300], Batch [42/43], Training Loss: 0.00002684
2024-11-06 14:07:04,266 - INFO - Epoch [81/300], Batch [43/43], Training Loss: 0.00002562
2024-11-06 14:07:04,280 - INFO - Epoch [81/300], Average Training Loss: 0.00002177, Validation Loss: 0.00002516
2024-11-06 14:07:04,284 - INFO - Epoch [82/300], Batch [1/43], Training Loss: 0.00003499
2024-11-06 14:07:04,287 - INFO - Epoch [82/300], Batch [2/43], Training Loss: 0.00002893
2024-11-06 14:07:04,292 - INFO - Epoch [82/300], Batch [3/43], Training Loss: 0.00001082
2024-11-06 14:07:04,296 - INFO - Epoch [82/300], Batch [4/43], Training Loss: 0.00002824
2024-11-06 14:07:04,300 - INFO - Epoch [82/300], Batch [5/43], Training Loss: 0.00002407
2024-11-06 14:07:04,305 - INFO - Epoch [82/300], Batch [6/43], Training Loss: 0.00002593
2024-11-06 14:07:04,309 - INFO - Epoch [82/300], Batch [7/43], Training Loss: 0.00001981
2024-11-06 14:07:04,314 - INFO - Epoch [82/300], Batch [8/43], Training Loss: 0.00003324
2024-11-06 14:07:04,318 - INFO - Epoch [82/300], Batch [9/43], Training Loss: 0.00002053
2024-11-06 14:07:04,322 - INFO - Epoch [82/300], Batch [10/43], Training Loss: 0.00002920
2024-11-06 14:07:04,326 - INFO - Epoch [82/300], Batch [11/43], Training Loss: 0.00002556
2024-11-06 14:07:04,330 - INFO - Epoch [82/300], Batch [12/43], Training Loss: 0.00002417
2024-11-06 14:07:04,334 - INFO - Epoch [82/300], Batch [13/43], Training Loss: 0.00002425
2024-11-06 14:07:04,338 - INFO - Epoch [82/300], Batch [14/43], Training Loss: 0.00003013
2024-11-06 14:07:04,342 - INFO - Epoch [82/300], Batch [15/43], Training Loss: 0.00003176
2024-11-06 14:07:04,346 - INFO - Epoch [82/300], Batch [16/43], Training Loss: 0.00002085
2024-11-06 14:07:04,350 - INFO - Epoch [82/300], Batch [17/43], Training Loss: 0.00001321
2024-11-06 14:07:04,353 - INFO - Epoch [82/300], Batch [18/43], Training Loss: 0.00004454
2024-11-06 14:07:04,358 - INFO - Epoch [82/300], Batch [19/43], Training Loss: 0.00002508
2024-11-06 14:07:04,361 - INFO - Epoch [82/300], Batch [20/43], Training Loss: 0.00002986
2024-11-06 14:07:04,365 - INFO - Epoch [82/300], Batch [21/43], Training Loss: 0.00001665
2024-11-06 14:07:04,370 - INFO - Epoch [82/300], Batch [22/43], Training Loss: 0.00004960
2024-11-06 14:07:04,374 - INFO - Epoch [82/300], Batch [23/43], Training Loss: 0.00002250
2024-11-06 14:07:04,377 - INFO - Epoch [82/300], Batch [24/43], Training Loss: 0.00002163
2024-11-06 14:07:04,382 - INFO - Epoch [82/300], Batch [25/43], Training Loss: 0.00001848
2024-11-06 14:07:04,385 - INFO - Epoch [82/300], Batch [26/43], Training Loss: 0.00001356
2024-11-06 14:07:04,389 - INFO - Epoch [82/300], Batch [27/43], Training Loss: 0.00001473
2024-11-06 14:07:04,392 - INFO - Epoch [82/300], Batch [28/43], Training Loss: 0.00002123
2024-11-06 14:07:04,396 - INFO - Epoch [82/300], Batch [29/43], Training Loss: 0.00000740
2024-11-06 14:07:04,400 - INFO - Epoch [82/300], Batch [30/43], Training Loss: 0.00004443
2024-11-06 14:07:04,405 - INFO - Epoch [82/300], Batch [31/43], Training Loss: 0.00002321
2024-11-06 14:07:04,409 - INFO - Epoch [82/300], Batch [32/43], Training Loss: 0.00001769
2024-11-06 14:07:04,414 - INFO - Epoch [82/300], Batch [33/43], Training Loss: 0.00002673
2024-11-06 14:07:04,418 - INFO - Epoch [82/300], Batch [34/43], Training Loss: 0.00002351
2024-11-06 14:07:04,422 - INFO - Epoch [82/300], Batch [35/43], Training Loss: 0.00001503
2024-11-06 14:07:04,427 - INFO - Epoch [82/300], Batch [36/43], Training Loss: 0.00003275
2024-11-06 14:07:04,431 - INFO - Epoch [82/300], Batch [37/43], Training Loss: 0.00004975
2024-11-06 14:07:04,435 - INFO - Epoch [82/300], Batch [38/43], Training Loss: 0.00002323
2024-11-06 14:07:04,438 - INFO - Epoch [82/300], Batch [39/43], Training Loss: 0.00001601
2024-11-06 14:07:04,442 - INFO - Epoch [82/300], Batch [40/43], Training Loss: 0.00006294
2024-11-06 14:07:04,445 - INFO - Epoch [82/300], Batch [41/43], Training Loss: 0.00004100
2024-11-06 14:07:04,450 - INFO - Epoch [82/300], Batch [42/43], Training Loss: 0.00001781
2024-11-06 14:07:04,455 - INFO - Epoch [82/300], Batch [43/43], Training Loss: 0.00002938
2024-11-06 14:07:04,467 - INFO - Epoch [82/300], Average Training Loss: 0.00002638, Validation Loss: 0.00003852
2024-11-06 14:07:04,471 - INFO - Epoch [83/300], Batch [1/43], Training Loss: 0.00001742
2024-11-06 14:07:04,475 - INFO - Epoch [83/300], Batch [2/43], Training Loss: 0.00002703
2024-11-06 14:07:04,479 - INFO - Epoch [83/300], Batch [3/43], Training Loss: 0.00002640
2024-11-06 14:07:04,482 - INFO - Epoch [83/300], Batch [4/43], Training Loss: 0.00003598
2024-11-06 14:07:04,486 - INFO - Epoch [83/300], Batch [5/43], Training Loss: 0.00000921
2024-11-06 14:07:04,489 - INFO - Epoch [83/300], Batch [6/43], Training Loss: 0.00001755
2024-11-06 14:07:04,492 - INFO - Epoch [83/300], Batch [7/43], Training Loss: 0.00002058
2024-11-06 14:07:04,495 - INFO - Epoch [83/300], Batch [8/43], Training Loss: 0.00001734
2024-11-06 14:07:04,499 - INFO - Epoch [83/300], Batch [9/43], Training Loss: 0.00003567
2024-11-06 14:07:04,502 - INFO - Epoch [83/300], Batch [10/43], Training Loss: 0.00002289
2024-11-06 14:07:04,506 - INFO - Epoch [83/300], Batch [11/43], Training Loss: 0.00002325
2024-11-06 14:07:04,509 - INFO - Epoch [83/300], Batch [12/43], Training Loss: 0.00001964
2024-11-06 14:07:04,513 - INFO - Epoch [83/300], Batch [13/43], Training Loss: 0.00002075
2024-11-06 14:07:04,516 - INFO - Epoch [83/300], Batch [14/43], Training Loss: 0.00000372
2024-11-06 14:07:04,520 - INFO - Epoch [83/300], Batch [15/43], Training Loss: 0.00001938
2024-11-06 14:07:04,523 - INFO - Epoch [83/300], Batch [16/43], Training Loss: 0.00002999
2024-11-06 14:07:04,526 - INFO - Epoch [83/300], Batch [17/43], Training Loss: 0.00003964
2024-11-06 14:07:04,529 - INFO - Epoch [83/300], Batch [18/43], Training Loss: 0.00002239
2024-11-06 14:07:04,533 - INFO - Epoch [83/300], Batch [19/43], Training Loss: 0.00001921
2024-11-06 14:07:04,537 - INFO - Epoch [83/300], Batch [20/43], Training Loss: 0.00003247
2024-11-06 14:07:04,540 - INFO - Epoch [83/300], Batch [21/43], Training Loss: 0.00002222
2024-11-06 14:07:04,543 - INFO - Epoch [83/300], Batch [22/43], Training Loss: 0.00002057
2024-11-06 14:07:04,547 - INFO - Epoch [83/300], Batch [23/43], Training Loss: 0.00003376
2024-11-06 14:07:04,549 - INFO - Epoch [83/300], Batch [24/43], Training Loss: 0.00002326
2024-11-06 14:07:04,552 - INFO - Epoch [83/300], Batch [25/43], Training Loss: 0.00002088
2024-11-06 14:07:04,555 - INFO - Epoch [83/300], Batch [26/43], Training Loss: 0.00003081
2024-11-06 14:07:04,559 - INFO - Epoch [83/300], Batch [27/43], Training Loss: 0.00002187
2024-11-06 14:07:04,562 - INFO - Epoch [83/300], Batch [28/43], Training Loss: 0.00000627
2024-11-06 14:07:04,566 - INFO - Epoch [83/300], Batch [29/43], Training Loss: 0.00001359
2024-11-06 14:07:04,569 - INFO - Epoch [83/300], Batch [30/43], Training Loss: 0.00001868
2024-11-06 14:07:04,573 - INFO - Epoch [83/300], Batch [31/43], Training Loss: 0.00000886
2024-11-06 14:07:04,576 - INFO - Epoch [83/300], Batch [32/43], Training Loss: 0.00001894
2024-11-06 14:07:04,579 - INFO - Epoch [83/300], Batch [33/43], Training Loss: 0.00001640
2024-11-06 14:07:04,582 - INFO - Epoch [83/300], Batch [34/43], Training Loss: 0.00001632
2024-11-06 14:07:04,585 - INFO - Epoch [83/300], Batch [35/43], Training Loss: 0.00001373
2024-11-06 14:07:04,588 - INFO - Epoch [83/300], Batch [36/43], Training Loss: 0.00001551
2024-11-06 14:07:04,593 - INFO - Epoch [83/300], Batch [37/43], Training Loss: 0.00001925
2024-11-06 14:07:04,596 - INFO - Epoch [83/300], Batch [38/43], Training Loss: 0.00000729
2024-11-06 14:07:04,600 - INFO - Epoch [83/300], Batch [39/43], Training Loss: 0.00002703
2024-11-06 14:07:04,604 - INFO - Epoch [83/300], Batch [40/43], Training Loss: 0.00001247
2024-11-06 14:07:04,608 - INFO - Epoch [83/300], Batch [41/43], Training Loss: 0.00002121
2024-11-06 14:07:04,611 - INFO - Epoch [83/300], Batch [42/43], Training Loss: 0.00001515
2024-11-06 14:07:04,615 - INFO - Epoch [83/300], Batch [43/43], Training Loss: 0.00001219
2024-11-06 14:07:04,627 - INFO - Epoch [83/300], Average Training Loss: 0.00002039, Validation Loss: 0.00002526
2024-11-06 14:07:04,632 - INFO - Epoch [84/300], Batch [1/43], Training Loss: 0.00002938
2024-11-06 14:07:04,636 - INFO - Epoch [84/300], Batch [2/43], Training Loss: 0.00001540
2024-11-06 14:07:04,640 - INFO - Epoch [84/300], Batch [3/43], Training Loss: 0.00001089
2024-11-06 14:07:04,645 - INFO - Epoch [84/300], Batch [4/43], Training Loss: 0.00000779
2024-11-06 14:07:04,649 - INFO - Epoch [84/300], Batch [5/43], Training Loss: 0.00002406
2024-11-06 14:07:04,653 - INFO - Epoch [84/300], Batch [6/43], Training Loss: 0.00001567
2024-11-06 14:07:04,657 - INFO - Epoch [84/300], Batch [7/43], Training Loss: 0.00001788
2024-11-06 14:07:04,661 - INFO - Epoch [84/300], Batch [8/43], Training Loss: 0.00003100
2024-11-06 14:07:04,665 - INFO - Epoch [84/300], Batch [9/43], Training Loss: 0.00003168
2024-11-06 14:07:04,668 - INFO - Epoch [84/300], Batch [10/43], Training Loss: 0.00001094
2024-11-06 14:07:04,672 - INFO - Epoch [84/300], Batch [11/43], Training Loss: 0.00002014
2024-11-06 14:07:04,676 - INFO - Epoch [84/300], Batch [12/43], Training Loss: 0.00004321
2024-11-06 14:07:04,680 - INFO - Epoch [84/300], Batch [13/43], Training Loss: 0.00003684
2024-11-06 14:07:04,684 - INFO - Epoch [84/300], Batch [14/43], Training Loss: 0.00002019
2024-11-06 14:07:04,689 - INFO - Epoch [84/300], Batch [15/43], Training Loss: 0.00001089
2024-11-06 14:07:04,692 - INFO - Epoch [84/300], Batch [16/43], Training Loss: 0.00002787
2024-11-06 14:07:04,695 - INFO - Epoch [84/300], Batch [17/43], Training Loss: 0.00001687
2024-11-06 14:07:04,699 - INFO - Epoch [84/300], Batch [18/43], Training Loss: 0.00001853
2024-11-06 14:07:04,703 - INFO - Epoch [84/300], Batch [19/43], Training Loss: 0.00001868
2024-11-06 14:07:04,706 - INFO - Epoch [84/300], Batch [20/43], Training Loss: 0.00000560
2024-11-06 14:07:04,710 - INFO - Epoch [84/300], Batch [21/43], Training Loss: 0.00003221
2024-11-06 14:07:04,713 - INFO - Epoch [84/300], Batch [22/43], Training Loss: 0.00005982
2024-11-06 14:07:04,716 - INFO - Epoch [84/300], Batch [23/43], Training Loss: 0.00000966
2024-11-06 14:07:04,719 - INFO - Epoch [84/300], Batch [24/43], Training Loss: 0.00005686
2024-11-06 14:07:04,722 - INFO - Epoch [84/300], Batch [25/43], Training Loss: 0.00002084
2024-11-06 14:07:04,725 - INFO - Epoch [84/300], Batch [26/43], Training Loss: 0.00000981
2024-11-06 14:07:04,728 - INFO - Epoch [84/300], Batch [27/43], Training Loss: 0.00002633
2024-11-06 14:07:04,731 - INFO - Epoch [84/300], Batch [28/43], Training Loss: 0.00003042
2024-11-06 14:07:04,734 - INFO - Epoch [84/300], Batch [29/43], Training Loss: 0.00002156
2024-11-06 14:07:04,738 - INFO - Epoch [84/300], Batch [30/43], Training Loss: 0.00003060
2024-11-06 14:07:04,741 - INFO - Epoch [84/300], Batch [31/43], Training Loss: 0.00001917
2024-11-06 14:07:04,744 - INFO - Epoch [84/300], Batch [32/43], Training Loss: 0.00002403
2024-11-06 14:07:04,747 - INFO - Epoch [84/300], Batch [33/43], Training Loss: 0.00002400
2024-11-06 14:07:04,751 - INFO - Epoch [84/300], Batch [34/43], Training Loss: 0.00001010
2024-11-06 14:07:04,754 - INFO - Epoch [84/300], Batch [35/43], Training Loss: 0.00001772
2024-11-06 14:07:04,758 - INFO - Epoch [84/300], Batch [36/43], Training Loss: 0.00002142
2024-11-06 14:07:04,761 - INFO - Epoch [84/300], Batch [37/43], Training Loss: 0.00002735
2024-11-06 14:07:04,764 - INFO - Epoch [84/300], Batch [38/43], Training Loss: 0.00001180
2024-11-06 14:07:04,769 - INFO - Epoch [84/300], Batch [39/43], Training Loss: 0.00002181
2024-11-06 14:07:04,772 - INFO - Epoch [84/300], Batch [40/43], Training Loss: 0.00002126
2024-11-06 14:07:04,775 - INFO - Epoch [84/300], Batch [41/43], Training Loss: 0.00001405
2024-11-06 14:07:04,778 - INFO - Epoch [84/300], Batch [42/43], Training Loss: 0.00002085
2024-11-06 14:07:04,782 - INFO - Epoch [84/300], Batch [43/43], Training Loss: 0.00003044
2024-11-06 14:07:04,794 - INFO - Epoch [84/300], Average Training Loss: 0.00002269, Validation Loss: 0.00002651
2024-11-06 14:07:04,797 - INFO - Epoch [85/300], Batch [1/43], Training Loss: 0.00001834
2024-11-06 14:07:04,801 - INFO - Epoch [85/300], Batch [2/43], Training Loss: 0.00002194
2024-11-06 14:07:04,805 - INFO - Epoch [85/300], Batch [3/43], Training Loss: 0.00001986
2024-11-06 14:07:04,810 - INFO - Epoch [85/300], Batch [4/43], Training Loss: 0.00002440
2024-11-06 14:07:04,814 - INFO - Epoch [85/300], Batch [5/43], Training Loss: 0.00002716
2024-11-06 14:07:04,818 - INFO - Epoch [85/300], Batch [6/43], Training Loss: 0.00002891
2024-11-06 14:07:04,822 - INFO - Epoch [85/300], Batch [7/43], Training Loss: 0.00000777
2024-11-06 14:07:04,826 - INFO - Epoch [85/300], Batch [8/43], Training Loss: 0.00001829
2024-11-06 14:07:04,830 - INFO - Epoch [85/300], Batch [9/43], Training Loss: 0.00002845
2024-11-06 14:07:04,833 - INFO - Epoch [85/300], Batch [10/43], Training Loss: 0.00004758
2024-11-06 14:07:04,836 - INFO - Epoch [85/300], Batch [11/43], Training Loss: 0.00001609
2024-11-06 14:07:04,840 - INFO - Epoch [85/300], Batch [12/43], Training Loss: 0.00001941
2024-11-06 14:07:04,844 - INFO - Epoch [85/300], Batch [13/43], Training Loss: 0.00003522
2024-11-06 14:07:04,847 - INFO - Epoch [85/300], Batch [14/43], Training Loss: 0.00004559
2024-11-06 14:07:04,851 - INFO - Epoch [85/300], Batch [15/43], Training Loss: 0.00002301
2024-11-06 14:07:04,854 - INFO - Epoch [85/300], Batch [16/43], Training Loss: 0.00002349
2024-11-06 14:07:04,857 - INFO - Epoch [85/300], Batch [17/43], Training Loss: 0.00005845
2024-11-06 14:07:04,861 - INFO - Epoch [85/300], Batch [18/43], Training Loss: 0.00004091
2024-11-06 14:07:04,864 - INFO - Epoch [85/300], Batch [19/43], Training Loss: 0.00001925
2024-11-06 14:07:04,868 - INFO - Epoch [85/300], Batch [20/43], Training Loss: 0.00003148
2024-11-06 14:07:04,872 - INFO - Epoch [85/300], Batch [21/43], Training Loss: 0.00008798
2024-11-06 14:07:04,877 - INFO - Epoch [85/300], Batch [22/43], Training Loss: 0.00003805
2024-11-06 14:07:04,881 - INFO - Epoch [85/300], Batch [23/43], Training Loss: 0.00001903
2024-11-06 14:07:04,885 - INFO - Epoch [85/300], Batch [24/43], Training Loss: 0.00001891
2024-11-06 14:07:04,889 - INFO - Epoch [85/300], Batch [25/43], Training Loss: 0.00005337
2024-11-06 14:07:04,892 - INFO - Epoch [85/300], Batch [26/43], Training Loss: 0.00002829
2024-11-06 14:07:04,896 - INFO - Epoch [85/300], Batch [27/43], Training Loss: 0.00001134
2024-11-06 14:07:04,900 - INFO - Epoch [85/300], Batch [28/43], Training Loss: 0.00002192
2024-11-06 14:07:04,903 - INFO - Epoch [85/300], Batch [29/43], Training Loss: 0.00005246
2024-11-06 14:07:04,907 - INFO - Epoch [85/300], Batch [30/43], Training Loss: 0.00002629
2024-11-06 14:07:04,911 - INFO - Epoch [85/300], Batch [31/43], Training Loss: 0.00003104
2024-11-06 14:07:04,915 - INFO - Epoch [85/300], Batch [32/43], Training Loss: 0.00002891
2024-11-06 14:07:04,919 - INFO - Epoch [85/300], Batch [33/43], Training Loss: 0.00002515
2024-11-06 14:07:04,923 - INFO - Epoch [85/300], Batch [34/43], Training Loss: 0.00002145
2024-11-06 14:07:04,927 - INFO - Epoch [85/300], Batch [35/43], Training Loss: 0.00001431
2024-11-06 14:07:04,931 - INFO - Epoch [85/300], Batch [36/43], Training Loss: 0.00001824
2024-11-06 14:07:04,935 - INFO - Epoch [85/300], Batch [37/43], Training Loss: 0.00001566
2024-11-06 14:07:04,940 - INFO - Epoch [85/300], Batch [38/43], Training Loss: 0.00001200
2024-11-06 14:07:04,943 - INFO - Epoch [85/300], Batch [39/43], Training Loss: 0.00001906
2024-11-06 14:07:04,947 - INFO - Epoch [85/300], Batch [40/43], Training Loss: 0.00003514
2024-11-06 14:07:04,952 - INFO - Epoch [85/300], Batch [41/43], Training Loss: 0.00002788
2024-11-06 14:07:04,956 - INFO - Epoch [85/300], Batch [42/43], Training Loss: 0.00003234
2024-11-06 14:07:04,960 - INFO - Epoch [85/300], Batch [43/43], Training Loss: 0.00000927
2024-11-06 14:07:04,973 - INFO - Epoch [85/300], Average Training Loss: 0.00002799, Validation Loss: 0.00002907
2024-11-06 14:07:04,978 - INFO - Epoch [86/300], Batch [1/43], Training Loss: 0.00001658
2024-11-06 14:07:04,982 - INFO - Epoch [86/300], Batch [2/43], Training Loss: 0.00001095
2024-11-06 14:07:04,987 - INFO - Epoch [86/300], Batch [3/43], Training Loss: 0.00002374
2024-11-06 14:07:04,991 - INFO - Epoch [86/300], Batch [4/43], Training Loss: 0.00002549
2024-11-06 14:07:04,995 - INFO - Epoch [86/300], Batch [5/43], Training Loss: 0.00003152
2024-11-06 14:07:05,000 - INFO - Epoch [86/300], Batch [6/43], Training Loss: 0.00002061
2024-11-06 14:07:05,004 - INFO - Epoch [86/300], Batch [7/43], Training Loss: 0.00001286
2024-11-06 14:07:05,008 - INFO - Epoch [86/300], Batch [8/43], Training Loss: 0.00001507
2024-11-06 14:07:05,012 - INFO - Epoch [86/300], Batch [9/43], Training Loss: 0.00001224
2024-11-06 14:07:05,016 - INFO - Epoch [86/300], Batch [10/43], Training Loss: 0.00001868
2024-11-06 14:07:05,020 - INFO - Epoch [86/300], Batch [11/43], Training Loss: 0.00002016
2024-11-06 14:07:05,024 - INFO - Epoch [86/300], Batch [12/43], Training Loss: 0.00002133
2024-11-06 14:07:05,028 - INFO - Epoch [86/300], Batch [13/43], Training Loss: 0.00002531
2024-11-06 14:07:05,032 - INFO - Epoch [86/300], Batch [14/43], Training Loss: 0.00003079
2024-11-06 14:07:05,036 - INFO - Epoch [86/300], Batch [15/43], Training Loss: 0.00000487
2024-11-06 14:07:05,040 - INFO - Epoch [86/300], Batch [16/43], Training Loss: 0.00003040
2024-11-06 14:07:05,045 - INFO - Epoch [86/300], Batch [17/43], Training Loss: 0.00001727
2024-11-06 14:07:05,049 - INFO - Epoch [86/300], Batch [18/43], Training Loss: 0.00001185
2024-11-06 14:07:05,053 - INFO - Epoch [86/300], Batch [19/43], Training Loss: 0.00004197
2024-11-06 14:07:05,056 - INFO - Epoch [86/300], Batch [20/43], Training Loss: 0.00002030
2024-11-06 14:07:05,060 - INFO - Epoch [86/300], Batch [21/43], Training Loss: 0.00002273
2024-11-06 14:07:05,065 - INFO - Epoch [86/300], Batch [22/43], Training Loss: 0.00002238
2024-11-06 14:07:05,069 - INFO - Epoch [86/300], Batch [23/43], Training Loss: 0.00001796
2024-11-06 14:07:05,074 - INFO - Epoch [86/300], Batch [24/43], Training Loss: 0.00002177
2024-11-06 14:07:05,079 - INFO - Epoch [86/300], Batch [25/43], Training Loss: 0.00001934
2024-11-06 14:07:05,082 - INFO - Epoch [86/300], Batch [26/43], Training Loss: 0.00001901
2024-11-06 14:07:05,087 - INFO - Epoch [86/300], Batch [27/43], Training Loss: 0.00001554
2024-11-06 14:07:05,091 - INFO - Epoch [86/300], Batch [28/43], Training Loss: 0.00003127
2024-11-06 14:07:05,095 - INFO - Epoch [86/300], Batch [29/43], Training Loss: 0.00002662
2024-11-06 14:07:05,099 - INFO - Epoch [86/300], Batch [30/43], Training Loss: 0.00002409
2024-11-06 14:07:05,104 - INFO - Epoch [86/300], Batch [31/43], Training Loss: 0.00000779
2024-11-06 14:07:05,110 - INFO - Epoch [86/300], Batch [32/43], Training Loss: 0.00003051
2024-11-06 14:07:05,117 - INFO - Epoch [86/300], Batch [33/43], Training Loss: 0.00000772
2024-11-06 14:07:05,123 - INFO - Epoch [86/300], Batch [34/43], Training Loss: 0.00003922
2024-11-06 14:07:05,130 - INFO - Epoch [86/300], Batch [35/43], Training Loss: 0.00001011
2024-11-06 14:07:05,135 - INFO - Epoch [86/300], Batch [36/43], Training Loss: 0.00001471
2024-11-06 14:07:05,143 - INFO - Epoch [86/300], Batch [37/43], Training Loss: 0.00001289
2024-11-06 14:07:05,149 - INFO - Epoch [86/300], Batch [38/43], Training Loss: 0.00000712
2024-11-06 14:07:05,154 - INFO - Epoch [86/300], Batch [39/43], Training Loss: 0.00005124
2024-11-06 14:07:05,159 - INFO - Epoch [86/300], Batch [40/43], Training Loss: 0.00001971
2024-11-06 14:07:05,164 - INFO - Epoch [86/300], Batch [41/43], Training Loss: 0.00003369
2024-11-06 14:07:05,169 - INFO - Epoch [86/300], Batch [42/43], Training Loss: 0.00001971
2024-11-06 14:07:05,174 - INFO - Epoch [86/300], Batch [43/43], Training Loss: 0.00001008
2024-11-06 14:07:05,189 - INFO - Epoch [86/300], Average Training Loss: 0.00002086, Validation Loss: 0.00002343
2024-11-06 14:07:05,193 - INFO - Epoch [87/300], Batch [1/43], Training Loss: 0.00002668
2024-11-06 14:07:05,200 - INFO - Epoch [87/300], Batch [2/43], Training Loss: 0.00001037
2024-11-06 14:07:05,206 - INFO - Epoch [87/300], Batch [3/43], Training Loss: 0.00001473
2024-11-06 14:07:05,212 - INFO - Epoch [87/300], Batch [4/43], Training Loss: 0.00001463
2024-11-06 14:07:05,218 - INFO - Epoch [87/300], Batch [5/43], Training Loss: 0.00003145
2024-11-06 14:07:05,223 - INFO - Epoch [87/300], Batch [6/43], Training Loss: 0.00001063
2024-11-06 14:07:05,228 - INFO - Epoch [87/300], Batch [7/43], Training Loss: 0.00003009
2024-11-06 14:07:05,234 - INFO - Epoch [87/300], Batch [8/43], Training Loss: 0.00001758
2024-11-06 14:07:05,239 - INFO - Epoch [87/300], Batch [9/43], Training Loss: 0.00001990
2024-11-06 14:07:05,243 - INFO - Epoch [87/300], Batch [10/43], Training Loss: 0.00002026
2024-11-06 14:07:05,248 - INFO - Epoch [87/300], Batch [11/43], Training Loss: 0.00001391
2024-11-06 14:07:05,252 - INFO - Epoch [87/300], Batch [12/43], Training Loss: 0.00001335
2024-11-06 14:07:05,257 - INFO - Epoch [87/300], Batch [13/43], Training Loss: 0.00003724
2024-11-06 14:07:05,261 - INFO - Epoch [87/300], Batch [14/43], Training Loss: 0.00001628
2024-11-06 14:07:05,266 - INFO - Epoch [87/300], Batch [15/43], Training Loss: 0.00001559
2024-11-06 14:07:05,270 - INFO - Epoch [87/300], Batch [16/43], Training Loss: 0.00001119
2024-11-06 14:07:05,274 - INFO - Epoch [87/300], Batch [17/43], Training Loss: 0.00002346
2024-11-06 14:07:05,279 - INFO - Epoch [87/300], Batch [18/43], Training Loss: 0.00000981
2024-11-06 14:07:05,284 - INFO - Epoch [87/300], Batch [19/43], Training Loss: 0.00001815
2024-11-06 14:07:05,289 - INFO - Epoch [87/300], Batch [20/43], Training Loss: 0.00002205
2024-11-06 14:07:05,293 - INFO - Epoch [87/300], Batch [21/43], Training Loss: 0.00001578
2024-11-06 14:07:05,299 - INFO - Epoch [87/300], Batch [22/43], Training Loss: 0.00001740
2024-11-06 14:07:05,305 - INFO - Epoch [87/300], Batch [23/43], Training Loss: 0.00001175
2024-11-06 14:07:05,310 - INFO - Epoch [87/300], Batch [24/43], Training Loss: 0.00000878
2024-11-06 14:07:05,315 - INFO - Epoch [87/300], Batch [25/43], Training Loss: 0.00002128
2024-11-06 14:07:05,320 - INFO - Epoch [87/300], Batch [26/43], Training Loss: 0.00003626
2024-11-06 14:07:05,325 - INFO - Epoch [87/300], Batch [27/43], Training Loss: 0.00002371
2024-11-06 14:07:05,329 - INFO - Epoch [87/300], Batch [28/43], Training Loss: 0.00004635
2024-11-06 14:07:05,333 - INFO - Epoch [87/300], Batch [29/43], Training Loss: 0.00004111
2024-11-06 14:07:05,336 - INFO - Epoch [87/300], Batch [30/43], Training Loss: 0.00001500
2024-11-06 14:07:05,340 - INFO - Epoch [87/300], Batch [31/43], Training Loss: 0.00004805
2024-11-06 14:07:05,344 - INFO - Epoch [87/300], Batch [32/43], Training Loss: 0.00002189
2024-11-06 14:07:05,349 - INFO - Epoch [87/300], Batch [33/43], Training Loss: 0.00001446
2024-11-06 14:07:05,354 - INFO - Epoch [87/300], Batch [34/43], Training Loss: 0.00003296
2024-11-06 14:07:05,358 - INFO - Epoch [87/300], Batch [35/43], Training Loss: 0.00001112
2024-11-06 14:07:05,363 - INFO - Epoch [87/300], Batch [36/43], Training Loss: 0.00004680
2024-11-06 14:07:05,369 - INFO - Epoch [87/300], Batch [37/43], Training Loss: 0.00002005
2024-11-06 14:07:05,373 - INFO - Epoch [87/300], Batch [38/43], Training Loss: 0.00001511
2024-11-06 14:07:05,377 - INFO - Epoch [87/300], Batch [39/43], Training Loss: 0.00001785
2024-11-06 14:07:05,382 - INFO - Epoch [87/300], Batch [40/43], Training Loss: 0.00001703
2024-11-06 14:07:05,385 - INFO - Epoch [87/300], Batch [41/43], Training Loss: 0.00006296
2024-11-06 14:07:05,389 - INFO - Epoch [87/300], Batch [42/43], Training Loss: 0.00001477
2024-11-06 14:07:05,393 - INFO - Epoch [87/300], Batch [43/43], Training Loss: 0.00002984
2024-11-06 14:07:05,406 - INFO - Epoch [87/300], Average Training Loss: 0.00002250, Validation Loss: 0.00006198
2024-11-06 14:07:05,411 - INFO - Epoch [88/300], Batch [1/43], Training Loss: 0.00004716
2024-11-06 14:07:05,415 - INFO - Epoch [88/300], Batch [2/43], Training Loss: 0.00002772
2024-11-06 14:07:05,420 - INFO - Epoch [88/300], Batch [3/43], Training Loss: 0.00001907
2024-11-06 14:07:05,425 - INFO - Epoch [88/300], Batch [4/43], Training Loss: 0.00001514
2024-11-06 14:07:05,429 - INFO - Epoch [88/300], Batch [5/43], Training Loss: 0.00005604
2024-11-06 14:07:05,433 - INFO - Epoch [88/300], Batch [6/43], Training Loss: 0.00003152
2024-11-06 14:07:05,437 - INFO - Epoch [88/300], Batch [7/43], Training Loss: 0.00001843
2024-11-06 14:07:05,441 - INFO - Epoch [88/300], Batch [8/43], Training Loss: 0.00001590
2024-11-06 14:07:05,447 - INFO - Epoch [88/300], Batch [9/43], Training Loss: 0.00002067
2024-11-06 14:07:05,452 - INFO - Epoch [88/300], Batch [10/43], Training Loss: 0.00003941
2024-11-06 14:07:05,457 - INFO - Epoch [88/300], Batch [11/43], Training Loss: 0.00001268
2024-11-06 14:07:05,461 - INFO - Epoch [88/300], Batch [12/43], Training Loss: 0.00001535
2024-11-06 14:07:05,466 - INFO - Epoch [88/300], Batch [13/43], Training Loss: 0.00002471
2024-11-06 14:07:05,472 - INFO - Epoch [88/300], Batch [14/43], Training Loss: 0.00004063
2024-11-06 14:07:05,476 - INFO - Epoch [88/300], Batch [15/43], Training Loss: 0.00001897
2024-11-06 14:07:05,481 - INFO - Epoch [88/300], Batch [16/43], Training Loss: 0.00000916
2024-11-06 14:07:05,487 - INFO - Epoch [88/300], Batch [17/43], Training Loss: 0.00002430
2024-11-06 14:07:05,491 - INFO - Epoch [88/300], Batch [18/43], Training Loss: 0.00001625
2024-11-06 14:07:05,496 - INFO - Epoch [88/300], Batch [19/43], Training Loss: 0.00002114
2024-11-06 14:07:05,500 - INFO - Epoch [88/300], Batch [20/43], Training Loss: 0.00003615
2024-11-06 14:07:05,505 - INFO - Epoch [88/300], Batch [21/43], Training Loss: 0.00001901
2024-11-06 14:07:05,509 - INFO - Epoch [88/300], Batch [22/43], Training Loss: 0.00002525
2024-11-06 14:07:05,514 - INFO - Epoch [88/300], Batch [23/43], Training Loss: 0.00001733
2024-11-06 14:07:05,520 - INFO - Epoch [88/300], Batch [24/43], Training Loss: 0.00002178
2024-11-06 14:07:05,525 - INFO - Epoch [88/300], Batch [25/43], Training Loss: 0.00002626
2024-11-06 14:07:05,529 - INFO - Epoch [88/300], Batch [26/43], Training Loss: 0.00001946
2024-11-06 14:07:05,534 - INFO - Epoch [88/300], Batch [27/43], Training Loss: 0.00000874
2024-11-06 14:07:05,540 - INFO - Epoch [88/300], Batch [28/43], Training Loss: 0.00002072
2024-11-06 14:07:05,544 - INFO - Epoch [88/300], Batch [29/43], Training Loss: 0.00002383
2024-11-06 14:07:05,548 - INFO - Epoch [88/300], Batch [30/43], Training Loss: 0.00000858
2024-11-06 14:07:05,551 - INFO - Epoch [88/300], Batch [31/43], Training Loss: 0.00002784
2024-11-06 14:07:05,555 - INFO - Epoch [88/300], Batch [32/43], Training Loss: 0.00003833
2024-11-06 14:07:05,558 - INFO - Epoch [88/300], Batch [33/43], Training Loss: 0.00003067
2024-11-06 14:07:05,563 - INFO - Epoch [88/300], Batch [34/43], Training Loss: 0.00002889
2024-11-06 14:07:05,566 - INFO - Epoch [88/300], Batch [35/43], Training Loss: 0.00003585
2024-11-06 14:07:05,571 - INFO - Epoch [88/300], Batch [36/43], Training Loss: 0.00000599
2024-11-06 14:07:05,575 - INFO - Epoch [88/300], Batch [37/43], Training Loss: 0.00002263
2024-11-06 14:07:05,579 - INFO - Epoch [88/300], Batch [38/43], Training Loss: 0.00003446
2024-11-06 14:07:05,583 - INFO - Epoch [88/300], Batch [39/43], Training Loss: 0.00003555
2024-11-06 14:07:05,586 - INFO - Epoch [88/300], Batch [40/43], Training Loss: 0.00001345
2024-11-06 14:07:05,589 - INFO - Epoch [88/300], Batch [41/43], Training Loss: 0.00004263
2024-11-06 14:07:05,593 - INFO - Epoch [88/300], Batch [42/43], Training Loss: 0.00002800
2024-11-06 14:07:05,597 - INFO - Epoch [88/300], Batch [43/43], Training Loss: 0.00001064
2024-11-06 14:07:05,611 - INFO - Epoch [88/300], Average Training Loss: 0.00002456, Validation Loss: 0.00002357
2024-11-06 14:07:05,615 - INFO - Epoch [89/300], Batch [1/43], Training Loss: 0.00003418
2024-11-06 14:07:05,619 - INFO - Epoch [89/300], Batch [2/43], Training Loss: 0.00001501
2024-11-06 14:07:05,623 - INFO - Epoch [89/300], Batch [3/43], Training Loss: 0.00000736
2024-11-06 14:07:05,627 - INFO - Epoch [89/300], Batch [4/43], Training Loss: 0.00001171
2024-11-06 14:07:05,631 - INFO - Epoch [89/300], Batch [5/43], Training Loss: 0.00001490
2024-11-06 14:07:05,635 - INFO - Epoch [89/300], Batch [6/43], Training Loss: 0.00000939
2024-11-06 14:07:05,639 - INFO - Epoch [89/300], Batch [7/43], Training Loss: 0.00002116
2024-11-06 14:07:05,644 - INFO - Epoch [89/300], Batch [8/43], Training Loss: 0.00002820
2024-11-06 14:07:05,648 - INFO - Epoch [89/300], Batch [9/43], Training Loss: 0.00000686
2024-11-06 14:07:05,652 - INFO - Epoch [89/300], Batch [10/43], Training Loss: 0.00002670
2024-11-06 14:07:05,656 - INFO - Epoch [89/300], Batch [11/43], Training Loss: 0.00002260
2024-11-06 14:07:05,660 - INFO - Epoch [89/300], Batch [12/43], Training Loss: 0.00003960
2024-11-06 14:07:05,664 - INFO - Epoch [89/300], Batch [13/43], Training Loss: 0.00003604
2024-11-06 14:07:05,668 - INFO - Epoch [89/300], Batch [14/43], Training Loss: 0.00002734
2024-11-06 14:07:05,673 - INFO - Epoch [89/300], Batch [15/43], Training Loss: 0.00002352
2024-11-06 14:07:05,676 - INFO - Epoch [89/300], Batch [16/43], Training Loss: 0.00003356
2024-11-06 14:07:05,680 - INFO - Epoch [89/300], Batch [17/43], Training Loss: 0.00002369
2024-11-06 14:07:05,684 - INFO - Epoch [89/300], Batch [18/43], Training Loss: 0.00002353
2024-11-06 14:07:05,688 - INFO - Epoch [89/300], Batch [19/43], Training Loss: 0.00003477
2024-11-06 14:07:05,692 - INFO - Epoch [89/300], Batch [20/43], Training Loss: 0.00001210
2024-11-06 14:07:05,695 - INFO - Epoch [89/300], Batch [21/43], Training Loss: 0.00002252
2024-11-06 14:07:05,699 - INFO - Epoch [89/300], Batch [22/43], Training Loss: 0.00003883
2024-11-06 14:07:05,702 - INFO - Epoch [89/300], Batch [23/43], Training Loss: 0.00003528
2024-11-06 14:07:05,707 - INFO - Epoch [89/300], Batch [24/43], Training Loss: 0.00002660
2024-11-06 14:07:05,711 - INFO - Epoch [89/300], Batch [25/43], Training Loss: 0.00003502
2024-11-06 14:07:05,714 - INFO - Epoch [89/300], Batch [26/43], Training Loss: 0.00001596
2024-11-06 14:07:05,717 - INFO - Epoch [89/300], Batch [27/43], Training Loss: 0.00002039
2024-11-06 14:07:05,721 - INFO - Epoch [89/300], Batch [28/43], Training Loss: 0.00004062
2024-11-06 14:07:05,725 - INFO - Epoch [89/300], Batch [29/43], Training Loss: 0.00001944
2024-11-06 14:07:05,729 - INFO - Epoch [89/300], Batch [30/43], Training Loss: 0.00008054
2024-11-06 14:07:05,732 - INFO - Epoch [89/300], Batch [31/43], Training Loss: 0.00002300
2024-11-06 14:07:05,735 - INFO - Epoch [89/300], Batch [32/43], Training Loss: 0.00000874
2024-11-06 14:07:05,739 - INFO - Epoch [89/300], Batch [33/43], Training Loss: 0.00002789
2024-11-06 14:07:05,742 - INFO - Epoch [89/300], Batch [34/43], Training Loss: 0.00006797
2024-11-06 14:07:05,745 - INFO - Epoch [89/300], Batch [35/43], Training Loss: 0.00001061
2024-11-06 14:07:05,749 - INFO - Epoch [89/300], Batch [36/43], Training Loss: 0.00001849
2024-11-06 14:07:05,753 - INFO - Epoch [89/300], Batch [37/43], Training Loss: 0.00001885
2024-11-06 14:07:05,756 - INFO - Epoch [89/300], Batch [38/43], Training Loss: 0.00005517
2024-11-06 14:07:05,759 - INFO - Epoch [89/300], Batch [39/43], Training Loss: 0.00002156
2024-11-06 14:07:05,762 - INFO - Epoch [89/300], Batch [40/43], Training Loss: 0.00001694
2024-11-06 14:07:05,765 - INFO - Epoch [89/300], Batch [41/43], Training Loss: 0.00006826
2024-11-06 14:07:05,769 - INFO - Epoch [89/300], Batch [42/43], Training Loss: 0.00002922
2024-11-06 14:07:05,773 - INFO - Epoch [89/300], Batch [43/43], Training Loss: 0.00003162
2024-11-06 14:07:05,785 - INFO - Epoch [89/300], Average Training Loss: 0.00002758, Validation Loss: 0.00003259
2024-11-06 14:07:05,788 - INFO - Epoch [90/300], Batch [1/43], Training Loss: 0.00002598
2024-11-06 14:07:05,791 - INFO - Epoch [90/300], Batch [2/43], Training Loss: 0.00003883
2024-11-06 14:07:05,794 - INFO - Epoch [90/300], Batch [3/43], Training Loss: 0.00002147
2024-11-06 14:07:05,797 - INFO - Epoch [90/300], Batch [4/43], Training Loss: 0.00002330
2024-11-06 14:07:05,800 - INFO - Epoch [90/300], Batch [5/43], Training Loss: 0.00001856
2024-11-06 14:07:05,803 - INFO - Epoch [90/300], Batch [6/43], Training Loss: 0.00004070
2024-11-06 14:07:05,806 - INFO - Epoch [90/300], Batch [7/43], Training Loss: 0.00004728
2024-11-06 14:07:05,809 - INFO - Epoch [90/300], Batch [8/43], Training Loss: 0.00001865
2024-11-06 14:07:05,811 - INFO - Epoch [90/300], Batch [9/43], Training Loss: 0.00001980
2024-11-06 14:07:05,815 - INFO - Epoch [90/300], Batch [10/43], Training Loss: 0.00001926
2024-11-06 14:07:05,818 - INFO - Epoch [90/300], Batch [11/43], Training Loss: 0.00002697
2024-11-06 14:07:05,821 - INFO - Epoch [90/300], Batch [12/43], Training Loss: 0.00002590
2024-11-06 14:07:05,824 - INFO - Epoch [90/300], Batch [13/43], Training Loss: 0.00002053
2024-11-06 14:07:05,827 - INFO - Epoch [90/300], Batch [14/43], Training Loss: 0.00002276
2024-11-06 14:07:05,831 - INFO - Epoch [90/300], Batch [15/43], Training Loss: 0.00004651
2024-11-06 14:07:05,835 - INFO - Epoch [90/300], Batch [16/43], Training Loss: 0.00001863
2024-11-06 14:07:05,838 - INFO - Epoch [90/300], Batch [17/43], Training Loss: 0.00001803
2024-11-06 14:07:05,842 - INFO - Epoch [90/300], Batch [18/43], Training Loss: 0.00003010
2024-11-06 14:07:05,846 - INFO - Epoch [90/300], Batch [19/43], Training Loss: 0.00002534
2024-11-06 14:07:05,850 - INFO - Epoch [90/300], Batch [20/43], Training Loss: 0.00001376
2024-11-06 14:07:05,853 - INFO - Epoch [90/300], Batch [21/43], Training Loss: 0.00003586
2024-11-06 14:07:05,857 - INFO - Epoch [90/300], Batch [22/43], Training Loss: 0.00003302
2024-11-06 14:07:05,861 - INFO - Epoch [90/300], Batch [23/43], Training Loss: 0.00000752
2024-11-06 14:07:05,865 - INFO - Epoch [90/300], Batch [24/43], Training Loss: 0.00003441
2024-11-06 14:07:05,869 - INFO - Epoch [90/300], Batch [25/43], Training Loss: 0.00003412
2024-11-06 14:07:05,873 - INFO - Epoch [90/300], Batch [26/43], Training Loss: 0.00003539
2024-11-06 14:07:05,878 - INFO - Epoch [90/300], Batch [27/43], Training Loss: 0.00001976
2024-11-06 14:07:05,882 - INFO - Epoch [90/300], Batch [28/43], Training Loss: 0.00001522
2024-11-06 14:07:05,886 - INFO - Epoch [90/300], Batch [29/43], Training Loss: 0.00005280
2024-11-06 14:07:05,889 - INFO - Epoch [90/300], Batch [30/43], Training Loss: 0.00002185
2024-11-06 14:07:05,893 - INFO - Epoch [90/300], Batch [31/43], Training Loss: 0.00000709
2024-11-06 14:07:05,897 - INFO - Epoch [90/300], Batch [32/43], Training Loss: 0.00003783
2024-11-06 14:07:05,901 - INFO - Epoch [90/300], Batch [33/43], Training Loss: 0.00002808
2024-11-06 14:07:05,904 - INFO - Epoch [90/300], Batch [34/43], Training Loss: 0.00001676
2024-11-06 14:07:05,907 - INFO - Epoch [90/300], Batch [35/43], Training Loss: 0.00001524
2024-11-06 14:07:05,910 - INFO - Epoch [90/300], Batch [36/43], Training Loss: 0.00001999
2024-11-06 14:07:05,913 - INFO - Epoch [90/300], Batch [37/43], Training Loss: 0.00001346
2024-11-06 14:07:05,917 - INFO - Epoch [90/300], Batch [38/43], Training Loss: 0.00002209
2024-11-06 14:07:05,921 - INFO - Epoch [90/300], Batch [39/43], Training Loss: 0.00002500
2024-11-06 14:07:05,925 - INFO - Epoch [90/300], Batch [40/43], Training Loss: 0.00002964
2024-11-06 14:07:05,928 - INFO - Epoch [90/300], Batch [41/43], Training Loss: 0.00001387
2024-11-06 14:07:05,931 - INFO - Epoch [90/300], Batch [42/43], Training Loss: 0.00001141
2024-11-06 14:07:05,934 - INFO - Epoch [90/300], Batch [43/43], Training Loss: 0.00002499
2024-11-06 14:07:05,946 - INFO - Epoch [90/300], Average Training Loss: 0.00002506, Validation Loss: 0.00003269
2024-11-06 14:07:05,950 - INFO - Epoch [91/300], Batch [1/43], Training Loss: 0.00002710
2024-11-06 14:07:05,955 - INFO - Epoch [91/300], Batch [2/43], Training Loss: 0.00000909
2024-11-06 14:07:05,958 - INFO - Epoch [91/300], Batch [3/43], Training Loss: 0.00002433
2024-11-06 14:07:05,962 - INFO - Epoch [91/300], Batch [4/43], Training Loss: 0.00002327
2024-11-06 14:07:05,965 - INFO - Epoch [91/300], Batch [5/43], Training Loss: 0.00001839
2024-11-06 14:07:05,969 - INFO - Epoch [91/300], Batch [6/43], Training Loss: 0.00001759
2024-11-06 14:07:05,973 - INFO - Epoch [91/300], Batch [7/43], Training Loss: 0.00003856
2024-11-06 14:07:05,976 - INFO - Epoch [91/300], Batch [8/43], Training Loss: 0.00003485
2024-11-06 14:07:05,980 - INFO - Epoch [91/300], Batch [9/43], Training Loss: 0.00001024
2024-11-06 14:07:05,983 - INFO - Epoch [91/300], Batch [10/43], Training Loss: 0.00003350
2024-11-06 14:07:05,986 - INFO - Epoch [91/300], Batch [11/43], Training Loss: 0.00003170
2024-11-06 14:07:05,989 - INFO - Epoch [91/300], Batch [12/43], Training Loss: 0.00002652
2024-11-06 14:07:05,993 - INFO - Epoch [91/300], Batch [13/43], Training Loss: 0.00001179
2024-11-06 14:07:05,996 - INFO - Epoch [91/300], Batch [14/43], Training Loss: 0.00005826
2024-11-06 14:07:06,000 - INFO - Epoch [91/300], Batch [15/43], Training Loss: 0.00005620
2024-11-06 14:07:06,004 - INFO - Epoch [91/300], Batch [16/43], Training Loss: 0.00001113
2024-11-06 14:07:06,007 - INFO - Epoch [91/300], Batch [17/43], Training Loss: 0.00004656
2024-11-06 14:07:06,010 - INFO - Epoch [91/300], Batch [18/43], Training Loss: 0.00003330
2024-11-06 14:07:06,015 - INFO - Epoch [91/300], Batch [19/43], Training Loss: 0.00002274
2024-11-06 14:07:06,019 - INFO - Epoch [91/300], Batch [20/43], Training Loss: 0.00002192
2024-11-06 14:07:06,023 - INFO - Epoch [91/300], Batch [21/43], Training Loss: 0.00002671
2024-11-06 14:07:06,026 - INFO - Epoch [91/300], Batch [22/43], Training Loss: 0.00002473
2024-11-06 14:07:06,030 - INFO - Epoch [91/300], Batch [23/43], Training Loss: 0.00001577
2024-11-06 14:07:06,034 - INFO - Epoch [91/300], Batch [24/43], Training Loss: 0.00002637
2024-11-06 14:07:06,038 - INFO - Epoch [91/300], Batch [25/43], Training Loss: 0.00002172
2024-11-06 14:07:06,041 - INFO - Epoch [91/300], Batch [26/43], Training Loss: 0.00001548
2024-11-06 14:07:06,045 - INFO - Epoch [91/300], Batch [27/43], Training Loss: 0.00001751
2024-11-06 14:07:06,049 - INFO - Epoch [91/300], Batch [28/43], Training Loss: 0.00003127
2024-11-06 14:07:06,053 - INFO - Epoch [91/300], Batch [29/43], Training Loss: 0.00001400
2024-11-06 14:07:06,057 - INFO - Epoch [91/300], Batch [30/43], Training Loss: 0.00003899
2024-11-06 14:07:06,062 - INFO - Epoch [91/300], Batch [31/43], Training Loss: 0.00001303
2024-11-06 14:07:06,066 - INFO - Epoch [91/300], Batch [32/43], Training Loss: 0.00001274
2024-11-06 14:07:06,070 - INFO - Epoch [91/300], Batch [33/43], Training Loss: 0.00001035
2024-11-06 14:07:06,074 - INFO - Epoch [91/300], Batch [34/43], Training Loss: 0.00001379
2024-11-06 14:07:06,077 - INFO - Epoch [91/300], Batch [35/43], Training Loss: 0.00002428
2024-11-06 14:07:06,080 - INFO - Epoch [91/300], Batch [36/43], Training Loss: 0.00002581
2024-11-06 14:07:06,083 - INFO - Epoch [91/300], Batch [37/43], Training Loss: 0.00001424
2024-11-06 14:07:06,086 - INFO - Epoch [91/300], Batch [38/43], Training Loss: 0.00002124
2024-11-06 14:07:06,089 - INFO - Epoch [91/300], Batch [39/43], Training Loss: 0.00001171
2024-11-06 14:07:06,092 - INFO - Epoch [91/300], Batch [40/43], Training Loss: 0.00001818
2024-11-06 14:07:06,095 - INFO - Epoch [91/300], Batch [41/43], Training Loss: 0.00002754
2024-11-06 14:07:06,098 - INFO - Epoch [91/300], Batch [42/43], Training Loss: 0.00002071
2024-11-06 14:07:06,101 - INFO - Epoch [91/300], Batch [43/43], Training Loss: 0.00002517
2024-11-06 14:07:06,113 - INFO - Epoch [91/300], Average Training Loss: 0.00002392, Validation Loss: 0.00002669
2024-11-06 14:07:06,116 - INFO - Epoch [92/300], Batch [1/43], Training Loss: 0.00001684
2024-11-06 14:07:06,119 - INFO - Epoch [92/300], Batch [2/43], Training Loss: 0.00003654
2024-11-06 14:07:06,123 - INFO - Epoch [92/300], Batch [3/43], Training Loss: 0.00002253
2024-11-06 14:07:06,126 - INFO - Epoch [92/300], Batch [4/43], Training Loss: 0.00001231
2024-11-06 14:07:06,129 - INFO - Epoch [92/300], Batch [5/43], Training Loss: 0.00002093
2024-11-06 14:07:06,133 - INFO - Epoch [92/300], Batch [6/43], Training Loss: 0.00004745
2024-11-06 14:07:06,137 - INFO - Epoch [92/300], Batch [7/43], Training Loss: 0.00002231
2024-11-06 14:07:06,140 - INFO - Epoch [92/300], Batch [8/43], Training Loss: 0.00002708
2024-11-06 14:07:06,143 - INFO - Epoch [92/300], Batch [9/43], Training Loss: 0.00000865
2024-11-06 14:07:06,147 - INFO - Epoch [92/300], Batch [10/43], Training Loss: 0.00002424
2024-11-06 14:07:06,150 - INFO - Epoch [92/300], Batch [11/43], Training Loss: 0.00002899
2024-11-06 14:07:06,155 - INFO - Epoch [92/300], Batch [12/43], Training Loss: 0.00004839
2024-11-06 14:07:06,158 - INFO - Epoch [92/300], Batch [13/43], Training Loss: 0.00001831
2024-11-06 14:07:06,161 - INFO - Epoch [92/300], Batch [14/43], Training Loss: 0.00002705
2024-11-06 14:07:06,164 - INFO - Epoch [92/300], Batch [15/43], Training Loss: 0.00008854
2024-11-06 14:07:06,167 - INFO - Epoch [92/300], Batch [16/43], Training Loss: 0.00001081
2024-11-06 14:07:06,170 - INFO - Epoch [92/300], Batch [17/43], Training Loss: 0.00001910
2024-11-06 14:07:06,173 - INFO - Epoch [92/300], Batch [18/43], Training Loss: 0.00002858
2024-11-06 14:07:06,176 - INFO - Epoch [92/300], Batch [19/43], Training Loss: 0.00002506
2024-11-06 14:07:06,180 - INFO - Epoch [92/300], Batch [20/43], Training Loss: 0.00003068
2024-11-06 14:07:06,183 - INFO - Epoch [92/300], Batch [21/43], Training Loss: 0.00001649
2024-11-06 14:07:06,187 - INFO - Epoch [92/300], Batch [22/43], Training Loss: 0.00001597
2024-11-06 14:07:06,191 - INFO - Epoch [92/300], Batch [23/43], Training Loss: 0.00004346
2024-11-06 14:07:06,194 - INFO - Epoch [92/300], Batch [24/43], Training Loss: 0.00001607
2024-11-06 14:07:06,198 - INFO - Epoch [92/300], Batch [25/43], Training Loss: 0.00001932
2024-11-06 14:07:06,201 - INFO - Epoch [92/300], Batch [26/43], Training Loss: 0.00002358
2024-11-06 14:07:06,206 - INFO - Epoch [92/300], Batch [27/43], Training Loss: 0.00001440
2024-11-06 14:07:06,210 - INFO - Epoch [92/300], Batch [28/43], Training Loss: 0.00002104
2024-11-06 14:07:06,214 - INFO - Epoch [92/300], Batch [29/43], Training Loss: 0.00001670
2024-11-06 14:07:06,217 - INFO - Epoch [92/300], Batch [30/43], Training Loss: 0.00003880
2024-11-06 14:07:06,221 - INFO - Epoch [92/300], Batch [31/43], Training Loss: 0.00002327
2024-11-06 14:07:06,224 - INFO - Epoch [92/300], Batch [32/43], Training Loss: 0.00002148
2024-11-06 14:07:06,228 - INFO - Epoch [92/300], Batch [33/43], Training Loss: 0.00002896
2024-11-06 14:07:06,233 - INFO - Epoch [92/300], Batch [34/43], Training Loss: 0.00002978
2024-11-06 14:07:06,238 - INFO - Epoch [92/300], Batch [35/43], Training Loss: 0.00001040
2024-11-06 14:07:06,242 - INFO - Epoch [92/300], Batch [36/43], Training Loss: 0.00002194
2024-11-06 14:07:06,245 - INFO - Epoch [92/300], Batch [37/43], Training Loss: 0.00002581
2024-11-06 14:07:06,250 - INFO - Epoch [92/300], Batch [38/43], Training Loss: 0.00001493
2024-11-06 14:07:06,253 - INFO - Epoch [92/300], Batch [39/43], Training Loss: 0.00002005
2024-11-06 14:07:06,257 - INFO - Epoch [92/300], Batch [40/43], Training Loss: 0.00000912
2024-11-06 14:07:06,261 - INFO - Epoch [92/300], Batch [41/43], Training Loss: 0.00001462
2024-11-06 14:07:06,265 - INFO - Epoch [92/300], Batch [42/43], Training Loss: 0.00002401
2024-11-06 14:07:06,268 - INFO - Epoch [92/300], Batch [43/43], Training Loss: 0.00002923
2024-11-06 14:07:06,279 - INFO - Epoch [92/300], Average Training Loss: 0.00002474, Validation Loss: 0.00002427
2024-11-06 14:07:06,283 - INFO - Epoch [93/300], Batch [1/43], Training Loss: 0.00001291
2024-11-06 14:07:06,286 - INFO - Epoch [93/300], Batch [2/43], Training Loss: 0.00000888
2024-11-06 14:07:06,289 - INFO - Epoch [93/300], Batch [3/43], Training Loss: 0.00001485
2024-11-06 14:07:06,293 - INFO - Epoch [93/300], Batch [4/43], Training Loss: 0.00001954
2024-11-06 14:07:06,296 - INFO - Epoch [93/300], Batch [5/43], Training Loss: 0.00002254
2024-11-06 14:07:06,299 - INFO - Epoch [93/300], Batch [6/43], Training Loss: 0.00002010
2024-11-06 14:07:06,302 - INFO - Epoch [93/300], Batch [7/43], Training Loss: 0.00001996
2024-11-06 14:07:06,305 - INFO - Epoch [93/300], Batch [8/43], Training Loss: 0.00002120
2024-11-06 14:07:06,307 - INFO - Epoch [93/300], Batch [9/43], Training Loss: 0.00001316
2024-11-06 14:07:06,310 - INFO - Epoch [93/300], Batch [10/43], Training Loss: 0.00002521
2024-11-06 14:07:06,314 - INFO - Epoch [93/300], Batch [11/43], Training Loss: 0.00001491
2024-11-06 14:07:06,317 - INFO - Epoch [93/300], Batch [12/43], Training Loss: 0.00001968
2024-11-06 14:07:06,320 - INFO - Epoch [93/300], Batch [13/43], Training Loss: 0.00001761
2024-11-06 14:07:06,323 - INFO - Epoch [93/300], Batch [14/43], Training Loss: 0.00000913
2024-11-06 14:07:06,327 - INFO - Epoch [93/300], Batch [15/43], Training Loss: 0.00002733
2024-11-06 14:07:06,330 - INFO - Epoch [93/300], Batch [16/43], Training Loss: 0.00000814
2024-11-06 14:07:06,333 - INFO - Epoch [93/300], Batch [17/43], Training Loss: 0.00001461
2024-11-06 14:07:06,337 - INFO - Epoch [93/300], Batch [18/43], Training Loss: 0.00001859
2024-11-06 14:07:06,340 - INFO - Epoch [93/300], Batch [19/43], Training Loss: 0.00001003
2024-11-06 14:07:06,344 - INFO - Epoch [93/300], Batch [20/43], Training Loss: 0.00000865
2024-11-06 14:07:06,348 - INFO - Epoch [93/300], Batch [21/43], Training Loss: 0.00002325
2024-11-06 14:07:06,352 - INFO - Epoch [93/300], Batch [22/43], Training Loss: 0.00001214
2024-11-06 14:07:06,355 - INFO - Epoch [93/300], Batch [23/43], Training Loss: 0.00000877
2024-11-06 14:07:06,359 - INFO - Epoch [93/300], Batch [24/43], Training Loss: 0.00002591
2024-11-06 14:07:06,364 - INFO - Epoch [93/300], Batch [25/43], Training Loss: 0.00001351
2024-11-06 14:07:06,368 - INFO - Epoch [93/300], Batch [26/43], Training Loss: 0.00001054
2024-11-06 14:07:06,371 - INFO - Epoch [93/300], Batch [27/43], Training Loss: 0.00001765
2024-11-06 14:07:06,375 - INFO - Epoch [93/300], Batch [28/43], Training Loss: 0.00001191
2024-11-06 14:07:06,379 - INFO - Epoch [93/300], Batch [29/43], Training Loss: 0.00001086
2024-11-06 14:07:06,382 - INFO - Epoch [93/300], Batch [30/43], Training Loss: 0.00002201
2024-11-06 14:07:06,386 - INFO - Epoch [93/300], Batch [31/43], Training Loss: 0.00003216
2024-11-06 14:07:06,389 - INFO - Epoch [93/300], Batch [32/43], Training Loss: 0.00001572
2024-11-06 14:07:06,393 - INFO - Epoch [93/300], Batch [33/43], Training Loss: 0.00002589
2024-11-06 14:07:06,397 - INFO - Epoch [93/300], Batch [34/43], Training Loss: 0.00002874
2024-11-06 14:07:06,401 - INFO - Epoch [93/300], Batch [35/43], Training Loss: 0.00001931
2024-11-06 14:07:06,406 - INFO - Epoch [93/300], Batch [36/43], Training Loss: 0.00002611
2024-11-06 14:07:06,410 - INFO - Epoch [93/300], Batch [37/43], Training Loss: 0.00002169
2024-11-06 14:07:06,413 - INFO - Epoch [93/300], Batch [38/43], Training Loss: 0.00002615
2024-11-06 14:07:06,417 - INFO - Epoch [93/300], Batch [39/43], Training Loss: 0.00001540
2024-11-06 14:07:06,422 - INFO - Epoch [93/300], Batch [40/43], Training Loss: 0.00001632
2024-11-06 14:07:06,427 - INFO - Epoch [93/300], Batch [41/43], Training Loss: 0.00002643
2024-11-06 14:07:06,431 - INFO - Epoch [93/300], Batch [42/43], Training Loss: 0.00004035
2024-11-06 14:07:06,435 - INFO - Epoch [93/300], Batch [43/43], Training Loss: 0.00000889
2024-11-06 14:07:06,449 - INFO - Epoch [93/300], Average Training Loss: 0.00001830, Validation Loss: 0.00002418
2024-11-06 14:07:06,453 - INFO - Epoch [94/300], Batch [1/43], Training Loss: 0.00001561
2024-11-06 14:07:06,458 - INFO - Epoch [94/300], Batch [2/43], Training Loss: 0.00000902
2024-11-06 14:07:06,461 - INFO - Epoch [94/300], Batch [3/43], Training Loss: 0.00002843
2024-11-06 14:07:06,465 - INFO - Epoch [94/300], Batch [4/43], Training Loss: 0.00001239
2024-11-06 14:07:06,470 - INFO - Epoch [94/300], Batch [5/43], Training Loss: 0.00002228
2024-11-06 14:07:06,474 - INFO - Epoch [94/300], Batch [6/43], Training Loss: 0.00001114
2024-11-06 14:07:06,478 - INFO - Epoch [94/300], Batch [7/43], Training Loss: 0.00001566
2024-11-06 14:07:06,482 - INFO - Epoch [94/300], Batch [8/43], Training Loss: 0.00003065
2024-11-06 14:07:06,486 - INFO - Epoch [94/300], Batch [9/43], Training Loss: 0.00001894
2024-11-06 14:07:06,490 - INFO - Epoch [94/300], Batch [10/43], Training Loss: 0.00002143
2024-11-06 14:07:06,493 - INFO - Epoch [94/300], Batch [11/43], Training Loss: 0.00003302
2024-11-06 14:07:06,497 - INFO - Epoch [94/300], Batch [12/43], Training Loss: 0.00001495
2024-11-06 14:07:06,500 - INFO - Epoch [94/300], Batch [13/43], Training Loss: 0.00003432
2024-11-06 14:07:06,502 - INFO - Epoch [94/300], Batch [14/43], Training Loss: 0.00001068
2024-11-06 14:07:06,505 - INFO - Epoch [94/300], Batch [15/43], Training Loss: 0.00002106
2024-11-06 14:07:06,508 - INFO - Epoch [94/300], Batch [16/43], Training Loss: 0.00005100
2024-11-06 14:07:06,511 - INFO - Epoch [94/300], Batch [17/43], Training Loss: 0.00001920
2024-11-06 14:07:06,514 - INFO - Epoch [94/300], Batch [18/43], Training Loss: 0.00001220
2024-11-06 14:07:06,517 - INFO - Epoch [94/300], Batch [19/43], Training Loss: 0.00000802
2024-11-06 14:07:06,520 - INFO - Epoch [94/300], Batch [20/43], Training Loss: 0.00003487
2024-11-06 14:07:06,523 - INFO - Epoch [94/300], Batch [21/43], Training Loss: 0.00002595
2024-11-06 14:07:06,526 - INFO - Epoch [94/300], Batch [22/43], Training Loss: 0.00003223
2024-11-06 14:07:06,530 - INFO - Epoch [94/300], Batch [23/43], Training Loss: 0.00002395
2024-11-06 14:07:06,534 - INFO - Epoch [94/300], Batch [24/43], Training Loss: 0.00001964
2024-11-06 14:07:06,537 - INFO - Epoch [94/300], Batch [25/43], Training Loss: 0.00003315
2024-11-06 14:07:06,540 - INFO - Epoch [94/300], Batch [26/43], Training Loss: 0.00000811
2024-11-06 14:07:06,543 - INFO - Epoch [94/300], Batch [27/43], Training Loss: 0.00001151
2024-11-06 14:07:06,546 - INFO - Epoch [94/300], Batch [28/43], Training Loss: 0.00003659
2024-11-06 14:07:06,549 - INFO - Epoch [94/300], Batch [29/43], Training Loss: 0.00003898
2024-11-06 14:07:06,552 - INFO - Epoch [94/300], Batch [30/43], Training Loss: 0.00001080
2024-11-06 14:07:06,555 - INFO - Epoch [94/300], Batch [31/43], Training Loss: 0.00003482
2024-11-06 14:07:06,558 - INFO - Epoch [94/300], Batch [32/43], Training Loss: 0.00004635
2024-11-06 14:07:06,561 - INFO - Epoch [94/300], Batch [33/43], Training Loss: 0.00001163
2024-11-06 14:07:06,565 - INFO - Epoch [94/300], Batch [34/43], Training Loss: 0.00002358
2024-11-06 14:07:06,569 - INFO - Epoch [94/300], Batch [35/43], Training Loss: 0.00001658
2024-11-06 14:07:06,573 - INFO - Epoch [94/300], Batch [36/43], Training Loss: 0.00003159
2024-11-06 14:07:06,578 - INFO - Epoch [94/300], Batch [37/43], Training Loss: 0.00003228
2024-11-06 14:07:06,582 - INFO - Epoch [94/300], Batch [38/43], Training Loss: 0.00002772
2024-11-06 14:07:06,585 - INFO - Epoch [94/300], Batch [39/43], Training Loss: 0.00001284
2024-11-06 14:07:06,589 - INFO - Epoch [94/300], Batch [40/43], Training Loss: 0.00004617
2024-11-06 14:07:06,593 - INFO - Epoch [94/300], Batch [41/43], Training Loss: 0.00001809
2024-11-06 14:07:06,597 - INFO - Epoch [94/300], Batch [42/43], Training Loss: 0.00002692
2024-11-06 14:07:06,602 - INFO - Epoch [94/300], Batch [43/43], Training Loss: 0.00003031
2024-11-06 14:07:06,614 - INFO - Epoch [94/300], Average Training Loss: 0.00002383, Validation Loss: 0.00003385
2024-11-06 14:07:06,618 - INFO - Epoch [95/300], Batch [1/43], Training Loss: 0.00003716
2024-11-06 14:07:06,621 - INFO - Epoch [95/300], Batch [2/43], Training Loss: 0.00001863
2024-11-06 14:07:06,625 - INFO - Epoch [95/300], Batch [3/43], Training Loss: 0.00000999
2024-11-06 14:07:06,628 - INFO - Epoch [95/300], Batch [4/43], Training Loss: 0.00001654
2024-11-06 14:07:06,632 - INFO - Epoch [95/300], Batch [5/43], Training Loss: 0.00002725
2024-11-06 14:07:06,636 - INFO - Epoch [95/300], Batch [6/43], Training Loss: 0.00002969
2024-11-06 14:07:06,639 - INFO - Epoch [95/300], Batch [7/43], Training Loss: 0.00002077
2024-11-06 14:07:06,643 - INFO - Epoch [95/300], Batch [8/43], Training Loss: 0.00003005
2024-11-06 14:07:06,647 - INFO - Epoch [95/300], Batch [9/43], Training Loss: 0.00004202
2024-11-06 14:07:06,650 - INFO - Epoch [95/300], Batch [10/43], Training Loss: 0.00001494
2024-11-06 14:07:06,654 - INFO - Epoch [95/300], Batch [11/43], Training Loss: 0.00003593
2024-11-06 14:07:06,657 - INFO - Epoch [95/300], Batch [12/43], Training Loss: 0.00001424
2024-11-06 14:07:06,661 - INFO - Epoch [95/300], Batch [13/43], Training Loss: 0.00001554
2024-11-06 14:07:06,665 - INFO - Epoch [95/300], Batch [14/43], Training Loss: 0.00001058
2024-11-06 14:07:06,669 - INFO - Epoch [95/300], Batch [15/43], Training Loss: 0.00000809
2024-11-06 14:07:06,673 - INFO - Epoch [95/300], Batch [16/43], Training Loss: 0.00002906
2024-11-06 14:07:06,676 - INFO - Epoch [95/300], Batch [17/43], Training Loss: 0.00002220
2024-11-06 14:07:06,680 - INFO - Epoch [95/300], Batch [18/43], Training Loss: 0.00002837
2024-11-06 14:07:06,684 - INFO - Epoch [95/300], Batch [19/43], Training Loss: 0.00004220
2024-11-06 14:07:06,688 - INFO - Epoch [95/300], Batch [20/43], Training Loss: 0.00002611
2024-11-06 14:07:06,692 - INFO - Epoch [95/300], Batch [21/43], Training Loss: 0.00002958
2024-11-06 14:07:06,696 - INFO - Epoch [95/300], Batch [22/43], Training Loss: 0.00002754
2024-11-06 14:07:06,699 - INFO - Epoch [95/300], Batch [23/43], Training Loss: 0.00001230
2024-11-06 14:07:06,703 - INFO - Epoch [95/300], Batch [24/43], Training Loss: 0.00001062
2024-11-06 14:07:06,707 - INFO - Epoch [95/300], Batch [25/43], Training Loss: 0.00001460
2024-11-06 14:07:06,710 - INFO - Epoch [95/300], Batch [26/43], Training Loss: 0.00002381
2024-11-06 14:07:06,714 - INFO - Epoch [95/300], Batch [27/43], Training Loss: 0.00002618
2024-11-06 14:07:06,717 - INFO - Epoch [95/300], Batch [28/43], Training Loss: 0.00002021
2024-11-06 14:07:06,721 - INFO - Epoch [95/300], Batch [29/43], Training Loss: 0.00001651
2024-11-06 14:07:06,724 - INFO - Epoch [95/300], Batch [30/43], Training Loss: 0.00003531
2024-11-06 14:07:06,728 - INFO - Epoch [95/300], Batch [31/43], Training Loss: 0.00002743
2024-11-06 14:07:06,731 - INFO - Epoch [95/300], Batch [32/43], Training Loss: 0.00001696
2024-11-06 14:07:06,735 - INFO - Epoch [95/300], Batch [33/43], Training Loss: 0.00002237
2024-11-06 14:07:06,739 - INFO - Epoch [95/300], Batch [34/43], Training Loss: 0.00001999
2024-11-06 14:07:06,742 - INFO - Epoch [95/300], Batch [35/43], Training Loss: 0.00001967
2024-11-06 14:07:06,745 - INFO - Epoch [95/300], Batch [36/43], Training Loss: 0.00001070
2024-11-06 14:07:06,748 - INFO - Epoch [95/300], Batch [37/43], Training Loss: 0.00003158
2024-11-06 14:07:06,751 - INFO - Epoch [95/300], Batch [38/43], Training Loss: 0.00001428
2024-11-06 14:07:06,755 - INFO - Epoch [95/300], Batch [39/43], Training Loss: 0.00000345
2024-11-06 14:07:06,759 - INFO - Epoch [95/300], Batch [40/43], Training Loss: 0.00002124
2024-11-06 14:07:06,763 - INFO - Epoch [95/300], Batch [41/43], Training Loss: 0.00002237
2024-11-06 14:07:06,767 - INFO - Epoch [95/300], Batch [42/43], Training Loss: 0.00002931
2024-11-06 14:07:06,772 - INFO - Epoch [95/300], Batch [43/43], Training Loss: 0.00001727
2024-11-06 14:07:06,784 - INFO - Epoch [95/300], Average Training Loss: 0.00002215, Validation Loss: 0.00002236
2024-11-06 14:07:06,790 - INFO - Epoch [96/300], Batch [1/43], Training Loss: 0.00001963
2024-11-06 14:07:06,794 - INFO - Epoch [96/300], Batch [2/43], Training Loss: 0.00001572
2024-11-06 14:07:06,799 - INFO - Epoch [96/300], Batch [3/43], Training Loss: 0.00001477
2024-11-06 14:07:06,804 - INFO - Epoch [96/300], Batch [4/43], Training Loss: 0.00002605
2024-11-06 14:07:06,808 - INFO - Epoch [96/300], Batch [5/43], Training Loss: 0.00001413
2024-11-06 14:07:06,813 - INFO - Epoch [96/300], Batch [6/43], Training Loss: 0.00001195
2024-11-06 14:07:06,817 - INFO - Epoch [96/300], Batch [7/43], Training Loss: 0.00001287
2024-11-06 14:07:06,822 - INFO - Epoch [96/300], Batch [8/43], Training Loss: 0.00002929
2024-11-06 14:07:06,829 - INFO - Epoch [96/300], Batch [9/43], Training Loss: 0.00001247
2024-11-06 14:07:06,834 - INFO - Epoch [96/300], Batch [10/43], Training Loss: 0.00001409
2024-11-06 14:07:06,838 - INFO - Epoch [96/300], Batch [11/43], Training Loss: 0.00001799
2024-11-06 14:07:06,843 - INFO - Epoch [96/300], Batch [12/43], Training Loss: 0.00001618
2024-11-06 14:07:06,848 - INFO - Epoch [96/300], Batch [13/43], Training Loss: 0.00001740
2024-11-06 14:07:06,852 - INFO - Epoch [96/300], Batch [14/43], Training Loss: 0.00003724
2024-11-06 14:07:06,857 - INFO - Epoch [96/300], Batch [15/43], Training Loss: 0.00000918
2024-11-06 14:07:06,861 - INFO - Epoch [96/300], Batch [16/43], Training Loss: 0.00002579
2024-11-06 14:07:06,866 - INFO - Epoch [96/300], Batch [17/43], Training Loss: 0.00001481
2024-11-06 14:07:06,871 - INFO - Epoch [96/300], Batch [18/43], Training Loss: 0.00001667
2024-11-06 14:07:06,876 - INFO - Epoch [96/300], Batch [19/43], Training Loss: 0.00001873
2024-11-06 14:07:06,880 - INFO - Epoch [96/300], Batch [20/43], Training Loss: 0.00001545
2024-11-06 14:07:06,885 - INFO - Epoch [96/300], Batch [21/43], Training Loss: 0.00001225
2024-11-06 14:07:06,890 - INFO - Epoch [96/300], Batch [22/43], Training Loss: 0.00001047
2024-11-06 14:07:06,894 - INFO - Epoch [96/300], Batch [23/43], Training Loss: 0.00003137
2024-11-06 14:07:06,899 - INFO - Epoch [96/300], Batch [24/43], Training Loss: 0.00001376
2024-11-06 14:07:06,903 - INFO - Epoch [96/300], Batch [25/43], Training Loss: 0.00002297
2024-11-06 14:07:06,908 - INFO - Epoch [96/300], Batch [26/43], Training Loss: 0.00001090
2024-11-06 14:07:06,912 - INFO - Epoch [96/300], Batch [27/43], Training Loss: 0.00001309
2024-11-06 14:07:06,916 - INFO - Epoch [96/300], Batch [28/43], Training Loss: 0.00004109
2024-11-06 14:07:06,921 - INFO - Epoch [96/300], Batch [29/43], Training Loss: 0.00002805
2024-11-06 14:07:06,925 - INFO - Epoch [96/300], Batch [30/43], Training Loss: 0.00005350
2024-11-06 14:07:06,928 - INFO - Epoch [96/300], Batch [31/43], Training Loss: 0.00002092
2024-11-06 14:07:06,931 - INFO - Epoch [96/300], Batch [32/43], Training Loss: 0.00001535
2024-11-06 14:07:06,935 - INFO - Epoch [96/300], Batch [33/43], Training Loss: 0.00001591
2024-11-06 14:07:06,939 - INFO - Epoch [96/300], Batch [34/43], Training Loss: 0.00003004
2024-11-06 14:07:06,943 - INFO - Epoch [96/300], Batch [35/43], Training Loss: 0.00001508
2024-11-06 14:07:06,947 - INFO - Epoch [96/300], Batch [36/43], Training Loss: 0.00001934
2024-11-06 14:07:06,951 - INFO - Epoch [96/300], Batch [37/43], Training Loss: 0.00002475
2024-11-06 14:07:06,954 - INFO - Epoch [96/300], Batch [38/43], Training Loss: 0.00001300
2024-11-06 14:07:06,957 - INFO - Epoch [96/300], Batch [39/43], Training Loss: 0.00001307
2024-11-06 14:07:06,961 - INFO - Epoch [96/300], Batch [40/43], Training Loss: 0.00002046
2024-11-06 14:07:06,965 - INFO - Epoch [96/300], Batch [41/43], Training Loss: 0.00001142
2024-11-06 14:07:06,968 - INFO - Epoch [96/300], Batch [42/43], Training Loss: 0.00001531
2024-11-06 14:07:06,971 - INFO - Epoch [96/300], Batch [43/43], Training Loss: 0.00000999
2024-11-06 14:07:06,981 - INFO - Epoch [96/300], Average Training Loss: 0.00001913, Validation Loss: 0.00002233
2024-11-06 14:07:06,985 - INFO - Epoch [97/300], Batch [1/43], Training Loss: 0.00000623
2024-11-06 14:07:06,988 - INFO - Epoch [97/300], Batch [2/43], Training Loss: 0.00001289
2024-11-06 14:07:06,991 - INFO - Epoch [97/300], Batch [3/43], Training Loss: 0.00001347
2024-11-06 14:07:06,994 - INFO - Epoch [97/300], Batch [4/43], Training Loss: 0.00002581
2024-11-06 14:07:06,997 - INFO - Epoch [97/300], Batch [5/43], Training Loss: 0.00002022
2024-11-06 14:07:07,001 - INFO - Epoch [97/300], Batch [6/43], Training Loss: 0.00002571
2024-11-06 14:07:07,005 - INFO - Epoch [97/300], Batch [7/43], Training Loss: 0.00001389
2024-11-06 14:07:07,008 - INFO - Epoch [97/300], Batch [8/43], Training Loss: 0.00002411
2024-11-06 14:07:07,011 - INFO - Epoch [97/300], Batch [9/43], Training Loss: 0.00001053
2024-11-06 14:07:07,016 - INFO - Epoch [97/300], Batch [10/43], Training Loss: 0.00001662
2024-11-06 14:07:07,019 - INFO - Epoch [97/300], Batch [11/43], Training Loss: 0.00002265
2024-11-06 14:07:07,023 - INFO - Epoch [97/300], Batch [12/43], Training Loss: 0.00000841
2024-11-06 14:07:07,027 - INFO - Epoch [97/300], Batch [13/43], Training Loss: 0.00000759
2024-11-06 14:07:07,029 - INFO - Epoch [97/300], Batch [14/43], Training Loss: 0.00001679
2024-11-06 14:07:07,032 - INFO - Epoch [97/300], Batch [15/43], Training Loss: 0.00002215
2024-11-06 14:07:07,035 - INFO - Epoch [97/300], Batch [16/43], Training Loss: 0.00001248
2024-11-06 14:07:07,038 - INFO - Epoch [97/300], Batch [17/43], Training Loss: 0.00002356
2024-11-06 14:07:07,041 - INFO - Epoch [97/300], Batch [18/43], Training Loss: 0.00000988
2024-11-06 14:07:07,044 - INFO - Epoch [97/300], Batch [19/43], Training Loss: 0.00000912
2024-11-06 14:07:07,047 - INFO - Epoch [97/300], Batch [20/43], Training Loss: 0.00001422
2024-11-06 14:07:07,050 - INFO - Epoch [97/300], Batch [21/43], Training Loss: 0.00001888
2024-11-06 14:07:07,053 - INFO - Epoch [97/300], Batch [22/43], Training Loss: 0.00001729
2024-11-06 14:07:07,056 - INFO - Epoch [97/300], Batch [23/43], Training Loss: 0.00001030
2024-11-06 14:07:07,059 - INFO - Epoch [97/300], Batch [24/43], Training Loss: 0.00001884
2024-11-06 14:07:07,063 - INFO - Epoch [97/300], Batch [25/43], Training Loss: 0.00001280
2024-11-06 14:07:07,067 - INFO - Epoch [97/300], Batch [26/43], Training Loss: 0.00002132
2024-11-06 14:07:07,070 - INFO - Epoch [97/300], Batch [27/43], Training Loss: 0.00002486
2024-11-06 14:07:07,073 - INFO - Epoch [97/300], Batch [28/43], Training Loss: 0.00000875
2024-11-06 14:07:07,077 - INFO - Epoch [97/300], Batch [29/43], Training Loss: 0.00001214
2024-11-06 14:07:07,082 - INFO - Epoch [97/300], Batch [30/43], Training Loss: 0.00001614
2024-11-06 14:07:07,086 - INFO - Epoch [97/300], Batch [31/43], Training Loss: 0.00001309
2024-11-06 14:07:07,089 - INFO - Epoch [97/300], Batch [32/43], Training Loss: 0.00001451
2024-11-06 14:07:07,092 - INFO - Epoch [97/300], Batch [33/43], Training Loss: 0.00001920
2024-11-06 14:07:07,095 - INFO - Epoch [97/300], Batch [34/43], Training Loss: 0.00002286
2024-11-06 14:07:07,099 - INFO - Epoch [97/300], Batch [35/43], Training Loss: 0.00001064
2024-11-06 14:07:07,103 - INFO - Epoch [97/300], Batch [36/43], Training Loss: 0.00002215
2024-11-06 14:07:07,108 - INFO - Epoch [97/300], Batch [37/43], Training Loss: 0.00001961
2024-11-06 14:07:07,114 - INFO - Epoch [97/300], Batch [38/43], Training Loss: 0.00002708
2024-11-06 14:07:07,118 - INFO - Epoch [97/300], Batch [39/43], Training Loss: 0.00001668
2024-11-06 14:07:07,122 - INFO - Epoch [97/300], Batch [40/43], Training Loss: 0.00003276
2024-11-06 14:07:07,125 - INFO - Epoch [97/300], Batch [41/43], Training Loss: 0.00002116
2024-11-06 14:07:07,129 - INFO - Epoch [97/300], Batch [42/43], Training Loss: 0.00002793
2024-11-06 14:07:07,133 - INFO - Epoch [97/300], Batch [43/43], Training Loss: 0.00002796
2024-11-06 14:07:07,144 - INFO - Epoch [97/300], Average Training Loss: 0.00001752, Validation Loss: 0.00002179
2024-11-06 14:07:07,148 - INFO - Epoch [98/300], Batch [1/43], Training Loss: 0.00002888
2024-11-06 14:07:07,151 - INFO - Epoch [98/300], Batch [2/43], Training Loss: 0.00000816
2024-11-06 14:07:07,155 - INFO - Epoch [98/300], Batch [3/43], Training Loss: 0.00002276
2024-11-06 14:07:07,159 - INFO - Epoch [98/300], Batch [4/43], Training Loss: 0.00002269
2024-11-06 14:07:07,162 - INFO - Epoch [98/300], Batch [5/43], Training Loss: 0.00002086
2024-11-06 14:07:07,166 - INFO - Epoch [98/300], Batch [6/43], Training Loss: 0.00002270
2024-11-06 14:07:07,169 - INFO - Epoch [98/300], Batch [7/43], Training Loss: 0.00001266
2024-11-06 14:07:07,172 - INFO - Epoch [98/300], Batch [8/43], Training Loss: 0.00002517
2024-11-06 14:07:07,176 - INFO - Epoch [98/300], Batch [9/43], Training Loss: 0.00002396
2024-11-06 14:07:07,179 - INFO - Epoch [98/300], Batch [10/43], Training Loss: 0.00001499
2024-11-06 14:07:07,182 - INFO - Epoch [98/300], Batch [11/43], Training Loss: 0.00002238
2024-11-06 14:07:07,185 - INFO - Epoch [98/300], Batch [12/43], Training Loss: 0.00001769
2024-11-06 14:07:07,188 - INFO - Epoch [98/300], Batch [13/43], Training Loss: 0.00000601
2024-11-06 14:07:07,191 - INFO - Epoch [98/300], Batch [14/43], Training Loss: 0.00001489
2024-11-06 14:07:07,195 - INFO - Epoch [98/300], Batch [15/43], Training Loss: 0.00002303
2024-11-06 14:07:07,198 - INFO - Epoch [98/300], Batch [16/43], Training Loss: 0.00001809
2024-11-06 14:07:07,201 - INFO - Epoch [98/300], Batch [17/43], Training Loss: 0.00001113
2024-11-06 14:07:07,204 - INFO - Epoch [98/300], Batch [18/43], Training Loss: 0.00001414
2024-11-06 14:07:07,207 - INFO - Epoch [98/300], Batch [19/43], Training Loss: 0.00000683
2024-11-06 14:07:07,210 - INFO - Epoch [98/300], Batch [20/43], Training Loss: 0.00002078
2024-11-06 14:07:07,213 - INFO - Epoch [98/300], Batch [21/43], Training Loss: 0.00001595
2024-11-06 14:07:07,216 - INFO - Epoch [98/300], Batch [22/43], Training Loss: 0.00001140
2024-11-06 14:07:07,219 - INFO - Epoch [98/300], Batch [23/43], Training Loss: 0.00002984
2024-11-06 14:07:07,222 - INFO - Epoch [98/300], Batch [24/43], Training Loss: 0.00000940
2024-11-06 14:07:07,225 - INFO - Epoch [98/300], Batch [25/43], Training Loss: 0.00002420
2024-11-06 14:07:07,228 - INFO - Epoch [98/300], Batch [26/43], Training Loss: 0.00001533
2024-11-06 14:07:07,231 - INFO - Epoch [98/300], Batch [27/43], Training Loss: 0.00001005
2024-11-06 14:07:07,235 - INFO - Epoch [98/300], Batch [28/43], Training Loss: 0.00002784
2024-11-06 14:07:07,238 - INFO - Epoch [98/300], Batch [29/43], Training Loss: 0.00002079
2024-11-06 14:07:07,241 - INFO - Epoch [98/300], Batch [30/43], Training Loss: 0.00000711
2024-11-06 14:07:07,245 - INFO - Epoch [98/300], Batch [31/43], Training Loss: 0.00002395
2024-11-06 14:07:07,249 - INFO - Epoch [98/300], Batch [32/43], Training Loss: 0.00000932
2024-11-06 14:07:07,252 - INFO - Epoch [98/300], Batch [33/43], Training Loss: 0.00002084
2024-11-06 14:07:07,256 - INFO - Epoch [98/300], Batch [34/43], Training Loss: 0.00000865
2024-11-06 14:07:07,259 - INFO - Epoch [98/300], Batch [35/43], Training Loss: 0.00002301
2024-11-06 14:07:07,263 - INFO - Epoch [98/300], Batch [36/43], Training Loss: 0.00001382
2024-11-06 14:07:07,268 - INFO - Epoch [98/300], Batch [37/43], Training Loss: 0.00001237
2024-11-06 14:07:07,272 - INFO - Epoch [98/300], Batch [38/43], Training Loss: 0.00002023
2024-11-06 14:07:07,278 - INFO - Epoch [98/300], Batch [39/43], Training Loss: 0.00002381
2024-11-06 14:07:07,282 - INFO - Epoch [98/300], Batch [40/43], Training Loss: 0.00001178
2024-11-06 14:07:07,287 - INFO - Epoch [98/300], Batch [41/43], Training Loss: 0.00002407
2024-11-06 14:07:07,292 - INFO - Epoch [98/300], Batch [42/43], Training Loss: 0.00001878
2024-11-06 14:07:07,297 - INFO - Epoch [98/300], Batch [43/43], Training Loss: 0.00003456
2024-11-06 14:07:07,311 - INFO - Epoch [98/300], Average Training Loss: 0.00001802, Validation Loss: 0.00002173
2024-11-06 14:07:07,316 - INFO - Epoch [99/300], Batch [1/43], Training Loss: 0.00004235
2024-11-06 14:07:07,320 - INFO - Epoch [99/300], Batch [2/43], Training Loss: 0.00001455
2024-11-06 14:07:07,325 - INFO - Epoch [99/300], Batch [3/43], Training Loss: 0.00001582
2024-11-06 14:07:07,329 - INFO - Epoch [99/300], Batch [4/43], Training Loss: 0.00003664
2024-11-06 14:07:07,333 - INFO - Epoch [99/300], Batch [5/43], Training Loss: 0.00000851
2024-11-06 14:07:07,338 - INFO - Epoch [99/300], Batch [6/43], Training Loss: 0.00003694
2024-11-06 14:07:07,342 - INFO - Epoch [99/300], Batch [7/43], Training Loss: 0.00001682
2024-11-06 14:07:07,347 - INFO - Epoch [99/300], Batch [8/43], Training Loss: 0.00000772
2024-11-06 14:07:07,352 - INFO - Epoch [99/300], Batch [9/43], Training Loss: 0.00001962
2024-11-06 14:07:07,358 - INFO - Epoch [99/300], Batch [10/43], Training Loss: 0.00001747
2024-11-06 14:07:07,362 - INFO - Epoch [99/300], Batch [11/43], Training Loss: 0.00000654
2024-11-06 14:07:07,366 - INFO - Epoch [99/300], Batch [12/43], Training Loss: 0.00001399
2024-11-06 14:07:07,371 - INFO - Epoch [99/300], Batch [13/43], Training Loss: 0.00001924
2024-11-06 14:07:07,376 - INFO - Epoch [99/300], Batch [14/43], Training Loss: 0.00001149
2024-11-06 14:07:07,381 - INFO - Epoch [99/300], Batch [15/43], Training Loss: 0.00000743
2024-11-06 14:07:07,385 - INFO - Epoch [99/300], Batch [16/43], Training Loss: 0.00002262
2024-11-06 14:07:07,390 - INFO - Epoch [99/300], Batch [17/43], Training Loss: 0.00002310
2024-11-06 14:07:07,394 - INFO - Epoch [99/300], Batch [18/43], Training Loss: 0.00003830
2024-11-06 14:07:07,397 - INFO - Epoch [99/300], Batch [19/43], Training Loss: 0.00001677
2024-11-06 14:07:07,402 - INFO - Epoch [99/300], Batch [20/43], Training Loss: 0.00003135
2024-11-06 14:07:07,405 - INFO - Epoch [99/300], Batch [21/43], Training Loss: 0.00000993
2024-11-06 14:07:07,409 - INFO - Epoch [99/300], Batch [22/43], Training Loss: 0.00002578
2024-11-06 14:07:07,412 - INFO - Epoch [99/300], Batch [23/43], Training Loss: 0.00001072
2024-11-06 14:07:07,415 - INFO - Epoch [99/300], Batch [24/43], Training Loss: 0.00002453
2024-11-06 14:07:07,418 - INFO - Epoch [99/300], Batch [25/43], Training Loss: 0.00002935
2024-11-06 14:07:07,423 - INFO - Epoch [99/300], Batch [26/43], Training Loss: 0.00001058
2024-11-06 14:07:07,426 - INFO - Epoch [99/300], Batch [27/43], Training Loss: 0.00001469
2024-11-06 14:07:07,431 - INFO - Epoch [99/300], Batch [28/43], Training Loss: 0.00002873
2024-11-06 14:07:07,435 - INFO - Epoch [99/300], Batch [29/43], Training Loss: 0.00001928
2024-11-06 14:07:07,439 - INFO - Epoch [99/300], Batch [30/43], Training Loss: 0.00001390
2024-11-06 14:07:07,442 - INFO - Epoch [99/300], Batch [31/43], Training Loss: 0.00002556
2024-11-06 14:07:07,446 - INFO - Epoch [99/300], Batch [32/43], Training Loss: 0.00001511
2024-11-06 14:07:07,449 - INFO - Epoch [99/300], Batch [33/43], Training Loss: 0.00002087
2024-11-06 14:07:07,453 - INFO - Epoch [99/300], Batch [34/43], Training Loss: 0.00001450
2024-11-06 14:07:07,457 - INFO - Epoch [99/300], Batch [35/43], Training Loss: 0.00002624
2024-11-06 14:07:07,461 - INFO - Epoch [99/300], Batch [36/43], Training Loss: 0.00001864
2024-11-06 14:07:07,465 - INFO - Epoch [99/300], Batch [37/43], Training Loss: 0.00001155
2024-11-06 14:07:07,470 - INFO - Epoch [99/300], Batch [38/43], Training Loss: 0.00002907
2024-11-06 14:07:07,474 - INFO - Epoch [99/300], Batch [39/43], Training Loss: 0.00002338
2024-11-06 14:07:07,478 - INFO - Epoch [99/300], Batch [40/43], Training Loss: 0.00001918
2024-11-06 14:07:07,484 - INFO - Epoch [99/300], Batch [41/43], Training Loss: 0.00005388
2024-11-06 14:07:07,489 - INFO - Epoch [99/300], Batch [42/43], Training Loss: 0.00004884
2024-11-06 14:07:07,494 - INFO - Epoch [99/300], Batch [43/43], Training Loss: 0.00001002
2024-11-06 14:07:07,508 - INFO - Epoch [99/300], Average Training Loss: 0.00002120, Validation Loss: 0.00002770
2024-11-06 14:07:07,513 - INFO - Epoch [100/300], Batch [1/43], Training Loss: 0.00001572
2024-11-06 14:07:07,517 - INFO - Epoch [100/300], Batch [2/43], Training Loss: 0.00006885
2024-11-06 14:07:07,522 - INFO - Epoch [100/300], Batch [3/43], Training Loss: 0.00002521
2024-11-06 14:07:07,526 - INFO - Epoch [100/300], Batch [4/43], Training Loss: 0.00001578
2024-11-06 14:07:07,531 - INFO - Epoch [100/300], Batch [5/43], Training Loss: 0.00003824
2024-11-06 14:07:07,535 - INFO - Epoch [100/300], Batch [6/43], Training Loss: 0.00002299
2024-11-06 14:07:07,539 - INFO - Epoch [100/300], Batch [7/43], Training Loss: 0.00000850
2024-11-06 14:07:07,543 - INFO - Epoch [100/300], Batch [8/43], Training Loss: 0.00001324
2024-11-06 14:07:07,548 - INFO - Epoch [100/300], Batch [9/43], Training Loss: 0.00002213
2024-11-06 14:07:07,552 - INFO - Epoch [100/300], Batch [10/43], Training Loss: 0.00003802
2024-11-06 14:07:07,557 - INFO - Epoch [100/300], Batch [11/43], Training Loss: 0.00001486
2024-11-06 14:07:07,561 - INFO - Epoch [100/300], Batch [12/43], Training Loss: 0.00001411
2024-11-06 14:07:07,565 - INFO - Epoch [100/300], Batch [13/43], Training Loss: 0.00000974
2024-11-06 14:07:07,569 - INFO - Epoch [100/300], Batch [14/43], Training Loss: 0.00001719
2024-11-06 14:07:07,573 - INFO - Epoch [100/300], Batch [15/43], Training Loss: 0.00002502
2024-11-06 14:07:07,577 - INFO - Epoch [100/300], Batch [16/43], Training Loss: 0.00000951
2024-11-06 14:07:07,580 - INFO - Epoch [100/300], Batch [17/43], Training Loss: 0.00001456
2024-11-06 14:07:07,583 - INFO - Epoch [100/300], Batch [18/43], Training Loss: 0.00001240
2024-11-06 14:07:07,587 - INFO - Epoch [100/300], Batch [19/43], Training Loss: 0.00001928
2024-11-06 14:07:07,591 - INFO - Epoch [100/300], Batch [20/43], Training Loss: 0.00000878
2024-11-06 14:07:07,593 - INFO - Epoch [100/300], Batch [21/43], Training Loss: 0.00001996
2024-11-06 14:07:07,597 - INFO - Epoch [100/300], Batch [22/43], Training Loss: 0.00003512
2024-11-06 14:07:07,600 - INFO - Epoch [100/300], Batch [23/43], Training Loss: 0.00002828
2024-11-06 14:07:07,604 - INFO - Epoch [100/300], Batch [24/43], Training Loss: 0.00001400
2024-11-06 14:07:07,607 - INFO - Epoch [100/300], Batch [25/43], Training Loss: 0.00001262
2024-11-06 14:07:07,611 - INFO - Epoch [100/300], Batch [26/43], Training Loss: 0.00001228
2024-11-06 14:07:07,615 - INFO - Epoch [100/300], Batch [27/43], Training Loss: 0.00002317
2024-11-06 14:07:07,621 - INFO - Epoch [100/300], Batch [28/43], Training Loss: 0.00003022
2024-11-06 14:07:07,625 - INFO - Epoch [100/300], Batch [29/43], Training Loss: 0.00000932
2024-11-06 14:07:07,629 - INFO - Epoch [100/300], Batch [30/43], Training Loss: 0.00000949
2024-11-06 14:07:07,633 - INFO - Epoch [100/300], Batch [31/43], Training Loss: 0.00002516
2024-11-06 14:07:07,638 - INFO - Epoch [100/300], Batch [32/43], Training Loss: 0.00000914
2024-11-06 14:07:07,641 - INFO - Epoch [100/300], Batch [33/43], Training Loss: 0.00001696
2024-11-06 14:07:07,645 - INFO - Epoch [100/300], Batch [34/43], Training Loss: 0.00002701
2024-11-06 14:07:07,650 - INFO - Epoch [100/300], Batch [35/43], Training Loss: 0.00002051
2024-11-06 14:07:07,655 - INFO - Epoch [100/300], Batch [36/43], Training Loss: 0.00001303
2024-11-06 14:07:07,659 - INFO - Epoch [100/300], Batch [37/43], Training Loss: 0.00000624
2024-11-06 14:07:07,663 - INFO - Epoch [100/300], Batch [38/43], Training Loss: 0.00002064
2024-11-06 14:07:07,667 - INFO - Epoch [100/300], Batch [39/43], Training Loss: 0.00001783
2024-11-06 14:07:07,672 - INFO - Epoch [100/300], Batch [40/43], Training Loss: 0.00002161
2024-11-06 14:07:07,676 - INFO - Epoch [100/300], Batch [41/43], Training Loss: 0.00001539
2024-11-06 14:07:07,680 - INFO - Epoch [100/300], Batch [42/43], Training Loss: 0.00004092
2024-11-06 14:07:07,684 - INFO - Epoch [100/300], Batch [43/43], Training Loss: 0.00000946
2024-11-06 14:07:07,696 - INFO - Epoch [100/300], Average Training Loss: 0.00001983, Validation Loss: 0.00003202
2024-11-06 14:07:07,700 - INFO - Epoch [101/300], Batch [1/43], Training Loss: 0.00002126
2024-11-06 14:07:07,704 - INFO - Epoch [101/300], Batch [2/43], Training Loss: 0.00007752
2024-11-06 14:07:07,707 - INFO - Epoch [101/300], Batch [3/43], Training Loss: 0.00002125
2024-11-06 14:07:07,710 - INFO - Epoch [101/300], Batch [4/43], Training Loss: 0.00001949
2024-11-06 14:07:07,712 - INFO - Epoch [101/300], Batch [5/43], Training Loss: 0.00002236
2024-11-06 14:07:07,715 - INFO - Epoch [101/300], Batch [6/43], Training Loss: 0.00001781
2024-11-06 14:07:07,719 - INFO - Epoch [101/300], Batch [7/43], Training Loss: 0.00001256
2024-11-06 14:07:07,723 - INFO - Epoch [101/300], Batch [8/43], Training Loss: 0.00002933
2024-11-06 14:07:07,726 - INFO - Epoch [101/300], Batch [9/43], Training Loss: 0.00002284
2024-11-06 14:07:07,730 - INFO - Epoch [101/300], Batch [10/43], Training Loss: 0.00002639
2024-11-06 14:07:07,733 - INFO - Epoch [101/300], Batch [11/43], Training Loss: 0.00001809
2024-11-06 14:07:07,736 - INFO - Epoch [101/300], Batch [12/43], Training Loss: 0.00000792
2024-11-06 14:07:07,740 - INFO - Epoch [101/300], Batch [13/43], Training Loss: 0.00002006
2024-11-06 14:07:07,743 - INFO - Epoch [101/300], Batch [14/43], Training Loss: 0.00001837
2024-11-06 14:07:07,745 - INFO - Epoch [101/300], Batch [15/43], Training Loss: 0.00001007
2024-11-06 14:07:07,749 - INFO - Epoch [101/300], Batch [16/43], Training Loss: 0.00003482
2024-11-06 14:07:07,752 - INFO - Epoch [101/300], Batch [17/43], Training Loss: 0.00001224
2024-11-06 14:07:07,755 - INFO - Epoch [101/300], Batch [18/43], Training Loss: 0.00001838
2024-11-06 14:07:07,758 - INFO - Epoch [101/300], Batch [19/43], Training Loss: 0.00002614
2024-11-06 14:07:07,761 - INFO - Epoch [101/300], Batch [20/43], Training Loss: 0.00000849
2024-11-06 14:07:07,764 - INFO - Epoch [101/300], Batch [21/43], Training Loss: 0.00001682
2024-11-06 14:07:07,768 - INFO - Epoch [101/300], Batch [22/43], Training Loss: 0.00001256
2024-11-06 14:07:07,770 - INFO - Epoch [101/300], Batch [23/43], Training Loss: 0.00001836
2024-11-06 14:07:07,773 - INFO - Epoch [101/300], Batch [24/43], Training Loss: 0.00001494
2024-11-06 14:07:07,776 - INFO - Epoch [101/300], Batch [25/43], Training Loss: 0.00000967
2024-11-06 14:07:07,780 - INFO - Epoch [101/300], Batch [26/43], Training Loss: 0.00003742
2024-11-06 14:07:07,783 - INFO - Epoch [101/300], Batch [27/43], Training Loss: 0.00001504
2024-11-06 14:07:07,786 - INFO - Epoch [101/300], Batch [28/43], Training Loss: 0.00000788
2024-11-06 14:07:07,789 - INFO - Epoch [101/300], Batch [29/43], Training Loss: 0.00002260
2024-11-06 14:07:07,792 - INFO - Epoch [101/300], Batch [30/43], Training Loss: 0.00001112
2024-11-06 14:07:07,796 - INFO - Epoch [101/300], Batch [31/43], Training Loss: 0.00002100
2024-11-06 14:07:07,799 - INFO - Epoch [101/300], Batch [32/43], Training Loss: 0.00001309
2024-11-06 14:07:07,803 - INFO - Epoch [101/300], Batch [33/43], Training Loss: 0.00000947
2024-11-06 14:07:07,806 - INFO - Epoch [101/300], Batch [34/43], Training Loss: 0.00000953
2024-11-06 14:07:07,810 - INFO - Epoch [101/300], Batch [35/43], Training Loss: 0.00001670
2024-11-06 14:07:07,813 - INFO - Epoch [101/300], Batch [36/43], Training Loss: 0.00004114
2024-11-06 14:07:07,817 - INFO - Epoch [101/300], Batch [37/43], Training Loss: 0.00001348
2024-11-06 14:07:07,821 - INFO - Epoch [101/300], Batch [38/43], Training Loss: 0.00004335
2024-11-06 14:07:07,824 - INFO - Epoch [101/300], Batch [39/43], Training Loss: 0.00001743
2024-11-06 14:07:07,828 - INFO - Epoch [101/300], Batch [40/43], Training Loss: 0.00001117
2024-11-06 14:07:07,833 - INFO - Epoch [101/300], Batch [41/43], Training Loss: 0.00002768
2024-11-06 14:07:07,837 - INFO - Epoch [101/300], Batch [42/43], Training Loss: 0.00002078
2024-11-06 14:07:07,842 - INFO - Epoch [101/300], Batch [43/43], Training Loss: 0.00001379
2024-11-06 14:07:07,856 - INFO - Epoch [101/300], Average Training Loss: 0.00002024, Validation Loss: 0.00002167
2024-11-06 14:07:07,862 - INFO - Epoch [102/300], Batch [1/43], Training Loss: 0.00004390
2024-11-06 14:07:07,867 - INFO - Epoch [102/300], Batch [2/43], Training Loss: 0.00002271
2024-11-06 14:07:07,871 - INFO - Epoch [102/300], Batch [3/43], Training Loss: 0.00002139
2024-11-06 14:07:07,876 - INFO - Epoch [102/300], Batch [4/43], Training Loss: 0.00002949
2024-11-06 14:07:07,880 - INFO - Epoch [102/300], Batch [5/43], Training Loss: 0.00001112
2024-11-06 14:07:07,884 - INFO - Epoch [102/300], Batch [6/43], Training Loss: 0.00000979
2024-11-06 14:07:07,889 - INFO - Epoch [102/300], Batch [7/43], Training Loss: 0.00002452
2024-11-06 14:07:07,893 - INFO - Epoch [102/300], Batch [8/43], Training Loss: 0.00001508
2024-11-06 14:07:07,898 - INFO - Epoch [102/300], Batch [9/43], Training Loss: 0.00002020
2024-11-06 14:07:07,903 - INFO - Epoch [102/300], Batch [10/43], Training Loss: 0.00000980
2024-11-06 14:07:07,907 - INFO - Epoch [102/300], Batch [11/43], Training Loss: 0.00002092
2024-11-06 14:07:07,911 - INFO - Epoch [102/300], Batch [12/43], Training Loss: 0.00001702
2024-11-06 14:07:07,915 - INFO - Epoch [102/300], Batch [13/43], Training Loss: 0.00001086
2024-11-06 14:07:07,918 - INFO - Epoch [102/300], Batch [14/43], Training Loss: 0.00001025
2024-11-06 14:07:07,924 - INFO - Epoch [102/300], Batch [15/43], Training Loss: 0.00001327
2024-11-06 14:07:07,928 - INFO - Epoch [102/300], Batch [16/43], Training Loss: 0.00001404
2024-11-06 14:07:07,932 - INFO - Epoch [102/300], Batch [17/43], Training Loss: 0.00002123
2024-11-06 14:07:07,937 - INFO - Epoch [102/300], Batch [18/43], Training Loss: 0.00001002
2024-11-06 14:07:07,941 - INFO - Epoch [102/300], Batch [19/43], Training Loss: 0.00002666
2024-11-06 14:07:07,945 - INFO - Epoch [102/300], Batch [20/43], Training Loss: 0.00002240
2024-11-06 14:07:07,948 - INFO - Epoch [102/300], Batch [21/43], Training Loss: 0.00003170
2024-11-06 14:07:07,952 - INFO - Epoch [102/300], Batch [22/43], Training Loss: 0.00002343
2024-11-06 14:07:07,956 - INFO - Epoch [102/300], Batch [23/43], Training Loss: 0.00001082
2024-11-06 14:07:07,959 - INFO - Epoch [102/300], Batch [24/43], Training Loss: 0.00000958
2024-11-06 14:07:07,963 - INFO - Epoch [102/300], Batch [25/43], Training Loss: 0.00002419
2024-11-06 14:07:07,967 - INFO - Epoch [102/300], Batch [26/43], Training Loss: 0.00000618
2024-11-06 14:07:07,970 - INFO - Epoch [102/300], Batch [27/43], Training Loss: 0.00001476
2024-11-06 14:07:07,973 - INFO - Epoch [102/300], Batch [28/43], Training Loss: 0.00001544
2024-11-06 14:07:07,977 - INFO - Epoch [102/300], Batch [29/43], Training Loss: 0.00002345
2024-11-06 14:07:07,981 - INFO - Epoch [102/300], Batch [30/43], Training Loss: 0.00002700
2024-11-06 14:07:07,984 - INFO - Epoch [102/300], Batch [31/43], Training Loss: 0.00002292
2024-11-06 14:07:07,988 - INFO - Epoch [102/300], Batch [32/43], Training Loss: 0.00001277
2024-11-06 14:07:07,992 - INFO - Epoch [102/300], Batch [33/43], Training Loss: 0.00001443
2024-11-06 14:07:07,996 - INFO - Epoch [102/300], Batch [34/43], Training Loss: 0.00002601
2024-11-06 14:07:08,001 - INFO - Epoch [102/300], Batch [35/43], Training Loss: 0.00003292
2024-11-06 14:07:08,006 - INFO - Epoch [102/300], Batch [36/43], Training Loss: 0.00001281
2024-11-06 14:07:08,009 - INFO - Epoch [102/300], Batch [37/43], Training Loss: 0.00001560
2024-11-06 14:07:08,014 - INFO - Epoch [102/300], Batch [38/43], Training Loss: 0.00004012
2024-11-06 14:07:08,018 - INFO - Epoch [102/300], Batch [39/43], Training Loss: 0.00001928
2024-11-06 14:07:08,022 - INFO - Epoch [102/300], Batch [40/43], Training Loss: 0.00001126
2024-11-06 14:07:08,025 - INFO - Epoch [102/300], Batch [41/43], Training Loss: 0.00001649
2024-11-06 14:07:08,029 - INFO - Epoch [102/300], Batch [42/43], Training Loss: 0.00002371
2024-11-06 14:07:08,034 - INFO - Epoch [102/300], Batch [43/43], Training Loss: 0.00001623
2024-11-06 14:07:08,047 - INFO - Epoch [102/300], Average Training Loss: 0.00001920, Validation Loss: 0.00002097
2024-11-06 14:07:08,051 - INFO - Epoch [103/300], Batch [1/43], Training Loss: 0.00003068
2024-11-06 14:07:08,055 - INFO - Epoch [103/300], Batch [2/43], Training Loss: 0.00003392
2024-11-06 14:07:08,058 - INFO - Epoch [103/300], Batch [3/43], Training Loss: 0.00000850
2024-11-06 14:07:08,063 - INFO - Epoch [103/300], Batch [4/43], Training Loss: 0.00001062
2024-11-06 14:07:08,068 - INFO - Epoch [103/300], Batch [5/43], Training Loss: 0.00001961
2024-11-06 14:07:08,072 - INFO - Epoch [103/300], Batch [6/43], Training Loss: 0.00001654
2024-11-06 14:07:08,076 - INFO - Epoch [103/300], Batch [7/43], Training Loss: 0.00000841
2024-11-06 14:07:08,079 - INFO - Epoch [103/300], Batch [8/43], Training Loss: 0.00001058
2024-11-06 14:07:08,083 - INFO - Epoch [103/300], Batch [9/43], Training Loss: 0.00000527
2024-11-06 14:07:08,088 - INFO - Epoch [103/300], Batch [10/43], Training Loss: 0.00001167
2024-11-06 14:07:08,092 - INFO - Epoch [103/300], Batch [11/43], Training Loss: 0.00001586
2024-11-06 14:07:08,096 - INFO - Epoch [103/300], Batch [12/43], Training Loss: 0.00001719
2024-11-06 14:07:08,099 - INFO - Epoch [103/300], Batch [13/43], Training Loss: 0.00001676
2024-11-06 14:07:08,102 - INFO - Epoch [103/300], Batch [14/43], Training Loss: 0.00000498
2024-11-06 14:07:08,105 - INFO - Epoch [103/300], Batch [15/43], Training Loss: 0.00001905
2024-11-06 14:07:08,109 - INFO - Epoch [103/300], Batch [16/43], Training Loss: 0.00001531
2024-11-06 14:07:08,113 - INFO - Epoch [103/300], Batch [17/43], Training Loss: 0.00002445
2024-11-06 14:07:08,116 - INFO - Epoch [103/300], Batch [18/43], Training Loss: 0.00002367
2024-11-06 14:07:08,119 - INFO - Epoch [103/300], Batch [19/43], Training Loss: 0.00002370
2024-11-06 14:07:08,122 - INFO - Epoch [103/300], Batch [20/43], Training Loss: 0.00002182
2024-11-06 14:07:08,125 - INFO - Epoch [103/300], Batch [21/43], Training Loss: 0.00001545
2024-11-06 14:07:08,129 - INFO - Epoch [103/300], Batch [22/43], Training Loss: 0.00001750
2024-11-06 14:07:08,132 - INFO - Epoch [103/300], Batch [23/43], Training Loss: 0.00001537
2024-11-06 14:07:08,136 - INFO - Epoch [103/300], Batch [24/43], Training Loss: 0.00001967
2024-11-06 14:07:08,140 - INFO - Epoch [103/300], Batch [25/43], Training Loss: 0.00000855
2024-11-06 14:07:08,143 - INFO - Epoch [103/300], Batch [26/43], Training Loss: 0.00003685
2024-11-06 14:07:08,147 - INFO - Epoch [103/300], Batch [27/43], Training Loss: 0.00001250
2024-11-06 14:07:08,150 - INFO - Epoch [103/300], Batch [28/43], Training Loss: 0.00002449
2024-11-06 14:07:08,153 - INFO - Epoch [103/300], Batch [29/43], Training Loss: 0.00002792
2024-11-06 14:07:08,157 - INFO - Epoch [103/300], Batch [30/43], Training Loss: 0.00001383
2024-11-06 14:07:08,161 - INFO - Epoch [103/300], Batch [31/43], Training Loss: 0.00001711
2024-11-06 14:07:08,165 - INFO - Epoch [103/300], Batch [32/43], Training Loss: 0.00001370
2024-11-06 14:07:08,168 - INFO - Epoch [103/300], Batch [33/43], Training Loss: 0.00001365
2024-11-06 14:07:08,171 - INFO - Epoch [103/300], Batch [34/43], Training Loss: 0.00000807
2024-11-06 14:07:08,175 - INFO - Epoch [103/300], Batch [35/43], Training Loss: 0.00001727
2024-11-06 14:07:08,179 - INFO - Epoch [103/300], Batch [36/43], Training Loss: 0.00000705
2024-11-06 14:07:08,184 - INFO - Epoch [103/300], Batch [37/43], Training Loss: 0.00000980
2024-11-06 14:07:08,189 - INFO - Epoch [103/300], Batch [38/43], Training Loss: 0.00002348
2024-11-06 14:07:08,193 - INFO - Epoch [103/300], Batch [39/43], Training Loss: 0.00001649
2024-11-06 14:07:08,197 - INFO - Epoch [103/300], Batch [40/43], Training Loss: 0.00001317
2024-11-06 14:07:08,200 - INFO - Epoch [103/300], Batch [41/43], Training Loss: 0.00002993
2024-11-06 14:07:08,205 - INFO - Epoch [103/300], Batch [42/43], Training Loss: 0.00000940
2024-11-06 14:07:08,209 - INFO - Epoch [103/300], Batch [43/43], Training Loss: 0.00002670
2024-11-06 14:07:08,219 - INFO - Epoch [103/300], Average Training Loss: 0.00001713, Validation Loss: 0.00002309
2024-11-06 14:07:08,223 - INFO - Epoch [104/300], Batch [1/43], Training Loss: 0.00002054
2024-11-06 14:07:08,226 - INFO - Epoch [104/300], Batch [2/43], Training Loss: 0.00001177
2024-11-06 14:07:08,229 - INFO - Epoch [104/300], Batch [3/43], Training Loss: 0.00002332
2024-11-06 14:07:08,232 - INFO - Epoch [104/300], Batch [4/43], Training Loss: 0.00003034
2024-11-06 14:07:08,236 - INFO - Epoch [104/300], Batch [5/43], Training Loss: 0.00003564
2024-11-06 14:07:08,239 - INFO - Epoch [104/300], Batch [6/43], Training Loss: 0.00002320
2024-11-06 14:07:08,243 - INFO - Epoch [104/300], Batch [7/43], Training Loss: 0.00001377
2024-11-06 14:07:08,246 - INFO - Epoch [104/300], Batch [8/43], Training Loss: 0.00001405
2024-11-06 14:07:08,249 - INFO - Epoch [104/300], Batch [9/43], Training Loss: 0.00002419
2024-11-06 14:07:08,252 - INFO - Epoch [104/300], Batch [10/43], Training Loss: 0.00002651
2024-11-06 14:07:08,255 - INFO - Epoch [104/300], Batch [11/43], Training Loss: 0.00001034
2024-11-06 14:07:08,257 - INFO - Epoch [104/300], Batch [12/43], Training Loss: 0.00002664
2024-11-06 14:07:08,260 - INFO - Epoch [104/300], Batch [13/43], Training Loss: 0.00002404
2024-11-06 14:07:08,263 - INFO - Epoch [104/300], Batch [14/43], Training Loss: 0.00003126
2024-11-06 14:07:08,266 - INFO - Epoch [104/300], Batch [15/43], Training Loss: 0.00000662
2024-11-06 14:07:08,269 - INFO - Epoch [104/300], Batch [16/43], Training Loss: 0.00001525
2024-11-06 14:07:08,271 - INFO - Epoch [104/300], Batch [17/43], Training Loss: 0.00000807
2024-11-06 14:07:08,274 - INFO - Epoch [104/300], Batch [18/43], Training Loss: 0.00002290
2024-11-06 14:07:08,277 - INFO - Epoch [104/300], Batch [19/43], Training Loss: 0.00001444
2024-11-06 14:07:08,280 - INFO - Epoch [104/300], Batch [20/43], Training Loss: 0.00000780
2024-11-06 14:07:08,283 - INFO - Epoch [104/300], Batch [21/43], Training Loss: 0.00002539
2024-11-06 14:07:08,286 - INFO - Epoch [104/300], Batch [22/43], Training Loss: 0.00001005
2024-11-06 14:07:08,289 - INFO - Epoch [104/300], Batch [23/43], Training Loss: 0.00001568
2024-11-06 14:07:08,292 - INFO - Epoch [104/300], Batch [24/43], Training Loss: 0.00001738
2024-11-06 14:07:08,295 - INFO - Epoch [104/300], Batch [25/43], Training Loss: 0.00001842
2024-11-06 14:07:08,298 - INFO - Epoch [104/300], Batch [26/43], Training Loss: 0.00001224
2024-11-06 14:07:08,301 - INFO - Epoch [104/300], Batch [27/43], Training Loss: 0.00001564
2024-11-06 14:07:08,305 - INFO - Epoch [104/300], Batch [28/43], Training Loss: 0.00001699
2024-11-06 14:07:08,308 - INFO - Epoch [104/300], Batch [29/43], Training Loss: 0.00001196
2024-11-06 14:07:08,313 - INFO - Epoch [104/300], Batch [30/43], Training Loss: 0.00001090
2024-11-06 14:07:08,318 - INFO - Epoch [104/300], Batch [31/43], Training Loss: 0.00002265
2024-11-06 14:07:08,321 - INFO - Epoch [104/300], Batch [32/43], Training Loss: 0.00001464
2024-11-06 14:07:08,324 - INFO - Epoch [104/300], Batch [33/43], Training Loss: 0.00001944
2024-11-06 14:07:08,328 - INFO - Epoch [104/300], Batch [34/43], Training Loss: 0.00001812
2024-11-06 14:07:08,332 - INFO - Epoch [104/300], Batch [35/43], Training Loss: 0.00000644
2024-11-06 14:07:08,335 - INFO - Epoch [104/300], Batch [36/43], Training Loss: 0.00002271
2024-11-06 14:07:08,339 - INFO - Epoch [104/300], Batch [37/43], Training Loss: 0.00002618
2024-11-06 14:07:08,342 - INFO - Epoch [104/300], Batch [38/43], Training Loss: 0.00001044
2024-11-06 14:07:08,346 - INFO - Epoch [104/300], Batch [39/43], Training Loss: 0.00001610
2024-11-06 14:07:08,350 - INFO - Epoch [104/300], Batch [40/43], Training Loss: 0.00001936
2024-11-06 14:07:08,354 - INFO - Epoch [104/300], Batch [41/43], Training Loss: 0.00001051
2024-11-06 14:07:08,358 - INFO - Epoch [104/300], Batch [42/43], Training Loss: 0.00001348
2024-11-06 14:07:08,362 - INFO - Epoch [104/300], Batch [43/43], Training Loss: 0.00000634
2024-11-06 14:07:08,375 - INFO - Epoch [104/300], Average Training Loss: 0.00001748, Validation Loss: 0.00003013
2024-11-06 14:07:08,379 - INFO - Epoch [105/300], Batch [1/43], Training Loss: 0.00001323
2024-11-06 14:07:08,382 - INFO - Epoch [105/300], Batch [2/43], Training Loss: 0.00002805
2024-11-06 14:07:08,385 - INFO - Epoch [105/300], Batch [3/43], Training Loss: 0.00002426
2024-11-06 14:07:08,389 - INFO - Epoch [105/300], Batch [4/43], Training Loss: 0.00000985
2024-11-06 14:07:08,392 - INFO - Epoch [105/300], Batch [5/43], Training Loss: 0.00001127
2024-11-06 14:07:08,396 - INFO - Epoch [105/300], Batch [6/43], Training Loss: 0.00001448
2024-11-06 14:07:08,399 - INFO - Epoch [105/300], Batch [7/43], Training Loss: 0.00000798
2024-11-06 14:07:08,402 - INFO - Epoch [105/300], Batch [8/43], Training Loss: 0.00001865
2024-11-06 14:07:08,405 - INFO - Epoch [105/300], Batch [9/43], Training Loss: 0.00001272
2024-11-06 14:07:08,408 - INFO - Epoch [105/300], Batch [10/43], Training Loss: 0.00001837
2024-11-06 14:07:08,411 - INFO - Epoch [105/300], Batch [11/43], Training Loss: 0.00001679
2024-11-06 14:07:08,414 - INFO - Epoch [105/300], Batch [12/43], Training Loss: 0.00002503
2024-11-06 14:07:08,417 - INFO - Epoch [105/300], Batch [13/43], Training Loss: 0.00000733
2024-11-06 14:07:08,420 - INFO - Epoch [105/300], Batch [14/43], Training Loss: 0.00001981
2024-11-06 14:07:08,423 - INFO - Epoch [105/300], Batch [15/43], Training Loss: 0.00001355
2024-11-06 14:07:08,426 - INFO - Epoch [105/300], Batch [16/43], Training Loss: 0.00000904
2024-11-06 14:07:08,429 - INFO - Epoch [105/300], Batch [17/43], Training Loss: 0.00001015
2024-11-06 14:07:08,432 - INFO - Epoch [105/300], Batch [18/43], Training Loss: 0.00000834
2024-11-06 14:07:08,435 - INFO - Epoch [105/300], Batch [19/43], Training Loss: 0.00002974
2024-11-06 14:07:08,438 - INFO - Epoch [105/300], Batch [20/43], Training Loss: 0.00000843
2024-11-06 14:07:08,441 - INFO - Epoch [105/300], Batch [21/43], Training Loss: 0.00000699
2024-11-06 14:07:08,444 - INFO - Epoch [105/300], Batch [22/43], Training Loss: 0.00002881
2024-11-06 14:07:08,447 - INFO - Epoch [105/300], Batch [23/43], Training Loss: 0.00004208
2024-11-06 14:07:08,450 - INFO - Epoch [105/300], Batch [24/43], Training Loss: 0.00000397
2024-11-06 14:07:08,453 - INFO - Epoch [105/300], Batch [25/43], Training Loss: 0.00001497
2024-11-06 14:07:08,456 - INFO - Epoch [105/300], Batch [26/43], Training Loss: 0.00001378
2024-11-06 14:07:08,459 - INFO - Epoch [105/300], Batch [27/43], Training Loss: 0.00000491
2024-11-06 14:07:08,462 - INFO - Epoch [105/300], Batch [28/43], Training Loss: 0.00001360
2024-11-06 14:07:08,464 - INFO - Epoch [105/300], Batch [29/43], Training Loss: 0.00002499
2024-11-06 14:07:08,468 - INFO - Epoch [105/300], Batch [30/43], Training Loss: 0.00002709
2024-11-06 14:07:08,471 - INFO - Epoch [105/300], Batch [31/43], Training Loss: 0.00003452
2024-11-06 14:07:08,474 - INFO - Epoch [105/300], Batch [32/43], Training Loss: 0.00002594
2024-11-06 14:07:08,477 - INFO - Epoch [105/300], Batch [33/43], Training Loss: 0.00001381
2024-11-06 14:07:08,481 - INFO - Epoch [105/300], Batch [34/43], Training Loss: 0.00001885
2024-11-06 14:07:08,484 - INFO - Epoch [105/300], Batch [35/43], Training Loss: 0.00001671
2024-11-06 14:07:08,487 - INFO - Epoch [105/300], Batch [36/43], Training Loss: 0.00001136
2024-11-06 14:07:08,491 - INFO - Epoch [105/300], Batch [37/43], Training Loss: 0.00000732
2024-11-06 14:07:08,494 - INFO - Epoch [105/300], Batch [38/43], Training Loss: 0.00001203
2024-11-06 14:07:08,499 - INFO - Epoch [105/300], Batch [39/43], Training Loss: 0.00001248
2024-11-06 14:07:08,504 - INFO - Epoch [105/300], Batch [40/43], Training Loss: 0.00001332
2024-11-06 14:07:08,508 - INFO - Epoch [105/300], Batch [41/43], Training Loss: 0.00000625
2024-11-06 14:07:08,512 - INFO - Epoch [105/300], Batch [42/43], Training Loss: 0.00002026
2024-11-06 14:07:08,516 - INFO - Epoch [105/300], Batch [43/43], Training Loss: 0.00000700
2024-11-06 14:07:08,528 - INFO - Epoch [105/300], Average Training Loss: 0.00001600, Validation Loss: 0.00002399
2024-11-06 14:07:08,532 - INFO - Epoch [106/300], Batch [1/43], Training Loss: 0.00001597
2024-11-06 14:07:08,536 - INFO - Epoch [106/300], Batch [2/43], Training Loss: 0.00002265
2024-11-06 14:07:08,541 - INFO - Epoch [106/300], Batch [3/43], Training Loss: 0.00000799
2024-11-06 14:07:08,545 - INFO - Epoch [106/300], Batch [4/43], Training Loss: 0.00002017
2024-11-06 14:07:08,550 - INFO - Epoch [106/300], Batch [5/43], Training Loss: 0.00003431
2024-11-06 14:07:08,554 - INFO - Epoch [106/300], Batch [6/43], Training Loss: 0.00002549
2024-11-06 14:07:08,558 - INFO - Epoch [106/300], Batch [7/43], Training Loss: 0.00000775
2024-11-06 14:07:08,563 - INFO - Epoch [106/300], Batch [8/43], Training Loss: 0.00002286
2024-11-06 14:07:08,568 - INFO - Epoch [106/300], Batch [9/43], Training Loss: 0.00000618
2024-11-06 14:07:08,571 - INFO - Epoch [106/300], Batch [10/43], Training Loss: 0.00001978
2024-11-06 14:07:08,575 - INFO - Epoch [106/300], Batch [11/43], Training Loss: 0.00003597
2024-11-06 14:07:08,580 - INFO - Epoch [106/300], Batch [12/43], Training Loss: 0.00000665
2024-11-06 14:07:08,584 - INFO - Epoch [106/300], Batch [13/43], Training Loss: 0.00001014
2024-11-06 14:07:08,587 - INFO - Epoch [106/300], Batch [14/43], Training Loss: 0.00003689
2024-11-06 14:07:08,591 - INFO - Epoch [106/300], Batch [15/43], Training Loss: 0.00001827
2024-11-06 14:07:08,595 - INFO - Epoch [106/300], Batch [16/43], Training Loss: 0.00003509
2024-11-06 14:07:08,599 - INFO - Epoch [106/300], Batch [17/43], Training Loss: 0.00003180
2024-11-06 14:07:08,603 - INFO - Epoch [106/300], Batch [18/43], Training Loss: 0.00001033
2024-11-06 14:07:08,606 - INFO - Epoch [106/300], Batch [19/43], Training Loss: 0.00001273
2024-11-06 14:07:08,610 - INFO - Epoch [106/300], Batch [20/43], Training Loss: 0.00004227
2024-11-06 14:07:08,615 - INFO - Epoch [106/300], Batch [21/43], Training Loss: 0.00002880
2024-11-06 14:07:08,619 - INFO - Epoch [106/300], Batch [22/43], Training Loss: 0.00001295
2024-11-06 14:07:08,623 - INFO - Epoch [106/300], Batch [23/43], Training Loss: 0.00004808
2024-11-06 14:07:08,626 - INFO - Epoch [106/300], Batch [24/43], Training Loss: 0.00003837
2024-11-06 14:07:08,630 - INFO - Epoch [106/300], Batch [25/43], Training Loss: 0.00000427
2024-11-06 14:07:08,634 - INFO - Epoch [106/300], Batch [26/43], Training Loss: 0.00001820
2024-11-06 14:07:08,638 - INFO - Epoch [106/300], Batch [27/43], Training Loss: 0.00002205
2024-11-06 14:07:08,642 - INFO - Epoch [106/300], Batch [28/43], Training Loss: 0.00001240
2024-11-06 14:07:08,646 - INFO - Epoch [106/300], Batch [29/43], Training Loss: 0.00001296
2024-11-06 14:07:08,650 - INFO - Epoch [106/300], Batch [30/43], Training Loss: 0.00004284
2024-11-06 14:07:08,653 - INFO - Epoch [106/300], Batch [31/43], Training Loss: 0.00001514
2024-11-06 14:07:08,657 - INFO - Epoch [106/300], Batch [32/43], Training Loss: 0.00000990
2024-11-06 14:07:08,661 - INFO - Epoch [106/300], Batch [33/43], Training Loss: 0.00005151
2024-11-06 14:07:08,665 - INFO - Epoch [106/300], Batch [34/43], Training Loss: 0.00002445
2024-11-06 14:07:08,669 - INFO - Epoch [106/300], Batch [35/43], Training Loss: 0.00001314
2024-11-06 14:07:08,672 - INFO - Epoch [106/300], Batch [36/43], Training Loss: 0.00001361
2024-11-06 14:07:08,676 - INFO - Epoch [106/300], Batch [37/43], Training Loss: 0.00002228
2024-11-06 14:07:08,680 - INFO - Epoch [106/300], Batch [38/43], Training Loss: 0.00001847
2024-11-06 14:07:08,684 - INFO - Epoch [106/300], Batch [39/43], Training Loss: 0.00002051
2024-11-06 14:07:08,688 - INFO - Epoch [106/300], Batch [40/43], Training Loss: 0.00001340
2024-11-06 14:07:08,693 - INFO - Epoch [106/300], Batch [41/43], Training Loss: 0.00002182
2024-11-06 14:07:08,696 - INFO - Epoch [106/300], Batch [42/43], Training Loss: 0.00001949
2024-11-06 14:07:08,701 - INFO - Epoch [106/300], Batch [43/43], Training Loss: 0.00001812
2024-11-06 14:07:08,711 - INFO - Epoch [106/300], Average Training Loss: 0.00002154, Validation Loss: 0.00002097
2024-11-06 14:07:08,716 - INFO - Epoch [107/300], Batch [1/43], Training Loss: 0.00001490
2024-11-06 14:07:08,720 - INFO - Epoch [107/300], Batch [2/43], Training Loss: 0.00000650
2024-11-06 14:07:08,725 - INFO - Epoch [107/300], Batch [3/43], Training Loss: 0.00001596
2024-11-06 14:07:08,730 - INFO - Epoch [107/300], Batch [4/43], Training Loss: 0.00001354
2024-11-06 14:07:08,735 - INFO - Epoch [107/300], Batch [5/43], Training Loss: 0.00002313
2024-11-06 14:07:08,739 - INFO - Epoch [107/300], Batch [6/43], Training Loss: 0.00000947
2024-11-06 14:07:08,743 - INFO - Epoch [107/300], Batch [7/43], Training Loss: 0.00001548
2024-11-06 14:07:08,747 - INFO - Epoch [107/300], Batch [8/43], Training Loss: 0.00001963
2024-11-06 14:07:08,751 - INFO - Epoch [107/300], Batch [9/43], Training Loss: 0.00002996
2024-11-06 14:07:08,755 - INFO - Epoch [107/300], Batch [10/43], Training Loss: 0.00003254
2024-11-06 14:07:08,759 - INFO - Epoch [107/300], Batch [11/43], Training Loss: 0.00001258
2024-11-06 14:07:08,763 - INFO - Epoch [107/300], Batch [12/43], Training Loss: 0.00000937
2024-11-06 14:07:08,767 - INFO - Epoch [107/300], Batch [13/43], Training Loss: 0.00001539
2024-11-06 14:07:08,770 - INFO - Epoch [107/300], Batch [14/43], Training Loss: 0.00001612
2024-11-06 14:07:08,773 - INFO - Epoch [107/300], Batch [15/43], Training Loss: 0.00000806
2024-11-06 14:07:08,776 - INFO - Epoch [107/300], Batch [16/43], Training Loss: 0.00001416
2024-11-06 14:07:08,781 - INFO - Epoch [107/300], Batch [17/43], Training Loss: 0.00000714
2024-11-06 14:07:08,784 - INFO - Epoch [107/300], Batch [18/43], Training Loss: 0.00001952
2024-11-06 14:07:08,787 - INFO - Epoch [107/300], Batch [19/43], Training Loss: 0.00002127
2024-11-06 14:07:08,791 - INFO - Epoch [107/300], Batch [20/43], Training Loss: 0.00002186
2024-11-06 14:07:08,794 - INFO - Epoch [107/300], Batch [21/43], Training Loss: 0.00001236
2024-11-06 14:07:08,798 - INFO - Epoch [107/300], Batch [22/43], Training Loss: 0.00001563
2024-11-06 14:07:08,801 - INFO - Epoch [107/300], Batch [23/43], Training Loss: 0.00001913
2024-11-06 14:07:08,805 - INFO - Epoch [107/300], Batch [24/43], Training Loss: 0.00003628
2024-11-06 14:07:08,808 - INFO - Epoch [107/300], Batch [25/43], Training Loss: 0.00000800
2024-11-06 14:07:08,811 - INFO - Epoch [107/300], Batch [26/43], Training Loss: 0.00001107
2024-11-06 14:07:08,815 - INFO - Epoch [107/300], Batch [27/43], Training Loss: 0.00002751
2024-11-06 14:07:08,818 - INFO - Epoch [107/300], Batch [28/43], Training Loss: 0.00001500
2024-11-06 14:07:08,821 - INFO - Epoch [107/300], Batch [29/43], Training Loss: 0.00001808
2024-11-06 14:07:08,825 - INFO - Epoch [107/300], Batch [30/43], Training Loss: 0.00002356
2024-11-06 14:07:08,828 - INFO - Epoch [107/300], Batch [31/43], Training Loss: 0.00000998
2024-11-06 14:07:08,832 - INFO - Epoch [107/300], Batch [32/43], Training Loss: 0.00001312
2024-11-06 14:07:08,836 - INFO - Epoch [107/300], Batch [33/43], Training Loss: 0.00003136
2024-11-06 14:07:08,840 - INFO - Epoch [107/300], Batch [34/43], Training Loss: 0.00001004
2024-11-06 14:07:08,845 - INFO - Epoch [107/300], Batch [35/43], Training Loss: 0.00002815
2024-11-06 14:07:08,849 - INFO - Epoch [107/300], Batch [36/43], Training Loss: 0.00002474
2024-11-06 14:07:08,853 - INFO - Epoch [107/300], Batch [37/43], Training Loss: 0.00001081
2024-11-06 14:07:08,857 - INFO - Epoch [107/300], Batch [38/43], Training Loss: 0.00001710
2024-11-06 14:07:08,861 - INFO - Epoch [107/300], Batch [39/43], Training Loss: 0.00001828
2024-11-06 14:07:08,865 - INFO - Epoch [107/300], Batch [40/43], Training Loss: 0.00002228
2024-11-06 14:07:08,869 - INFO - Epoch [107/300], Batch [41/43], Training Loss: 0.00001684
2024-11-06 14:07:08,873 - INFO - Epoch [107/300], Batch [42/43], Training Loss: 0.00001898
2024-11-06 14:07:08,877 - INFO - Epoch [107/300], Batch [43/43], Training Loss: 0.00002544
2024-11-06 14:07:08,890 - INFO - Epoch [107/300], Average Training Loss: 0.00001768, Validation Loss: 0.00002054
2024-11-06 14:07:08,894 - INFO - Epoch [108/300], Batch [1/43], Training Loss: 0.00001120
2024-11-06 14:07:08,899 - INFO - Epoch [108/300], Batch [2/43], Training Loss: 0.00001205
2024-11-06 14:07:08,903 - INFO - Epoch [108/300], Batch [3/43], Training Loss: 0.00002210
2024-11-06 14:07:08,908 - INFO - Epoch [108/300], Batch [4/43], Training Loss: 0.00001171
2024-11-06 14:07:08,912 - INFO - Epoch [108/300], Batch [5/43], Training Loss: 0.00004312
2024-11-06 14:07:08,916 - INFO - Epoch [108/300], Batch [6/43], Training Loss: 0.00000490
2024-11-06 14:07:08,920 - INFO - Epoch [108/300], Batch [7/43], Training Loss: 0.00001017
2024-11-06 14:07:08,923 - INFO - Epoch [108/300], Batch [8/43], Training Loss: 0.00002813
2024-11-06 14:07:08,927 - INFO - Epoch [108/300], Batch [9/43], Training Loss: 0.00001546
2024-11-06 14:07:08,930 - INFO - Epoch [108/300], Batch [10/43], Training Loss: 0.00000831
2024-11-06 14:07:08,934 - INFO - Epoch [108/300], Batch [11/43], Training Loss: 0.00001446
2024-11-06 14:07:08,938 - INFO - Epoch [108/300], Batch [12/43], Training Loss: 0.00002626
2024-11-06 14:07:08,943 - INFO - Epoch [108/300], Batch [13/43], Training Loss: 0.00000945
2024-11-06 14:07:08,946 - INFO - Epoch [108/300], Batch [14/43], Training Loss: 0.00001926
2024-11-06 14:07:08,950 - INFO - Epoch [108/300], Batch [15/43], Training Loss: 0.00000536
2024-11-06 14:07:08,954 - INFO - Epoch [108/300], Batch [16/43], Training Loss: 0.00001898
2024-11-06 14:07:08,958 - INFO - Epoch [108/300], Batch [17/43], Training Loss: 0.00002472
2024-11-06 14:07:08,961 - INFO - Epoch [108/300], Batch [18/43], Training Loss: 0.00000657
2024-11-06 14:07:08,965 - INFO - Epoch [108/300], Batch [19/43], Training Loss: 0.00001781
2024-11-06 14:07:08,969 - INFO - Epoch [108/300], Batch [20/43], Training Loss: 0.00003430
2024-11-06 14:07:08,973 - INFO - Epoch [108/300], Batch [21/43], Training Loss: 0.00002151
2024-11-06 14:07:08,977 - INFO - Epoch [108/300], Batch [22/43], Training Loss: 0.00001332
2024-11-06 14:07:08,981 - INFO - Epoch [108/300], Batch [23/43], Training Loss: 0.00002521
2024-11-06 14:07:08,984 - INFO - Epoch [108/300], Batch [24/43], Training Loss: 0.00004239
2024-11-06 14:07:08,989 - INFO - Epoch [108/300], Batch [25/43], Training Loss: 0.00000933
2024-11-06 14:07:08,992 - INFO - Epoch [108/300], Batch [26/43], Training Loss: 0.00000999
2024-11-06 14:07:08,996 - INFO - Epoch [108/300], Batch [27/43], Training Loss: 0.00003737
2024-11-06 14:07:09,000 - INFO - Epoch [108/300], Batch [28/43], Training Loss: 0.00002467
2024-11-06 14:07:09,003 - INFO - Epoch [108/300], Batch [29/43], Training Loss: 0.00000839
2024-11-06 14:07:09,007 - INFO - Epoch [108/300], Batch [30/43], Training Loss: 0.00002283
2024-11-06 14:07:09,010 - INFO - Epoch [108/300], Batch [31/43], Training Loss: 0.00001750
2024-11-06 14:07:09,014 - INFO - Epoch [108/300], Batch [32/43], Training Loss: 0.00001564
2024-11-06 14:07:09,018 - INFO - Epoch [108/300], Batch [33/43], Training Loss: 0.00002372
2024-11-06 14:07:09,021 - INFO - Epoch [108/300], Batch [34/43], Training Loss: 0.00000942
2024-11-06 14:07:09,025 - INFO - Epoch [108/300], Batch [35/43], Training Loss: 0.00003246
2024-11-06 14:07:09,028 - INFO - Epoch [108/300], Batch [36/43], Training Loss: 0.00002756
2024-11-06 14:07:09,032 - INFO - Epoch [108/300], Batch [37/43], Training Loss: 0.00001906
2024-11-06 14:07:09,036 - INFO - Epoch [108/300], Batch [38/43], Training Loss: 0.00002033
2024-11-06 14:07:09,040 - INFO - Epoch [108/300], Batch [39/43], Training Loss: 0.00002261
2024-11-06 14:07:09,045 - INFO - Epoch [108/300], Batch [40/43], Training Loss: 0.00001075
2024-11-06 14:07:09,049 - INFO - Epoch [108/300], Batch [41/43], Training Loss: 0.00001939
2024-11-06 14:07:09,053 - INFO - Epoch [108/300], Batch [42/43], Training Loss: 0.00001607
2024-11-06 14:07:09,058 - INFO - Epoch [108/300], Batch [43/43], Training Loss: 0.00001031
2024-11-06 14:07:09,071 - INFO - Epoch [108/300], Average Training Loss: 0.00001870, Validation Loss: 0.00002162
2024-11-06 14:07:09,076 - INFO - Epoch [109/300], Batch [1/43], Training Loss: 0.00001456
2024-11-06 14:07:09,081 - INFO - Epoch [109/300], Batch [2/43], Training Loss: 0.00000533
2024-11-06 14:07:09,086 - INFO - Epoch [109/300], Batch [3/43], Training Loss: 0.00001718
2024-11-06 14:07:09,090 - INFO - Epoch [109/300], Batch [4/43], Training Loss: 0.00001760
2024-11-06 14:07:09,095 - INFO - Epoch [109/300], Batch [5/43], Training Loss: 0.00001748
2024-11-06 14:07:09,098 - INFO - Epoch [109/300], Batch [6/43], Training Loss: 0.00001220
2024-11-06 14:07:09,103 - INFO - Epoch [109/300], Batch [7/43], Training Loss: 0.00002334
2024-11-06 14:07:09,106 - INFO - Epoch [109/300], Batch [8/43], Training Loss: 0.00001426
2024-11-06 14:07:09,110 - INFO - Epoch [109/300], Batch [9/43], Training Loss: 0.00001805
2024-11-06 14:07:09,113 - INFO - Epoch [109/300], Batch [10/43], Training Loss: 0.00000634
2024-11-06 14:07:09,116 - INFO - Epoch [109/300], Batch [11/43], Training Loss: 0.00001706
2024-11-06 14:07:09,120 - INFO - Epoch [109/300], Batch [12/43], Training Loss: 0.00001615
2024-11-06 14:07:09,123 - INFO - Epoch [109/300], Batch [13/43], Training Loss: 0.00001920
2024-11-06 14:07:09,127 - INFO - Epoch [109/300], Batch [14/43], Training Loss: 0.00001076
2024-11-06 14:07:09,131 - INFO - Epoch [109/300], Batch [15/43], Training Loss: 0.00001652
2024-11-06 14:07:09,134 - INFO - Epoch [109/300], Batch [16/43], Training Loss: 0.00003412
2024-11-06 14:07:09,140 - INFO - Epoch [109/300], Batch [17/43], Training Loss: 0.00002318
2024-11-06 14:07:09,144 - INFO - Epoch [109/300], Batch [18/43], Training Loss: 0.00001208
2024-11-06 14:07:09,148 - INFO - Epoch [109/300], Batch [19/43], Training Loss: 0.00002516
2024-11-06 14:07:09,151 - INFO - Epoch [109/300], Batch [20/43], Training Loss: 0.00003932
2024-11-06 14:07:09,154 - INFO - Epoch [109/300], Batch [21/43], Training Loss: 0.00002836
2024-11-06 14:07:09,158 - INFO - Epoch [109/300], Batch [22/43], Training Loss: 0.00001633
2024-11-06 14:07:09,162 - INFO - Epoch [109/300], Batch [23/43], Training Loss: 0.00001540
2024-11-06 14:07:09,166 - INFO - Epoch [109/300], Batch [24/43], Training Loss: 0.00003584
2024-11-06 14:07:09,169 - INFO - Epoch [109/300], Batch [25/43], Training Loss: 0.00001801
2024-11-06 14:07:09,173 - INFO - Epoch [109/300], Batch [26/43], Training Loss: 0.00002060
2024-11-06 14:07:09,177 - INFO - Epoch [109/300], Batch [27/43], Training Loss: 0.00001679
2024-11-06 14:07:09,181 - INFO - Epoch [109/300], Batch [28/43], Training Loss: 0.00003431
2024-11-06 14:07:09,184 - INFO - Epoch [109/300], Batch [29/43], Training Loss: 0.00001764
2024-11-06 14:07:09,189 - INFO - Epoch [109/300], Batch [30/43], Training Loss: 0.00001094
2024-11-06 14:07:09,193 - INFO - Epoch [109/300], Batch [31/43], Training Loss: 0.00003604
2024-11-06 14:07:09,197 - INFO - Epoch [109/300], Batch [32/43], Training Loss: 0.00003573
2024-11-06 14:07:09,202 - INFO - Epoch [109/300], Batch [33/43], Training Loss: 0.00000874
2024-11-06 14:07:09,206 - INFO - Epoch [109/300], Batch [34/43], Training Loss: 0.00004015
2024-11-06 14:07:09,210 - INFO - Epoch [109/300], Batch [35/43], Training Loss: 0.00003361
2024-11-06 14:07:09,213 - INFO - Epoch [109/300], Batch [36/43], Training Loss: 0.00000747
2024-11-06 14:07:09,217 - INFO - Epoch [109/300], Batch [37/43], Training Loss: 0.00002005
2024-11-06 14:07:09,221 - INFO - Epoch [109/300], Batch [38/43], Training Loss: 0.00002491
2024-11-06 14:07:09,225 - INFO - Epoch [109/300], Batch [39/43], Training Loss: 0.00004092
2024-11-06 14:07:09,229 - INFO - Epoch [109/300], Batch [40/43], Training Loss: 0.00002474
2024-11-06 14:07:09,234 - INFO - Epoch [109/300], Batch [41/43], Training Loss: 0.00004021
2024-11-06 14:07:09,239 - INFO - Epoch [109/300], Batch [42/43], Training Loss: 0.00002753
2024-11-06 14:07:09,243 - INFO - Epoch [109/300], Batch [43/43], Training Loss: 0.00001332
2024-11-06 14:07:09,255 - INFO - Epoch [109/300], Average Training Loss: 0.00002157, Validation Loss: 0.00002023
2024-11-06 14:07:09,259 - INFO - Epoch [110/300], Batch [1/43], Training Loss: 0.00003294
2024-11-06 14:07:09,264 - INFO - Epoch [110/300], Batch [2/43], Training Loss: 0.00000741
2024-11-06 14:07:09,268 - INFO - Epoch [110/300], Batch [3/43], Training Loss: 0.00001994
2024-11-06 14:07:09,272 - INFO - Epoch [110/300], Batch [4/43], Training Loss: 0.00001831
2024-11-06 14:07:09,276 - INFO - Epoch [110/300], Batch [5/43], Training Loss: 0.00000767
2024-11-06 14:07:09,281 - INFO - Epoch [110/300], Batch [6/43], Training Loss: 0.00000757
2024-11-06 14:07:09,285 - INFO - Epoch [110/300], Batch [7/43], Training Loss: 0.00002038
2024-11-06 14:07:09,289 - INFO - Epoch [110/300], Batch [8/43], Training Loss: 0.00001408
2024-11-06 14:07:09,293 - INFO - Epoch [110/300], Batch [9/43], Training Loss: 0.00001953
2024-11-06 14:07:09,297 - INFO - Epoch [110/300], Batch [10/43], Training Loss: 0.00001953
2024-11-06 14:07:09,301 - INFO - Epoch [110/300], Batch [11/43], Training Loss: 0.00002570
2024-11-06 14:07:09,306 - INFO - Epoch [110/300], Batch [12/43], Training Loss: 0.00003251
2024-11-06 14:07:09,310 - INFO - Epoch [110/300], Batch [13/43], Training Loss: 0.00001995
2024-11-06 14:07:09,314 - INFO - Epoch [110/300], Batch [14/43], Training Loss: 0.00001258
2024-11-06 14:07:09,318 - INFO - Epoch [110/300], Batch [15/43], Training Loss: 0.00000695
2024-11-06 14:07:09,322 - INFO - Epoch [110/300], Batch [16/43], Training Loss: 0.00001146
2024-11-06 14:07:09,326 - INFO - Epoch [110/300], Batch [17/43], Training Loss: 0.00001537
2024-11-06 14:07:09,330 - INFO - Epoch [110/300], Batch [18/43], Training Loss: 0.00002058
2024-11-06 14:07:09,334 - INFO - Epoch [110/300], Batch [19/43], Training Loss: 0.00001757
2024-11-06 14:07:09,338 - INFO - Epoch [110/300], Batch [20/43], Training Loss: 0.00001796
2024-11-06 14:07:09,342 - INFO - Epoch [110/300], Batch [21/43], Training Loss: 0.00001072
2024-11-06 14:07:09,346 - INFO - Epoch [110/300], Batch [22/43], Training Loss: 0.00002291
2024-11-06 14:07:09,350 - INFO - Epoch [110/300], Batch [23/43], Training Loss: 0.00000944
2024-11-06 14:07:09,355 - INFO - Epoch [110/300], Batch [24/43], Training Loss: 0.00001171
2024-11-06 14:07:09,360 - INFO - Epoch [110/300], Batch [25/43], Training Loss: 0.00002501
2024-11-06 14:07:09,364 - INFO - Epoch [110/300], Batch [26/43], Training Loss: 0.00001875
2024-11-06 14:07:09,368 - INFO - Epoch [110/300], Batch [27/43], Training Loss: 0.00001091
2024-11-06 14:07:09,372 - INFO - Epoch [110/300], Batch [28/43], Training Loss: 0.00002604
2024-11-06 14:07:09,376 - INFO - Epoch [110/300], Batch [29/43], Training Loss: 0.00002477
2024-11-06 14:07:09,379 - INFO - Epoch [110/300], Batch [30/43], Training Loss: 0.00002338
2024-11-06 14:07:09,382 - INFO - Epoch [110/300], Batch [31/43], Training Loss: 0.00001085
2024-11-06 14:07:09,387 - INFO - Epoch [110/300], Batch [32/43], Training Loss: 0.00003114
2024-11-06 14:07:09,391 - INFO - Epoch [110/300], Batch [33/43], Training Loss: 0.00002587
2024-11-06 14:07:09,395 - INFO - Epoch [110/300], Batch [34/43], Training Loss: 0.00001882
2024-11-06 14:07:09,399 - INFO - Epoch [110/300], Batch [35/43], Training Loss: 0.00000930
2024-11-06 14:07:09,403 - INFO - Epoch [110/300], Batch [36/43], Training Loss: 0.00002164
2024-11-06 14:07:09,408 - INFO - Epoch [110/300], Batch [37/43], Training Loss: 0.00001822
2024-11-06 14:07:09,412 - INFO - Epoch [110/300], Batch [38/43], Training Loss: 0.00000946
2024-11-06 14:07:09,416 - INFO - Epoch [110/300], Batch [39/43], Training Loss: 0.00002083
2024-11-06 14:07:09,420 - INFO - Epoch [110/300], Batch [40/43], Training Loss: 0.00001315
2024-11-06 14:07:09,424 - INFO - Epoch [110/300], Batch [41/43], Training Loss: 0.00001096
2024-11-06 14:07:09,428 - INFO - Epoch [110/300], Batch [42/43], Training Loss: 0.00001648
2024-11-06 14:07:09,433 - INFO - Epoch [110/300], Batch [43/43], Training Loss: 0.00001576
2024-11-06 14:07:09,445 - INFO - Epoch [110/300], Average Training Loss: 0.00001754, Validation Loss: 0.00002118
2024-11-06 14:07:09,450 - INFO - Epoch [111/300], Batch [1/43], Training Loss: 0.00001188
2024-11-06 14:07:09,454 - INFO - Epoch [111/300], Batch [2/43], Training Loss: 0.00004120
2024-11-06 14:07:09,458 - INFO - Epoch [111/300], Batch [3/43], Training Loss: 0.00000996
2024-11-06 14:07:09,462 - INFO - Epoch [111/300], Batch [4/43], Training Loss: 0.00001395
2024-11-06 14:07:09,466 - INFO - Epoch [111/300], Batch [5/43], Training Loss: 0.00001649
2024-11-06 14:07:09,471 - INFO - Epoch [111/300], Batch [6/43], Training Loss: 0.00001540
2024-11-06 14:07:09,475 - INFO - Epoch [111/300], Batch [7/43], Training Loss: 0.00000733
2024-11-06 14:07:09,479 - INFO - Epoch [111/300], Batch [8/43], Training Loss: 0.00002859
2024-11-06 14:07:09,483 - INFO - Epoch [111/300], Batch [9/43], Training Loss: 0.00001820
2024-11-06 14:07:09,488 - INFO - Epoch [111/300], Batch [10/43], Training Loss: 0.00000673
2024-11-06 14:07:09,492 - INFO - Epoch [111/300], Batch [11/43], Training Loss: 0.00000160
2024-11-06 14:07:09,496 - INFO - Epoch [111/300], Batch [12/43], Training Loss: 0.00000585
2024-11-06 14:07:09,500 - INFO - Epoch [111/300], Batch [13/43], Training Loss: 0.00002637
2024-11-06 14:07:09,505 - INFO - Epoch [111/300], Batch [14/43], Training Loss: 0.00001736
2024-11-06 14:07:09,509 - INFO - Epoch [111/300], Batch [15/43], Training Loss: 0.00001022
2024-11-06 14:07:09,516 - INFO - Epoch [111/300], Batch [16/43], Training Loss: 0.00002887
2024-11-06 14:07:09,520 - INFO - Epoch [111/300], Batch [17/43], Training Loss: 0.00001007
2024-11-06 14:07:09,527 - INFO - Epoch [111/300], Batch [18/43], Training Loss: 0.00001801
2024-11-06 14:07:09,532 - INFO - Epoch [111/300], Batch [19/43], Training Loss: 0.00004215
2024-11-06 14:07:09,537 - INFO - Epoch [111/300], Batch [20/43], Training Loss: 0.00001396
2024-11-06 14:07:09,541 - INFO - Epoch [111/300], Batch [21/43], Training Loss: 0.00001419
2024-11-06 14:07:09,546 - INFO - Epoch [111/300], Batch [22/43], Training Loss: 0.00000415
2024-11-06 14:07:09,552 - INFO - Epoch [111/300], Batch [23/43], Training Loss: 0.00000757
2024-11-06 14:07:09,557 - INFO - Epoch [111/300], Batch [24/43], Training Loss: 0.00001389
2024-11-06 14:07:09,563 - INFO - Epoch [111/300], Batch [25/43], Training Loss: 0.00001497
2024-11-06 14:07:09,569 - INFO - Epoch [111/300], Batch [26/43], Training Loss: 0.00001041
2024-11-06 14:07:09,573 - INFO - Epoch [111/300], Batch [27/43], Training Loss: 0.00000916
2024-11-06 14:07:09,579 - INFO - Epoch [111/300], Batch [28/43], Training Loss: 0.00002409
2024-11-06 14:07:09,585 - INFO - Epoch [111/300], Batch [29/43], Training Loss: 0.00002099
2024-11-06 14:07:09,590 - INFO - Epoch [111/300], Batch [30/43], Training Loss: 0.00001413
2024-11-06 14:07:09,595 - INFO - Epoch [111/300], Batch [31/43], Training Loss: 0.00001063
2024-11-06 14:07:09,600 - INFO - Epoch [111/300], Batch [32/43], Training Loss: 0.00000817
2024-11-06 14:07:09,605 - INFO - Epoch [111/300], Batch [33/43], Training Loss: 0.00000755
2024-11-06 14:07:09,611 - INFO - Epoch [111/300], Batch [34/43], Training Loss: 0.00001267
2024-11-06 14:07:09,616 - INFO - Epoch [111/300], Batch [35/43], Training Loss: 0.00000464
2024-11-06 14:07:09,623 - INFO - Epoch [111/300], Batch [36/43], Training Loss: 0.00001355
2024-11-06 14:07:09,627 - INFO - Epoch [111/300], Batch [37/43], Training Loss: 0.00001835
2024-11-06 14:07:09,634 - INFO - Epoch [111/300], Batch [38/43], Training Loss: 0.00001761
2024-11-06 14:07:09,644 - INFO - Epoch [111/300], Batch [39/43], Training Loss: 0.00001712
2024-11-06 14:07:09,649 - INFO - Epoch [111/300], Batch [40/43], Training Loss: 0.00001766
2024-11-06 14:07:09,653 - INFO - Epoch [111/300], Batch [41/43], Training Loss: 0.00002208
2024-11-06 14:07:09,657 - INFO - Epoch [111/300], Batch [42/43], Training Loss: 0.00001206
2024-11-06 14:07:09,662 - INFO - Epoch [111/300], Batch [43/43], Training Loss: 0.00002732
2024-11-06 14:07:09,674 - INFO - Epoch [111/300], Average Training Loss: 0.00001551, Validation Loss: 0.00001950
2024-11-06 14:07:09,680 - INFO - Epoch [112/300], Batch [1/43], Training Loss: 0.00002191
2024-11-06 14:07:09,685 - INFO - Epoch [112/300], Batch [2/43], Training Loss: 0.00001546
2024-11-06 14:07:09,689 - INFO - Epoch [112/300], Batch [3/43], Training Loss: 0.00003448
2024-11-06 14:07:09,695 - INFO - Epoch [112/300], Batch [4/43], Training Loss: 0.00002737
2024-11-06 14:07:09,700 - INFO - Epoch [112/300], Batch [5/43], Training Loss: 0.00001945
2024-11-06 14:07:09,704 - INFO - Epoch [112/300], Batch [6/43], Training Loss: 0.00001182
2024-11-06 14:07:09,708 - INFO - Epoch [112/300], Batch [7/43], Training Loss: 0.00001134
2024-11-06 14:07:09,712 - INFO - Epoch [112/300], Batch [8/43], Training Loss: 0.00002297
2024-11-06 14:07:09,716 - INFO - Epoch [112/300], Batch [9/43], Training Loss: 0.00002020
2024-11-06 14:07:09,720 - INFO - Epoch [112/300], Batch [10/43], Training Loss: 0.00000880
2024-11-06 14:07:09,725 - INFO - Epoch [112/300], Batch [11/43], Training Loss: 0.00001766
2024-11-06 14:07:09,731 - INFO - Epoch [112/300], Batch [12/43], Training Loss: 0.00001189
2024-11-06 14:07:09,737 - INFO - Epoch [112/300], Batch [13/43], Training Loss: 0.00003494
2024-11-06 14:07:09,741 - INFO - Epoch [112/300], Batch [14/43], Training Loss: 0.00001731
2024-11-06 14:07:09,746 - INFO - Epoch [112/300], Batch [15/43], Training Loss: 0.00001871
2024-11-06 14:07:09,752 - INFO - Epoch [112/300], Batch [16/43], Training Loss: 0.00002355
2024-11-06 14:07:09,756 - INFO - Epoch [112/300], Batch [17/43], Training Loss: 0.00001401
2024-11-06 14:07:09,760 - INFO - Epoch [112/300], Batch [18/43], Training Loss: 0.00001172
2024-11-06 14:07:09,765 - INFO - Epoch [112/300], Batch [19/43], Training Loss: 0.00001165
2024-11-06 14:07:09,769 - INFO - Epoch [112/300], Batch [20/43], Training Loss: 0.00002223
2024-11-06 14:07:09,774 - INFO - Epoch [112/300], Batch [21/43], Training Loss: 0.00001781
2024-11-06 14:07:09,783 - INFO - Epoch [112/300], Batch [22/43], Training Loss: 0.00001128
2024-11-06 14:07:09,794 - INFO - Epoch [112/300], Batch [23/43], Training Loss: 0.00001972
2024-11-06 14:07:09,799 - INFO - Epoch [112/300], Batch [24/43], Training Loss: 0.00002298
2024-11-06 14:07:09,806 - INFO - Epoch [112/300], Batch [25/43], Training Loss: 0.00001299
2024-11-06 14:07:09,813 - INFO - Epoch [112/300], Batch [26/43], Training Loss: 0.00003232
2024-11-06 14:07:09,817 - INFO - Epoch [112/300], Batch [27/43], Training Loss: 0.00001512
2024-11-06 14:07:09,821 - INFO - Epoch [112/300], Batch [28/43], Training Loss: 0.00000809
2024-11-06 14:07:09,830 - INFO - Epoch [112/300], Batch [29/43], Training Loss: 0.00001294
2024-11-06 14:07:09,835 - INFO - Epoch [112/300], Batch [30/43], Training Loss: 0.00002057
2024-11-06 14:07:09,841 - INFO - Epoch [112/300], Batch [31/43], Training Loss: 0.00000613
2024-11-06 14:07:09,844 - INFO - Epoch [112/300], Batch [32/43], Training Loss: 0.00002256
2024-11-06 14:07:09,848 - INFO - Epoch [112/300], Batch [33/43], Training Loss: 0.00003128
2024-11-06 14:07:09,852 - INFO - Epoch [112/300], Batch [34/43], Training Loss: 0.00002085
2024-11-06 14:07:09,856 - INFO - Epoch [112/300], Batch [35/43], Training Loss: 0.00002411
2024-11-06 14:07:09,859 - INFO - Epoch [112/300], Batch [36/43], Training Loss: 0.00001686
2024-11-06 14:07:09,863 - INFO - Epoch [112/300], Batch [37/43], Training Loss: 0.00000697
2024-11-06 14:07:09,867 - INFO - Epoch [112/300], Batch [38/43], Training Loss: 0.00001460
2024-11-06 14:07:09,872 - INFO - Epoch [112/300], Batch [39/43], Training Loss: 0.00001840
2024-11-06 14:07:09,876 - INFO - Epoch [112/300], Batch [40/43], Training Loss: 0.00001027
2024-11-06 14:07:09,880 - INFO - Epoch [112/300], Batch [41/43], Training Loss: 0.00002348
2024-11-06 14:07:09,884 - INFO - Epoch [112/300], Batch [42/43], Training Loss: 0.00001898
2024-11-06 14:07:09,889 - INFO - Epoch [112/300], Batch [43/43], Training Loss: 0.00001325
2024-11-06 14:07:09,901 - INFO - Epoch [112/300], Average Training Loss: 0.00001812, Validation Loss: 0.00002018
2024-11-06 14:07:09,905 - INFO - Epoch [113/300], Batch [1/43], Training Loss: 0.00002173
2024-11-06 14:07:09,909 - INFO - Epoch [113/300], Batch [2/43], Training Loss: 0.00002468
2024-11-06 14:07:09,914 - INFO - Epoch [113/300], Batch [3/43], Training Loss: 0.00002257
2024-11-06 14:07:09,918 - INFO - Epoch [113/300], Batch [4/43], Training Loss: 0.00001066
2024-11-06 14:07:09,922 - INFO - Epoch [113/300], Batch [5/43], Training Loss: 0.00001593
2024-11-06 14:07:09,926 - INFO - Epoch [113/300], Batch [6/43], Training Loss: 0.00000598
2024-11-06 14:07:09,930 - INFO - Epoch [113/300], Batch [7/43], Training Loss: 0.00000986
2024-11-06 14:07:09,934 - INFO - Epoch [113/300], Batch [8/43], Training Loss: 0.00001225
2024-11-06 14:07:09,939 - INFO - Epoch [113/300], Batch [9/43], Training Loss: 0.00001627
2024-11-06 14:07:09,943 - INFO - Epoch [113/300], Batch [10/43], Training Loss: 0.00001045
2024-11-06 14:07:09,947 - INFO - Epoch [113/300], Batch [11/43], Training Loss: 0.00001084
2024-11-06 14:07:09,951 - INFO - Epoch [113/300], Batch [12/43], Training Loss: 0.00002467
2024-11-06 14:07:09,954 - INFO - Epoch [113/300], Batch [13/43], Training Loss: 0.00000946
2024-11-06 14:07:09,958 - INFO - Epoch [113/300], Batch [14/43], Training Loss: 0.00000780
2024-11-06 14:07:09,961 - INFO - Epoch [113/300], Batch [15/43], Training Loss: 0.00002107
2024-11-06 14:07:09,964 - INFO - Epoch [113/300], Batch [16/43], Training Loss: 0.00001732
2024-11-06 14:07:09,967 - INFO - Epoch [113/300], Batch [17/43], Training Loss: 0.00002015
2024-11-06 14:07:09,970 - INFO - Epoch [113/300], Batch [18/43], Training Loss: 0.00000409
2024-11-06 14:07:09,974 - INFO - Epoch [113/300], Batch [19/43], Training Loss: 0.00000800
2024-11-06 14:07:09,977 - INFO - Epoch [113/300], Batch [20/43], Training Loss: 0.00000756
2024-11-06 14:07:09,981 - INFO - Epoch [113/300], Batch [21/43], Training Loss: 0.00000949
2024-11-06 14:07:09,984 - INFO - Epoch [113/300], Batch [22/43], Training Loss: 0.00001072
2024-11-06 14:07:09,989 - INFO - Epoch [113/300], Batch [23/43], Training Loss: 0.00002188
2024-11-06 14:07:09,992 - INFO - Epoch [113/300], Batch [24/43], Training Loss: 0.00001007
2024-11-06 14:07:09,996 - INFO - Epoch [113/300], Batch [25/43], Training Loss: 0.00003634
2024-11-06 14:07:09,999 - INFO - Epoch [113/300], Batch [26/43], Training Loss: 0.00001637
2024-11-06 14:07:10,004 - INFO - Epoch [113/300], Batch [27/43], Training Loss: 0.00001618
2024-11-06 14:07:10,008 - INFO - Epoch [113/300], Batch [28/43], Training Loss: 0.00001503
2024-11-06 14:07:10,011 - INFO - Epoch [113/300], Batch [29/43], Training Loss: 0.00001109
2024-11-06 14:07:10,015 - INFO - Epoch [113/300], Batch [30/43], Training Loss: 0.00002910
2024-11-06 14:07:10,018 - INFO - Epoch [113/300], Batch [31/43], Training Loss: 0.00000894
2024-11-06 14:07:10,022 - INFO - Epoch [113/300], Batch [32/43], Training Loss: 0.00002405
2024-11-06 14:07:10,025 - INFO - Epoch [113/300], Batch [33/43], Training Loss: 0.00001259
2024-11-06 14:07:10,029 - INFO - Epoch [113/300], Batch [34/43], Training Loss: 0.00001201
2024-11-06 14:07:10,032 - INFO - Epoch [113/300], Batch [35/43], Training Loss: 0.00000971
2024-11-06 14:07:10,035 - INFO - Epoch [113/300], Batch [36/43], Training Loss: 0.00002445
2024-11-06 14:07:10,038 - INFO - Epoch [113/300], Batch [37/43], Training Loss: 0.00001349
2024-11-06 14:07:10,040 - INFO - Epoch [113/300], Batch [38/43], Training Loss: 0.00000728
2024-11-06 14:07:10,044 - INFO - Epoch [113/300], Batch [39/43], Training Loss: 0.00000913
2024-11-06 14:07:10,047 - INFO - Epoch [113/300], Batch [40/43], Training Loss: 0.00001560
2024-11-06 14:07:10,050 - INFO - Epoch [113/300], Batch [41/43], Training Loss: 0.00000671
2024-11-06 14:07:10,053 - INFO - Epoch [113/300], Batch [42/43], Training Loss: 0.00002398
2024-11-06 14:07:10,055 - INFO - Epoch [113/300], Batch [43/43], Training Loss: 0.00002906
2024-11-06 14:07:10,066 - INFO - Epoch [113/300], Average Training Loss: 0.00001522, Validation Loss: 0.00002174
2024-11-06 14:07:10,069 - INFO - Epoch [114/300], Batch [1/43], Training Loss: 0.00001886
2024-11-06 14:07:10,072 - INFO - Epoch [114/300], Batch [2/43], Training Loss: 0.00003396
2024-11-06 14:07:10,075 - INFO - Epoch [114/300], Batch [3/43], Training Loss: 0.00001416
2024-11-06 14:07:10,078 - INFO - Epoch [114/300], Batch [4/43], Training Loss: 0.00001361
2024-11-06 14:07:10,082 - INFO - Epoch [114/300], Batch [5/43], Training Loss: 0.00002241
2024-11-06 14:07:10,084 - INFO - Epoch [114/300], Batch [6/43], Training Loss: 0.00000725
2024-11-06 14:07:10,087 - INFO - Epoch [114/300], Batch [7/43], Training Loss: 0.00002146
2024-11-06 14:07:10,091 - INFO - Epoch [114/300], Batch [8/43], Training Loss: 0.00000811
2024-11-06 14:07:10,095 - INFO - Epoch [114/300], Batch [9/43], Training Loss: 0.00001557
2024-11-06 14:07:10,100 - INFO - Epoch [114/300], Batch [10/43], Training Loss: 0.00001129
2024-11-06 14:07:10,104 - INFO - Epoch [114/300], Batch [11/43], Training Loss: 0.00000900
2024-11-06 14:07:10,108 - INFO - Epoch [114/300], Batch [12/43], Training Loss: 0.00001096
2024-11-06 14:07:10,112 - INFO - Epoch [114/300], Batch [13/43], Training Loss: 0.00001744
2024-11-06 14:07:10,116 - INFO - Epoch [114/300], Batch [14/43], Training Loss: 0.00001166
2024-11-06 14:07:10,119 - INFO - Epoch [114/300], Batch [15/43], Training Loss: 0.00001736
2024-11-06 14:07:10,124 - INFO - Epoch [114/300], Batch [16/43], Training Loss: 0.00001248
2024-11-06 14:07:10,127 - INFO - Epoch [114/300], Batch [17/43], Training Loss: 0.00001523
2024-11-06 14:07:10,131 - INFO - Epoch [114/300], Batch [18/43], Training Loss: 0.00000814
2024-11-06 14:07:10,134 - INFO - Epoch [114/300], Batch [19/43], Training Loss: 0.00001541
2024-11-06 14:07:10,138 - INFO - Epoch [114/300], Batch [20/43], Training Loss: 0.00002296
2024-11-06 14:07:10,141 - INFO - Epoch [114/300], Batch [21/43], Training Loss: 0.00001077
2024-11-06 14:07:10,144 - INFO - Epoch [114/300], Batch [22/43], Training Loss: 0.00000587
2024-11-06 14:07:10,147 - INFO - Epoch [114/300], Batch [23/43], Training Loss: 0.00003019
2024-11-06 14:07:10,151 - INFO - Epoch [114/300], Batch [24/43], Training Loss: 0.00001565
2024-11-06 14:07:10,154 - INFO - Epoch [114/300], Batch [25/43], Training Loss: 0.00001178
2024-11-06 14:07:10,158 - INFO - Epoch [114/300], Batch [26/43], Training Loss: 0.00001455
2024-11-06 14:07:10,162 - INFO - Epoch [114/300], Batch [27/43], Training Loss: 0.00001479
2024-11-06 14:07:10,165 - INFO - Epoch [114/300], Batch [28/43], Training Loss: 0.00000662
2024-11-06 14:07:10,170 - INFO - Epoch [114/300], Batch [29/43], Training Loss: 0.00002575
2024-11-06 14:07:10,174 - INFO - Epoch [114/300], Batch [30/43], Training Loss: 0.00002669
2024-11-06 14:07:10,177 - INFO - Epoch [114/300], Batch [31/43], Training Loss: 0.00000598
2024-11-06 14:07:10,181 - INFO - Epoch [114/300], Batch [32/43], Training Loss: 0.00001364
2024-11-06 14:07:10,185 - INFO - Epoch [114/300], Batch [33/43], Training Loss: 0.00001679
2024-11-06 14:07:10,188 - INFO - Epoch [114/300], Batch [34/43], Training Loss: 0.00002472
2024-11-06 14:07:10,192 - INFO - Epoch [114/300], Batch [35/43], Training Loss: 0.00001145
2024-11-06 14:07:10,196 - INFO - Epoch [114/300], Batch [36/43], Training Loss: 0.00001410
2024-11-06 14:07:10,200 - INFO - Epoch [114/300], Batch [37/43], Training Loss: 0.00000608
2024-11-06 14:07:10,203 - INFO - Epoch [114/300], Batch [38/43], Training Loss: 0.00000815
2024-11-06 14:07:10,206 - INFO - Epoch [114/300], Batch [39/43], Training Loss: 0.00000341
2024-11-06 14:07:10,209 - INFO - Epoch [114/300], Batch [40/43], Training Loss: 0.00002118
2024-11-06 14:07:10,212 - INFO - Epoch [114/300], Batch [41/43], Training Loss: 0.00002317
2024-11-06 14:07:10,215 - INFO - Epoch [114/300], Batch [42/43], Training Loss: 0.00001947
2024-11-06 14:07:10,219 - INFO - Epoch [114/300], Batch [43/43], Training Loss: 0.00001630
2024-11-06 14:07:10,228 - INFO - Epoch [114/300], Average Training Loss: 0.00001522, Validation Loss: 0.00003508
2024-11-06 14:07:10,231 - INFO - Epoch [115/300], Batch [1/43], Training Loss: 0.00002207
2024-11-06 14:07:10,234 - INFO - Epoch [115/300], Batch [2/43], Training Loss: 0.00001878
2024-11-06 14:07:10,237 - INFO - Epoch [115/300], Batch [3/43], Training Loss: 0.00001552
2024-11-06 14:07:10,240 - INFO - Epoch [115/300], Batch [4/43], Training Loss: 0.00001480
2024-11-06 14:07:10,243 - INFO - Epoch [115/300], Batch [5/43], Training Loss: 0.00002774
2024-11-06 14:07:10,246 - INFO - Epoch [115/300], Batch [6/43], Training Loss: 0.00001333
2024-11-06 14:07:10,249 - INFO - Epoch [115/300], Batch [7/43], Training Loss: 0.00001076
2024-11-06 14:07:10,252 - INFO - Epoch [115/300], Batch [8/43], Training Loss: 0.00001702
2024-11-06 14:07:10,255 - INFO - Epoch [115/300], Batch [9/43], Training Loss: 0.00001808
2024-11-06 14:07:10,258 - INFO - Epoch [115/300], Batch [10/43], Training Loss: 0.00001267
2024-11-06 14:07:10,261 - INFO - Epoch [115/300], Batch [11/43], Training Loss: 0.00001475
2024-11-06 14:07:10,265 - INFO - Epoch [115/300], Batch [12/43], Training Loss: 0.00001540
2024-11-06 14:07:10,269 - INFO - Epoch [115/300], Batch [13/43], Training Loss: 0.00001434
2024-11-06 14:07:10,273 - INFO - Epoch [115/300], Batch [14/43], Training Loss: 0.00003178
2024-11-06 14:07:10,277 - INFO - Epoch [115/300], Batch [15/43], Training Loss: 0.00001073
2024-11-06 14:07:10,280 - INFO - Epoch [115/300], Batch [16/43], Training Loss: 0.00000964
2024-11-06 14:07:10,284 - INFO - Epoch [115/300], Batch [17/43], Training Loss: 0.00001754
2024-11-06 14:07:10,287 - INFO - Epoch [115/300], Batch [18/43], Training Loss: 0.00001977
2024-11-06 14:07:10,291 - INFO - Epoch [115/300], Batch [19/43], Training Loss: 0.00000745
2024-11-06 14:07:10,295 - INFO - Epoch [115/300], Batch [20/43], Training Loss: 0.00000867
2024-11-06 14:07:10,298 - INFO - Epoch [115/300], Batch [21/43], Training Loss: 0.00001559
2024-11-06 14:07:10,303 - INFO - Epoch [115/300], Batch [22/43], Training Loss: 0.00001967
2024-11-06 14:07:10,306 - INFO - Epoch [115/300], Batch [23/43], Training Loss: 0.00001663
2024-11-06 14:07:10,310 - INFO - Epoch [115/300], Batch [24/43], Training Loss: 0.00003052
2024-11-06 14:07:10,314 - INFO - Epoch [115/300], Batch [25/43], Training Loss: 0.00001276
2024-11-06 14:07:10,318 - INFO - Epoch [115/300], Batch [26/43], Training Loss: 0.00001311
2024-11-06 14:07:10,322 - INFO - Epoch [115/300], Batch [27/43], Training Loss: 0.00002049
2024-11-06 14:07:10,325 - INFO - Epoch [115/300], Batch [28/43], Training Loss: 0.00002110
2024-11-06 14:07:10,329 - INFO - Epoch [115/300], Batch [29/43], Training Loss: 0.00001319
2024-11-06 14:07:10,333 - INFO - Epoch [115/300], Batch [30/43], Training Loss: 0.00003398
2024-11-06 14:07:10,338 - INFO - Epoch [115/300], Batch [31/43], Training Loss: 0.00002758
2024-11-06 14:07:10,343 - INFO - Epoch [115/300], Batch [32/43], Training Loss: 0.00000512
2024-11-06 14:07:10,346 - INFO - Epoch [115/300], Batch [33/43], Training Loss: 0.00001683
2024-11-06 14:07:10,349 - INFO - Epoch [115/300], Batch [34/43], Training Loss: 0.00003780
2024-11-06 14:07:10,354 - INFO - Epoch [115/300], Batch [35/43], Training Loss: 0.00002382
2024-11-06 14:07:10,358 - INFO - Epoch [115/300], Batch [36/43], Training Loss: 0.00000709
2024-11-06 14:07:10,362 - INFO - Epoch [115/300], Batch [37/43], Training Loss: 0.00002354
2024-11-06 14:07:10,366 - INFO - Epoch [115/300], Batch [38/43], Training Loss: 0.00004752
2024-11-06 14:07:10,370 - INFO - Epoch [115/300], Batch [39/43], Training Loss: 0.00003377
2024-11-06 14:07:10,374 - INFO - Epoch [115/300], Batch [40/43], Training Loss: 0.00000735
2024-11-06 14:07:10,378 - INFO - Epoch [115/300], Batch [41/43], Training Loss: 0.00003338
2024-11-06 14:07:10,383 - INFO - Epoch [115/300], Batch [42/43], Training Loss: 0.00001489
2024-11-06 14:07:10,423 - INFO - Epoch [115/300], Batch [43/43], Training Loss: 0.00002282
2024-11-06 14:07:10,481 - INFO - Epoch [115/300], Average Training Loss: 0.00001906, Validation Loss: 0.00002123
2024-11-06 14:07:10,487 - INFO - Epoch [116/300], Batch [1/43], Training Loss: 0.00002131
2024-11-06 14:07:10,491 - INFO - Epoch [116/300], Batch [2/43], Training Loss: 0.00003426
2024-11-06 14:07:10,495 - INFO - Epoch [116/300], Batch [3/43], Training Loss: 0.00001697
2024-11-06 14:07:10,498 - INFO - Epoch [116/300], Batch [4/43], Training Loss: 0.00001093
2024-11-06 14:07:10,502 - INFO - Epoch [116/300], Batch [5/43], Training Loss: 0.00003502
2024-11-06 14:07:10,506 - INFO - Epoch [116/300], Batch [6/43], Training Loss: 0.00001325
2024-11-06 14:07:10,510 - INFO - Epoch [116/300], Batch [7/43], Training Loss: 0.00001935
2024-11-06 14:07:10,515 - INFO - Epoch [116/300], Batch [8/43], Training Loss: 0.00000776
2024-11-06 14:07:10,520 - INFO - Epoch [116/300], Batch [9/43], Training Loss: 0.00002427
2024-11-06 14:07:10,524 - INFO - Epoch [116/300], Batch [10/43], Training Loss: 0.00002113
2024-11-06 14:07:10,528 - INFO - Epoch [116/300], Batch [11/43], Training Loss: 0.00002220
2024-11-06 14:07:10,534 - INFO - Epoch [116/300], Batch [12/43], Training Loss: 0.00000936
2024-11-06 14:07:10,538 - INFO - Epoch [116/300], Batch [13/43], Training Loss: 0.00001955
2024-11-06 14:07:10,542 - INFO - Epoch [116/300], Batch [14/43], Training Loss: 0.00002078
2024-11-06 14:07:10,547 - INFO - Epoch [116/300], Batch [15/43], Training Loss: 0.00003246
2024-11-06 14:07:10,551 - INFO - Epoch [116/300], Batch [16/43], Training Loss: 0.00001542
2024-11-06 14:07:10,554 - INFO - Epoch [116/300], Batch [17/43], Training Loss: 0.00003502
2024-11-06 14:07:10,558 - INFO - Epoch [116/300], Batch [18/43], Training Loss: 0.00001026
2024-11-06 14:07:10,561 - INFO - Epoch [116/300], Batch [19/43], Training Loss: 0.00002365
2024-11-06 14:07:10,565 - INFO - Epoch [116/300], Batch [20/43], Training Loss: 0.00001736
2024-11-06 14:07:10,568 - INFO - Epoch [116/300], Batch [21/43], Training Loss: 0.00003366
2024-11-06 14:07:10,571 - INFO - Epoch [116/300], Batch [22/43], Training Loss: 0.00001961
2024-11-06 14:07:10,575 - INFO - Epoch [116/300], Batch [23/43], Training Loss: 0.00000867
2024-11-06 14:07:10,580 - INFO - Epoch [116/300], Batch [24/43], Training Loss: 0.00004218
2024-11-06 14:07:10,585 - INFO - Epoch [116/300], Batch [25/43], Training Loss: 0.00001352
2024-11-06 14:07:10,590 - INFO - Epoch [116/300], Batch [26/43], Training Loss: 0.00001879
2024-11-06 14:07:10,594 - INFO - Epoch [116/300], Batch [27/43], Training Loss: 0.00001219
2024-11-06 14:07:10,599 - INFO - Epoch [116/300], Batch [28/43], Training Loss: 0.00001569
2024-11-06 14:07:10,602 - INFO - Epoch [116/300], Batch [29/43], Training Loss: 0.00002155
2024-11-06 14:07:10,605 - INFO - Epoch [116/300], Batch [30/43], Training Loss: 0.00001795
2024-11-06 14:07:10,609 - INFO - Epoch [116/300], Batch [31/43], Training Loss: 0.00001611
2024-11-06 14:07:10,613 - INFO - Epoch [116/300], Batch [32/43], Training Loss: 0.00001528
2024-11-06 14:07:10,616 - INFO - Epoch [116/300], Batch [33/43], Training Loss: 0.00000726
2024-11-06 14:07:10,619 - INFO - Epoch [116/300], Batch [34/43], Training Loss: 0.00001645
2024-11-06 14:07:10,622 - INFO - Epoch [116/300], Batch [35/43], Training Loss: 0.00001611
2024-11-06 14:07:10,625 - INFO - Epoch [116/300], Batch [36/43], Training Loss: 0.00002168
2024-11-06 14:07:10,629 - INFO - Epoch [116/300], Batch [37/43], Training Loss: 0.00002146
2024-11-06 14:07:10,633 - INFO - Epoch [116/300], Batch [38/43], Training Loss: 0.00000946
2024-11-06 14:07:10,636 - INFO - Epoch [116/300], Batch [39/43], Training Loss: 0.00003356
2024-11-06 14:07:10,640 - INFO - Epoch [116/300], Batch [40/43], Training Loss: 0.00002358
2024-11-06 14:07:10,644 - INFO - Epoch [116/300], Batch [41/43], Training Loss: 0.00001877
2024-11-06 14:07:10,648 - INFO - Epoch [116/300], Batch [42/43], Training Loss: 0.00003861
2024-11-06 14:07:10,652 - INFO - Epoch [116/300], Batch [43/43], Training Loss: 0.00001458
2024-11-06 14:07:10,663 - INFO - Epoch [116/300], Average Training Loss: 0.00002016, Validation Loss: 0.00001942
2024-11-06 14:07:10,667 - INFO - Epoch [117/300], Batch [1/43], Training Loss: 0.00001532
2024-11-06 14:07:10,670 - INFO - Epoch [117/300], Batch [2/43], Training Loss: 0.00001429
2024-11-06 14:07:10,673 - INFO - Epoch [117/300], Batch [3/43], Training Loss: 0.00002474
2024-11-06 14:07:10,677 - INFO - Epoch [117/300], Batch [4/43], Training Loss: 0.00001685
2024-11-06 14:07:10,680 - INFO - Epoch [117/300], Batch [5/43], Training Loss: 0.00001256
2024-11-06 14:07:10,685 - INFO - Epoch [117/300], Batch [6/43], Training Loss: 0.00001060
2024-11-06 14:07:10,690 - INFO - Epoch [117/300], Batch [7/43], Training Loss: 0.00001616
2024-11-06 14:07:10,694 - INFO - Epoch [117/300], Batch [8/43], Training Loss: 0.00001047
2024-11-06 14:07:10,697 - INFO - Epoch [117/300], Batch [9/43], Training Loss: 0.00001096
2024-11-06 14:07:10,701 - INFO - Epoch [117/300], Batch [10/43], Training Loss: 0.00002153
2024-11-06 14:07:10,705 - INFO - Epoch [117/300], Batch [11/43], Training Loss: 0.00003190
2024-11-06 14:07:10,709 - INFO - Epoch [117/300], Batch [12/43], Training Loss: 0.00000891
2024-11-06 14:07:10,713 - INFO - Epoch [117/300], Batch [13/43], Training Loss: 0.00004029
2024-11-06 14:07:10,717 - INFO - Epoch [117/300], Batch [14/43], Training Loss: 0.00002022
2024-11-06 14:07:10,721 - INFO - Epoch [117/300], Batch [15/43], Training Loss: 0.00001013
2024-11-06 14:07:10,725 - INFO - Epoch [117/300], Batch [16/43], Training Loss: 0.00000272
2024-11-06 14:07:10,728 - INFO - Epoch [117/300], Batch [17/43], Training Loss: 0.00001880
2024-11-06 14:07:10,731 - INFO - Epoch [117/300], Batch [18/43], Training Loss: 0.00001356
2024-11-06 14:07:10,735 - INFO - Epoch [117/300], Batch [19/43], Training Loss: 0.00001445
2024-11-06 14:07:10,738 - INFO - Epoch [117/300], Batch [20/43], Training Loss: 0.00002119
2024-11-06 14:07:10,741 - INFO - Epoch [117/300], Batch [21/43], Training Loss: 0.00001540
2024-11-06 14:07:10,744 - INFO - Epoch [117/300], Batch [22/43], Training Loss: 0.00000939
2024-11-06 14:07:10,748 - INFO - Epoch [117/300], Batch [23/43], Training Loss: 0.00002158
2024-11-06 14:07:10,752 - INFO - Epoch [117/300], Batch [24/43], Training Loss: 0.00003474
2024-11-06 14:07:10,755 - INFO - Epoch [117/300], Batch [25/43], Training Loss: 0.00001707
2024-11-06 14:07:10,759 - INFO - Epoch [117/300], Batch [26/43], Training Loss: 0.00001512
2024-11-06 14:07:10,763 - INFO - Epoch [117/300], Batch [27/43], Training Loss: 0.00002544
2024-11-06 14:07:10,766 - INFO - Epoch [117/300], Batch [28/43], Training Loss: 0.00001489
2024-11-06 14:07:10,770 - INFO - Epoch [117/300], Batch [29/43], Training Loss: 0.00002415
2024-11-06 14:07:10,774 - INFO - Epoch [117/300], Batch [30/43], Training Loss: 0.00001586
2024-11-06 14:07:10,777 - INFO - Epoch [117/300], Batch [31/43], Training Loss: 0.00000416
2024-11-06 14:07:10,780 - INFO - Epoch [117/300], Batch [32/43], Training Loss: 0.00001156
2024-11-06 14:07:10,783 - INFO - Epoch [117/300], Batch [33/43], Training Loss: 0.00001219
2024-11-06 14:07:10,786 - INFO - Epoch [117/300], Batch [34/43], Training Loss: 0.00001546
2024-11-06 14:07:10,790 - INFO - Epoch [117/300], Batch [35/43], Training Loss: 0.00001371
2024-11-06 14:07:10,794 - INFO - Epoch [117/300], Batch [36/43], Training Loss: 0.00001379
2024-11-06 14:07:10,797 - INFO - Epoch [117/300], Batch [37/43], Training Loss: 0.00002877
2024-11-06 14:07:10,800 - INFO - Epoch [117/300], Batch [38/43], Training Loss: 0.00001137
2024-11-06 14:07:10,804 - INFO - Epoch [117/300], Batch [39/43], Training Loss: 0.00000772
2024-11-06 14:07:10,807 - INFO - Epoch [117/300], Batch [40/43], Training Loss: 0.00001720
2024-11-06 14:07:10,811 - INFO - Epoch [117/300], Batch [41/43], Training Loss: 0.00002116
2024-11-06 14:07:10,814 - INFO - Epoch [117/300], Batch [42/43], Training Loss: 0.00000717
2024-11-06 14:07:10,818 - INFO - Epoch [117/300], Batch [43/43], Training Loss: 0.00000966
2024-11-06 14:07:10,830 - INFO - Epoch [117/300], Average Training Loss: 0.00001635, Validation Loss: 0.00001815
2024-11-06 14:07:10,834 - INFO - Epoch [118/300], Batch [1/43], Training Loss: 0.00000905
2024-11-06 14:07:10,840 - INFO - Epoch [118/300], Batch [2/43], Training Loss: 0.00001826
2024-11-06 14:07:10,844 - INFO - Epoch [118/300], Batch [3/43], Training Loss: 0.00001073
2024-11-06 14:07:10,848 - INFO - Epoch [118/300], Batch [4/43], Training Loss: 0.00002029
2024-11-06 14:07:10,852 - INFO - Epoch [118/300], Batch [5/43], Training Loss: 0.00001441
2024-11-06 14:07:10,856 - INFO - Epoch [118/300], Batch [6/43], Training Loss: 0.00001298
2024-11-06 14:07:10,861 - INFO - Epoch [118/300], Batch [7/43], Training Loss: 0.00001581
2024-11-06 14:07:10,867 - INFO - Epoch [118/300], Batch [8/43], Training Loss: 0.00001488
2024-11-06 14:07:10,872 - INFO - Epoch [118/300], Batch [9/43], Training Loss: 0.00002261
2024-11-06 14:07:10,877 - INFO - Epoch [118/300], Batch [10/43], Training Loss: 0.00001248
2024-11-06 14:07:10,881 - INFO - Epoch [118/300], Batch [11/43], Training Loss: 0.00001470
2024-11-06 14:07:10,885 - INFO - Epoch [118/300], Batch [12/43], Training Loss: 0.00000657
2024-11-06 14:07:10,890 - INFO - Epoch [118/300], Batch [13/43], Training Loss: 0.00001886
2024-11-06 14:07:10,895 - INFO - Epoch [118/300], Batch [14/43], Training Loss: 0.00001322
2024-11-06 14:07:10,900 - INFO - Epoch [118/300], Batch [15/43], Training Loss: 0.00001702
2024-11-06 14:07:10,905 - INFO - Epoch [118/300], Batch [16/43], Training Loss: 0.00001859
2024-11-06 14:07:10,910 - INFO - Epoch [118/300], Batch [17/43], Training Loss: 0.00001033
2024-11-06 14:07:10,917 - INFO - Epoch [118/300], Batch [18/43], Training Loss: 0.00000391
2024-11-06 14:07:10,922 - INFO - Epoch [118/300], Batch [19/43], Training Loss: 0.00002078
2024-11-06 14:07:10,928 - INFO - Epoch [118/300], Batch [20/43], Training Loss: 0.00002629
2024-11-06 14:07:10,931 - INFO - Epoch [118/300], Batch [21/43], Training Loss: 0.00001814
2024-11-06 14:07:10,935 - INFO - Epoch [118/300], Batch [22/43], Training Loss: 0.00000918
2024-11-06 14:07:10,940 - INFO - Epoch [118/300], Batch [23/43], Training Loss: 0.00001563
2024-11-06 14:07:10,946 - INFO - Epoch [118/300], Batch [24/43], Training Loss: 0.00001555
2024-11-06 14:07:10,952 - INFO - Epoch [118/300], Batch [25/43], Training Loss: 0.00001669
2024-11-06 14:07:10,956 - INFO - Epoch [118/300], Batch [26/43], Training Loss: 0.00001424
2024-11-06 14:07:10,961 - INFO - Epoch [118/300], Batch [27/43], Training Loss: 0.00004072
2024-11-06 14:07:10,965 - INFO - Epoch [118/300], Batch [28/43], Training Loss: 0.00001496
2024-11-06 14:07:10,969 - INFO - Epoch [118/300], Batch [29/43], Training Loss: 0.00003473
2024-11-06 14:07:10,975 - INFO - Epoch [118/300], Batch [30/43], Training Loss: 0.00000663
2024-11-06 14:07:10,978 - INFO - Epoch [118/300], Batch [31/43], Training Loss: 0.00001990
2024-11-06 14:07:10,983 - INFO - Epoch [118/300], Batch [32/43], Training Loss: 0.00001951
2024-11-06 14:07:10,987 - INFO - Epoch [118/300], Batch [33/43], Training Loss: 0.00002794
2024-11-06 14:07:10,991 - INFO - Epoch [118/300], Batch [34/43], Training Loss: 0.00000919
2024-11-06 14:07:10,995 - INFO - Epoch [118/300], Batch [35/43], Training Loss: 0.00002112
2024-11-06 14:07:10,998 - INFO - Epoch [118/300], Batch [36/43], Training Loss: 0.00002237
2024-11-06 14:07:11,002 - INFO - Epoch [118/300], Batch [37/43], Training Loss: 0.00000947
2024-11-06 14:07:11,006 - INFO - Epoch [118/300], Batch [38/43], Training Loss: 0.00001238
2024-11-06 14:07:11,011 - INFO - Epoch [118/300], Batch [39/43], Training Loss: 0.00000728
2024-11-06 14:07:11,016 - INFO - Epoch [118/300], Batch [40/43], Training Loss: 0.00000634
2024-11-06 14:07:11,020 - INFO - Epoch [118/300], Batch [41/43], Training Loss: 0.00000594
2024-11-06 14:07:11,025 - INFO - Epoch [118/300], Batch [42/43], Training Loss: 0.00000818
2024-11-06 14:07:11,029 - INFO - Epoch [118/300], Batch [43/43], Training Loss: 0.00001287
2024-11-06 14:07:11,040 - INFO - Epoch [118/300], Average Training Loss: 0.00001560, Validation Loss: 0.00001814
2024-11-06 14:07:11,044 - INFO - Epoch [119/300], Batch [1/43], Training Loss: 0.00001871
2024-11-06 14:07:11,048 - INFO - Epoch [119/300], Batch [2/43], Training Loss: 0.00000973
2024-11-06 14:07:11,052 - INFO - Epoch [119/300], Batch [3/43], Training Loss: 0.00000521
2024-11-06 14:07:11,056 - INFO - Epoch [119/300], Batch [4/43], Training Loss: 0.00001111
2024-11-06 14:07:11,059 - INFO - Epoch [119/300], Batch [5/43], Training Loss: 0.00001176
2024-11-06 14:07:11,062 - INFO - Epoch [119/300], Batch [6/43], Training Loss: 0.00001322
2024-11-06 14:07:11,066 - INFO - Epoch [119/300], Batch [7/43], Training Loss: 0.00001226
2024-11-06 14:07:11,069 - INFO - Epoch [119/300], Batch [8/43], Training Loss: 0.00001896
2024-11-06 14:07:11,073 - INFO - Epoch [119/300], Batch [9/43], Training Loss: 0.00002997
2024-11-06 14:07:11,076 - INFO - Epoch [119/300], Batch [10/43], Training Loss: 0.00002271
2024-11-06 14:07:11,080 - INFO - Epoch [119/300], Batch [11/43], Training Loss: 0.00001572
2024-11-06 14:07:11,085 - INFO - Epoch [119/300], Batch [12/43], Training Loss: 0.00000829
2024-11-06 14:07:11,088 - INFO - Epoch [119/300], Batch [13/43], Training Loss: 0.00000997
2024-11-06 14:07:11,093 - INFO - Epoch [119/300], Batch [14/43], Training Loss: 0.00001759
2024-11-06 14:07:11,097 - INFO - Epoch [119/300], Batch [15/43], Training Loss: 0.00001444
2024-11-06 14:07:11,102 - INFO - Epoch [119/300], Batch [16/43], Training Loss: 0.00001972
2024-11-06 14:07:11,106 - INFO - Epoch [119/300], Batch [17/43], Training Loss: 0.00002085
2024-11-06 14:07:11,110 - INFO - Epoch [119/300], Batch [18/43], Training Loss: 0.00002060
2024-11-06 14:07:11,114 - INFO - Epoch [119/300], Batch [19/43], Training Loss: 0.00000735
2024-11-06 14:07:11,119 - INFO - Epoch [119/300], Batch [20/43], Training Loss: 0.00001340
2024-11-06 14:07:11,123 - INFO - Epoch [119/300], Batch [21/43], Training Loss: 0.00002218
2024-11-06 14:07:11,127 - INFO - Epoch [119/300], Batch [22/43], Training Loss: 0.00001261
2024-11-06 14:07:11,131 - INFO - Epoch [119/300], Batch [23/43], Training Loss: 0.00002941
2024-11-06 14:07:11,134 - INFO - Epoch [119/300], Batch [24/43], Training Loss: 0.00002572
2024-11-06 14:07:11,138 - INFO - Epoch [119/300], Batch [25/43], Training Loss: 0.00003159
2024-11-06 14:07:11,141 - INFO - Epoch [119/300], Batch [26/43], Training Loss: 0.00002094
2024-11-06 14:07:11,145 - INFO - Epoch [119/300], Batch [27/43], Training Loss: 0.00002337
2024-11-06 14:07:11,150 - INFO - Epoch [119/300], Batch [28/43], Training Loss: 0.00002070
2024-11-06 14:07:11,154 - INFO - Epoch [119/300], Batch [29/43], Training Loss: 0.00001816
2024-11-06 14:07:11,157 - INFO - Epoch [119/300], Batch [30/43], Training Loss: 0.00001392
2024-11-06 14:07:11,161 - INFO - Epoch [119/300], Batch [31/43], Training Loss: 0.00002201
2024-11-06 14:07:11,165 - INFO - Epoch [119/300], Batch [32/43], Training Loss: 0.00001993
2024-11-06 14:07:11,169 - INFO - Epoch [119/300], Batch [33/43], Training Loss: 0.00001759
2024-11-06 14:07:11,173 - INFO - Epoch [119/300], Batch [34/43], Training Loss: 0.00001631
2024-11-06 14:07:11,177 - INFO - Epoch [119/300], Batch [35/43], Training Loss: 0.00002162
2024-11-06 14:07:11,182 - INFO - Epoch [119/300], Batch [36/43], Training Loss: 0.00001364
2024-11-06 14:07:11,185 - INFO - Epoch [119/300], Batch [37/43], Training Loss: 0.00000511
2024-11-06 14:07:11,188 - INFO - Epoch [119/300], Batch [38/43], Training Loss: 0.00001383
2024-11-06 14:07:11,191 - INFO - Epoch [119/300], Batch [39/43], Training Loss: 0.00001412
2024-11-06 14:07:11,195 - INFO - Epoch [119/300], Batch [40/43], Training Loss: 0.00001026
2024-11-06 14:07:11,198 - INFO - Epoch [119/300], Batch [41/43], Training Loss: 0.00001169
2024-11-06 14:07:11,201 - INFO - Epoch [119/300], Batch [42/43], Training Loss: 0.00001904
2024-11-06 14:07:11,205 - INFO - Epoch [119/300], Batch [43/43], Training Loss: 0.00001341
2024-11-06 14:07:11,216 - INFO - Epoch [119/300], Average Training Loss: 0.00001671, Validation Loss: 0.00001772
2024-11-06 14:07:11,220 - INFO - Epoch [120/300], Batch [1/43], Training Loss: 0.00000614
2024-11-06 14:07:11,223 - INFO - Epoch [120/300], Batch [2/43], Training Loss: 0.00001830
2024-11-06 14:07:11,226 - INFO - Epoch [120/300], Batch [3/43], Training Loss: 0.00001413
2024-11-06 14:07:11,228 - INFO - Epoch [120/300], Batch [4/43], Training Loss: 0.00000618
2024-11-06 14:07:11,231 - INFO - Epoch [120/300], Batch [5/43], Training Loss: 0.00001080
2024-11-06 14:07:11,234 - INFO - Epoch [120/300], Batch [6/43], Training Loss: 0.00001720
2024-11-06 14:07:11,237 - INFO - Epoch [120/300], Batch [7/43], Training Loss: 0.00002337
2024-11-06 14:07:11,240 - INFO - Epoch [120/300], Batch [8/43], Training Loss: 0.00001093
2024-11-06 14:07:11,243 - INFO - Epoch [120/300], Batch [9/43], Training Loss: 0.00004895
2024-11-06 14:07:11,246 - INFO - Epoch [120/300], Batch [10/43], Training Loss: 0.00001377
2024-11-06 14:07:11,248 - INFO - Epoch [120/300], Batch [11/43], Training Loss: 0.00000943
2024-11-06 14:07:11,251 - INFO - Epoch [120/300], Batch [12/43], Training Loss: 0.00001509
2024-11-06 14:07:11,255 - INFO - Epoch [120/300], Batch [13/43], Training Loss: 0.00001853
2024-11-06 14:07:11,258 - INFO - Epoch [120/300], Batch [14/43], Training Loss: 0.00001698
2024-11-06 14:07:11,261 - INFO - Epoch [120/300], Batch [15/43], Training Loss: 0.00001525
2024-11-06 14:07:11,265 - INFO - Epoch [120/300], Batch [16/43], Training Loss: 0.00001504
2024-11-06 14:07:11,268 - INFO - Epoch [120/300], Batch [17/43], Training Loss: 0.00002244
2024-11-06 14:07:11,272 - INFO - Epoch [120/300], Batch [18/43], Training Loss: 0.00001802
2024-11-06 14:07:11,275 - INFO - Epoch [120/300], Batch [19/43], Training Loss: 0.00001203
2024-11-06 14:07:11,278 - INFO - Epoch [120/300], Batch [20/43], Training Loss: 0.00001803
2024-11-06 14:07:11,281 - INFO - Epoch [120/300], Batch [21/43], Training Loss: 0.00000596
2024-11-06 14:07:11,284 - INFO - Epoch [120/300], Batch [22/43], Training Loss: 0.00000792
2024-11-06 14:07:11,289 - INFO - Epoch [120/300], Batch [23/43], Training Loss: 0.00001300
2024-11-06 14:07:11,293 - INFO - Epoch [120/300], Batch [24/43], Training Loss: 0.00000380
2024-11-06 14:07:11,296 - INFO - Epoch [120/300], Batch [25/43], Training Loss: 0.00002714
2024-11-06 14:07:11,299 - INFO - Epoch [120/300], Batch [26/43], Training Loss: 0.00001312
2024-11-06 14:07:11,302 - INFO - Epoch [120/300], Batch [27/43], Training Loss: 0.00000574
2024-11-06 14:07:11,305 - INFO - Epoch [120/300], Batch [28/43], Training Loss: 0.00001788
2024-11-06 14:07:11,309 - INFO - Epoch [120/300], Batch [29/43], Training Loss: 0.00001057
2024-11-06 14:07:11,313 - INFO - Epoch [120/300], Batch [30/43], Training Loss: 0.00001534
2024-11-06 14:07:11,317 - INFO - Epoch [120/300], Batch [31/43], Training Loss: 0.00002098
2024-11-06 14:07:11,320 - INFO - Epoch [120/300], Batch [32/43], Training Loss: 0.00003266
2024-11-06 14:07:11,324 - INFO - Epoch [120/300], Batch [33/43], Training Loss: 0.00000341
2024-11-06 14:07:11,328 - INFO - Epoch [120/300], Batch [34/43], Training Loss: 0.00000941
2024-11-06 14:07:11,331 - INFO - Epoch [120/300], Batch [35/43], Training Loss: 0.00004035
2024-11-06 14:07:11,337 - INFO - Epoch [120/300], Batch [36/43], Training Loss: 0.00002602
2024-11-06 14:07:11,341 - INFO - Epoch [120/300], Batch [37/43], Training Loss: 0.00003063
2024-11-06 14:07:11,345 - INFO - Epoch [120/300], Batch [38/43], Training Loss: 0.00003836
2024-11-06 14:07:11,349 - INFO - Epoch [120/300], Batch [39/43], Training Loss: 0.00000996
2024-11-06 14:07:11,353 - INFO - Epoch [120/300], Batch [40/43], Training Loss: 0.00001133
2024-11-06 14:07:11,357 - INFO - Epoch [120/300], Batch [41/43], Training Loss: 0.00002231
2024-11-06 14:07:11,361 - INFO - Epoch [120/300], Batch [42/43], Training Loss: 0.00004789
2024-11-06 14:07:11,364 - INFO - Epoch [120/300], Batch [43/43], Training Loss: 0.00001891
2024-11-06 14:07:11,374 - INFO - Epoch [120/300], Average Training Loss: 0.00001775, Validation Loss: 0.00002322
2024-11-06 14:07:11,377 - INFO - Epoch [121/300], Batch [1/43], Training Loss: 0.00001003
2024-11-06 14:07:11,380 - INFO - Epoch [121/300], Batch [2/43], Training Loss: 0.00002782
2024-11-06 14:07:11,384 - INFO - Epoch [121/300], Batch [3/43], Training Loss: 0.00001223
2024-11-06 14:07:11,387 - INFO - Epoch [121/300], Batch [4/43], Training Loss: 0.00002509
2024-11-06 14:07:11,391 - INFO - Epoch [121/300], Batch [5/43], Training Loss: 0.00001768
2024-11-06 14:07:11,394 - INFO - Epoch [121/300], Batch [6/43], Training Loss: 0.00001995
2024-11-06 14:07:11,398 - INFO - Epoch [121/300], Batch [7/43], Training Loss: 0.00003208
2024-11-06 14:07:11,402 - INFO - Epoch [121/300], Batch [8/43], Training Loss: 0.00001396
2024-11-06 14:07:11,405 - INFO - Epoch [121/300], Batch [9/43], Training Loss: 0.00001180
2024-11-06 14:07:11,408 - INFO - Epoch [121/300], Batch [10/43], Training Loss: 0.00000515
2024-11-06 14:07:11,411 - INFO - Epoch [121/300], Batch [11/43], Training Loss: 0.00002845
2024-11-06 14:07:11,414 - INFO - Epoch [121/300], Batch [12/43], Training Loss: 0.00002072
2024-11-06 14:07:11,417 - INFO - Epoch [121/300], Batch [13/43], Training Loss: 0.00000900
2024-11-06 14:07:11,420 - INFO - Epoch [121/300], Batch [14/43], Training Loss: 0.00003347
2024-11-06 14:07:11,423 - INFO - Epoch [121/300], Batch [15/43], Training Loss: 0.00002155
2024-11-06 14:07:11,426 - INFO - Epoch [121/300], Batch [16/43], Training Loss: 0.00003133
2024-11-06 14:07:11,430 - INFO - Epoch [121/300], Batch [17/43], Training Loss: 0.00002774
2024-11-06 14:07:11,435 - INFO - Epoch [121/300], Batch [18/43], Training Loss: 0.00002671
2024-11-06 14:07:11,439 - INFO - Epoch [121/300], Batch [19/43], Training Loss: 0.00001769
2024-11-06 14:07:11,443 - INFO - Epoch [121/300], Batch [20/43], Training Loss: 0.00002127
2024-11-06 14:07:11,447 - INFO - Epoch [121/300], Batch [21/43], Training Loss: 0.00001203
2024-11-06 14:07:11,450 - INFO - Epoch [121/300], Batch [22/43], Training Loss: 0.00002601
2024-11-06 14:07:11,454 - INFO - Epoch [121/300], Batch [23/43], Training Loss: 0.00001271
2024-11-06 14:07:11,457 - INFO - Epoch [121/300], Batch [24/43], Training Loss: 0.00001278
2024-11-06 14:07:11,461 - INFO - Epoch [121/300], Batch [25/43], Training Loss: 0.00001734
2024-11-06 14:07:11,466 - INFO - Epoch [121/300], Batch [26/43], Training Loss: 0.00002072
2024-11-06 14:07:11,469 - INFO - Epoch [121/300], Batch [27/43], Training Loss: 0.00001186
2024-11-06 14:07:11,472 - INFO - Epoch [121/300], Batch [28/43], Training Loss: 0.00002509
2024-11-06 14:07:11,476 - INFO - Epoch [121/300], Batch [29/43], Training Loss: 0.00001323
2024-11-06 14:07:11,480 - INFO - Epoch [121/300], Batch [30/43], Training Loss: 0.00001216
2024-11-06 14:07:11,484 - INFO - Epoch [121/300], Batch [31/43], Training Loss: 0.00000563
2024-11-06 14:07:11,487 - INFO - Epoch [121/300], Batch [32/43], Training Loss: 0.00001099
2024-11-06 14:07:11,491 - INFO - Epoch [121/300], Batch [33/43], Training Loss: 0.00001914
2024-11-06 14:07:11,495 - INFO - Epoch [121/300], Batch [34/43], Training Loss: 0.00000902
2024-11-06 14:07:11,500 - INFO - Epoch [121/300], Batch [35/43], Training Loss: 0.00001214
2024-11-06 14:07:11,505 - INFO - Epoch [121/300], Batch [36/43], Training Loss: 0.00002472
2024-11-06 14:07:11,510 - INFO - Epoch [121/300], Batch [37/43], Training Loss: 0.00002429
2024-11-06 14:07:11,514 - INFO - Epoch [121/300], Batch [38/43], Training Loss: 0.00001506
2024-11-06 14:07:11,518 - INFO - Epoch [121/300], Batch [39/43], Training Loss: 0.00000959
2024-11-06 14:07:11,522 - INFO - Epoch [121/300], Batch [40/43], Training Loss: 0.00003151
2024-11-06 14:07:11,527 - INFO - Epoch [121/300], Batch [41/43], Training Loss: 0.00000963
2024-11-06 14:07:11,532 - INFO - Epoch [121/300], Batch [42/43], Training Loss: 0.00001879
2024-11-06 14:07:11,536 - INFO - Epoch [121/300], Batch [43/43], Training Loss: 0.00000802
2024-11-06 14:07:11,548 - INFO - Epoch [121/300], Average Training Loss: 0.00001805, Validation Loss: 0.00002054
2024-11-06 14:07:11,551 - INFO - Epoch [122/300], Batch [1/43], Training Loss: 0.00000777
2024-11-06 14:07:11,555 - INFO - Epoch [122/300], Batch [2/43], Training Loss: 0.00001322
2024-11-06 14:07:11,558 - INFO - Epoch [122/300], Batch [3/43], Training Loss: 0.00001831
2024-11-06 14:07:11,562 - INFO - Epoch [122/300], Batch [4/43], Training Loss: 0.00001785
2024-11-06 14:07:11,566 - INFO - Epoch [122/300], Batch [5/43], Training Loss: 0.00002038
2024-11-06 14:07:11,570 - INFO - Epoch [122/300], Batch [6/43], Training Loss: 0.00001133
2024-11-06 14:07:11,574 - INFO - Epoch [122/300], Batch [7/43], Training Loss: 0.00001304
2024-11-06 14:07:11,577 - INFO - Epoch [122/300], Batch [8/43], Training Loss: 0.00002620
2024-11-06 14:07:11,581 - INFO - Epoch [122/300], Batch [9/43], Training Loss: 0.00001871
2024-11-06 14:07:11,584 - INFO - Epoch [122/300], Batch [10/43], Training Loss: 0.00001329
2024-11-06 14:07:11,588 - INFO - Epoch [122/300], Batch [11/43], Training Loss: 0.00001639
2024-11-06 14:07:11,591 - INFO - Epoch [122/300], Batch [12/43], Training Loss: 0.00003533
2024-11-06 14:07:11,595 - INFO - Epoch [122/300], Batch [13/43], Training Loss: 0.00001243
2024-11-06 14:07:11,600 - INFO - Epoch [122/300], Batch [14/43], Training Loss: 0.00001974
2024-11-06 14:07:11,606 - INFO - Epoch [122/300], Batch [15/43], Training Loss: 0.00001483
2024-11-06 14:07:11,610 - INFO - Epoch [122/300], Batch [16/43], Training Loss: 0.00003549
2024-11-06 14:07:11,614 - INFO - Epoch [122/300], Batch [17/43], Training Loss: 0.00002985
2024-11-06 14:07:11,618 - INFO - Epoch [122/300], Batch [18/43], Training Loss: 0.00000429
2024-11-06 14:07:11,621 - INFO - Epoch [122/300], Batch [19/43], Training Loss: 0.00002312
2024-11-06 14:07:11,625 - INFO - Epoch [122/300], Batch [20/43], Training Loss: 0.00001578
2024-11-06 14:07:11,629 - INFO - Epoch [122/300], Batch [21/43], Training Loss: 0.00002520
2024-11-06 14:07:11,633 - INFO - Epoch [122/300], Batch [22/43], Training Loss: 0.00000707
2024-11-06 14:07:11,638 - INFO - Epoch [122/300], Batch [23/43], Training Loss: 0.00000548
2024-11-06 14:07:11,642 - INFO - Epoch [122/300], Batch [24/43], Training Loss: 0.00001647
2024-11-06 14:07:11,646 - INFO - Epoch [122/300], Batch [25/43], Training Loss: 0.00002299
2024-11-06 14:07:11,651 - INFO - Epoch [122/300], Batch [26/43], Training Loss: 0.00001002
2024-11-06 14:07:11,655 - INFO - Epoch [122/300], Batch [27/43], Training Loss: 0.00001626
2024-11-06 14:07:11,659 - INFO - Epoch [122/300], Batch [28/43], Training Loss: 0.00001203
2024-11-06 14:07:11,663 - INFO - Epoch [122/300], Batch [29/43], Training Loss: 0.00000428
2024-11-06 14:07:11,668 - INFO - Epoch [122/300], Batch [30/43], Training Loss: 0.00002107
2024-11-06 14:07:11,672 - INFO - Epoch [122/300], Batch [31/43], Training Loss: 0.00002475
2024-11-06 14:07:11,678 - INFO - Epoch [122/300], Batch [32/43], Training Loss: 0.00001275
2024-11-06 14:07:11,682 - INFO - Epoch [122/300], Batch [33/43], Training Loss: 0.00000957
2024-11-06 14:07:11,686 - INFO - Epoch [122/300], Batch [34/43], Training Loss: 0.00002722
2024-11-06 14:07:11,691 - INFO - Epoch [122/300], Batch [35/43], Training Loss: 0.00000727
2024-11-06 14:07:11,695 - INFO - Epoch [122/300], Batch [36/43], Training Loss: 0.00002543
2024-11-06 14:07:11,701 - INFO - Epoch [122/300], Batch [37/43], Training Loss: 0.00001494
2024-11-06 14:07:11,705 - INFO - Epoch [122/300], Batch [38/43], Training Loss: 0.00001409
2024-11-06 14:07:11,709 - INFO - Epoch [122/300], Batch [39/43], Training Loss: 0.00001126
2024-11-06 14:07:11,712 - INFO - Epoch [122/300], Batch [40/43], Training Loss: 0.00001845
2024-11-06 14:07:11,716 - INFO - Epoch [122/300], Batch [41/43], Training Loss: 0.00002036
2024-11-06 14:07:11,721 - INFO - Epoch [122/300], Batch [42/43], Training Loss: 0.00001622
2024-11-06 14:07:11,726 - INFO - Epoch [122/300], Batch [43/43], Training Loss: 0.00001391
2024-11-06 14:07:11,738 - INFO - Epoch [122/300], Average Training Loss: 0.00001685, Validation Loss: 0.00002096
2024-11-06 14:07:11,742 - INFO - Epoch [123/300], Batch [1/43], Training Loss: 0.00001567
2024-11-06 14:07:11,746 - INFO - Epoch [123/300], Batch [2/43], Training Loss: 0.00001207
2024-11-06 14:07:11,750 - INFO - Epoch [123/300], Batch [3/43], Training Loss: 0.00000807
2024-11-06 14:07:11,754 - INFO - Epoch [123/300], Batch [4/43], Training Loss: 0.00001438
2024-11-06 14:07:11,758 - INFO - Epoch [123/300], Batch [5/43], Training Loss: 0.00000505
2024-11-06 14:07:11,761 - INFO - Epoch [123/300], Batch [6/43], Training Loss: 0.00002072
2024-11-06 14:07:11,764 - INFO - Epoch [123/300], Batch [7/43], Training Loss: 0.00003118
2024-11-06 14:07:11,768 - INFO - Epoch [123/300], Batch [8/43], Training Loss: 0.00000415
2024-11-06 14:07:11,773 - INFO - Epoch [123/300], Batch [9/43], Training Loss: 0.00000407
2024-11-06 14:07:11,776 - INFO - Epoch [123/300], Batch [10/43], Training Loss: 0.00001113
2024-11-06 14:07:11,780 - INFO - Epoch [123/300], Batch [11/43], Training Loss: 0.00001393
2024-11-06 14:07:11,783 - INFO - Epoch [123/300], Batch [12/43], Training Loss: 0.00000791
2024-11-06 14:07:11,788 - INFO - Epoch [123/300], Batch [13/43], Training Loss: 0.00001721
2024-11-06 14:07:11,792 - INFO - Epoch [123/300], Batch [14/43], Training Loss: 0.00001941
2024-11-06 14:07:11,798 - INFO - Epoch [123/300], Batch [15/43], Training Loss: 0.00002016
2024-11-06 14:07:11,803 - INFO - Epoch [123/300], Batch [16/43], Training Loss: 0.00001316
2024-11-06 14:07:11,807 - INFO - Epoch [123/300], Batch [17/43], Training Loss: 0.00002807
2024-11-06 14:07:11,810 - INFO - Epoch [123/300], Batch [18/43], Training Loss: 0.00001244
2024-11-06 14:07:11,814 - INFO - Epoch [123/300], Batch [19/43], Training Loss: 0.00000875
2024-11-06 14:07:11,818 - INFO - Epoch [123/300], Batch [20/43], Training Loss: 0.00003001
2024-11-06 14:07:11,821 - INFO - Epoch [123/300], Batch [21/43], Training Loss: 0.00000968
2024-11-06 14:07:11,824 - INFO - Epoch [123/300], Batch [22/43], Training Loss: 0.00002207
2024-11-06 14:07:11,829 - INFO - Epoch [123/300], Batch [23/43], Training Loss: 0.00001433
2024-11-06 14:07:11,833 - INFO - Epoch [123/300], Batch [24/43], Training Loss: 0.00001417
2024-11-06 14:07:11,837 - INFO - Epoch [123/300], Batch [25/43], Training Loss: 0.00002472
2024-11-06 14:07:11,841 - INFO - Epoch [123/300], Batch [26/43], Training Loss: 0.00000561
2024-11-06 14:07:11,845 - INFO - Epoch [123/300], Batch [27/43], Training Loss: 0.00001820
2024-11-06 14:07:11,849 - INFO - Epoch [123/300], Batch [28/43], Training Loss: 0.00001154
2024-11-06 14:07:11,854 - INFO - Epoch [123/300], Batch [29/43], Training Loss: 0.00001674
2024-11-06 14:07:11,858 - INFO - Epoch [123/300], Batch [30/43], Training Loss: 0.00001210
2024-11-06 14:07:11,862 - INFO - Epoch [123/300], Batch [31/43], Training Loss: 0.00002369
2024-11-06 14:07:11,867 - INFO - Epoch [123/300], Batch [32/43], Training Loss: 0.00001331
2024-11-06 14:07:11,870 - INFO - Epoch [123/300], Batch [33/43], Training Loss: 0.00001645
2024-11-06 14:07:11,874 - INFO - Epoch [123/300], Batch [34/43], Training Loss: 0.00002057
2024-11-06 14:07:11,878 - INFO - Epoch [123/300], Batch [35/43], Training Loss: 0.00002194
2024-11-06 14:07:11,881 - INFO - Epoch [123/300], Batch [36/43], Training Loss: 0.00001825
2024-11-06 14:07:11,884 - INFO - Epoch [123/300], Batch [37/43], Training Loss: 0.00002311
2024-11-06 14:07:11,887 - INFO - Epoch [123/300], Batch [38/43], Training Loss: 0.00002544
2024-11-06 14:07:11,890 - INFO - Epoch [123/300], Batch [39/43], Training Loss: 0.00002024
2024-11-06 14:07:11,894 - INFO - Epoch [123/300], Batch [40/43], Training Loss: 0.00001252
2024-11-06 14:07:11,898 - INFO - Epoch [123/300], Batch [41/43], Training Loss: 0.00001580
2024-11-06 14:07:11,902 - INFO - Epoch [123/300], Batch [42/43], Training Loss: 0.00001362
2024-11-06 14:07:11,906 - INFO - Epoch [123/300], Batch [43/43], Training Loss: 0.00002104
2024-11-06 14:07:11,918 - INFO - Epoch [123/300], Average Training Loss: 0.00001611, Validation Loss: 0.00001756
2024-11-06 14:07:11,923 - INFO - Epoch [124/300], Batch [1/43], Training Loss: 0.00000618
2024-11-06 14:07:11,927 - INFO - Epoch [124/300], Batch [2/43], Training Loss: 0.00002383
2024-11-06 14:07:11,931 - INFO - Epoch [124/300], Batch [3/43], Training Loss: 0.00001177
2024-11-06 14:07:11,935 - INFO - Epoch [124/300], Batch [4/43], Training Loss: 0.00001464
2024-11-06 14:07:11,939 - INFO - Epoch [124/300], Batch [5/43], Training Loss: 0.00001446
2024-11-06 14:07:11,943 - INFO - Epoch [124/300], Batch [6/43], Training Loss: 0.00001376
2024-11-06 14:07:11,946 - INFO - Epoch [124/300], Batch [7/43], Training Loss: 0.00002561
2024-11-06 14:07:11,950 - INFO - Epoch [124/300], Batch [8/43], Training Loss: 0.00003080
2024-11-06 14:07:11,954 - INFO - Epoch [124/300], Batch [9/43], Training Loss: 0.00000777
2024-11-06 14:07:11,957 - INFO - Epoch [124/300], Batch [10/43], Training Loss: 0.00002951
2024-11-06 14:07:11,962 - INFO - Epoch [124/300], Batch [11/43], Training Loss: 0.00001854
2024-11-06 14:07:11,967 - INFO - Epoch [124/300], Batch [12/43], Training Loss: 0.00003205
2024-11-06 14:07:11,972 - INFO - Epoch [124/300], Batch [13/43], Training Loss: 0.00002483
2024-11-06 14:07:11,976 - INFO - Epoch [124/300], Batch [14/43], Training Loss: 0.00003332
2024-11-06 14:07:11,980 - INFO - Epoch [124/300], Batch [15/43], Training Loss: 0.00000687
2024-11-06 14:07:11,985 - INFO - Epoch [124/300], Batch [16/43], Training Loss: 0.00002479
2024-11-06 14:07:11,991 - INFO - Epoch [124/300], Batch [17/43], Training Loss: 0.00003653
2024-11-06 14:07:11,997 - INFO - Epoch [124/300], Batch [18/43], Training Loss: 0.00002879
2024-11-06 14:07:12,002 - INFO - Epoch [124/300], Batch [19/43], Training Loss: 0.00000943
2024-11-06 14:07:12,009 - INFO - Epoch [124/300], Batch [20/43], Training Loss: 0.00003853
2024-11-06 14:07:12,014 - INFO - Epoch [124/300], Batch [21/43], Training Loss: 0.00003137
2024-11-06 14:07:12,020 - INFO - Epoch [124/300], Batch [22/43], Training Loss: 0.00000679
2024-11-06 14:07:12,025 - INFO - Epoch [124/300], Batch [23/43], Training Loss: 0.00000607
2024-11-06 14:07:12,030 - INFO - Epoch [124/300], Batch [24/43], Training Loss: 0.00001450
2024-11-06 14:07:12,037 - INFO - Epoch [124/300], Batch [25/43], Training Loss: 0.00003057
2024-11-06 14:07:12,043 - INFO - Epoch [124/300], Batch [26/43], Training Loss: 0.00002784
2024-11-06 14:07:12,047 - INFO - Epoch [124/300], Batch [27/43], Training Loss: 0.00001444
2024-11-06 14:07:12,052 - INFO - Epoch [124/300], Batch [28/43], Training Loss: 0.00002042
2024-11-06 14:07:12,056 - INFO - Epoch [124/300], Batch [29/43], Training Loss: 0.00003191
2024-11-06 14:07:12,060 - INFO - Epoch [124/300], Batch [30/43], Training Loss: 0.00001893
2024-11-06 14:07:12,064 - INFO - Epoch [124/300], Batch [31/43], Training Loss: 0.00003352
2024-11-06 14:07:12,069 - INFO - Epoch [124/300], Batch [32/43], Training Loss: 0.00003045
2024-11-06 14:07:12,073 - INFO - Epoch [124/300], Batch [33/43], Training Loss: 0.00001186
2024-11-06 14:07:12,077 - INFO - Epoch [124/300], Batch [34/43], Training Loss: 0.00000541
2024-11-06 14:07:12,080 - INFO - Epoch [124/300], Batch [35/43], Training Loss: 0.00001784
2024-11-06 14:07:12,084 - INFO - Epoch [124/300], Batch [36/43], Training Loss: 0.00001717
2024-11-06 14:07:12,087 - INFO - Epoch [124/300], Batch [37/43], Training Loss: 0.00001928
2024-11-06 14:07:12,090 - INFO - Epoch [124/300], Batch [38/43], Training Loss: 0.00001243
2024-11-06 14:07:12,093 - INFO - Epoch [124/300], Batch [39/43], Training Loss: 0.00001015
2024-11-06 14:07:12,097 - INFO - Epoch [124/300], Batch [40/43], Training Loss: 0.00002008
2024-11-06 14:07:12,100 - INFO - Epoch [124/300], Batch [41/43], Training Loss: 0.00001442
2024-11-06 14:07:12,104 - INFO - Epoch [124/300], Batch [42/43], Training Loss: 0.00001630
2024-11-06 14:07:12,107 - INFO - Epoch [124/300], Batch [43/43], Training Loss: 0.00002747
2024-11-06 14:07:12,119 - INFO - Epoch [124/300], Average Training Loss: 0.00002026, Validation Loss: 0.00002498
2024-11-06 14:07:12,123 - INFO - Epoch [125/300], Batch [1/43], Training Loss: 0.00002875
2024-11-06 14:07:12,125 - INFO - Epoch [125/300], Batch [2/43], Training Loss: 0.00000726
2024-11-06 14:07:12,129 - INFO - Epoch [125/300], Batch [3/43], Training Loss: 0.00001466
2024-11-06 14:07:12,133 - INFO - Epoch [125/300], Batch [4/43], Training Loss: 0.00001280
2024-11-06 14:07:12,138 - INFO - Epoch [125/300], Batch [5/43], Training Loss: 0.00001255
2024-11-06 14:07:12,141 - INFO - Epoch [125/300], Batch [6/43], Training Loss: 0.00000771
2024-11-06 14:07:12,144 - INFO - Epoch [125/300], Batch [7/43], Training Loss: 0.00000670
2024-11-06 14:07:12,147 - INFO - Epoch [125/300], Batch [8/43], Training Loss: 0.00001163
2024-11-06 14:07:12,152 - INFO - Epoch [125/300], Batch [9/43], Training Loss: 0.00001801
2024-11-06 14:07:12,157 - INFO - Epoch [125/300], Batch [10/43], Training Loss: 0.00001001
2024-11-06 14:07:12,161 - INFO - Epoch [125/300], Batch [11/43], Training Loss: 0.00000661
2024-11-06 14:07:12,165 - INFO - Epoch [125/300], Batch [12/43], Training Loss: 0.00001822
2024-11-06 14:07:12,171 - INFO - Epoch [125/300], Batch [13/43], Training Loss: 0.00002894
2024-11-06 14:07:12,175 - INFO - Epoch [125/300], Batch [14/43], Training Loss: 0.00001934
2024-11-06 14:07:12,180 - INFO - Epoch [125/300], Batch [15/43], Training Loss: 0.00001158
2024-11-06 14:07:12,186 - INFO - Epoch [125/300], Batch [16/43], Training Loss: 0.00006137
2024-11-06 14:07:12,190 - INFO - Epoch [125/300], Batch [17/43], Training Loss: 0.00003273
2024-11-06 14:07:12,195 - INFO - Epoch [125/300], Batch [18/43], Training Loss: 0.00001775
2024-11-06 14:07:12,199 - INFO - Epoch [125/300], Batch [19/43], Training Loss: 0.00003837
2024-11-06 14:07:12,203 - INFO - Epoch [125/300], Batch [20/43], Training Loss: 0.00002902
2024-11-06 14:07:12,207 - INFO - Epoch [125/300], Batch [21/43], Training Loss: 0.00001054
2024-11-06 14:07:12,211 - INFO - Epoch [125/300], Batch [22/43], Training Loss: 0.00003833
2024-11-06 14:07:12,214 - INFO - Epoch [125/300], Batch [23/43], Training Loss: 0.00004312
2024-11-06 14:07:12,218 - INFO - Epoch [125/300], Batch [24/43], Training Loss: 0.00002981
2024-11-06 14:07:12,222 - INFO - Epoch [125/300], Batch [25/43], Training Loss: 0.00000735
2024-11-06 14:07:12,225 - INFO - Epoch [125/300], Batch [26/43], Training Loss: 0.00003898
2024-11-06 14:07:12,231 - INFO - Epoch [125/300], Batch [27/43], Training Loss: 0.00004627
2024-11-06 14:07:12,235 - INFO - Epoch [125/300], Batch [28/43], Training Loss: 0.00000451
2024-11-06 14:07:12,238 - INFO - Epoch [125/300], Batch [29/43], Training Loss: 0.00003052
2024-11-06 14:07:12,243 - INFO - Epoch [125/300], Batch [30/43], Training Loss: 0.00001434
2024-11-06 14:07:12,246 - INFO - Epoch [125/300], Batch [31/43], Training Loss: 0.00000727
2024-11-06 14:07:12,251 - INFO - Epoch [125/300], Batch [32/43], Training Loss: 0.00002466
2024-11-06 14:07:12,254 - INFO - Epoch [125/300], Batch [33/43], Training Loss: 0.00001707
2024-11-06 14:07:12,258 - INFO - Epoch [125/300], Batch [34/43], Training Loss: 0.00002023
2024-11-06 14:07:12,262 - INFO - Epoch [125/300], Batch [35/43], Training Loss: 0.00001039
2024-11-06 14:07:12,266 - INFO - Epoch [125/300], Batch [36/43], Training Loss: 0.00001772
2024-11-06 14:07:12,270 - INFO - Epoch [125/300], Batch [37/43], Training Loss: 0.00002775
2024-11-06 14:07:12,275 - INFO - Epoch [125/300], Batch [38/43], Training Loss: 0.00002073
2024-11-06 14:07:12,280 - INFO - Epoch [125/300], Batch [39/43], Training Loss: 0.00001146
2024-11-06 14:07:12,283 - INFO - Epoch [125/300], Batch [40/43], Training Loss: 0.00002399
2024-11-06 14:07:12,287 - INFO - Epoch [125/300], Batch [41/43], Training Loss: 0.00002013
2024-11-06 14:07:12,291 - INFO - Epoch [125/300], Batch [42/43], Training Loss: 0.00003985
2024-11-06 14:07:12,296 - INFO - Epoch [125/300], Batch [43/43], Training Loss: 0.00000906
2024-11-06 14:07:12,308 - INFO - Epoch [125/300], Average Training Loss: 0.00002112, Validation Loss: 0.00002957
2024-11-06 14:07:12,312 - INFO - Epoch [126/300], Batch [1/43], Training Loss: 0.00001853
2024-11-06 14:07:12,316 - INFO - Epoch [126/300], Batch [2/43], Training Loss: 0.00002976
2024-11-06 14:07:12,319 - INFO - Epoch [126/300], Batch [3/43], Training Loss: 0.00001400
2024-11-06 14:07:12,324 - INFO - Epoch [126/300], Batch [4/43], Training Loss: 0.00000977
2024-11-06 14:07:12,328 - INFO - Epoch [126/300], Batch [5/43], Training Loss: 0.00001319
2024-11-06 14:07:12,333 - INFO - Epoch [126/300], Batch [6/43], Training Loss: 0.00002345
2024-11-06 14:07:12,337 - INFO - Epoch [126/300], Batch [7/43], Training Loss: 0.00001026
2024-11-06 14:07:12,341 - INFO - Epoch [126/300], Batch [8/43], Training Loss: 0.00000699
2024-11-06 14:07:12,344 - INFO - Epoch [126/300], Batch [9/43], Training Loss: 0.00002749
2024-11-06 14:07:12,347 - INFO - Epoch [126/300], Batch [10/43], Training Loss: 0.00001324
2024-11-06 14:07:12,351 - INFO - Epoch [126/300], Batch [11/43], Training Loss: 0.00001132
2024-11-06 14:07:12,355 - INFO - Epoch [126/300], Batch [12/43], Training Loss: 0.00002992
2024-11-06 14:07:12,359 - INFO - Epoch [126/300], Batch [13/43], Training Loss: 0.00001315
2024-11-06 14:07:12,363 - INFO - Epoch [126/300], Batch [14/43], Training Loss: 0.00001151
2024-11-06 14:07:12,367 - INFO - Epoch [126/300], Batch [15/43], Training Loss: 0.00002757
2024-11-06 14:07:12,371 - INFO - Epoch [126/300], Batch [16/43], Training Loss: 0.00001446
2024-11-06 14:07:12,376 - INFO - Epoch [126/300], Batch [17/43], Training Loss: 0.00001541
2024-11-06 14:07:12,380 - INFO - Epoch [126/300], Batch [18/43], Training Loss: 0.00001287
2024-11-06 14:07:12,383 - INFO - Epoch [126/300], Batch [19/43], Training Loss: 0.00000937
2024-11-06 14:07:12,388 - INFO - Epoch [126/300], Batch [20/43], Training Loss: 0.00001224
2024-11-06 14:07:12,392 - INFO - Epoch [126/300], Batch [21/43], Training Loss: 0.00000996
2024-11-06 14:07:12,396 - INFO - Epoch [126/300], Batch [22/43], Training Loss: 0.00001738
2024-11-06 14:07:12,400 - INFO - Epoch [126/300], Batch [23/43], Training Loss: 0.00001450
2024-11-06 14:07:12,405 - INFO - Epoch [126/300], Batch [24/43], Training Loss: 0.00001531
2024-11-06 14:07:12,409 - INFO - Epoch [126/300], Batch [25/43], Training Loss: 0.00002364
2024-11-06 14:07:12,413 - INFO - Epoch [126/300], Batch [26/43], Training Loss: 0.00001290
2024-11-06 14:07:12,417 - INFO - Epoch [126/300], Batch [27/43], Training Loss: 0.00001100
2024-11-06 14:07:12,421 - INFO - Epoch [126/300], Batch [28/43], Training Loss: 0.00000656
2024-11-06 14:07:12,425 - INFO - Epoch [126/300], Batch [29/43], Training Loss: 0.00001964
2024-11-06 14:07:12,429 - INFO - Epoch [126/300], Batch [30/43], Training Loss: 0.00002115
2024-11-06 14:07:12,434 - INFO - Epoch [126/300], Batch [31/43], Training Loss: 0.00001333
2024-11-06 14:07:12,439 - INFO - Epoch [126/300], Batch [32/43], Training Loss: 0.00004064
2024-11-06 14:07:12,443 - INFO - Epoch [126/300], Batch [33/43], Training Loss: 0.00001155
2024-11-06 14:07:12,448 - INFO - Epoch [126/300], Batch [34/43], Training Loss: 0.00001901
2024-11-06 14:07:12,453 - INFO - Epoch [126/300], Batch [35/43], Training Loss: 0.00000971
2024-11-06 14:07:12,456 - INFO - Epoch [126/300], Batch [36/43], Training Loss: 0.00001083
2024-11-06 14:07:12,460 - INFO - Epoch [126/300], Batch [37/43], Training Loss: 0.00001359
2024-11-06 14:07:12,464 - INFO - Epoch [126/300], Batch [38/43], Training Loss: 0.00000415
2024-11-06 14:07:12,468 - INFO - Epoch [126/300], Batch [39/43], Training Loss: 0.00000601
2024-11-06 14:07:12,472 - INFO - Epoch [126/300], Batch [40/43], Training Loss: 0.00000885
2024-11-06 14:07:12,476 - INFO - Epoch [126/300], Batch [41/43], Training Loss: 0.00002303
2024-11-06 14:07:12,480 - INFO - Epoch [126/300], Batch [42/43], Training Loss: 0.00001426
2024-11-06 14:07:12,484 - INFO - Epoch [126/300], Batch [43/43], Training Loss: 0.00002979
2024-11-06 14:07:12,496 - INFO - Epoch [126/300], Average Training Loss: 0.00001584, Validation Loss: 0.00002488
2024-11-06 14:07:12,500 - INFO - Epoch [127/300], Batch [1/43], Training Loss: 0.00002869
2024-11-06 14:07:12,505 - INFO - Epoch [127/300], Batch [2/43], Training Loss: 0.00002432
2024-11-06 14:07:12,510 - INFO - Epoch [127/300], Batch [3/43], Training Loss: 0.00001142
2024-11-06 14:07:12,514 - INFO - Epoch [127/300], Batch [4/43], Training Loss: 0.00000996
2024-11-06 14:07:12,518 - INFO - Epoch [127/300], Batch [5/43], Training Loss: 0.00001954
2024-11-06 14:07:12,522 - INFO - Epoch [127/300], Batch [6/43], Training Loss: 0.00002365
2024-11-06 14:07:12,527 - INFO - Epoch [127/300], Batch [7/43], Training Loss: 0.00000847
2024-11-06 14:07:12,531 - INFO - Epoch [127/300], Batch [8/43], Training Loss: 0.00001071
2024-11-06 14:07:12,536 - INFO - Epoch [127/300], Batch [9/43], Training Loss: 0.00001522
2024-11-06 14:07:12,541 - INFO - Epoch [127/300], Batch [10/43], Training Loss: 0.00002092
2024-11-06 14:07:12,545 - INFO - Epoch [127/300], Batch [11/43], Training Loss: 0.00000485
2024-11-06 14:07:12,550 - INFO - Epoch [127/300], Batch [12/43], Training Loss: 0.00001097
2024-11-06 14:07:12,554 - INFO - Epoch [127/300], Batch [13/43], Training Loss: 0.00001039
2024-11-06 14:07:12,559 - INFO - Epoch [127/300], Batch [14/43], Training Loss: 0.00000928
2024-11-06 14:07:12,564 - INFO - Epoch [127/300], Batch [15/43], Training Loss: 0.00000630
2024-11-06 14:07:12,569 - INFO - Epoch [127/300], Batch [16/43], Training Loss: 0.00001561
2024-11-06 14:07:12,573 - INFO - Epoch [127/300], Batch [17/43], Training Loss: 0.00001951
2024-11-06 14:07:12,577 - INFO - Epoch [127/300], Batch [18/43], Training Loss: 0.00001466
2024-11-06 14:07:12,581 - INFO - Epoch [127/300], Batch [19/43], Training Loss: 0.00000719
2024-11-06 14:07:12,585 - INFO - Epoch [127/300], Batch [20/43], Training Loss: 0.00000718
2024-11-06 14:07:12,588 - INFO - Epoch [127/300], Batch [21/43], Training Loss: 0.00000692
2024-11-06 14:07:12,593 - INFO - Epoch [127/300], Batch [22/43], Training Loss: 0.00001091
2024-11-06 14:07:12,597 - INFO - Epoch [127/300], Batch [23/43], Training Loss: 0.00001867
2024-11-06 14:07:12,601 - INFO - Epoch [127/300], Batch [24/43], Training Loss: 0.00000931
2024-11-06 14:07:12,604 - INFO - Epoch [127/300], Batch [25/43], Training Loss: 0.00001426
2024-11-06 14:07:12,608 - INFO - Epoch [127/300], Batch [26/43], Training Loss: 0.00000646
2024-11-06 14:07:12,612 - INFO - Epoch [127/300], Batch [27/43], Training Loss: 0.00002828
2024-11-06 14:07:12,617 - INFO - Epoch [127/300], Batch [28/43], Training Loss: 0.00001714
2024-11-06 14:07:12,621 - INFO - Epoch [127/300], Batch [29/43], Training Loss: 0.00001766
2024-11-06 14:07:12,625 - INFO - Epoch [127/300], Batch [30/43], Training Loss: 0.00001486
2024-11-06 14:07:12,629 - INFO - Epoch [127/300], Batch [31/43], Training Loss: 0.00001801
2024-11-06 14:07:12,632 - INFO - Epoch [127/300], Batch [32/43], Training Loss: 0.00000750
2024-11-06 14:07:12,637 - INFO - Epoch [127/300], Batch [33/43], Training Loss: 0.00001339
2024-11-06 14:07:12,641 - INFO - Epoch [127/300], Batch [34/43], Training Loss: 0.00001560
2024-11-06 14:07:12,645 - INFO - Epoch [127/300], Batch [35/43], Training Loss: 0.00001298
2024-11-06 14:07:12,649 - INFO - Epoch [127/300], Batch [36/43], Training Loss: 0.00003379
2024-11-06 14:07:12,652 - INFO - Epoch [127/300], Batch [37/43], Training Loss: 0.00000734
2024-11-06 14:07:12,656 - INFO - Epoch [127/300], Batch [38/43], Training Loss: 0.00001273
2024-11-06 14:07:12,659 - INFO - Epoch [127/300], Batch [39/43], Training Loss: 0.00001096
2024-11-06 14:07:12,663 - INFO - Epoch [127/300], Batch [40/43], Training Loss: 0.00001292
2024-11-06 14:07:12,667 - INFO - Epoch [127/300], Batch [41/43], Training Loss: 0.00000779
2024-11-06 14:07:12,670 - INFO - Epoch [127/300], Batch [42/43], Training Loss: 0.00002730
2024-11-06 14:07:12,675 - INFO - Epoch [127/300], Batch [43/43], Training Loss: 0.00001929
2024-11-06 14:07:12,689 - INFO - Epoch [127/300], Average Training Loss: 0.00001449, Validation Loss: 0.00001788
2024-11-06 14:07:12,693 - INFO - Epoch [128/300], Batch [1/43], Training Loss: 0.00001822
2024-11-06 14:07:12,697 - INFO - Epoch [128/300], Batch [2/43], Training Loss: 0.00000625
2024-11-06 14:07:12,701 - INFO - Epoch [128/300], Batch [3/43], Training Loss: 0.00001493
2024-11-06 14:07:12,705 - INFO - Epoch [128/300], Batch [4/43], Training Loss: 0.00001836
2024-11-06 14:07:12,709 - INFO - Epoch [128/300], Batch [5/43], Training Loss: 0.00000737
2024-11-06 14:07:12,714 - INFO - Epoch [128/300], Batch [6/43], Training Loss: 0.00001120
2024-11-06 14:07:12,718 - INFO - Epoch [128/300], Batch [7/43], Training Loss: 0.00001459
2024-11-06 14:07:12,723 - INFO - Epoch [128/300], Batch [8/43], Training Loss: 0.00000919
2024-11-06 14:07:12,728 - INFO - Epoch [128/300], Batch [9/43], Training Loss: 0.00001365
2024-11-06 14:07:12,732 - INFO - Epoch [128/300], Batch [10/43], Training Loss: 0.00000444
2024-11-06 14:07:12,737 - INFO - Epoch [128/300], Batch [11/43], Training Loss: 0.00000886
2024-11-06 14:07:12,740 - INFO - Epoch [128/300], Batch [12/43], Training Loss: 0.00000865
2024-11-06 14:07:12,745 - INFO - Epoch [128/300], Batch [13/43], Training Loss: 0.00002703
2024-11-06 14:07:12,749 - INFO - Epoch [128/300], Batch [14/43], Training Loss: 0.00001644
2024-11-06 14:07:12,752 - INFO - Epoch [128/300], Batch [15/43], Training Loss: 0.00001435
2024-11-06 14:07:12,756 - INFO - Epoch [128/300], Batch [16/43], Training Loss: 0.00000961
2024-11-06 14:07:12,760 - INFO - Epoch [128/300], Batch [17/43], Training Loss: 0.00000745
2024-11-06 14:07:12,764 - INFO - Epoch [128/300], Batch [18/43], Training Loss: 0.00000527
2024-11-06 14:07:12,767 - INFO - Epoch [128/300], Batch [19/43], Training Loss: 0.00000939
2024-11-06 14:07:12,771 - INFO - Epoch [128/300], Batch [20/43], Training Loss: 0.00000606
2024-11-06 14:07:12,776 - INFO - Epoch [128/300], Batch [21/43], Training Loss: 0.00002086
2024-11-06 14:07:12,781 - INFO - Epoch [128/300], Batch [22/43], Training Loss: 0.00004579
2024-11-06 14:07:12,785 - INFO - Epoch [128/300], Batch [23/43], Training Loss: 0.00000884
2024-11-06 14:07:12,788 - INFO - Epoch [128/300], Batch [24/43], Training Loss: 0.00001340
2024-11-06 14:07:12,792 - INFO - Epoch [128/300], Batch [25/43], Training Loss: 0.00001662
2024-11-06 14:07:12,795 - INFO - Epoch [128/300], Batch [26/43], Training Loss: 0.00000557
2024-11-06 14:07:12,798 - INFO - Epoch [128/300], Batch [27/43], Training Loss: 0.00001372
2024-11-06 14:07:12,802 - INFO - Epoch [128/300], Batch [28/43], Training Loss: 0.00000850
2024-11-06 14:07:12,807 - INFO - Epoch [128/300], Batch [29/43], Training Loss: 0.00000683
2024-11-06 14:07:12,812 - INFO - Epoch [128/300], Batch [30/43], Training Loss: 0.00000497
2024-11-06 14:07:12,816 - INFO - Epoch [128/300], Batch [31/43], Training Loss: 0.00002088
2024-11-06 14:07:12,820 - INFO - Epoch [128/300], Batch [32/43], Training Loss: 0.00001701
2024-11-06 14:07:12,825 - INFO - Epoch [128/300], Batch [33/43], Training Loss: 0.00001453
2024-11-06 14:07:12,829 - INFO - Epoch [128/300], Batch [34/43], Training Loss: 0.00000612
2024-11-06 14:07:12,834 - INFO - Epoch [128/300], Batch [35/43], Training Loss: 0.00004240
2024-11-06 14:07:12,838 - INFO - Epoch [128/300], Batch [36/43], Training Loss: 0.00001308
2024-11-06 14:07:12,843 - INFO - Epoch [128/300], Batch [37/43], Training Loss: 0.00001430
2024-11-06 14:07:12,847 - INFO - Epoch [128/300], Batch [38/43], Training Loss: 0.00000704
2024-11-06 14:07:12,852 - INFO - Epoch [128/300], Batch [39/43], Training Loss: 0.00000727
2024-11-06 14:07:12,856 - INFO - Epoch [128/300], Batch [40/43], Training Loss: 0.00000912
2024-11-06 14:07:12,860 - INFO - Epoch [128/300], Batch [41/43], Training Loss: 0.00001190
2024-11-06 14:07:12,865 - INFO - Epoch [128/300], Batch [42/43], Training Loss: 0.00001671
2024-11-06 14:07:12,870 - INFO - Epoch [128/300], Batch [43/43], Training Loss: 0.00001129
2024-11-06 14:07:12,884 - INFO - Epoch [128/300], Average Training Loss: 0.00001321, Validation Loss: 0.00001993
2024-11-06 14:07:12,889 - INFO - Epoch [129/300], Batch [1/43], Training Loss: 0.00002048
2024-11-06 14:07:12,894 - INFO - Epoch [129/300], Batch [2/43], Training Loss: 0.00001508
2024-11-06 14:07:12,899 - INFO - Epoch [129/300], Batch [3/43], Training Loss: 0.00002434
2024-11-06 14:07:12,903 - INFO - Epoch [129/300], Batch [4/43], Training Loss: 0.00001653
2024-11-06 14:07:12,907 - INFO - Epoch [129/300], Batch [5/43], Training Loss: 0.00000379
2024-11-06 14:07:12,913 - INFO - Epoch [129/300], Batch [6/43], Training Loss: 0.00001524
2024-11-06 14:07:12,918 - INFO - Epoch [129/300], Batch [7/43], Training Loss: 0.00000424
2024-11-06 14:07:12,923 - INFO - Epoch [129/300], Batch [8/43], Training Loss: 0.00001012
2024-11-06 14:07:12,927 - INFO - Epoch [129/300], Batch [9/43], Training Loss: 0.00001289
2024-11-06 14:07:12,932 - INFO - Epoch [129/300], Batch [10/43], Training Loss: 0.00001907
2024-11-06 14:07:12,936 - INFO - Epoch [129/300], Batch [11/43], Training Loss: 0.00001955
2024-11-06 14:07:12,940 - INFO - Epoch [129/300], Batch [12/43], Training Loss: 0.00001373
2024-11-06 14:07:12,945 - INFO - Epoch [129/300], Batch [13/43], Training Loss: 0.00001252
2024-11-06 14:07:12,950 - INFO - Epoch [129/300], Batch [14/43], Training Loss: 0.00001297
2024-11-06 14:07:12,954 - INFO - Epoch [129/300], Batch [15/43], Training Loss: 0.00001142
2024-11-06 14:07:12,959 - INFO - Epoch [129/300], Batch [16/43], Training Loss: 0.00001425
2024-11-06 14:07:12,964 - INFO - Epoch [129/300], Batch [17/43], Training Loss: 0.00001792
2024-11-06 14:07:12,968 - INFO - Epoch [129/300], Batch [18/43], Training Loss: 0.00001585
2024-11-06 14:07:12,972 - INFO - Epoch [129/300], Batch [19/43], Training Loss: 0.00000828
2024-11-06 14:07:12,977 - INFO - Epoch [129/300], Batch [20/43], Training Loss: 0.00001484
2024-11-06 14:07:12,981 - INFO - Epoch [129/300], Batch [21/43], Training Loss: 0.00002556
2024-11-06 14:07:12,986 - INFO - Epoch [129/300], Batch [22/43], Training Loss: 0.00001340
2024-11-06 14:07:12,990 - INFO - Epoch [129/300], Batch [23/43], Training Loss: 0.00000840
2024-11-06 14:07:12,995 - INFO - Epoch [129/300], Batch [24/43], Training Loss: 0.00000624
2024-11-06 14:07:12,999 - INFO - Epoch [129/300], Batch [25/43], Training Loss: 0.00001473
2024-11-06 14:07:13,003 - INFO - Epoch [129/300], Batch [26/43], Training Loss: 0.00002949
2024-11-06 14:07:13,008 - INFO - Epoch [129/300], Batch [27/43], Training Loss: 0.00001724
2024-11-06 14:07:13,013 - INFO - Epoch [129/300], Batch [28/43], Training Loss: 0.00000982
2024-11-06 14:07:13,018 - INFO - Epoch [129/300], Batch [29/43], Training Loss: 0.00001858
2024-11-06 14:07:13,022 - INFO - Epoch [129/300], Batch [30/43], Training Loss: 0.00000701
2024-11-06 14:07:13,027 - INFO - Epoch [129/300], Batch [31/43], Training Loss: 0.00000938
2024-11-06 14:07:13,031 - INFO - Epoch [129/300], Batch [32/43], Training Loss: 0.00000981
2024-11-06 14:07:13,035 - INFO - Epoch [129/300], Batch [33/43], Training Loss: 0.00000869
2024-11-06 14:07:13,040 - INFO - Epoch [129/300], Batch [34/43], Training Loss: 0.00002015
2024-11-06 14:07:13,045 - INFO - Epoch [129/300], Batch [35/43], Training Loss: 0.00000647
2024-11-06 14:07:13,049 - INFO - Epoch [129/300], Batch [36/43], Training Loss: 0.00001223
2024-11-06 14:07:13,053 - INFO - Epoch [129/300], Batch [37/43], Training Loss: 0.00001569
2024-11-06 14:07:13,058 - INFO - Epoch [129/300], Batch [38/43], Training Loss: 0.00001860
2024-11-06 14:07:13,062 - INFO - Epoch [129/300], Batch [39/43], Training Loss: 0.00001669
2024-11-06 14:07:13,067 - INFO - Epoch [129/300], Batch [40/43], Training Loss: 0.00001033
2024-11-06 14:07:13,071 - INFO - Epoch [129/300], Batch [41/43], Training Loss: 0.00000787
2024-11-06 14:07:13,076 - INFO - Epoch [129/300], Batch [42/43], Training Loss: 0.00000919
2024-11-06 14:07:13,081 - INFO - Epoch [129/300], Batch [43/43], Training Loss: 0.00001166
2024-11-06 14:07:13,095 - INFO - Epoch [129/300], Average Training Loss: 0.00001373, Validation Loss: 0.00001876
2024-11-06 14:07:13,100 - INFO - Epoch [130/300], Batch [1/43], Training Loss: 0.00001760
2024-11-06 14:07:13,105 - INFO - Epoch [130/300], Batch [2/43], Training Loss: 0.00001204
2024-11-06 14:07:13,111 - INFO - Epoch [130/300], Batch [3/43], Training Loss: 0.00001355
2024-11-06 14:07:13,115 - INFO - Epoch [130/300], Batch [4/43], Training Loss: 0.00000925
2024-11-06 14:07:13,119 - INFO - Epoch [130/300], Batch [5/43], Training Loss: 0.00001811
2024-11-06 14:07:13,124 - INFO - Epoch [130/300], Batch [6/43], Training Loss: 0.00001781
2024-11-06 14:07:13,129 - INFO - Epoch [130/300], Batch [7/43], Training Loss: 0.00001169
2024-11-06 14:07:13,133 - INFO - Epoch [130/300], Batch [8/43], Training Loss: 0.00001678
2024-11-06 14:07:13,138 - INFO - Epoch [130/300], Batch [9/43], Training Loss: 0.00001137
2024-11-06 14:07:13,142 - INFO - Epoch [130/300], Batch [10/43], Training Loss: 0.00001130
2024-11-06 14:07:13,146 - INFO - Epoch [130/300], Batch [11/43], Training Loss: 0.00001820
2024-11-06 14:07:13,151 - INFO - Epoch [130/300], Batch [12/43], Training Loss: 0.00001245
2024-11-06 14:07:13,156 - INFO - Epoch [130/300], Batch [13/43], Training Loss: 0.00001137
2024-11-06 14:07:13,161 - INFO - Epoch [130/300], Batch [14/43], Training Loss: 0.00000888
2024-11-06 14:07:13,164 - INFO - Epoch [130/300], Batch [15/43], Training Loss: 0.00001771
2024-11-06 14:07:13,168 - INFO - Epoch [130/300], Batch [16/43], Training Loss: 0.00001337
2024-11-06 14:07:13,172 - INFO - Epoch [130/300], Batch [17/43], Training Loss: 0.00000838
2024-11-06 14:07:13,177 - INFO - Epoch [130/300], Batch [18/43], Training Loss: 0.00000957
2024-11-06 14:07:13,181 - INFO - Epoch [130/300], Batch [19/43], Training Loss: 0.00001328
2024-11-06 14:07:13,185 - INFO - Epoch [130/300], Batch [20/43], Training Loss: 0.00001068
2024-11-06 14:07:13,190 - INFO - Epoch [130/300], Batch [21/43], Training Loss: 0.00002400
2024-11-06 14:07:13,195 - INFO - Epoch [130/300], Batch [22/43], Training Loss: 0.00000595
2024-11-06 14:07:13,200 - INFO - Epoch [130/300], Batch [23/43], Training Loss: 0.00001004
2024-11-06 14:07:13,204 - INFO - Epoch [130/300], Batch [24/43], Training Loss: 0.00000379
2024-11-06 14:07:13,208 - INFO - Epoch [130/300], Batch [25/43], Training Loss: 0.00000360
2024-11-06 14:07:13,213 - INFO - Epoch [130/300], Batch [26/43], Training Loss: 0.00000353
2024-11-06 14:07:13,218 - INFO - Epoch [130/300], Batch [27/43], Training Loss: 0.00002317
2024-11-06 14:07:13,222 - INFO - Epoch [130/300], Batch [28/43], Training Loss: 0.00001292
2024-11-06 14:07:13,227 - INFO - Epoch [130/300], Batch [29/43], Training Loss: 0.00001064
2024-11-06 14:07:13,232 - INFO - Epoch [130/300], Batch [30/43], Training Loss: 0.00000786
2024-11-06 14:07:13,236 - INFO - Epoch [130/300], Batch [31/43], Training Loss: 0.00000386
2024-11-06 14:07:13,240 - INFO - Epoch [130/300], Batch [32/43], Training Loss: 0.00000841
2024-11-06 14:07:13,245 - INFO - Epoch [130/300], Batch [33/43], Training Loss: 0.00001533
2024-11-06 14:07:13,249 - INFO - Epoch [130/300], Batch [34/43], Training Loss: 0.00000973
2024-11-06 14:07:13,253 - INFO - Epoch [130/300], Batch [35/43], Training Loss: 0.00000822
2024-11-06 14:07:13,258 - INFO - Epoch [130/300], Batch [36/43], Training Loss: 0.00002889
2024-11-06 14:07:13,262 - INFO - Epoch [130/300], Batch [37/43], Training Loss: 0.00002333
2024-11-06 14:07:13,267 - INFO - Epoch [130/300], Batch [38/43], Training Loss: 0.00001985
2024-11-06 14:07:13,272 - INFO - Epoch [130/300], Batch [39/43], Training Loss: 0.00001708
2024-11-06 14:07:13,276 - INFO - Epoch [130/300], Batch [40/43], Training Loss: 0.00000438
2024-11-06 14:07:13,280 - INFO - Epoch [130/300], Batch [41/43], Training Loss: 0.00002054
2024-11-06 14:07:13,284 - INFO - Epoch [130/300], Batch [42/43], Training Loss: 0.00003355
2024-11-06 14:07:13,289 - INFO - Epoch [130/300], Batch [43/43], Training Loss: 0.00001292
2024-11-06 14:07:13,302 - INFO - Epoch [130/300], Average Training Loss: 0.00001337, Validation Loss: 0.00002468
2024-11-06 14:07:13,307 - INFO - Epoch [131/300], Batch [1/43], Training Loss: 0.00001413
2024-11-06 14:07:13,311 - INFO - Epoch [131/300], Batch [2/43], Training Loss: 0.00001435
2024-11-06 14:07:13,315 - INFO - Epoch [131/300], Batch [3/43], Training Loss: 0.00001393
2024-11-06 14:07:13,319 - INFO - Epoch [131/300], Batch [4/43], Training Loss: 0.00001823
2024-11-06 14:07:13,323 - INFO - Epoch [131/300], Batch [5/43], Training Loss: 0.00000913
2024-11-06 14:07:13,377 - INFO - Epoch [131/300], Batch [6/43], Training Loss: 0.00001369
2024-11-06 14:07:13,388 - INFO - Epoch [131/300], Batch [7/43], Training Loss: 0.00001136
2024-11-06 14:07:13,399 - INFO - Epoch [131/300], Batch [8/43], Training Loss: 0.00001317
2024-11-06 14:07:13,404 - INFO - Epoch [131/300], Batch [9/43], Training Loss: 0.00002848
2024-11-06 14:07:13,407 - INFO - Epoch [131/300], Batch [10/43], Training Loss: 0.00001636
2024-11-06 14:07:13,411 - INFO - Epoch [131/300], Batch [11/43], Training Loss: 0.00001286
2024-11-06 14:07:13,415 - INFO - Epoch [131/300], Batch [12/43], Training Loss: 0.00001077
2024-11-06 14:07:13,419 - INFO - Epoch [131/300], Batch [13/43], Training Loss: 0.00002637
2024-11-06 14:07:13,423 - INFO - Epoch [131/300], Batch [14/43], Training Loss: 0.00002661
2024-11-06 14:07:13,427 - INFO - Epoch [131/300], Batch [15/43], Training Loss: 0.00001616
2024-11-06 14:07:13,431 - INFO - Epoch [131/300], Batch [16/43], Training Loss: 0.00002335
2024-11-06 14:07:13,435 - INFO - Epoch [131/300], Batch [17/43], Training Loss: 0.00003616
2024-11-06 14:07:13,440 - INFO - Epoch [131/300], Batch [18/43], Training Loss: 0.00001315
2024-11-06 14:07:13,445 - INFO - Epoch [131/300], Batch [19/43], Training Loss: 0.00000450
2024-11-06 14:07:13,449 - INFO - Epoch [131/300], Batch [20/43], Training Loss: 0.00001403
2024-11-06 14:07:13,454 - INFO - Epoch [131/300], Batch [21/43], Training Loss: 0.00000959
2024-11-06 14:07:13,460 - INFO - Epoch [131/300], Batch [22/43], Training Loss: 0.00001609
2024-11-06 14:07:13,469 - INFO - Epoch [131/300], Batch [23/43], Training Loss: 0.00001229
2024-11-06 14:07:13,473 - INFO - Epoch [131/300], Batch [24/43], Training Loss: 0.00002924
2024-11-06 14:07:13,478 - INFO - Epoch [131/300], Batch [25/43], Training Loss: 0.00000881
2024-11-06 14:07:13,482 - INFO - Epoch [131/300], Batch [26/43], Training Loss: 0.00000968
2024-11-06 14:07:13,487 - INFO - Epoch [131/300], Batch [27/43], Training Loss: 0.00002584
2024-11-06 14:07:13,492 - INFO - Epoch [131/300], Batch [28/43], Training Loss: 0.00001840
2024-11-06 14:07:13,497 - INFO - Epoch [131/300], Batch [29/43], Training Loss: 0.00001394
2024-11-06 14:07:13,503 - INFO - Epoch [131/300], Batch [30/43], Training Loss: 0.00001934
2024-11-06 14:07:13,508 - INFO - Epoch [131/300], Batch [31/43], Training Loss: 0.00000358
2024-11-06 14:07:13,512 - INFO - Epoch [131/300], Batch [32/43], Training Loss: 0.00000738
2024-11-06 14:07:13,518 - INFO - Epoch [131/300], Batch [33/43], Training Loss: 0.00001985
2024-11-06 14:07:13,523 - INFO - Epoch [131/300], Batch [34/43], Training Loss: 0.00002114
2024-11-06 14:07:13,527 - INFO - Epoch [131/300], Batch [35/43], Training Loss: 0.00001187
2024-11-06 14:07:13,531 - INFO - Epoch [131/300], Batch [36/43], Training Loss: 0.00001098
2024-11-06 14:07:13,536 - INFO - Epoch [131/300], Batch [37/43], Training Loss: 0.00001104
2024-11-06 14:07:13,541 - INFO - Epoch [131/300], Batch [38/43], Training Loss: 0.00002954
2024-11-06 14:07:13,545 - INFO - Epoch [131/300], Batch [39/43], Training Loss: 0.00000770
2024-11-06 14:07:13,549 - INFO - Epoch [131/300], Batch [40/43], Training Loss: 0.00002377
2024-11-06 14:07:13,552 - INFO - Epoch [131/300], Batch [41/43], Training Loss: 0.00000932
2024-11-06 14:07:13,556 - INFO - Epoch [131/300], Batch [42/43], Training Loss: 0.00001410
2024-11-06 14:07:13,561 - INFO - Epoch [131/300], Batch [43/43], Training Loss: 0.00000947
2024-11-06 14:07:13,573 - INFO - Epoch [131/300], Average Training Loss: 0.00001581, Validation Loss: 0.00001823
2024-11-06 14:07:13,577 - INFO - Epoch [132/300], Batch [1/43], Training Loss: 0.00001220
2024-11-06 14:07:13,581 - INFO - Epoch [132/300], Batch [2/43], Training Loss: 0.00002501
2024-11-06 14:07:13,584 - INFO - Epoch [132/300], Batch [3/43], Training Loss: 0.00002570
2024-11-06 14:07:13,588 - INFO - Epoch [132/300], Batch [4/43], Training Loss: 0.00000269
2024-11-06 14:07:13,593 - INFO - Epoch [132/300], Batch [5/43], Training Loss: 0.00000655
2024-11-06 14:07:13,597 - INFO - Epoch [132/300], Batch [6/43], Training Loss: 0.00000931
2024-11-06 14:07:13,602 - INFO - Epoch [132/300], Batch [7/43], Training Loss: 0.00001040
2024-11-06 14:07:13,606 - INFO - Epoch [132/300], Batch [8/43], Training Loss: 0.00001096
2024-11-06 14:07:13,610 - INFO - Epoch [132/300], Batch [9/43], Training Loss: 0.00000976
2024-11-06 14:07:13,614 - INFO - Epoch [132/300], Batch [10/43], Training Loss: 0.00002550
2024-11-06 14:07:13,618 - INFO - Epoch [132/300], Batch [11/43], Training Loss: 0.00000886
2024-11-06 14:07:13,623 - INFO - Epoch [132/300], Batch [12/43], Training Loss: 0.00000859
2024-11-06 14:07:13,626 - INFO - Epoch [132/300], Batch [13/43], Training Loss: 0.00001955
2024-11-06 14:07:13,630 - INFO - Epoch [132/300], Batch [14/43], Training Loss: 0.00002164
2024-11-06 14:07:13,634 - INFO - Epoch [132/300], Batch [15/43], Training Loss: 0.00000588
2024-11-06 14:07:13,639 - INFO - Epoch [132/300], Batch [16/43], Training Loss: 0.00001272
2024-11-06 14:07:13,643 - INFO - Epoch [132/300], Batch [17/43], Training Loss: 0.00001654
2024-11-06 14:07:13,648 - INFO - Epoch [132/300], Batch [18/43], Training Loss: 0.00001486
2024-11-06 14:07:13,651 - INFO - Epoch [132/300], Batch [19/43], Training Loss: 0.00001004
2024-11-06 14:07:13,655 - INFO - Epoch [132/300], Batch [20/43], Training Loss: 0.00002310
2024-11-06 14:07:13,659 - INFO - Epoch [132/300], Batch [21/43], Training Loss: 0.00000576
2024-11-06 14:07:13,662 - INFO - Epoch [132/300], Batch [22/43], Training Loss: 0.00001992
2024-11-06 14:07:13,666 - INFO - Epoch [132/300], Batch [23/43], Training Loss: 0.00002047
2024-11-06 14:07:13,670 - INFO - Epoch [132/300], Batch [24/43], Training Loss: 0.00000689
2024-11-06 14:07:13,673 - INFO - Epoch [132/300], Batch [25/43], Training Loss: 0.00001226
2024-11-06 14:07:13,676 - INFO - Epoch [132/300], Batch [26/43], Training Loss: 0.00000994
2024-11-06 14:07:13,679 - INFO - Epoch [132/300], Batch [27/43], Training Loss: 0.00001972
2024-11-06 14:07:13,683 - INFO - Epoch [132/300], Batch [28/43], Training Loss: 0.00000987
2024-11-06 14:07:13,687 - INFO - Epoch [132/300], Batch [29/43], Training Loss: 0.00001074
2024-11-06 14:07:13,690 - INFO - Epoch [132/300], Batch [30/43], Training Loss: 0.00001209
2024-11-06 14:07:13,693 - INFO - Epoch [132/300], Batch [31/43], Training Loss: 0.00000912
2024-11-06 14:07:13,697 - INFO - Epoch [132/300], Batch [32/43], Training Loss: 0.00001241
2024-11-06 14:07:13,701 - INFO - Epoch [132/300], Batch [33/43], Training Loss: 0.00000338
2024-11-06 14:07:13,704 - INFO - Epoch [132/300], Batch [34/43], Training Loss: 0.00001374
2024-11-06 14:07:13,708 - INFO - Epoch [132/300], Batch [35/43], Training Loss: 0.00003117
2024-11-06 14:07:13,712 - INFO - Epoch [132/300], Batch [36/43], Training Loss: 0.00002336
2024-11-06 14:07:13,716 - INFO - Epoch [132/300], Batch [37/43], Training Loss: 0.00002208
2024-11-06 14:07:13,721 - INFO - Epoch [132/300], Batch [38/43], Training Loss: 0.00001920
2024-11-06 14:07:13,727 - INFO - Epoch [132/300], Batch [39/43], Training Loss: 0.00000592
2024-11-06 14:07:13,732 - INFO - Epoch [132/300], Batch [40/43], Training Loss: 0.00001782
2024-11-06 14:07:13,737 - INFO - Epoch [132/300], Batch [41/43], Training Loss: 0.00000873
2024-11-06 14:07:13,741 - INFO - Epoch [132/300], Batch [42/43], Training Loss: 0.00000783
2024-11-06 14:07:13,745 - INFO - Epoch [132/300], Batch [43/43], Training Loss: 0.00002355
2024-11-06 14:07:13,757 - INFO - Epoch [132/300], Average Training Loss: 0.00001409, Validation Loss: 0.00001643
2024-11-06 14:07:13,762 - INFO - Epoch [133/300], Batch [1/43], Training Loss: 0.00001296
2024-11-06 14:07:13,765 - INFO - Epoch [133/300], Batch [2/43], Training Loss: 0.00001461
2024-11-06 14:07:13,769 - INFO - Epoch [133/300], Batch [3/43], Training Loss: 0.00001627
2024-11-06 14:07:13,773 - INFO - Epoch [133/300], Batch [4/43], Training Loss: 0.00001192
2024-11-06 14:07:13,777 - INFO - Epoch [133/300], Batch [5/43], Training Loss: 0.00000864
2024-11-06 14:07:13,781 - INFO - Epoch [133/300], Batch [6/43], Training Loss: 0.00002323
2024-11-06 14:07:13,784 - INFO - Epoch [133/300], Batch [7/43], Training Loss: 0.00002684
2024-11-06 14:07:13,788 - INFO - Epoch [133/300], Batch [8/43], Training Loss: 0.00001885
2024-11-06 14:07:13,790 - INFO - Epoch [133/300], Batch [9/43], Training Loss: 0.00000435
2024-11-06 14:07:13,794 - INFO - Epoch [133/300], Batch [10/43], Training Loss: 0.00002043
2024-11-06 14:07:13,798 - INFO - Epoch [133/300], Batch [11/43], Training Loss: 0.00003180
2024-11-06 14:07:13,801 - INFO - Epoch [133/300], Batch [12/43], Training Loss: 0.00000782
2024-11-06 14:07:13,804 - INFO - Epoch [133/300], Batch [13/43], Training Loss: 0.00001704
2024-11-06 14:07:13,808 - INFO - Epoch [133/300], Batch [14/43], Training Loss: 0.00001756
2024-11-06 14:07:13,811 - INFO - Epoch [133/300], Batch [15/43], Training Loss: 0.00001653
2024-11-06 14:07:13,813 - INFO - Epoch [133/300], Batch [16/43], Training Loss: 0.00002099
2024-11-06 14:07:13,817 - INFO - Epoch [133/300], Batch [17/43], Training Loss: 0.00002149
2024-11-06 14:07:13,821 - INFO - Epoch [133/300], Batch [18/43], Training Loss: 0.00002119
2024-11-06 14:07:13,824 - INFO - Epoch [133/300], Batch [19/43], Training Loss: 0.00002235
2024-11-06 14:07:13,828 - INFO - Epoch [133/300], Batch [20/43], Training Loss: 0.00001090
2024-11-06 14:07:13,832 - INFO - Epoch [133/300], Batch [21/43], Training Loss: 0.00001194
2024-11-06 14:07:13,835 - INFO - Epoch [133/300], Batch [22/43], Training Loss: 0.00001942
2024-11-06 14:07:13,839 - INFO - Epoch [133/300], Batch [23/43], Training Loss: 0.00000961
2024-11-06 14:07:13,842 - INFO - Epoch [133/300], Batch [24/43], Training Loss: 0.00001130
2024-11-06 14:07:13,845 - INFO - Epoch [133/300], Batch [25/43], Training Loss: 0.00001003
2024-11-06 14:07:13,849 - INFO - Epoch [133/300], Batch [26/43], Training Loss: 0.00001544
2024-11-06 14:07:13,851 - INFO - Epoch [133/300], Batch [27/43], Training Loss: 0.00001198
2024-11-06 14:07:13,855 - INFO - Epoch [133/300], Batch [28/43], Training Loss: 0.00001153
2024-11-06 14:07:13,859 - INFO - Epoch [133/300], Batch [29/43], Training Loss: 0.00000520
2024-11-06 14:07:13,862 - INFO - Epoch [133/300], Batch [30/43], Training Loss: 0.00001058
2024-11-06 14:07:13,865 - INFO - Epoch [133/300], Batch [31/43], Training Loss: 0.00002127
2024-11-06 14:07:13,868 - INFO - Epoch [133/300], Batch [32/43], Training Loss: 0.00001176
2024-11-06 14:07:13,873 - INFO - Epoch [133/300], Batch [33/43], Training Loss: 0.00001001
2024-11-06 14:07:13,878 - INFO - Epoch [133/300], Batch [34/43], Training Loss: 0.00002502
2024-11-06 14:07:13,882 - INFO - Epoch [133/300], Batch [35/43], Training Loss: 0.00000608
2024-11-06 14:07:13,885 - INFO - Epoch [133/300], Batch [36/43], Training Loss: 0.00002813
2024-11-06 14:07:13,890 - INFO - Epoch [133/300], Batch [37/43], Training Loss: 0.00001155
2024-11-06 14:07:13,894 - INFO - Epoch [133/300], Batch [38/43], Training Loss: 0.00000508
2024-11-06 14:07:13,898 - INFO - Epoch [133/300], Batch [39/43], Training Loss: 0.00001787
2024-11-06 14:07:13,901 - INFO - Epoch [133/300], Batch [40/43], Training Loss: 0.00002044
2024-11-06 14:07:13,905 - INFO - Epoch [133/300], Batch [41/43], Training Loss: 0.00001654
2024-11-06 14:07:13,909 - INFO - Epoch [133/300], Batch [42/43], Training Loss: 0.00000869
2024-11-06 14:07:13,913 - INFO - Epoch [133/300], Batch [43/43], Training Loss: 0.00003094
2024-11-06 14:07:13,925 - INFO - Epoch [133/300], Average Training Loss: 0.00001572, Validation Loss: 0.00002334
2024-11-06 14:07:13,929 - INFO - Epoch [134/300], Batch [1/43], Training Loss: 0.00001078
2024-11-06 14:07:13,933 - INFO - Epoch [134/300], Batch [2/43], Training Loss: 0.00001863
2024-11-06 14:07:13,936 - INFO - Epoch [134/300], Batch [3/43], Training Loss: 0.00000660
2024-11-06 14:07:13,939 - INFO - Epoch [134/300], Batch [4/43], Training Loss: 0.00000956
2024-11-06 14:07:13,942 - INFO - Epoch [134/300], Batch [5/43], Training Loss: 0.00000766
2024-11-06 14:07:13,945 - INFO - Epoch [134/300], Batch [6/43], Training Loss: 0.00001680
2024-11-06 14:07:13,949 - INFO - Epoch [134/300], Batch [7/43], Training Loss: 0.00000716
2024-11-06 14:07:13,952 - INFO - Epoch [134/300], Batch [8/43], Training Loss: 0.00000680
2024-11-06 14:07:13,956 - INFO - Epoch [134/300], Batch [9/43], Training Loss: 0.00002584
2024-11-06 14:07:13,960 - INFO - Epoch [134/300], Batch [10/43], Training Loss: 0.00000432
2024-11-06 14:07:13,963 - INFO - Epoch [134/300], Batch [11/43], Training Loss: 0.00001304
2024-11-06 14:07:13,967 - INFO - Epoch [134/300], Batch [12/43], Training Loss: 0.00000731
2024-11-06 14:07:13,970 - INFO - Epoch [134/300], Batch [13/43], Training Loss: 0.00000940
2024-11-06 14:07:13,973 - INFO - Epoch [134/300], Batch [14/43], Training Loss: 0.00000915
2024-11-06 14:07:13,976 - INFO - Epoch [134/300], Batch [15/43], Training Loss: 0.00001422
2024-11-06 14:07:13,980 - INFO - Epoch [134/300], Batch [16/43], Training Loss: 0.00000565
2024-11-06 14:07:13,983 - INFO - Epoch [134/300], Batch [17/43], Training Loss: 0.00000936
2024-11-06 14:07:13,986 - INFO - Epoch [134/300], Batch [18/43], Training Loss: 0.00000686
2024-11-06 14:07:13,990 - INFO - Epoch [134/300], Batch [19/43], Training Loss: 0.00002209
2024-11-06 14:07:13,993 - INFO - Epoch [134/300], Batch [20/43], Training Loss: 0.00002483
2024-11-06 14:07:13,996 - INFO - Epoch [134/300], Batch [21/43], Training Loss: 0.00001508
2024-11-06 14:07:14,000 - INFO - Epoch [134/300], Batch [22/43], Training Loss: 0.00000871
2024-11-06 14:07:14,003 - INFO - Epoch [134/300], Batch [23/43], Training Loss: 0.00001743
2024-11-06 14:07:14,007 - INFO - Epoch [134/300], Batch [24/43], Training Loss: 0.00002320
2024-11-06 14:07:14,009 - INFO - Epoch [134/300], Batch [25/43], Training Loss: 0.00001080
2024-11-06 14:07:14,013 - INFO - Epoch [134/300], Batch [26/43], Training Loss: 0.00000578
2024-11-06 14:07:14,017 - INFO - Epoch [134/300], Batch [27/43], Training Loss: 0.00001146
2024-11-06 14:07:14,021 - INFO - Epoch [134/300], Batch [28/43], Training Loss: 0.00001821
2024-11-06 14:07:14,025 - INFO - Epoch [134/300], Batch [29/43], Training Loss: 0.00003193
2024-11-06 14:07:14,030 - INFO - Epoch [134/300], Batch [30/43], Training Loss: 0.00002844
2024-11-06 14:07:14,034 - INFO - Epoch [134/300], Batch [31/43], Training Loss: 0.00002152
2024-11-06 14:07:14,038 - INFO - Epoch [134/300], Batch [32/43], Training Loss: 0.00001313
2024-11-06 14:07:14,042 - INFO - Epoch [134/300], Batch [33/43], Training Loss: 0.00002378
2024-11-06 14:07:14,048 - INFO - Epoch [134/300], Batch [34/43], Training Loss: 0.00001593
2024-11-06 14:07:14,052 - INFO - Epoch [134/300], Batch [35/43], Training Loss: 0.00000603
2024-11-06 14:07:14,055 - INFO - Epoch [134/300], Batch [36/43], Training Loss: 0.00003358
2024-11-06 14:07:14,058 - INFO - Epoch [134/300], Batch [37/43], Training Loss: 0.00002174
2024-11-06 14:07:14,062 - INFO - Epoch [134/300], Batch [38/43], Training Loss: 0.00000943
2024-11-06 14:07:14,065 - INFO - Epoch [134/300], Batch [39/43], Training Loss: 0.00001213
2024-11-06 14:07:14,069 - INFO - Epoch [134/300], Batch [40/43], Training Loss: 0.00002319
2024-11-06 14:07:14,073 - INFO - Epoch [134/300], Batch [41/43], Training Loss: 0.00001170
2024-11-06 14:07:14,077 - INFO - Epoch [134/300], Batch [42/43], Training Loss: 0.00001305
2024-11-06 14:07:14,081 - INFO - Epoch [134/300], Batch [43/43], Training Loss: 0.00001036
2024-11-06 14:07:14,093 - INFO - Epoch [134/300], Average Training Loss: 0.00001448, Validation Loss: 0.00001754
2024-11-06 14:07:14,097 - INFO - Epoch [135/300], Batch [1/43], Training Loss: 0.00001554
2024-11-06 14:07:14,101 - INFO - Epoch [135/300], Batch [2/43], Training Loss: 0.00001087
2024-11-06 14:07:14,105 - INFO - Epoch [135/300], Batch [3/43], Training Loss: 0.00000906
2024-11-06 14:07:14,108 - INFO - Epoch [135/300], Batch [4/43], Training Loss: 0.00000801
2024-11-06 14:07:14,111 - INFO - Epoch [135/300], Batch [5/43], Training Loss: 0.00002532
2024-11-06 14:07:14,115 - INFO - Epoch [135/300], Batch [6/43], Training Loss: 0.00000915
2024-11-06 14:07:14,118 - INFO - Epoch [135/300], Batch [7/43], Training Loss: 0.00000549
2024-11-06 14:07:14,122 - INFO - Epoch [135/300], Batch [8/43], Training Loss: 0.00000962
2024-11-06 14:07:14,126 - INFO - Epoch [135/300], Batch [9/43], Training Loss: 0.00002220
2024-11-06 14:07:14,130 - INFO - Epoch [135/300], Batch [10/43], Training Loss: 0.00001293
2024-11-06 14:07:14,133 - INFO - Epoch [135/300], Batch [11/43], Training Loss: 0.00000417
2024-11-06 14:07:14,136 - INFO - Epoch [135/300], Batch [12/43], Training Loss: 0.00001961
2024-11-06 14:07:14,140 - INFO - Epoch [135/300], Batch [13/43], Training Loss: 0.00003126
2024-11-06 14:07:14,143 - INFO - Epoch [135/300], Batch [14/43], Training Loss: 0.00001282
2024-11-06 14:07:14,146 - INFO - Epoch [135/300], Batch [15/43], Training Loss: 0.00001979
2024-11-06 14:07:14,149 - INFO - Epoch [135/300], Batch [16/43], Training Loss: 0.00001997
2024-11-06 14:07:14,153 - INFO - Epoch [135/300], Batch [17/43], Training Loss: 0.00003571
2024-11-06 14:07:14,157 - INFO - Epoch [135/300], Batch [18/43], Training Loss: 0.00002459
2024-11-06 14:07:14,161 - INFO - Epoch [135/300], Batch [19/43], Training Loss: 0.00001753
2024-11-06 14:07:14,164 - INFO - Epoch [135/300], Batch [20/43], Training Loss: 0.00001416
2024-11-06 14:07:14,167 - INFO - Epoch [135/300], Batch [21/43], Training Loss: 0.00001899
2024-11-06 14:07:14,171 - INFO - Epoch [135/300], Batch [22/43], Training Loss: 0.00001960
2024-11-06 14:07:14,174 - INFO - Epoch [135/300], Batch [23/43], Training Loss: 0.00002334
2024-11-06 14:07:14,178 - INFO - Epoch [135/300], Batch [24/43], Training Loss: 0.00000588
2024-11-06 14:07:14,182 - INFO - Epoch [135/300], Batch [25/43], Training Loss: 0.00000909
2024-11-06 14:07:14,186 - INFO - Epoch [135/300], Batch [26/43], Training Loss: 0.00000948
2024-11-06 14:07:14,190 - INFO - Epoch [135/300], Batch [27/43], Training Loss: 0.00000615
2024-11-06 14:07:14,193 - INFO - Epoch [135/300], Batch [28/43], Training Loss: 0.00002064
2024-11-06 14:07:14,196 - INFO - Epoch [135/300], Batch [29/43], Training Loss: 0.00000985
2024-11-06 14:07:14,200 - INFO - Epoch [135/300], Batch [30/43], Training Loss: 0.00001400
2024-11-06 14:07:14,207 - INFO - Epoch [135/300], Batch [31/43], Training Loss: 0.00000718
2024-11-06 14:07:14,212 - INFO - Epoch [135/300], Batch [32/43], Training Loss: 0.00002177
2024-11-06 14:07:14,217 - INFO - Epoch [135/300], Batch [33/43], Training Loss: 0.00001290
2024-11-06 14:07:14,221 - INFO - Epoch [135/300], Batch [34/43], Training Loss: 0.00000618
2024-11-06 14:07:14,225 - INFO - Epoch [135/300], Batch [35/43], Training Loss: 0.00002125
2024-11-06 14:07:14,228 - INFO - Epoch [135/300], Batch [36/43], Training Loss: 0.00000911
2024-11-06 14:07:14,232 - INFO - Epoch [135/300], Batch [37/43], Training Loss: 0.00000642
2024-11-06 14:07:14,235 - INFO - Epoch [135/300], Batch [38/43], Training Loss: 0.00001370
2024-11-06 14:07:14,239 - INFO - Epoch [135/300], Batch [39/43], Training Loss: 0.00001167
2024-11-06 14:07:14,243 - INFO - Epoch [135/300], Batch [40/43], Training Loss: 0.00000697
2024-11-06 14:07:14,246 - INFO - Epoch [135/300], Batch [41/43], Training Loss: 0.00000363
2024-11-06 14:07:14,251 - INFO - Epoch [135/300], Batch [42/43], Training Loss: 0.00001325
2024-11-06 14:07:14,256 - INFO - Epoch [135/300], Batch [43/43], Training Loss: 0.00002550
2024-11-06 14:07:14,269 - INFO - Epoch [135/300], Average Training Loss: 0.00001452, Validation Loss: 0.00001602
2024-11-06 14:07:14,273 - INFO - Epoch [136/300], Batch [1/43], Training Loss: 0.00000781
2024-11-06 14:07:14,277 - INFO - Epoch [136/300], Batch [2/43], Training Loss: 0.00001426
2024-11-06 14:07:14,280 - INFO - Epoch [136/300], Batch [3/43], Training Loss: 0.00001193
2024-11-06 14:07:14,284 - INFO - Epoch [136/300], Batch [4/43], Training Loss: 0.00001883
2024-11-06 14:07:14,287 - INFO - Epoch [136/300], Batch [5/43], Training Loss: 0.00001043
2024-11-06 14:07:14,291 - INFO - Epoch [136/300], Batch [6/43], Training Loss: 0.00002156
2024-11-06 14:07:14,295 - INFO - Epoch [136/300], Batch [7/43], Training Loss: 0.00001431
2024-11-06 14:07:14,299 - INFO - Epoch [136/300], Batch [8/43], Training Loss: 0.00002104
2024-11-06 14:07:14,303 - INFO - Epoch [136/300], Batch [9/43], Training Loss: 0.00003137
2024-11-06 14:07:14,306 - INFO - Epoch [136/300], Batch [10/43], Training Loss: 0.00002860
2024-11-06 14:07:14,310 - INFO - Epoch [136/300], Batch [11/43], Training Loss: 0.00002007
2024-11-06 14:07:14,314 - INFO - Epoch [136/300], Batch [12/43], Training Loss: 0.00001135
2024-11-06 14:07:14,317 - INFO - Epoch [136/300], Batch [13/43], Training Loss: 0.00001262
2024-11-06 14:07:14,322 - INFO - Epoch [136/300], Batch [14/43], Training Loss: 0.00001739
2024-11-06 14:07:14,329 - INFO - Epoch [136/300], Batch [15/43], Training Loss: 0.00001883
2024-11-06 14:07:14,334 - INFO - Epoch [136/300], Batch [16/43], Training Loss: 0.00001895
2024-11-06 14:07:14,339 - INFO - Epoch [136/300], Batch [17/43], Training Loss: 0.00000681
2024-11-06 14:07:14,343 - INFO - Epoch [136/300], Batch [18/43], Training Loss: 0.00001044
2024-11-06 14:07:14,347 - INFO - Epoch [136/300], Batch [19/43], Training Loss: 0.00001800
2024-11-06 14:07:14,352 - INFO - Epoch [136/300], Batch [20/43], Training Loss: 0.00001541
2024-11-06 14:07:14,356 - INFO - Epoch [136/300], Batch [21/43], Training Loss: 0.00002362
2024-11-06 14:07:14,359 - INFO - Epoch [136/300], Batch [22/43], Training Loss: 0.00002041
2024-11-06 14:07:14,363 - INFO - Epoch [136/300], Batch [23/43], Training Loss: 0.00002167
2024-11-06 14:07:14,367 - INFO - Epoch [136/300], Batch [24/43], Training Loss: 0.00000962
2024-11-06 14:07:14,371 - INFO - Epoch [136/300], Batch [25/43], Training Loss: 0.00001586
2024-11-06 14:07:14,375 - INFO - Epoch [136/300], Batch [26/43], Training Loss: 0.00002262
2024-11-06 14:07:14,378 - INFO - Epoch [136/300], Batch [27/43], Training Loss: 0.00003777
2024-11-06 14:07:14,382 - INFO - Epoch [136/300], Batch [28/43], Training Loss: 0.00001334
2024-11-06 14:07:14,384 - INFO - Epoch [136/300], Batch [29/43], Training Loss: 0.00003342
2024-11-06 14:07:14,388 - INFO - Epoch [136/300], Batch [30/43], Training Loss: 0.00002152
2024-11-06 14:07:14,392 - INFO - Epoch [136/300], Batch [31/43], Training Loss: 0.00000462
2024-11-06 14:07:14,396 - INFO - Epoch [136/300], Batch [32/43], Training Loss: 0.00000659
2024-11-06 14:07:14,400 - INFO - Epoch [136/300], Batch [33/43], Training Loss: 0.00002153
2024-11-06 14:07:14,404 - INFO - Epoch [136/300], Batch [34/43], Training Loss: 0.00002741
2024-11-06 14:07:14,407 - INFO - Epoch [136/300], Batch [35/43], Training Loss: 0.00000795
2024-11-06 14:07:14,411 - INFO - Epoch [136/300], Batch [36/43], Training Loss: 0.00000514
2024-11-06 14:07:14,415 - INFO - Epoch [136/300], Batch [37/43], Training Loss: 0.00002433
2024-11-06 14:07:14,419 - INFO - Epoch [136/300], Batch [38/43], Training Loss: 0.00000723
2024-11-06 14:07:14,423 - INFO - Epoch [136/300], Batch [39/43], Training Loss: 0.00000735
2024-11-06 14:07:14,427 - INFO - Epoch [136/300], Batch [40/43], Training Loss: 0.00001015
2024-11-06 14:07:14,431 - INFO - Epoch [136/300], Batch [41/43], Training Loss: 0.00001066
2024-11-06 14:07:14,435 - INFO - Epoch [136/300], Batch [42/43], Training Loss: 0.00002819
2024-11-06 14:07:14,439 - INFO - Epoch [136/300], Batch [43/43], Training Loss: 0.00000975
2024-11-06 14:07:14,451 - INFO - Epoch [136/300], Average Training Loss: 0.00001676, Validation Loss: 0.00001653
2024-11-06 14:07:14,456 - INFO - Epoch [137/300], Batch [1/43], Training Loss: 0.00001873
2024-11-06 14:07:14,460 - INFO - Epoch [137/300], Batch [2/43], Training Loss: 0.00001240
2024-11-06 14:07:14,463 - INFO - Epoch [137/300], Batch [3/43], Training Loss: 0.00001091
2024-11-06 14:07:14,467 - INFO - Epoch [137/300], Batch [4/43], Training Loss: 0.00001985
2024-11-06 14:07:14,472 - INFO - Epoch [137/300], Batch [5/43], Training Loss: 0.00001296
2024-11-06 14:07:14,476 - INFO - Epoch [137/300], Batch [6/43], Training Loss: 0.00001521
2024-11-06 14:07:14,481 - INFO - Epoch [137/300], Batch [7/43], Training Loss: 0.00002267
2024-11-06 14:07:14,486 - INFO - Epoch [137/300], Batch [8/43], Training Loss: 0.00000984
2024-11-06 14:07:14,491 - INFO - Epoch [137/300], Batch [9/43], Training Loss: 0.00000961
2024-11-06 14:07:14,494 - INFO - Epoch [137/300], Batch [10/43], Training Loss: 0.00001574
2024-11-06 14:07:14,499 - INFO - Epoch [137/300], Batch [11/43], Training Loss: 0.00001327
2024-11-06 14:07:14,503 - INFO - Epoch [137/300], Batch [12/43], Training Loss: 0.00001073
2024-11-06 14:07:14,507 - INFO - Epoch [137/300], Batch [13/43], Training Loss: 0.00002296
2024-11-06 14:07:14,511 - INFO - Epoch [137/300], Batch [14/43], Training Loss: 0.00001024
2024-11-06 14:07:14,515 - INFO - Epoch [137/300], Batch [15/43], Training Loss: 0.00000900
2024-11-06 14:07:14,519 - INFO - Epoch [137/300], Batch [16/43], Training Loss: 0.00000757
2024-11-06 14:07:14,522 - INFO - Epoch [137/300], Batch [17/43], Training Loss: 0.00002274
2024-11-06 14:07:14,525 - INFO - Epoch [137/300], Batch [18/43], Training Loss: 0.00000775
2024-11-06 14:07:14,529 - INFO - Epoch [137/300], Batch [19/43], Training Loss: 0.00000812
2024-11-06 14:07:14,534 - INFO - Epoch [137/300], Batch [20/43], Training Loss: 0.00000881
2024-11-06 14:07:14,538 - INFO - Epoch [137/300], Batch [21/43], Training Loss: 0.00001745
2024-11-06 14:07:14,541 - INFO - Epoch [137/300], Batch [22/43], Training Loss: 0.00000578
2024-11-06 14:07:14,545 - INFO - Epoch [137/300], Batch [23/43], Training Loss: 0.00001236
2024-11-06 14:07:14,549 - INFO - Epoch [137/300], Batch [24/43], Training Loss: 0.00000995
2024-11-06 14:07:14,552 - INFO - Epoch [137/300], Batch [25/43], Training Loss: 0.00001566
2024-11-06 14:07:14,555 - INFO - Epoch [137/300], Batch [26/43], Training Loss: 0.00000832
2024-11-06 14:07:14,560 - INFO - Epoch [137/300], Batch [27/43], Training Loss: 0.00001053
2024-11-06 14:07:14,564 - INFO - Epoch [137/300], Batch [28/43], Training Loss: 0.00002339
2024-11-06 14:07:14,567 - INFO - Epoch [137/300], Batch [29/43], Training Loss: 0.00001092
2024-11-06 14:07:14,571 - INFO - Epoch [137/300], Batch [30/43], Training Loss: 0.00002606
2024-11-06 14:07:14,574 - INFO - Epoch [137/300], Batch [31/43], Training Loss: 0.00000559
2024-11-06 14:07:14,578 - INFO - Epoch [137/300], Batch [32/43], Training Loss: 0.00001088
2024-11-06 14:07:14,582 - INFO - Epoch [137/300], Batch [33/43], Training Loss: 0.00000392
2024-11-06 14:07:14,585 - INFO - Epoch [137/300], Batch [34/43], Training Loss: 0.00002739
2024-11-06 14:07:14,588 - INFO - Epoch [137/300], Batch [35/43], Training Loss: 0.00001115
2024-11-06 14:07:14,592 - INFO - Epoch [137/300], Batch [36/43], Training Loss: 0.00000673
2024-11-06 14:07:14,596 - INFO - Epoch [137/300], Batch [37/43], Training Loss: 0.00000545
2024-11-06 14:07:14,599 - INFO - Epoch [137/300], Batch [38/43], Training Loss: 0.00001483
2024-11-06 14:07:14,603 - INFO - Epoch [137/300], Batch [39/43], Training Loss: 0.00000476
2024-11-06 14:07:14,607 - INFO - Epoch [137/300], Batch [40/43], Training Loss: 0.00001615
2024-11-06 14:07:14,611 - INFO - Epoch [137/300], Batch [41/43], Training Loss: 0.00001747
2024-11-06 14:07:14,616 - INFO - Epoch [137/300], Batch [42/43], Training Loss: 0.00000538
2024-11-06 14:07:14,621 - INFO - Epoch [137/300], Batch [43/43], Training Loss: 0.00003457
2024-11-06 14:07:14,634 - INFO - Epoch [137/300], Average Training Loss: 0.00001334, Validation Loss: 0.00002423
2024-11-06 14:07:14,639 - INFO - Epoch [138/300], Batch [1/43], Training Loss: 0.00001420
2024-11-06 14:07:14,643 - INFO - Epoch [138/300], Batch [2/43], Training Loss: 0.00001012
2024-11-06 14:07:14,647 - INFO - Epoch [138/300], Batch [3/43], Training Loss: 0.00001619
2024-11-06 14:07:14,651 - INFO - Epoch [138/300], Batch [4/43], Training Loss: 0.00001099
2024-11-06 14:07:14,655 - INFO - Epoch [138/300], Batch [5/43], Training Loss: 0.00002003
2024-11-06 14:07:14,658 - INFO - Epoch [138/300], Batch [6/43], Training Loss: 0.00000831
2024-11-06 14:07:14,662 - INFO - Epoch [138/300], Batch [7/43], Training Loss: 0.00002893
2024-11-06 14:07:14,665 - INFO - Epoch [138/300], Batch [8/43], Training Loss: 0.00000706
2024-11-06 14:07:14,668 - INFO - Epoch [138/300], Batch [9/43], Training Loss: 0.00001751
2024-11-06 14:07:14,672 - INFO - Epoch [138/300], Batch [10/43], Training Loss: 0.00001540
2024-11-06 14:07:14,676 - INFO - Epoch [138/300], Batch [11/43], Training Loss: 0.00002293
2024-11-06 14:07:14,680 - INFO - Epoch [138/300], Batch [12/43], Training Loss: 0.00002648
2024-11-06 14:07:14,683 - INFO - Epoch [138/300], Batch [13/43], Training Loss: 0.00000643
2024-11-06 14:07:14,686 - INFO - Epoch [138/300], Batch [14/43], Training Loss: 0.00000708
2024-11-06 14:07:14,690 - INFO - Epoch [138/300], Batch [15/43], Training Loss: 0.00001468
2024-11-06 14:07:14,694 - INFO - Epoch [138/300], Batch [16/43], Training Loss: 0.00001955
2024-11-06 14:07:14,698 - INFO - Epoch [138/300], Batch [17/43], Training Loss: 0.00001360
2024-11-06 14:07:14,702 - INFO - Epoch [138/300], Batch [18/43], Training Loss: 0.00000560
2024-11-06 14:07:14,706 - INFO - Epoch [138/300], Batch [19/43], Training Loss: 0.00001243
2024-11-06 14:07:14,709 - INFO - Epoch [138/300], Batch [20/43], Training Loss: 0.00000931
2024-11-06 14:07:14,713 - INFO - Epoch [138/300], Batch [21/43], Training Loss: 0.00000791
2024-11-06 14:07:14,717 - INFO - Epoch [138/300], Batch [22/43], Training Loss: 0.00001693
2024-11-06 14:07:14,722 - INFO - Epoch [138/300], Batch [23/43], Training Loss: 0.00002560
2024-11-06 14:07:14,725 - INFO - Epoch [138/300], Batch [24/43], Training Loss: 0.00000832
2024-11-06 14:07:14,729 - INFO - Epoch [138/300], Batch [25/43], Training Loss: 0.00001386
2024-11-06 14:07:14,732 - INFO - Epoch [138/300], Batch [26/43], Training Loss: 0.00003149
2024-11-06 14:07:14,735 - INFO - Epoch [138/300], Batch [27/43], Training Loss: 0.00003408
2024-11-06 14:07:14,739 - INFO - Epoch [138/300], Batch [28/43], Training Loss: 0.00001847
2024-11-06 14:07:14,742 - INFO - Epoch [138/300], Batch [29/43], Training Loss: 0.00002335
2024-11-06 14:07:14,746 - INFO - Epoch [138/300], Batch [30/43], Training Loss: 0.00006355
2024-11-06 14:07:14,749 - INFO - Epoch [138/300], Batch [31/43], Training Loss: 0.00002488
2024-11-06 14:07:14,753 - INFO - Epoch [138/300], Batch [32/43], Training Loss: 0.00002549
2024-11-06 14:07:14,757 - INFO - Epoch [138/300], Batch [33/43], Training Loss: 0.00003928
2024-11-06 14:07:14,761 - INFO - Epoch [138/300], Batch [34/43], Training Loss: 0.00001368
2024-11-06 14:07:14,766 - INFO - Epoch [138/300], Batch [35/43], Training Loss: 0.00001783
2024-11-06 14:07:14,771 - INFO - Epoch [138/300], Batch [36/43], Training Loss: 0.00001032
2024-11-06 14:07:14,776 - INFO - Epoch [138/300], Batch [37/43], Training Loss: 0.00004660
2024-11-06 14:07:14,781 - INFO - Epoch [138/300], Batch [38/43], Training Loss: 0.00002312
2024-11-06 14:07:14,785 - INFO - Epoch [138/300], Batch [39/43], Training Loss: 0.00001156
2024-11-06 14:07:14,789 - INFO - Epoch [138/300], Batch [40/43], Training Loss: 0.00001565
2024-11-06 14:07:14,793 - INFO - Epoch [138/300], Batch [41/43], Training Loss: 0.00003588
2024-11-06 14:07:14,797 - INFO - Epoch [138/300], Batch [42/43], Training Loss: 0.00001459
2024-11-06 14:07:14,801 - INFO - Epoch [138/300], Batch [43/43], Training Loss: 0.00000816
2024-11-06 14:07:14,817 - INFO - Epoch [138/300], Average Training Loss: 0.00001901, Validation Loss: 0.00001855
2024-11-06 14:07:14,821 - INFO - Epoch [139/300], Batch [1/43], Training Loss: 0.00002743
2024-11-06 14:07:14,826 - INFO - Epoch [139/300], Batch [2/43], Training Loss: 0.00002136
2024-11-06 14:07:14,831 - INFO - Epoch [139/300], Batch [3/43], Training Loss: 0.00002228
2024-11-06 14:07:14,835 - INFO - Epoch [139/300], Batch [4/43], Training Loss: 0.00002590
2024-11-06 14:07:14,839 - INFO - Epoch [139/300], Batch [5/43], Training Loss: 0.00000843
2024-11-06 14:07:14,843 - INFO - Epoch [139/300], Batch [6/43], Training Loss: 0.00002304
2024-11-06 14:07:14,846 - INFO - Epoch [139/300], Batch [7/43], Training Loss: 0.00001455
2024-11-06 14:07:14,850 - INFO - Epoch [139/300], Batch [8/43], Training Loss: 0.00001165
2024-11-06 14:07:14,854 - INFO - Epoch [139/300], Batch [9/43], Training Loss: 0.00000575
2024-11-06 14:07:14,858 - INFO - Epoch [139/300], Batch [10/43], Training Loss: 0.00001301
2024-11-06 14:07:14,862 - INFO - Epoch [139/300], Batch [11/43], Training Loss: 0.00001707
2024-11-06 14:07:14,866 - INFO - Epoch [139/300], Batch [12/43], Training Loss: 0.00001323
2024-11-06 14:07:14,870 - INFO - Epoch [139/300], Batch [13/43], Training Loss: 0.00001775
2024-11-06 14:07:14,874 - INFO - Epoch [139/300], Batch [14/43], Training Loss: 0.00001415
2024-11-06 14:07:14,879 - INFO - Epoch [139/300], Batch [15/43], Training Loss: 0.00000925
2024-11-06 14:07:14,883 - INFO - Epoch [139/300], Batch [16/43], Training Loss: 0.00001349
2024-11-06 14:07:14,888 - INFO - Epoch [139/300], Batch [17/43], Training Loss: 0.00000669
2024-11-06 14:07:14,893 - INFO - Epoch [139/300], Batch [18/43], Training Loss: 0.00000691
2024-11-06 14:07:14,897 - INFO - Epoch [139/300], Batch [19/43], Training Loss: 0.00001287
2024-11-06 14:07:14,901 - INFO - Epoch [139/300], Batch [20/43], Training Loss: 0.00000526
2024-11-06 14:07:14,906 - INFO - Epoch [139/300], Batch [21/43], Training Loss: 0.00001774
2024-11-06 14:07:14,910 - INFO - Epoch [139/300], Batch [22/43], Training Loss: 0.00001969
2024-11-06 14:07:14,914 - INFO - Epoch [139/300], Batch [23/43], Training Loss: 0.00001309
2024-11-06 14:07:14,918 - INFO - Epoch [139/300], Batch [24/43], Training Loss: 0.00000681
2024-11-06 14:07:14,921 - INFO - Epoch [139/300], Batch [25/43], Training Loss: 0.00001337
2024-11-06 14:07:14,925 - INFO - Epoch [139/300], Batch [26/43], Training Loss: 0.00000428
2024-11-06 14:07:14,929 - INFO - Epoch [139/300], Batch [27/43], Training Loss: 0.00001209
2024-11-06 14:07:14,933 - INFO - Epoch [139/300], Batch [28/43], Training Loss: 0.00002206
2024-11-06 14:07:14,937 - INFO - Epoch [139/300], Batch [29/43], Training Loss: 0.00000840
2024-11-06 14:07:14,941 - INFO - Epoch [139/300], Batch [30/43], Training Loss: 0.00000880
2024-11-06 14:07:14,944 - INFO - Epoch [139/300], Batch [31/43], Training Loss: 0.00000651
2024-11-06 14:07:14,948 - INFO - Epoch [139/300], Batch [32/43], Training Loss: 0.00000365
2024-11-06 14:07:14,951 - INFO - Epoch [139/300], Batch [33/43], Training Loss: 0.00001427
2024-11-06 14:07:14,955 - INFO - Epoch [139/300], Batch [34/43], Training Loss: 0.00000418
2024-11-06 14:07:14,960 - INFO - Epoch [139/300], Batch [35/43], Training Loss: 0.00001070
2024-11-06 14:07:14,964 - INFO - Epoch [139/300], Batch [36/43], Training Loss: 0.00000742
2024-11-06 14:07:14,969 - INFO - Epoch [139/300], Batch [37/43], Training Loss: 0.00001715
2024-11-06 14:07:14,974 - INFO - Epoch [139/300], Batch [38/43], Training Loss: 0.00001316
2024-11-06 14:07:14,979 - INFO - Epoch [139/300], Batch [39/43], Training Loss: 0.00000493
2024-11-06 14:07:14,984 - INFO - Epoch [139/300], Batch [40/43], Training Loss: 0.00002487
2024-11-06 14:07:14,989 - INFO - Epoch [139/300], Batch [41/43], Training Loss: 0.00001489
2024-11-06 14:07:14,993 - INFO - Epoch [139/300], Batch [42/43], Training Loss: 0.00001408
2024-11-06 14:07:14,998 - INFO - Epoch [139/300], Batch [43/43], Training Loss: 0.00001411
2024-11-06 14:07:15,011 - INFO - Epoch [139/300], Average Training Loss: 0.00001317, Validation Loss: 0.00001549
2024-11-06 14:07:15,014 - INFO - Epoch [140/300], Batch [1/43], Training Loss: 0.00001014
2024-11-06 14:07:15,018 - INFO - Epoch [140/300], Batch [2/43], Training Loss: 0.00001472
2024-11-06 14:07:15,022 - INFO - Epoch [140/300], Batch [3/43], Training Loss: 0.00001168
2024-11-06 14:07:15,025 - INFO - Epoch [140/300], Batch [4/43], Training Loss: 0.00001940
2024-11-06 14:07:15,028 - INFO - Epoch [140/300], Batch [5/43], Training Loss: 0.00001473
2024-11-06 14:07:15,031 - INFO - Epoch [140/300], Batch [6/43], Training Loss: 0.00000766
2024-11-06 14:07:15,034 - INFO - Epoch [140/300], Batch [7/43], Training Loss: 0.00001407
2024-11-06 14:07:15,037 - INFO - Epoch [140/300], Batch [8/43], Training Loss: 0.00000555
2024-11-06 14:07:15,040 - INFO - Epoch [140/300], Batch [9/43], Training Loss: 0.00001536
2024-11-06 14:07:15,043 - INFO - Epoch [140/300], Batch [10/43], Training Loss: 0.00001108
2024-11-06 14:07:15,046 - INFO - Epoch [140/300], Batch [11/43], Training Loss: 0.00001189
2024-11-06 14:07:15,049 - INFO - Epoch [140/300], Batch [12/43], Training Loss: 0.00000656
2024-11-06 14:07:15,053 - INFO - Epoch [140/300], Batch [13/43], Training Loss: 0.00000624
2024-11-06 14:07:15,057 - INFO - Epoch [140/300], Batch [14/43], Training Loss: 0.00000989
2024-11-06 14:07:15,060 - INFO - Epoch [140/300], Batch [15/43], Training Loss: 0.00000847
2024-11-06 14:07:15,063 - INFO - Epoch [140/300], Batch [16/43], Training Loss: 0.00000881
2024-11-06 14:07:15,067 - INFO - Epoch [140/300], Batch [17/43], Training Loss: 0.00001064
2024-11-06 14:07:15,071 - INFO - Epoch [140/300], Batch [18/43], Training Loss: 0.00000403
2024-11-06 14:07:15,074 - INFO - Epoch [140/300], Batch [19/43], Training Loss: 0.00000566
2024-11-06 14:07:15,077 - INFO - Epoch [140/300], Batch [20/43], Training Loss: 0.00000764
2024-11-06 14:07:15,080 - INFO - Epoch [140/300], Batch [21/43], Training Loss: 0.00000718
2024-11-06 14:07:15,084 - INFO - Epoch [140/300], Batch [22/43], Training Loss: 0.00002167
2024-11-06 14:07:15,087 - INFO - Epoch [140/300], Batch [23/43], Training Loss: 0.00001173
2024-11-06 14:07:15,091 - INFO - Epoch [140/300], Batch [24/43], Training Loss: 0.00000747
2024-11-06 14:07:15,094 - INFO - Epoch [140/300], Batch [25/43], Training Loss: 0.00000452
2024-11-06 14:07:15,098 - INFO - Epoch [140/300], Batch [26/43], Training Loss: 0.00002581
2024-11-06 14:07:15,102 - INFO - Epoch [140/300], Batch [27/43], Training Loss: 0.00001739
2024-11-06 14:07:15,105 - INFO - Epoch [140/300], Batch [28/43], Training Loss: 0.00000651
2024-11-06 14:07:15,109 - INFO - Epoch [140/300], Batch [29/43], Training Loss: 0.00000233
2024-11-06 14:07:15,112 - INFO - Epoch [140/300], Batch [30/43], Training Loss: 0.00001693
2024-11-06 14:07:15,116 - INFO - Epoch [140/300], Batch [31/43], Training Loss: 0.00002745
2024-11-06 14:07:15,119 - INFO - Epoch [140/300], Batch [32/43], Training Loss: 0.00001659
2024-11-06 14:07:15,122 - INFO - Epoch [140/300], Batch [33/43], Training Loss: 0.00003258
2024-11-06 14:07:15,125 - INFO - Epoch [140/300], Batch [34/43], Training Loss: 0.00000455
2024-11-06 14:07:15,128 - INFO - Epoch [140/300], Batch [35/43], Training Loss: 0.00000883
2024-11-06 14:07:15,132 - INFO - Epoch [140/300], Batch [36/43], Training Loss: 0.00000834
2024-11-06 14:07:15,136 - INFO - Epoch [140/300], Batch [37/43], Training Loss: 0.00001682
2024-11-06 14:07:15,140 - INFO - Epoch [140/300], Batch [38/43], Training Loss: 0.00001595
2024-11-06 14:07:15,144 - INFO - Epoch [140/300], Batch [39/43], Training Loss: 0.00000474
2024-11-06 14:07:15,148 - INFO - Epoch [140/300], Batch [40/43], Training Loss: 0.00001636
2024-11-06 14:07:15,152 - INFO - Epoch [140/300], Batch [41/43], Training Loss: 0.00001400
2024-11-06 14:07:15,156 - INFO - Epoch [140/300], Batch [42/43], Training Loss: 0.00002252
2024-11-06 14:07:15,161 - INFO - Epoch [140/300], Batch [43/43], Training Loss: 0.00002507
2024-11-06 14:07:15,172 - INFO - Epoch [140/300], Average Training Loss: 0.00001255, Validation Loss: 0.00001619
2024-11-06 14:07:15,176 - INFO - Epoch [141/300], Batch [1/43], Training Loss: 0.00001584
2024-11-06 14:07:15,179 - INFO - Epoch [141/300], Batch [2/43], Training Loss: 0.00000924
2024-11-06 14:07:15,183 - INFO - Epoch [141/300], Batch [3/43], Training Loss: 0.00001018
2024-11-06 14:07:15,186 - INFO - Epoch [141/300], Batch [4/43], Training Loss: 0.00002249
2024-11-06 14:07:15,189 - INFO - Epoch [141/300], Batch [5/43], Training Loss: 0.00001128
2024-11-06 14:07:15,193 - INFO - Epoch [141/300], Batch [6/43], Training Loss: 0.00000838
2024-11-06 14:07:15,196 - INFO - Epoch [141/300], Batch [7/43], Training Loss: 0.00001216
2024-11-06 14:07:15,200 - INFO - Epoch [141/300], Batch [8/43], Training Loss: 0.00000545
2024-11-06 14:07:15,203 - INFO - Epoch [141/300], Batch [9/43], Training Loss: 0.00000664
2024-11-06 14:07:15,207 - INFO - Epoch [141/300], Batch [10/43], Training Loss: 0.00001456
2024-11-06 14:07:15,211 - INFO - Epoch [141/300], Batch [11/43], Training Loss: 0.00001022
2024-11-06 14:07:15,214 - INFO - Epoch [141/300], Batch [12/43], Training Loss: 0.00001221
2024-11-06 14:07:15,217 - INFO - Epoch [141/300], Batch [13/43], Training Loss: 0.00001231
2024-11-06 14:07:15,220 - INFO - Epoch [141/300], Batch [14/43], Training Loss: 0.00000999
2024-11-06 14:07:15,223 - INFO - Epoch [141/300], Batch [15/43], Training Loss: 0.00000489
2024-11-06 14:07:15,226 - INFO - Epoch [141/300], Batch [16/43], Training Loss: 0.00001040
2024-11-06 14:07:15,229 - INFO - Epoch [141/300], Batch [17/43], Training Loss: 0.00001642
2024-11-06 14:07:15,232 - INFO - Epoch [141/300], Batch [18/43], Training Loss: 0.00002775
2024-11-06 14:07:15,235 - INFO - Epoch [141/300], Batch [19/43], Training Loss: 0.00000827
2024-11-06 14:07:15,238 - INFO - Epoch [141/300], Batch [20/43], Training Loss: 0.00001003
2024-11-06 14:07:15,242 - INFO - Epoch [141/300], Batch [21/43], Training Loss: 0.00002959
2024-11-06 14:07:15,245 - INFO - Epoch [141/300], Batch [22/43], Training Loss: 0.00003101
2024-11-06 14:07:15,248 - INFO - Epoch [141/300], Batch [23/43], Training Loss: 0.00000775
2024-11-06 14:07:15,251 - INFO - Epoch [141/300], Batch [24/43], Training Loss: 0.00002040
2024-11-06 14:07:15,253 - INFO - Epoch [141/300], Batch [25/43], Training Loss: 0.00001054
2024-11-06 14:07:15,256 - INFO - Epoch [141/300], Batch [26/43], Training Loss: 0.00001110
2024-11-06 14:07:15,259 - INFO - Epoch [141/300], Batch [27/43], Training Loss: 0.00002345
2024-11-06 14:07:15,262 - INFO - Epoch [141/300], Batch [28/43], Training Loss: 0.00002866
2024-11-06 14:07:15,265 - INFO - Epoch [141/300], Batch [29/43], Training Loss: 0.00001884
2024-11-06 14:07:15,268 - INFO - Epoch [141/300], Batch [30/43], Training Loss: 0.00001151
2024-11-06 14:07:15,271 - INFO - Epoch [141/300], Batch [31/43], Training Loss: 0.00002367
2024-11-06 14:07:15,275 - INFO - Epoch [141/300], Batch [32/43], Training Loss: 0.00001758
2024-11-06 14:07:15,278 - INFO - Epoch [141/300], Batch [33/43], Training Loss: 0.00000652
2024-11-06 14:07:15,281 - INFO - Epoch [141/300], Batch [34/43], Training Loss: 0.00001041
2024-11-06 14:07:15,285 - INFO - Epoch [141/300], Batch [35/43], Training Loss: 0.00000601
2024-11-06 14:07:15,289 - INFO - Epoch [141/300], Batch [36/43], Training Loss: 0.00000996
2024-11-06 14:07:15,293 - INFO - Epoch [141/300], Batch [37/43], Training Loss: 0.00002160
2024-11-06 14:07:15,297 - INFO - Epoch [141/300], Batch [38/43], Training Loss: 0.00000829
2024-11-06 14:07:15,301 - INFO - Epoch [141/300], Batch [39/43], Training Loss: 0.00001261
2024-11-06 14:07:15,306 - INFO - Epoch [141/300], Batch [40/43], Training Loss: 0.00001977
2024-11-06 14:07:15,309 - INFO - Epoch [141/300], Batch [41/43], Training Loss: 0.00001225
2024-11-06 14:07:15,313 - INFO - Epoch [141/300], Batch [42/43], Training Loss: 0.00001408
2024-11-06 14:07:15,316 - INFO - Epoch [141/300], Batch [43/43], Training Loss: 0.00002485
2024-11-06 14:07:15,326 - INFO - Epoch [141/300], Average Training Loss: 0.00001440, Validation Loss: 0.00002106
2024-11-06 14:07:15,329 - INFO - Epoch [142/300], Batch [1/43], Training Loss: 0.00001197
2024-11-06 14:07:15,333 - INFO - Epoch [142/300], Batch [2/43], Training Loss: 0.00001492
2024-11-06 14:07:15,336 - INFO - Epoch [142/300], Batch [3/43], Training Loss: 0.00002280
2024-11-06 14:07:15,341 - INFO - Epoch [142/300], Batch [4/43], Training Loss: 0.00001007
2024-11-06 14:07:15,345 - INFO - Epoch [142/300], Batch [5/43], Training Loss: 0.00001099
2024-11-06 14:07:15,349 - INFO - Epoch [142/300], Batch [6/43], Training Loss: 0.00001478
2024-11-06 14:07:15,353 - INFO - Epoch [142/300], Batch [7/43], Training Loss: 0.00000699
2024-11-06 14:07:15,355 - INFO - Epoch [142/300], Batch [8/43], Training Loss: 0.00000398
2024-11-06 14:07:15,358 - INFO - Epoch [142/300], Batch [9/43], Training Loss: 0.00002219
2024-11-06 14:07:15,362 - INFO - Epoch [142/300], Batch [10/43], Training Loss: 0.00001573
2024-11-06 14:07:15,364 - INFO - Epoch [142/300], Batch [11/43], Training Loss: 0.00001124
2024-11-06 14:07:15,369 - INFO - Epoch [142/300], Batch [12/43], Training Loss: 0.00000842
2024-11-06 14:07:15,372 - INFO - Epoch [142/300], Batch [13/43], Training Loss: 0.00001023
2024-11-06 14:07:15,374 - INFO - Epoch [142/300], Batch [14/43], Training Loss: 0.00000929
2024-11-06 14:07:15,377 - INFO - Epoch [142/300], Batch [15/43], Training Loss: 0.00000914
2024-11-06 14:07:15,380 - INFO - Epoch [142/300], Batch [16/43], Training Loss: 0.00000747
2024-11-06 14:07:15,383 - INFO - Epoch [142/300], Batch [17/43], Training Loss: 0.00001396
2024-11-06 14:07:15,386 - INFO - Epoch [142/300], Batch [18/43], Training Loss: 0.00000353
2024-11-06 14:07:15,389 - INFO - Epoch [142/300], Batch [19/43], Training Loss: 0.00001057
2024-11-06 14:07:15,392 - INFO - Epoch [142/300], Batch [20/43], Training Loss: 0.00001024
2024-11-06 14:07:15,396 - INFO - Epoch [142/300], Batch [21/43], Training Loss: 0.00000895
2024-11-06 14:07:15,400 - INFO - Epoch [142/300], Batch [22/43], Training Loss: 0.00001840
2024-11-06 14:07:15,403 - INFO - Epoch [142/300], Batch [23/43], Training Loss: 0.00001682
2024-11-06 14:07:15,406 - INFO - Epoch [142/300], Batch [24/43], Training Loss: 0.00000579
2024-11-06 14:07:15,410 - INFO - Epoch [142/300], Batch [25/43], Training Loss: 0.00001046
2024-11-06 14:07:15,413 - INFO - Epoch [142/300], Batch [26/43], Training Loss: 0.00000764
2024-11-06 14:07:15,416 - INFO - Epoch [142/300], Batch [27/43], Training Loss: 0.00000676
2024-11-06 14:07:15,420 - INFO - Epoch [142/300], Batch [28/43], Training Loss: 0.00000788
2024-11-06 14:07:15,424 - INFO - Epoch [142/300], Batch [29/43], Training Loss: 0.00002114
2024-11-06 14:07:15,428 - INFO - Epoch [142/300], Batch [30/43], Training Loss: 0.00001202
2024-11-06 14:07:15,431 - INFO - Epoch [142/300], Batch [31/43], Training Loss: 0.00000843
2024-11-06 14:07:15,435 - INFO - Epoch [142/300], Batch [32/43], Training Loss: 0.00001925
2024-11-06 14:07:15,440 - INFO - Epoch [142/300], Batch [33/43], Training Loss: 0.00002303
2024-11-06 14:07:15,444 - INFO - Epoch [142/300], Batch [34/43], Training Loss: 0.00001379
2024-11-06 14:07:15,449 - INFO - Epoch [142/300], Batch [35/43], Training Loss: 0.00001718
2024-11-06 14:07:15,452 - INFO - Epoch [142/300], Batch [36/43], Training Loss: 0.00000932
2024-11-06 14:07:15,456 - INFO - Epoch [142/300], Batch [37/43], Training Loss: 0.00002199
2024-11-06 14:07:15,460 - INFO - Epoch [142/300], Batch [38/43], Training Loss: 0.00000626
2024-11-06 14:07:15,465 - INFO - Epoch [142/300], Batch [39/43], Training Loss: 0.00001107
2024-11-06 14:07:15,469 - INFO - Epoch [142/300], Batch [40/43], Training Loss: 0.00001799
2024-11-06 14:07:15,472 - INFO - Epoch [142/300], Batch [41/43], Training Loss: 0.00001134
2024-11-06 14:07:15,476 - INFO - Epoch [142/300], Batch [42/43], Training Loss: 0.00002117
2024-11-06 14:07:15,481 - INFO - Epoch [142/300], Batch [43/43], Training Loss: 0.00001565
2024-11-06 14:07:15,492 - INFO - Epoch [142/300], Average Training Loss: 0.00001258, Validation Loss: 0.00001800
2024-11-06 14:07:15,496 - INFO - Epoch [143/300], Batch [1/43], Training Loss: 0.00002294
2024-11-06 14:07:15,500 - INFO - Epoch [143/300], Batch [2/43], Training Loss: 0.00001847
2024-11-06 14:07:15,503 - INFO - Epoch [143/300], Batch [3/43], Training Loss: 0.00001107
2024-11-06 14:07:15,507 - INFO - Epoch [143/300], Batch [4/43], Training Loss: 0.00000388
2024-11-06 14:07:15,510 - INFO - Epoch [143/300], Batch [5/43], Training Loss: 0.00001171
2024-11-06 14:07:15,514 - INFO - Epoch [143/300], Batch [6/43], Training Loss: 0.00001935
2024-11-06 14:07:15,518 - INFO - Epoch [143/300], Batch [7/43], Training Loss: 0.00001364
2024-11-06 14:07:15,521 - INFO - Epoch [143/300], Batch [8/43], Training Loss: 0.00001564
2024-11-06 14:07:15,525 - INFO - Epoch [143/300], Batch [9/43], Training Loss: 0.00002018
2024-11-06 14:07:15,529 - INFO - Epoch [143/300], Batch [10/43], Training Loss: 0.00000498
2024-11-06 14:07:15,533 - INFO - Epoch [143/300], Batch [11/43], Training Loss: 0.00001574
2024-11-06 14:07:15,538 - INFO - Epoch [143/300], Batch [12/43], Training Loss: 0.00001914
2024-11-06 14:07:15,541 - INFO - Epoch [143/300], Batch [13/43], Training Loss: 0.00001057
2024-11-06 14:07:15,545 - INFO - Epoch [143/300], Batch [14/43], Training Loss: 0.00002827
2024-11-06 14:07:15,549 - INFO - Epoch [143/300], Batch [15/43], Training Loss: 0.00001715
2024-11-06 14:07:15,553 - INFO - Epoch [143/300], Batch [16/43], Training Loss: 0.00002212
2024-11-06 14:07:15,556 - INFO - Epoch [143/300], Batch [17/43], Training Loss: 0.00001936
2024-11-06 14:07:15,560 - INFO - Epoch [143/300], Batch [18/43], Training Loss: 0.00000574
2024-11-06 14:07:15,563 - INFO - Epoch [143/300], Batch [19/43], Training Loss: 0.00002206
2024-11-06 14:07:15,567 - INFO - Epoch [143/300], Batch [20/43], Training Loss: 0.00001965
2024-11-06 14:07:15,572 - INFO - Epoch [143/300], Batch [21/43], Training Loss: 0.00001955
2024-11-06 14:07:15,577 - INFO - Epoch [143/300], Batch [22/43], Training Loss: 0.00001262
2024-11-06 14:07:15,585 - INFO - Epoch [143/300], Batch [23/43], Training Loss: 0.00001088
2024-11-06 14:07:15,591 - INFO - Epoch [143/300], Batch [24/43], Training Loss: 0.00002609
2024-11-06 14:07:15,595 - INFO - Epoch [143/300], Batch [25/43], Training Loss: 0.00002305
2024-11-06 14:07:15,600 - INFO - Epoch [143/300], Batch [26/43], Training Loss: 0.00000770
2024-11-06 14:07:15,604 - INFO - Epoch [143/300], Batch [27/43], Training Loss: 0.00001163
2024-11-06 14:07:15,608 - INFO - Epoch [143/300], Batch [28/43], Training Loss: 0.00001595
2024-11-06 14:07:15,613 - INFO - Epoch [143/300], Batch [29/43], Training Loss: 0.00001383
2024-11-06 14:07:15,617 - INFO - Epoch [143/300], Batch [30/43], Training Loss: 0.00001075
2024-11-06 14:07:15,621 - INFO - Epoch [143/300], Batch [31/43], Training Loss: 0.00001208
2024-11-06 14:07:15,625 - INFO - Epoch [143/300], Batch [32/43], Training Loss: 0.00001738
2024-11-06 14:07:15,629 - INFO - Epoch [143/300], Batch [33/43], Training Loss: 0.00001270
2024-11-06 14:07:15,634 - INFO - Epoch [143/300], Batch [34/43], Training Loss: 0.00002189
2024-11-06 14:07:15,639 - INFO - Epoch [143/300], Batch [35/43], Training Loss: 0.00001369
2024-11-06 14:07:15,643 - INFO - Epoch [143/300], Batch [36/43], Training Loss: 0.00001091
2024-11-06 14:07:15,647 - INFO - Epoch [143/300], Batch [37/43], Training Loss: 0.00000714
2024-11-06 14:07:15,650 - INFO - Epoch [143/300], Batch [38/43], Training Loss: 0.00002626
2024-11-06 14:07:15,655 - INFO - Epoch [143/300], Batch [39/43], Training Loss: 0.00001070
2024-11-06 14:07:15,659 - INFO - Epoch [143/300], Batch [40/43], Training Loss: 0.00001088
2024-11-06 14:07:15,663 - INFO - Epoch [143/300], Batch [41/43], Training Loss: 0.00001242
2024-11-06 14:07:15,668 - INFO - Epoch [143/300], Batch [42/43], Training Loss: 0.00000629
2024-11-06 14:07:15,673 - INFO - Epoch [143/300], Batch [43/43], Training Loss: 0.00000655
2024-11-06 14:07:15,685 - INFO - Epoch [143/300], Average Training Loss: 0.00001494, Validation Loss: 0.00001610
2024-11-06 14:07:15,689 - INFO - Epoch [144/300], Batch [1/43], Training Loss: 0.00003045
2024-11-06 14:07:15,693 - INFO - Epoch [144/300], Batch [2/43], Training Loss: 0.00000435
2024-11-06 14:07:15,697 - INFO - Epoch [144/300], Batch [3/43], Training Loss: 0.00001645
2024-11-06 14:07:15,701 - INFO - Epoch [144/300], Batch [4/43], Training Loss: 0.00001111
2024-11-06 14:07:15,706 - INFO - Epoch [144/300], Batch [5/43], Training Loss: 0.00000337
2024-11-06 14:07:15,709 - INFO - Epoch [144/300], Batch [6/43], Training Loss: 0.00000368
2024-11-06 14:07:15,713 - INFO - Epoch [144/300], Batch [7/43], Training Loss: 0.00000894
2024-11-06 14:07:15,717 - INFO - Epoch [144/300], Batch [8/43], Training Loss: 0.00000432
2024-11-06 14:07:15,721 - INFO - Epoch [144/300], Batch [9/43], Training Loss: 0.00000902
2024-11-06 14:07:15,726 - INFO - Epoch [144/300], Batch [10/43], Training Loss: 0.00000848
2024-11-06 14:07:15,730 - INFO - Epoch [144/300], Batch [11/43], Training Loss: 0.00000898
2024-11-06 14:07:15,734 - INFO - Epoch [144/300], Batch [12/43], Training Loss: 0.00000969
2024-11-06 14:07:15,737 - INFO - Epoch [144/300], Batch [13/43], Training Loss: 0.00002614
2024-11-06 14:07:15,741 - INFO - Epoch [144/300], Batch [14/43], Training Loss: 0.00000646
2024-11-06 14:07:15,746 - INFO - Epoch [144/300], Batch [15/43], Training Loss: 0.00002231
2024-11-06 14:07:15,750 - INFO - Epoch [144/300], Batch [16/43], Training Loss: 0.00002456
2024-11-06 14:07:15,754 - INFO - Epoch [144/300], Batch [17/43], Training Loss: 0.00000817
2024-11-06 14:07:15,757 - INFO - Epoch [144/300], Batch [18/43], Training Loss: 0.00001129
2024-11-06 14:07:15,761 - INFO - Epoch [144/300], Batch [19/43], Training Loss: 0.00000891
2024-11-06 14:07:15,765 - INFO - Epoch [144/300], Batch [20/43], Training Loss: 0.00001022
2024-11-06 14:07:15,769 - INFO - Epoch [144/300], Batch [21/43], Training Loss: 0.00000685
2024-11-06 14:07:15,773 - INFO - Epoch [144/300], Batch [22/43], Training Loss: 0.00000680
2024-11-06 14:07:15,777 - INFO - Epoch [144/300], Batch [23/43], Training Loss: 0.00000855
2024-11-06 14:07:15,781 - INFO - Epoch [144/300], Batch [24/43], Training Loss: 0.00002002
2024-11-06 14:07:15,785 - INFO - Epoch [144/300], Batch [25/43], Training Loss: 0.00000865
2024-11-06 14:07:15,789 - INFO - Epoch [144/300], Batch [26/43], Training Loss: 0.00000716
2024-11-06 14:07:15,793 - INFO - Epoch [144/300], Batch [27/43], Training Loss: 0.00000958
2024-11-06 14:07:15,796 - INFO - Epoch [144/300], Batch [28/43], Training Loss: 0.00002165
2024-11-06 14:07:15,800 - INFO - Epoch [144/300], Batch [29/43], Training Loss: 0.00001763
2024-11-06 14:07:15,803 - INFO - Epoch [144/300], Batch [30/43], Training Loss: 0.00001012
2024-11-06 14:07:15,807 - INFO - Epoch [144/300], Batch [31/43], Training Loss: 0.00001420
2024-11-06 14:07:15,811 - INFO - Epoch [144/300], Batch [32/43], Training Loss: 0.00001504
2024-11-06 14:07:15,815 - INFO - Epoch [144/300], Batch [33/43], Training Loss: 0.00002762
2024-11-06 14:07:15,819 - INFO - Epoch [144/300], Batch [34/43], Training Loss: 0.00001448
2024-11-06 14:07:15,823 - INFO - Epoch [144/300], Batch [35/43], Training Loss: 0.00001837
2024-11-06 14:07:15,828 - INFO - Epoch [144/300], Batch [36/43], Training Loss: 0.00001026
2024-11-06 14:07:15,833 - INFO - Epoch [144/300], Batch [37/43], Training Loss: 0.00002405
2024-11-06 14:07:15,838 - INFO - Epoch [144/300], Batch [38/43], Training Loss: 0.00001013
2024-11-06 14:07:15,843 - INFO - Epoch [144/300], Batch [39/43], Training Loss: 0.00001455
2024-11-06 14:07:15,848 - INFO - Epoch [144/300], Batch [40/43], Training Loss: 0.00000856
2024-11-06 14:07:15,853 - INFO - Epoch [144/300], Batch [41/43], Training Loss: 0.00001178
2024-11-06 14:07:15,857 - INFO - Epoch [144/300], Batch [42/43], Training Loss: 0.00000795
2024-11-06 14:07:15,862 - INFO - Epoch [144/300], Batch [43/43], Training Loss: 0.00000471
2024-11-06 14:07:15,875 - INFO - Epoch [144/300], Average Training Loss: 0.00001246, Validation Loss: 0.00001491
2024-11-06 14:07:15,879 - INFO - Epoch [145/300], Batch [1/43], Training Loss: 0.00001110
2024-11-06 14:07:15,884 - INFO - Epoch [145/300], Batch [2/43], Training Loss: 0.00001613
2024-11-06 14:07:15,890 - INFO - Epoch [145/300], Batch [3/43], Training Loss: 0.00000346
2024-11-06 14:07:15,895 - INFO - Epoch [145/300], Batch [4/43], Training Loss: 0.00000775
2024-11-06 14:07:15,900 - INFO - Epoch [145/300], Batch [5/43], Training Loss: 0.00002134
2024-11-06 14:07:15,905 - INFO - Epoch [145/300], Batch [6/43], Training Loss: 0.00000668
2024-11-06 14:07:15,910 - INFO - Epoch [145/300], Batch [7/43], Training Loss: 0.00002428
2024-11-06 14:07:15,914 - INFO - Epoch [145/300], Batch [8/43], Training Loss: 0.00001279
2024-11-06 14:07:15,918 - INFO - Epoch [145/300], Batch [9/43], Training Loss: 0.00000707
2024-11-06 14:07:15,923 - INFO - Epoch [145/300], Batch [10/43], Training Loss: 0.00001112
2024-11-06 14:07:15,926 - INFO - Epoch [145/300], Batch [11/43], Training Loss: 0.00000369
2024-11-06 14:07:15,930 - INFO - Epoch [145/300], Batch [12/43], Training Loss: 0.00000519
2024-11-06 14:07:15,934 - INFO - Epoch [145/300], Batch [13/43], Training Loss: 0.00001406
2024-11-06 14:07:15,938 - INFO - Epoch [145/300], Batch [14/43], Training Loss: 0.00000752
2024-11-06 14:07:15,941 - INFO - Epoch [145/300], Batch [15/43], Training Loss: 0.00000766
2024-11-06 14:07:15,945 - INFO - Epoch [145/300], Batch [16/43], Training Loss: 0.00000691
2024-11-06 14:07:15,948 - INFO - Epoch [145/300], Batch [17/43], Training Loss: 0.00000748
2024-11-06 14:07:15,952 - INFO - Epoch [145/300], Batch [18/43], Training Loss: 0.00000383
2024-11-06 14:07:15,955 - INFO - Epoch [145/300], Batch [19/43], Training Loss: 0.00001822
2024-11-06 14:07:15,959 - INFO - Epoch [145/300], Batch [20/43], Training Loss: 0.00000882
2024-11-06 14:07:15,963 - INFO - Epoch [145/300], Batch [21/43], Training Loss: 0.00001372
2024-11-06 14:07:15,967 - INFO - Epoch [145/300], Batch [22/43], Training Loss: 0.00001698
2024-11-06 14:07:15,971 - INFO - Epoch [145/300], Batch [23/43], Training Loss: 0.00001436
2024-11-06 14:07:15,976 - INFO - Epoch [145/300], Batch [24/43], Training Loss: 0.00001133
2024-11-06 14:07:15,980 - INFO - Epoch [145/300], Batch [25/43], Training Loss: 0.00000808
2024-11-06 14:07:15,984 - INFO - Epoch [145/300], Batch [26/43], Training Loss: 0.00002809
2024-11-06 14:07:15,988 - INFO - Epoch [145/300], Batch [27/43], Training Loss: 0.00001561
2024-11-06 14:07:15,992 - INFO - Epoch [145/300], Batch [28/43], Training Loss: 0.00000977
2024-11-06 14:07:15,996 - INFO - Epoch [145/300], Batch [29/43], Training Loss: 0.00002727
2024-11-06 14:07:16,000 - INFO - Epoch [145/300], Batch [30/43], Training Loss: 0.00000671
2024-11-06 14:07:16,003 - INFO - Epoch [145/300], Batch [31/43], Training Loss: 0.00001502
2024-11-06 14:07:16,007 - INFO - Epoch [145/300], Batch [32/43], Training Loss: 0.00001864
2024-11-06 14:07:16,010 - INFO - Epoch [145/300], Batch [33/43], Training Loss: 0.00001796
2024-11-06 14:07:16,014 - INFO - Epoch [145/300], Batch [34/43], Training Loss: 0.00000544
2024-11-06 14:07:16,017 - INFO - Epoch [145/300], Batch [35/43], Training Loss: 0.00001263
2024-11-06 14:07:16,021 - INFO - Epoch [145/300], Batch [36/43], Training Loss: 0.00001715
2024-11-06 14:07:16,025 - INFO - Epoch [145/300], Batch [37/43], Training Loss: 0.00000759
2024-11-06 14:07:16,029 - INFO - Epoch [145/300], Batch [38/43], Training Loss: 0.00000667
2024-11-06 14:07:16,034 - INFO - Epoch [145/300], Batch [39/43], Training Loss: 0.00001668
2024-11-06 14:07:16,038 - INFO - Epoch [145/300], Batch [40/43], Training Loss: 0.00000626
2024-11-06 14:07:16,042 - INFO - Epoch [145/300], Batch [41/43], Training Loss: 0.00000826
2024-11-06 14:07:16,046 - INFO - Epoch [145/300], Batch [42/43], Training Loss: 0.00001129
2024-11-06 14:07:16,052 - INFO - Epoch [145/300], Batch [43/43], Training Loss: 0.00001662
2024-11-06 14:07:16,064 - INFO - Epoch [145/300], Average Training Loss: 0.00001203, Validation Loss: 0.00001449
2024-11-06 14:07:16,069 - INFO - Epoch [146/300], Batch [1/43], Training Loss: 0.00002124
2024-11-06 14:07:16,074 - INFO - Epoch [146/300], Batch [2/43], Training Loss: 0.00000408
2024-11-06 14:07:16,078 - INFO - Epoch [146/300], Batch [3/43], Training Loss: 0.00000991
2024-11-06 14:07:16,083 - INFO - Epoch [146/300], Batch [4/43], Training Loss: 0.00001434
2024-11-06 14:07:16,087 - INFO - Epoch [146/300], Batch [5/43], Training Loss: 0.00001733
2024-11-06 14:07:16,091 - INFO - Epoch [146/300], Batch [6/43], Training Loss: 0.00001437
2024-11-06 14:07:16,096 - INFO - Epoch [146/300], Batch [7/43], Training Loss: 0.00001718
2024-11-06 14:07:16,101 - INFO - Epoch [146/300], Batch [8/43], Training Loss: 0.00000656
2024-11-06 14:07:16,105 - INFO - Epoch [146/300], Batch [9/43], Training Loss: 0.00000347
2024-11-06 14:07:16,109 - INFO - Epoch [146/300], Batch [10/43], Training Loss: 0.00001950
2024-11-06 14:07:16,114 - INFO - Epoch [146/300], Batch [11/43], Training Loss: 0.00000557
2024-11-06 14:07:16,118 - INFO - Epoch [146/300], Batch [12/43], Training Loss: 0.00000840
2024-11-06 14:07:16,122 - INFO - Epoch [146/300], Batch [13/43], Training Loss: 0.00000587
2024-11-06 14:07:16,126 - INFO - Epoch [146/300], Batch [14/43], Training Loss: 0.00001044
2024-11-06 14:07:16,131 - INFO - Epoch [146/300], Batch [15/43], Training Loss: 0.00001914
2024-11-06 14:07:16,135 - INFO - Epoch [146/300], Batch [16/43], Training Loss: 0.00001360
2024-11-06 14:07:16,139 - INFO - Epoch [146/300], Batch [17/43], Training Loss: 0.00000982
2024-11-06 14:07:16,142 - INFO - Epoch [146/300], Batch [18/43], Training Loss: 0.00001721
2024-11-06 14:07:16,147 - INFO - Epoch [146/300], Batch [19/43], Training Loss: 0.00000925
2024-11-06 14:07:16,151 - INFO - Epoch [146/300], Batch [20/43], Training Loss: 0.00001718
2024-11-06 14:07:16,154 - INFO - Epoch [146/300], Batch [21/43], Training Loss: 0.00002019
2024-11-06 14:07:16,157 - INFO - Epoch [146/300], Batch [22/43], Training Loss: 0.00001297
2024-11-06 14:07:16,162 - INFO - Epoch [146/300], Batch [23/43], Training Loss: 0.00001552
2024-11-06 14:07:16,166 - INFO - Epoch [146/300], Batch [24/43], Training Loss: 0.00000882
2024-11-06 14:07:16,170 - INFO - Epoch [146/300], Batch [25/43], Training Loss: 0.00000967
2024-11-06 14:07:16,175 - INFO - Epoch [146/300], Batch [26/43], Training Loss: 0.00000747
2024-11-06 14:07:16,178 - INFO - Epoch [146/300], Batch [27/43], Training Loss: 0.00000817
2024-11-06 14:07:16,182 - INFO - Epoch [146/300], Batch [28/43], Training Loss: 0.00001564
2024-11-06 14:07:16,186 - INFO - Epoch [146/300], Batch [29/43], Training Loss: 0.00001028
2024-11-06 14:07:16,189 - INFO - Epoch [146/300], Batch [30/43], Training Loss: 0.00001060
2024-11-06 14:07:16,193 - INFO - Epoch [146/300], Batch [31/43], Training Loss: 0.00001224
2024-11-06 14:07:16,197 - INFO - Epoch [146/300], Batch [32/43], Training Loss: 0.00000735
2024-11-06 14:07:16,201 - INFO - Epoch [146/300], Batch [33/43], Training Loss: 0.00001083
2024-11-06 14:07:16,206 - INFO - Epoch [146/300], Batch [34/43], Training Loss: 0.00002739
2024-11-06 14:07:16,210 - INFO - Epoch [146/300], Batch [35/43], Training Loss: 0.00001349
2024-11-06 14:07:16,215 - INFO - Epoch [146/300], Batch [36/43], Training Loss: 0.00000580
2024-11-06 14:07:16,219 - INFO - Epoch [146/300], Batch [37/43], Training Loss: 0.00002388
2024-11-06 14:07:16,223 - INFO - Epoch [146/300], Batch [38/43], Training Loss: 0.00001576
2024-11-06 14:07:16,228 - INFO - Epoch [146/300], Batch [39/43], Training Loss: 0.00002972
2024-11-06 14:07:16,232 - INFO - Epoch [146/300], Batch [40/43], Training Loss: 0.00001654
2024-11-06 14:07:16,237 - INFO - Epoch [146/300], Batch [41/43], Training Loss: 0.00000528
2024-11-06 14:07:16,242 - INFO - Epoch [146/300], Batch [42/43], Training Loss: 0.00001679
2024-11-06 14:07:16,247 - INFO - Epoch [146/300], Batch [43/43], Training Loss: 0.00000536
2024-11-06 14:07:16,260 - INFO - Epoch [146/300], Average Training Loss: 0.00001289, Validation Loss: 0.00001630
2024-11-06 14:07:16,266 - INFO - Epoch [147/300], Batch [1/43], Training Loss: 0.00002889
2024-11-06 14:07:16,271 - INFO - Epoch [147/300], Batch [2/43], Training Loss: 0.00001427
2024-11-06 14:07:16,276 - INFO - Epoch [147/300], Batch [3/43], Training Loss: 0.00000925
2024-11-06 14:07:16,286 - INFO - Epoch [147/300], Batch [4/43], Training Loss: 0.00001963
2024-11-06 14:07:16,292 - INFO - Epoch [147/300], Batch [5/43], Training Loss: 0.00000624
2024-11-06 14:07:16,297 - INFO - Epoch [147/300], Batch [6/43], Training Loss: 0.00000584
2024-11-06 14:07:16,302 - INFO - Epoch [147/300], Batch [7/43], Training Loss: 0.00001440
2024-11-06 14:07:16,308 - INFO - Epoch [147/300], Batch [8/43], Training Loss: 0.00000634
2024-11-06 14:07:16,312 - INFO - Epoch [147/300], Batch [9/43], Training Loss: 0.00000691
2024-11-06 14:07:16,317 - INFO - Epoch [147/300], Batch [10/43], Training Loss: 0.00001121
2024-11-06 14:07:16,322 - INFO - Epoch [147/300], Batch [11/43], Training Loss: 0.00000738
2024-11-06 14:07:16,327 - INFO - Epoch [147/300], Batch [12/43], Training Loss: 0.00001508
2024-11-06 14:07:16,333 - INFO - Epoch [147/300], Batch [13/43], Training Loss: 0.00000317
2024-11-06 14:07:16,338 - INFO - Epoch [147/300], Batch [14/43], Training Loss: 0.00001128
2024-11-06 14:07:16,342 - INFO - Epoch [147/300], Batch [15/43], Training Loss: 0.00000753
2024-11-06 14:07:16,347 - INFO - Epoch [147/300], Batch [16/43], Training Loss: 0.00000402
2024-11-06 14:07:16,352 - INFO - Epoch [147/300], Batch [17/43], Training Loss: 0.00001122
2024-11-06 14:07:16,358 - INFO - Epoch [147/300], Batch [18/43], Training Loss: 0.00000621
2024-11-06 14:07:16,364 - INFO - Epoch [147/300], Batch [19/43], Training Loss: 0.00001062
2024-11-06 14:07:16,369 - INFO - Epoch [147/300], Batch [20/43], Training Loss: 0.00000915
2024-11-06 14:07:16,373 - INFO - Epoch [147/300], Batch [21/43], Training Loss: 0.00002153
2024-11-06 14:07:16,377 - INFO - Epoch [147/300], Batch [22/43], Training Loss: 0.00001141
2024-11-06 14:07:16,382 - INFO - Epoch [147/300], Batch [23/43], Training Loss: 0.00001430
2024-11-06 14:07:16,387 - INFO - Epoch [147/300], Batch [24/43], Training Loss: 0.00002112
2024-11-06 14:07:16,391 - INFO - Epoch [147/300], Batch [25/43], Training Loss: 0.00000547
2024-11-06 14:07:16,394 - INFO - Epoch [147/300], Batch [26/43], Training Loss: 0.00001298
2024-11-06 14:07:16,398 - INFO - Epoch [147/300], Batch [27/43], Training Loss: 0.00001417
2024-11-06 14:07:16,402 - INFO - Epoch [147/300], Batch [28/43], Training Loss: 0.00002393
2024-11-06 14:07:16,405 - INFO - Epoch [147/300], Batch [29/43], Training Loss: 0.00001624
2024-11-06 14:07:16,409 - INFO - Epoch [147/300], Batch [30/43], Training Loss: 0.00005505
2024-11-06 14:07:16,412 - INFO - Epoch [147/300], Batch [31/43], Training Loss: 0.00001481
2024-11-06 14:07:16,416 - INFO - Epoch [147/300], Batch [32/43], Training Loss: 0.00000299
2024-11-06 14:07:16,419 - INFO - Epoch [147/300], Batch [33/43], Training Loss: 0.00001873
2024-11-06 14:07:16,423 - INFO - Epoch [147/300], Batch [34/43], Training Loss: 0.00003117
2024-11-06 14:07:16,429 - INFO - Epoch [147/300], Batch [35/43], Training Loss: 0.00001197
2024-11-06 14:07:16,433 - INFO - Epoch [147/300], Batch [36/43], Training Loss: 0.00002898
2024-11-06 14:07:16,438 - INFO - Epoch [147/300], Batch [37/43], Training Loss: 0.00006483
2024-11-06 14:07:16,444 - INFO - Epoch [147/300], Batch [38/43], Training Loss: 0.00002549
2024-11-06 14:07:16,450 - INFO - Epoch [147/300], Batch [39/43], Training Loss: 0.00002163
2024-11-06 14:07:16,454 - INFO - Epoch [147/300], Batch [40/43], Training Loss: 0.00002902
2024-11-06 14:07:16,458 - INFO - Epoch [147/300], Batch [41/43], Training Loss: 0.00001687
2024-11-06 14:07:16,465 - INFO - Epoch [147/300], Batch [42/43], Training Loss: 0.00000744
2024-11-06 14:07:16,471 - INFO - Epoch [147/300], Batch [43/43], Training Loss: 0.00002216
2024-11-06 14:07:16,485 - INFO - Epoch [147/300], Average Training Loss: 0.00001630, Validation Loss: 0.00004280
2024-11-06 14:07:16,490 - INFO - Epoch [148/300], Batch [1/43], Training Loss: 0.00002302
2024-11-06 14:07:16,495 - INFO - Epoch [148/300], Batch [2/43], Training Loss: 0.00002923
2024-11-06 14:07:16,499 - INFO - Epoch [148/300], Batch [3/43], Training Loss: 0.00001218
2024-11-06 14:07:16,502 - INFO - Epoch [148/300], Batch [4/43], Training Loss: 0.00002889
2024-11-06 14:07:16,506 - INFO - Epoch [148/300], Batch [5/43], Training Loss: 0.00003833
2024-11-06 14:07:16,511 - INFO - Epoch [148/300], Batch [6/43], Training Loss: 0.00001663
2024-11-06 14:07:16,515 - INFO - Epoch [148/300], Batch [7/43], Training Loss: 0.00000798
2024-11-06 14:07:16,520 - INFO - Epoch [148/300], Batch [8/43], Training Loss: 0.00002680
2024-11-06 14:07:16,523 - INFO - Epoch [148/300], Batch [9/43], Training Loss: 0.00003501
2024-11-06 14:07:16,527 - INFO - Epoch [148/300], Batch [10/43], Training Loss: 0.00001749
2024-11-06 14:07:16,531 - INFO - Epoch [148/300], Batch [11/43], Training Loss: 0.00001630
2024-11-06 14:07:16,534 - INFO - Epoch [148/300], Batch [12/43], Training Loss: 0.00004081
2024-11-06 14:07:16,537 - INFO - Epoch [148/300], Batch [13/43], Training Loss: 0.00001557
2024-11-06 14:07:16,541 - INFO - Epoch [148/300], Batch [14/43], Training Loss: 0.00000685
2024-11-06 14:07:16,545 - INFO - Epoch [148/300], Batch [15/43], Training Loss: 0.00002465
2024-11-06 14:07:16,547 - INFO - Epoch [148/300], Batch [16/43], Training Loss: 0.00002738
2024-11-06 14:07:16,551 - INFO - Epoch [148/300], Batch [17/43], Training Loss: 0.00001479
2024-11-06 14:07:16,554 - INFO - Epoch [148/300], Batch [18/43], Training Loss: 0.00001552
2024-11-06 14:07:16,556 - INFO - Epoch [148/300], Batch [19/43], Training Loss: 0.00002034
2024-11-06 14:07:16,560 - INFO - Epoch [148/300], Batch [20/43], Training Loss: 0.00002212
2024-11-06 14:07:16,563 - INFO - Epoch [148/300], Batch [21/43], Training Loss: 0.00001112
2024-11-06 14:07:16,566 - INFO - Epoch [148/300], Batch [22/43], Training Loss: 0.00002289
2024-11-06 14:07:16,569 - INFO - Epoch [148/300], Batch [23/43], Training Loss: 0.00001594
2024-11-06 14:07:16,571 - INFO - Epoch [148/300], Batch [24/43], Training Loss: 0.00003253
2024-11-06 14:07:16,574 - INFO - Epoch [148/300], Batch [25/43], Training Loss: 0.00002750
2024-11-06 14:07:16,577 - INFO - Epoch [148/300], Batch [26/43], Training Loss: 0.00001954
2024-11-06 14:07:16,579 - INFO - Epoch [148/300], Batch [27/43], Training Loss: 0.00003963
2024-11-06 14:07:16,585 - INFO - Epoch [148/300], Batch [28/43], Training Loss: 0.00002018
2024-11-06 14:07:16,589 - INFO - Epoch [148/300], Batch [29/43], Training Loss: 0.00001414
2024-11-06 14:07:16,593 - INFO - Epoch [148/300], Batch [30/43], Training Loss: 0.00002427
2024-11-06 14:07:16,597 - INFO - Epoch [148/300], Batch [31/43], Training Loss: 0.00002291
2024-11-06 14:07:16,601 - INFO - Epoch [148/300], Batch [32/43], Training Loss: 0.00002081
2024-11-06 14:07:16,605 - INFO - Epoch [148/300], Batch [33/43], Training Loss: 0.00000991
2024-11-06 14:07:16,609 - INFO - Epoch [148/300], Batch [34/43], Training Loss: 0.00005899
2024-11-06 14:07:16,612 - INFO - Epoch [148/300], Batch [35/43], Training Loss: 0.00002154
2024-11-06 14:07:16,617 - INFO - Epoch [148/300], Batch [36/43], Training Loss: 0.00000356
2024-11-06 14:07:16,620 - INFO - Epoch [148/300], Batch [37/43], Training Loss: 0.00002750
2024-11-06 14:07:16,624 - INFO - Epoch [148/300], Batch [38/43], Training Loss: 0.00001578
2024-11-06 14:07:16,628 - INFO - Epoch [148/300], Batch [39/43], Training Loss: 0.00000964
2024-11-06 14:07:16,632 - INFO - Epoch [148/300], Batch [40/43], Training Loss: 0.00001610
2024-11-06 14:07:16,636 - INFO - Epoch [148/300], Batch [41/43], Training Loss: 0.00001527
2024-11-06 14:07:16,671 - INFO - Epoch [148/300], Batch [42/43], Training Loss: 0.00002518
2024-11-06 14:07:16,686 - INFO - Epoch [148/300], Batch [43/43], Training Loss: 0.00001985
2024-11-06 14:07:16,708 - INFO - Epoch [148/300], Average Training Loss: 0.00002174, Validation Loss: 0.00002231
2024-11-06 14:07:16,713 - INFO - Epoch [149/300], Batch [1/43], Training Loss: 0.00001515
2024-11-06 14:07:16,717 - INFO - Epoch [149/300], Batch [2/43], Training Loss: 0.00005370
2024-11-06 14:07:16,720 - INFO - Epoch [149/300], Batch [3/43], Training Loss: 0.00001467
2024-11-06 14:07:16,725 - INFO - Epoch [149/300], Batch [4/43], Training Loss: 0.00002446
2024-11-06 14:07:16,728 - INFO - Epoch [149/300], Batch [5/43], Training Loss: 0.00001955
2024-11-06 14:07:16,733 - INFO - Epoch [149/300], Batch [6/43], Training Loss: 0.00002560
2024-11-06 14:07:16,737 - INFO - Epoch [149/300], Batch [7/43], Training Loss: 0.00000952
2024-11-06 14:07:16,742 - INFO - Epoch [149/300], Batch [8/43], Training Loss: 0.00000917
2024-11-06 14:07:16,746 - INFO - Epoch [149/300], Batch [9/43], Training Loss: 0.00000997
2024-11-06 14:07:16,750 - INFO - Epoch [149/300], Batch [10/43], Training Loss: 0.00003205
2024-11-06 14:07:16,756 - INFO - Epoch [149/300], Batch [11/43], Training Loss: 0.00000637
2024-11-06 14:07:16,760 - INFO - Epoch [149/300], Batch [12/43], Training Loss: 0.00001585
2024-11-06 14:07:16,764 - INFO - Epoch [149/300], Batch [13/43], Training Loss: 0.00001726
2024-11-06 14:07:16,767 - INFO - Epoch [149/300], Batch [14/43], Training Loss: 0.00001546
2024-11-06 14:07:16,772 - INFO - Epoch [149/300], Batch [15/43], Training Loss: 0.00001600
2024-11-06 14:07:16,776 - INFO - Epoch [149/300], Batch [16/43], Training Loss: 0.00000679
2024-11-06 14:07:16,779 - INFO - Epoch [149/300], Batch [17/43], Training Loss: 0.00001790
2024-11-06 14:07:16,783 - INFO - Epoch [149/300], Batch [18/43], Training Loss: 0.00002021
2024-11-06 14:07:16,801 - INFO - Epoch [149/300], Batch [19/43], Training Loss: 0.00000486
2024-11-06 14:07:16,806 - INFO - Epoch [149/300], Batch [20/43], Training Loss: 0.00001359
2024-11-06 14:07:16,810 - INFO - Epoch [149/300], Batch [21/43], Training Loss: 0.00001813
2024-11-06 14:07:16,814 - INFO - Epoch [149/300], Batch [22/43], Training Loss: 0.00000828
2024-11-06 14:07:16,818 - INFO - Epoch [149/300], Batch [23/43], Training Loss: 0.00001361
2024-11-06 14:07:16,821 - INFO - Epoch [149/300], Batch [24/43], Training Loss: 0.00000325
2024-11-06 14:07:16,825 - INFO - Epoch [149/300], Batch [25/43], Training Loss: 0.00001177
2024-11-06 14:07:16,828 - INFO - Epoch [149/300], Batch [26/43], Training Loss: 0.00001811
2024-11-06 14:07:16,831 - INFO - Epoch [149/300], Batch [27/43], Training Loss: 0.00001536
2024-11-06 14:07:16,835 - INFO - Epoch [149/300], Batch [28/43], Training Loss: 0.00000808
2024-11-06 14:07:16,839 - INFO - Epoch [149/300], Batch [29/43], Training Loss: 0.00002375
2024-11-06 14:07:16,844 - INFO - Epoch [149/300], Batch [30/43], Training Loss: 0.00001050
2024-11-06 14:07:16,848 - INFO - Epoch [149/300], Batch [31/43], Training Loss: 0.00000387
2024-11-06 14:07:16,852 - INFO - Epoch [149/300], Batch [32/43], Training Loss: 0.00000758
2024-11-06 14:07:16,856 - INFO - Epoch [149/300], Batch [33/43], Training Loss: 0.00001233
2024-11-06 14:07:16,860 - INFO - Epoch [149/300], Batch [34/43], Training Loss: 0.00000728
2024-11-06 14:07:16,866 - INFO - Epoch [149/300], Batch [35/43], Training Loss: 0.00001104
2024-11-06 14:07:16,871 - INFO - Epoch [149/300], Batch [36/43], Training Loss: 0.00000872
2024-11-06 14:07:16,875 - INFO - Epoch [149/300], Batch [37/43], Training Loss: 0.00000886
2024-11-06 14:07:16,879 - INFO - Epoch [149/300], Batch [38/43], Training Loss: 0.00001647
2024-11-06 14:07:16,885 - INFO - Epoch [149/300], Batch [39/43], Training Loss: 0.00001895
2024-11-06 14:07:16,890 - INFO - Epoch [149/300], Batch [40/43], Training Loss: 0.00001111
2024-11-06 14:07:16,894 - INFO - Epoch [149/300], Batch [41/43], Training Loss: 0.00001536
2024-11-06 14:07:16,898 - INFO - Epoch [149/300], Batch [42/43], Training Loss: 0.00004045
2024-11-06 14:07:16,903 - INFO - Epoch [149/300], Batch [43/43], Training Loss: 0.00000996
2024-11-06 14:07:16,917 - INFO - Epoch [149/300], Average Training Loss: 0.00001514, Validation Loss: 0.00001791
2024-11-06 14:07:16,921 - INFO - Epoch [150/300], Batch [1/43], Training Loss: 0.00001245
2024-11-06 14:07:16,926 - INFO - Epoch [150/300], Batch [2/43], Training Loss: 0.00001852
2024-11-06 14:07:16,931 - INFO - Epoch [150/300], Batch [3/43], Training Loss: 0.00001450
2024-11-06 14:07:16,935 - INFO - Epoch [150/300], Batch [4/43], Training Loss: 0.00001460
2024-11-06 14:07:16,940 - INFO - Epoch [150/300], Batch [5/43], Training Loss: 0.00000277
2024-11-06 14:07:16,943 - INFO - Epoch [150/300], Batch [6/43], Training Loss: 0.00003700
2024-11-06 14:07:16,947 - INFO - Epoch [150/300], Batch [7/43], Training Loss: 0.00001144
2024-11-06 14:07:16,951 - INFO - Epoch [150/300], Batch [8/43], Training Loss: 0.00001013
2024-11-06 14:07:16,956 - INFO - Epoch [150/300], Batch [9/43], Training Loss: 0.00000992
2024-11-06 14:07:16,960 - INFO - Epoch [150/300], Batch [10/43], Training Loss: 0.00001491
2024-11-06 14:07:16,964 - INFO - Epoch [150/300], Batch [11/43], Training Loss: 0.00001375
2024-11-06 14:07:16,968 - INFO - Epoch [150/300], Batch [12/43], Training Loss: 0.00003332
2024-11-06 14:07:16,971 - INFO - Epoch [150/300], Batch [13/43], Training Loss: 0.00000586
2024-11-06 14:07:16,975 - INFO - Epoch [150/300], Batch [14/43], Training Loss: 0.00001130
2024-11-06 14:07:16,978 - INFO - Epoch [150/300], Batch [15/43], Training Loss: 0.00000775
2024-11-06 14:07:16,982 - INFO - Epoch [150/300], Batch [16/43], Training Loss: 0.00001250
2024-11-06 14:07:16,987 - INFO - Epoch [150/300], Batch [17/43], Training Loss: 0.00001527
2024-11-06 14:07:16,991 - INFO - Epoch [150/300], Batch [18/43], Training Loss: 0.00001633
2024-11-06 14:07:16,996 - INFO - Epoch [150/300], Batch [19/43], Training Loss: 0.00000935
2024-11-06 14:07:17,000 - INFO - Epoch [150/300], Batch [20/43], Training Loss: 0.00001483
2024-11-06 14:07:17,004 - INFO - Epoch [150/300], Batch [21/43], Training Loss: 0.00001436
2024-11-06 14:07:17,009 - INFO - Epoch [150/300], Batch [22/43], Training Loss: 0.00001470
2024-11-06 14:07:17,012 - INFO - Epoch [150/300], Batch [23/43], Training Loss: 0.00001649
2024-11-06 14:07:17,016 - INFO - Epoch [150/300], Batch [24/43], Training Loss: 0.00000830
2024-11-06 14:07:17,020 - INFO - Epoch [150/300], Batch [25/43], Training Loss: 0.00001608
2024-11-06 14:07:17,023 - INFO - Epoch [150/300], Batch [26/43], Training Loss: 0.00002203
2024-11-06 14:07:17,026 - INFO - Epoch [150/300], Batch [27/43], Training Loss: 0.00001029
2024-11-06 14:07:17,029 - INFO - Epoch [150/300], Batch [28/43], Training Loss: 0.00000843
2024-11-06 14:07:17,033 - INFO - Epoch [150/300], Batch [29/43], Training Loss: 0.00001415
2024-11-06 14:07:17,036 - INFO - Epoch [150/300], Batch [30/43], Training Loss: 0.00002966
2024-11-06 14:07:17,039 - INFO - Epoch [150/300], Batch [31/43], Training Loss: 0.00000553
2024-11-06 14:07:17,042 - INFO - Epoch [150/300], Batch [32/43], Training Loss: 0.00000743
2024-11-06 14:07:17,045 - INFO - Epoch [150/300], Batch [33/43], Training Loss: 0.00001452
2024-11-06 14:07:17,048 - INFO - Epoch [150/300], Batch [34/43], Training Loss: 0.00000536
2024-11-06 14:07:17,052 - INFO - Epoch [150/300], Batch [35/43], Training Loss: 0.00000564
2024-11-06 14:07:17,055 - INFO - Epoch [150/300], Batch [36/43], Training Loss: 0.00000559
2024-11-06 14:07:17,058 - INFO - Epoch [150/300], Batch [37/43], Training Loss: 0.00001274
2024-11-06 14:07:17,061 - INFO - Epoch [150/300], Batch [38/43], Training Loss: 0.00000856
2024-11-06 14:07:17,064 - INFO - Epoch [150/300], Batch [39/43], Training Loss: 0.00000524
2024-11-06 14:07:17,068 - INFO - Epoch [150/300], Batch [40/43], Training Loss: 0.00001886
2024-11-06 14:07:17,072 - INFO - Epoch [150/300], Batch [41/43], Training Loss: 0.00000658
2024-11-06 14:07:17,076 - INFO - Epoch [150/300], Batch [42/43], Training Loss: 0.00000411
2024-11-06 14:07:17,080 - INFO - Epoch [150/300], Batch [43/43], Training Loss: 0.00001454
2024-11-06 14:07:17,092 - INFO - Epoch [150/300], Average Training Loss: 0.00001292, Validation Loss: 0.00001487
2024-11-06 14:07:17,098 - INFO - Epoch [151/300], Batch [1/43], Training Loss: 0.00000660
2024-11-06 14:07:17,102 - INFO - Epoch [151/300], Batch [2/43], Training Loss: 0.00002190
2024-11-06 14:07:17,105 - INFO - Epoch [151/300], Batch [3/43], Training Loss: 0.00000963
2024-11-06 14:07:17,108 - INFO - Epoch [151/300], Batch [4/43], Training Loss: 0.00001975
2024-11-06 14:07:17,111 - INFO - Epoch [151/300], Batch [5/43], Training Loss: 0.00001510
2024-11-06 14:07:17,115 - INFO - Epoch [151/300], Batch [6/43], Training Loss: 0.00000390
2024-11-06 14:07:17,119 - INFO - Epoch [151/300], Batch [7/43], Training Loss: 0.00000586
2024-11-06 14:07:17,123 - INFO - Epoch [151/300], Batch [8/43], Training Loss: 0.00000684
2024-11-06 14:07:17,126 - INFO - Epoch [151/300], Batch [9/43], Training Loss: 0.00000888
2024-11-06 14:07:17,130 - INFO - Epoch [151/300], Batch [10/43], Training Loss: 0.00002178
2024-11-06 14:07:17,134 - INFO - Epoch [151/300], Batch [11/43], Training Loss: 0.00001228
2024-11-06 14:07:17,138 - INFO - Epoch [151/300], Batch [12/43], Training Loss: 0.00000936
2024-11-06 14:07:17,142 - INFO - Epoch [151/300], Batch [13/43], Training Loss: 0.00000283
2024-11-06 14:07:17,146 - INFO - Epoch [151/300], Batch [14/43], Training Loss: 0.00001496
2024-11-06 14:07:17,149 - INFO - Epoch [151/300], Batch [15/43], Training Loss: 0.00000746
2024-11-06 14:07:17,152 - INFO - Epoch [151/300], Batch [16/43], Training Loss: 0.00000820
2024-11-06 14:07:17,155 - INFO - Epoch [151/300], Batch [17/43], Training Loss: 0.00001885
2024-11-06 14:07:17,158 - INFO - Epoch [151/300], Batch [18/43], Training Loss: 0.00001714
2024-11-06 14:07:17,161 - INFO - Epoch [151/300], Batch [19/43], Training Loss: 0.00001268
2024-11-06 14:07:17,164 - INFO - Epoch [151/300], Batch [20/43], Training Loss: 0.00000560
2024-11-06 14:07:17,167 - INFO - Epoch [151/300], Batch [21/43], Training Loss: 0.00000250
2024-11-06 14:07:17,170 - INFO - Epoch [151/300], Batch [22/43], Training Loss: 0.00002492
2024-11-06 14:07:17,174 - INFO - Epoch [151/300], Batch [23/43], Training Loss: 0.00000586
2024-11-06 14:07:17,178 - INFO - Epoch [151/300], Batch [24/43], Training Loss: 0.00002975
2024-11-06 14:07:17,181 - INFO - Epoch [151/300], Batch [25/43], Training Loss: 0.00001202
2024-11-06 14:07:17,185 - INFO - Epoch [151/300], Batch [26/43], Training Loss: 0.00000692
2024-11-06 14:07:17,188 - INFO - Epoch [151/300], Batch [27/43], Training Loss: 0.00001368
2024-11-06 14:07:17,192 - INFO - Epoch [151/300], Batch [28/43], Training Loss: 0.00001605
2024-11-06 14:07:17,196 - INFO - Epoch [151/300], Batch [29/43], Training Loss: 0.00001719
2024-11-06 14:07:17,199 - INFO - Epoch [151/300], Batch [30/43], Training Loss: 0.00000782
2024-11-06 14:07:17,203 - INFO - Epoch [151/300], Batch [31/43], Training Loss: 0.00000490
2024-11-06 14:07:17,206 - INFO - Epoch [151/300], Batch [32/43], Training Loss: 0.00002744
2024-11-06 14:07:17,211 - INFO - Epoch [151/300], Batch [33/43], Training Loss: 0.00001487
2024-11-06 14:07:17,215 - INFO - Epoch [151/300], Batch [34/43], Training Loss: 0.00001874
2024-11-06 14:07:17,219 - INFO - Epoch [151/300], Batch [35/43], Training Loss: 0.00001291
2024-11-06 14:07:17,223 - INFO - Epoch [151/300], Batch [36/43], Training Loss: 0.00000821
2024-11-06 14:07:17,227 - INFO - Epoch [151/300], Batch [37/43], Training Loss: 0.00001289
2024-11-06 14:07:17,232 - INFO - Epoch [151/300], Batch [38/43], Training Loss: 0.00000531
2024-11-06 14:07:17,237 - INFO - Epoch [151/300], Batch [39/43], Training Loss: 0.00000846
2024-11-06 14:07:17,241 - INFO - Epoch [151/300], Batch [40/43], Training Loss: 0.00000702
2024-11-06 14:07:17,246 - INFO - Epoch [151/300], Batch [41/43], Training Loss: 0.00000773
2024-11-06 14:07:17,250 - INFO - Epoch [151/300], Batch [42/43], Training Loss: 0.00001660
2024-11-06 14:07:17,254 - INFO - Epoch [151/300], Batch [43/43], Training Loss: 0.00000796
2024-11-06 14:07:17,266 - INFO - Epoch [151/300], Average Training Loss: 0.00001208, Validation Loss: 0.00002050
2024-11-06 14:07:17,270 - INFO - Epoch [152/300], Batch [1/43], Training Loss: 0.00002304
2024-11-06 14:07:17,273 - INFO - Epoch [152/300], Batch [2/43], Training Loss: 0.00002597
2024-11-06 14:07:17,278 - INFO - Epoch [152/300], Batch [3/43], Training Loss: 0.00000841
2024-11-06 14:07:17,283 - INFO - Epoch [152/300], Batch [4/43], Training Loss: 0.00002164
2024-11-06 14:07:17,288 - INFO - Epoch [152/300], Batch [5/43], Training Loss: 0.00000992
2024-11-06 14:07:17,292 - INFO - Epoch [152/300], Batch [6/43], Training Loss: 0.00001192
2024-11-06 14:07:17,297 - INFO - Epoch [152/300], Batch [7/43], Training Loss: 0.00001136
2024-11-06 14:07:17,301 - INFO - Epoch [152/300], Batch [8/43], Training Loss: 0.00002707
2024-11-06 14:07:17,306 - INFO - Epoch [152/300], Batch [9/43], Training Loss: 0.00002784
2024-11-06 14:07:17,309 - INFO - Epoch [152/300], Batch [10/43], Training Loss: 0.00002288
2024-11-06 14:07:17,315 - INFO - Epoch [152/300], Batch [11/43], Training Loss: 0.00002411
2024-11-06 14:07:17,320 - INFO - Epoch [152/300], Batch [12/43], Training Loss: 0.00002231
2024-11-06 14:07:17,325 - INFO - Epoch [152/300], Batch [13/43], Training Loss: 0.00001348
2024-11-06 14:07:17,329 - INFO - Epoch [152/300], Batch [14/43], Training Loss: 0.00001155
2024-11-06 14:07:17,333 - INFO - Epoch [152/300], Batch [15/43], Training Loss: 0.00001475
2024-11-06 14:07:17,337 - INFO - Epoch [152/300], Batch [16/43], Training Loss: 0.00005111
2024-11-06 14:07:17,341 - INFO - Epoch [152/300], Batch [17/43], Training Loss: 0.00001504
2024-11-06 14:07:17,346 - INFO - Epoch [152/300], Batch [18/43], Training Loss: 0.00000646
2024-11-06 14:07:17,350 - INFO - Epoch [152/300], Batch [19/43], Training Loss: 0.00003312
2024-11-06 14:07:17,356 - INFO - Epoch [152/300], Batch [20/43], Training Loss: 0.00001716
2024-11-06 14:07:17,360 - INFO - Epoch [152/300], Batch [21/43], Training Loss: 0.00000765
2024-11-06 14:07:17,366 - INFO - Epoch [152/300], Batch [22/43], Training Loss: 0.00001853
2024-11-06 14:07:17,370 - INFO - Epoch [152/300], Batch [23/43], Training Loss: 0.00001325
2024-11-06 14:07:17,374 - INFO - Epoch [152/300], Batch [24/43], Training Loss: 0.00000979
2024-11-06 14:07:17,378 - INFO - Epoch [152/300], Batch [25/43], Training Loss: 0.00000976
2024-11-06 14:07:17,383 - INFO - Epoch [152/300], Batch [26/43], Training Loss: 0.00002671
2024-11-06 14:07:17,387 - INFO - Epoch [152/300], Batch [27/43], Training Loss: 0.00001444
2024-11-06 14:07:17,391 - INFO - Epoch [152/300], Batch [28/43], Training Loss: 0.00000882
2024-11-06 14:07:17,395 - INFO - Epoch [152/300], Batch [29/43], Training Loss: 0.00001995
2024-11-06 14:07:17,399 - INFO - Epoch [152/300], Batch [30/43], Training Loss: 0.00002304
2024-11-06 14:07:17,403 - INFO - Epoch [152/300], Batch [31/43], Training Loss: 0.00001104
2024-11-06 14:07:17,406 - INFO - Epoch [152/300], Batch [32/43], Training Loss: 0.00000536
2024-11-06 14:07:17,411 - INFO - Epoch [152/300], Batch [33/43], Training Loss: 0.00001249
2024-11-06 14:07:17,415 - INFO - Epoch [152/300], Batch [34/43], Training Loss: 0.00000809
2024-11-06 14:07:17,419 - INFO - Epoch [152/300], Batch [35/43], Training Loss: 0.00001584
2024-11-06 14:07:17,423 - INFO - Epoch [152/300], Batch [36/43], Training Loss: 0.00001593
2024-11-06 14:07:17,426 - INFO - Epoch [152/300], Batch [37/43], Training Loss: 0.00001912
2024-11-06 14:07:17,430 - INFO - Epoch [152/300], Batch [38/43], Training Loss: 0.00001448
2024-11-06 14:07:17,433 - INFO - Epoch [152/300], Batch [39/43], Training Loss: 0.00001276
2024-11-06 14:07:17,436 - INFO - Epoch [152/300], Batch [40/43], Training Loss: 0.00001190
2024-11-06 14:07:17,440 - INFO - Epoch [152/300], Batch [41/43], Training Loss: 0.00000825
2024-11-06 14:07:17,444 - INFO - Epoch [152/300], Batch [42/43], Training Loss: 0.00000821
2024-11-06 14:07:17,448 - INFO - Epoch [152/300], Batch [43/43], Training Loss: 0.00000906
2024-11-06 14:07:17,461 - INFO - Epoch [152/300], Average Training Loss: 0.00001636, Validation Loss: 0.00001467
2024-11-06 14:07:17,465 - INFO - Epoch [153/300], Batch [1/43], Training Loss: 0.00000845
2024-11-06 14:07:17,469 - INFO - Epoch [153/300], Batch [2/43], Training Loss: 0.00001308
2024-11-06 14:07:17,473 - INFO - Epoch [153/300], Batch [3/43], Training Loss: 0.00000724
2024-11-06 14:07:17,477 - INFO - Epoch [153/300], Batch [4/43], Training Loss: 0.00000766
2024-11-06 14:07:17,481 - INFO - Epoch [153/300], Batch [5/43], Training Loss: 0.00002227
2024-11-06 14:07:17,485 - INFO - Epoch [153/300], Batch [6/43], Training Loss: 0.00002532
2024-11-06 14:07:17,488 - INFO - Epoch [153/300], Batch [7/43], Training Loss: 0.00002037
2024-11-06 14:07:17,492 - INFO - Epoch [153/300], Batch [8/43], Training Loss: 0.00000637
2024-11-06 14:07:17,495 - INFO - Epoch [153/300], Batch [9/43], Training Loss: 0.00001447
2024-11-06 14:07:17,501 - INFO - Epoch [153/300], Batch [10/43], Training Loss: 0.00002968
2024-11-06 14:07:17,505 - INFO - Epoch [153/300], Batch [11/43], Training Loss: 0.00000522
2024-11-06 14:07:17,509 - INFO - Epoch [153/300], Batch [12/43], Training Loss: 0.00000882
2024-11-06 14:07:17,513 - INFO - Epoch [153/300], Batch [13/43], Training Loss: 0.00002535
2024-11-06 14:07:17,518 - INFO - Epoch [153/300], Batch [14/43], Training Loss: 0.00002581
2024-11-06 14:07:17,521 - INFO - Epoch [153/300], Batch [15/43], Training Loss: 0.00001672
2024-11-06 14:07:17,524 - INFO - Epoch [153/300], Batch [16/43], Training Loss: 0.00000802
2024-11-06 14:07:17,528 - INFO - Epoch [153/300], Batch [17/43], Training Loss: 0.00002312
2024-11-06 14:07:17,531 - INFO - Epoch [153/300], Batch [18/43], Training Loss: 0.00000827
2024-11-06 14:07:17,534 - INFO - Epoch [153/300], Batch [19/43], Training Loss: 0.00000440
2024-11-06 14:07:17,538 - INFO - Epoch [153/300], Batch [20/43], Training Loss: 0.00000710
2024-11-06 14:07:17,542 - INFO - Epoch [153/300], Batch [21/43], Training Loss: 0.00001805
2024-11-06 14:07:17,545 - INFO - Epoch [153/300], Batch [22/43], Training Loss: 0.00001474
2024-11-06 14:07:17,548 - INFO - Epoch [153/300], Batch [23/43], Training Loss: 0.00001485
2024-11-06 14:07:17,552 - INFO - Epoch [153/300], Batch [24/43], Training Loss: 0.00001694
2024-11-06 14:07:17,555 - INFO - Epoch [153/300], Batch [25/43], Training Loss: 0.00001405
2024-11-06 14:07:17,559 - INFO - Epoch [153/300], Batch [26/43], Training Loss: 0.00000978
2024-11-06 14:07:17,563 - INFO - Epoch [153/300], Batch [27/43], Training Loss: 0.00000926
2024-11-06 14:07:17,567 - INFO - Epoch [153/300], Batch [28/43], Training Loss: 0.00001861
2024-11-06 14:07:17,571 - INFO - Epoch [153/300], Batch [29/43], Training Loss: 0.00000533
2024-11-06 14:07:17,574 - INFO - Epoch [153/300], Batch [30/43], Training Loss: 0.00001395
2024-11-06 14:07:17,578 - INFO - Epoch [153/300], Batch [31/43], Training Loss: 0.00000766
2024-11-06 14:07:17,582 - INFO - Epoch [153/300], Batch [32/43], Training Loss: 0.00000822
2024-11-06 14:07:17,586 - INFO - Epoch [153/300], Batch [33/43], Training Loss: 0.00001269
2024-11-06 14:07:17,589 - INFO - Epoch [153/300], Batch [34/43], Training Loss: 0.00001356
2024-11-06 14:07:17,592 - INFO - Epoch [153/300], Batch [35/43], Training Loss: 0.00002700
2024-11-06 14:07:17,596 - INFO - Epoch [153/300], Batch [36/43], Training Loss: 0.00002664
2024-11-06 14:07:17,600 - INFO - Epoch [153/300], Batch [37/43], Training Loss: 0.00001094
2024-11-06 14:07:17,604 - INFO - Epoch [153/300], Batch [38/43], Training Loss: 0.00001050
2024-11-06 14:07:17,607 - INFO - Epoch [153/300], Batch [39/43], Training Loss: 0.00001161
2024-11-06 14:07:17,611 - INFO - Epoch [153/300], Batch [40/43], Training Loss: 0.00001014
2024-11-06 14:07:17,614 - INFO - Epoch [153/300], Batch [41/43], Training Loss: 0.00000554
2024-11-06 14:07:17,617 - INFO - Epoch [153/300], Batch [42/43], Training Loss: 0.00000863
2024-11-06 14:07:17,620 - INFO - Epoch [153/300], Batch [43/43], Training Loss: 0.00000357
2024-11-06 14:07:17,630 - INFO - Epoch [153/300], Average Training Loss: 0.00001349, Validation Loss: 0.00002236
2024-11-06 14:07:17,634 - INFO - Epoch [154/300], Batch [1/43], Training Loss: 0.00002847
2024-11-06 14:07:17,637 - INFO - Epoch [154/300], Batch [2/43], Training Loss: 0.00001934
2024-11-06 14:07:17,641 - INFO - Epoch [154/300], Batch [3/43], Training Loss: 0.00001379
2024-11-06 14:07:17,644 - INFO - Epoch [154/300], Batch [4/43], Training Loss: 0.00000924
2024-11-06 14:07:17,647 - INFO - Epoch [154/300], Batch [5/43], Training Loss: 0.00000831
2024-11-06 14:07:17,650 - INFO - Epoch [154/300], Batch [6/43], Training Loss: 0.00000842
2024-11-06 14:07:17,653 - INFO - Epoch [154/300], Batch [7/43], Training Loss: 0.00000465
2024-11-06 14:07:17,656 - INFO - Epoch [154/300], Batch [8/43], Training Loss: 0.00002041
2024-11-06 14:07:17,658 - INFO - Epoch [154/300], Batch [9/43], Training Loss: 0.00001594
2024-11-06 14:07:17,663 - INFO - Epoch [154/300], Batch [10/43], Training Loss: 0.00000367
2024-11-06 14:07:17,666 - INFO - Epoch [154/300], Batch [11/43], Training Loss: 0.00001513
2024-11-06 14:07:17,671 - INFO - Epoch [154/300], Batch [12/43], Training Loss: 0.00000354
2024-11-06 14:07:17,676 - INFO - Epoch [154/300], Batch [13/43], Training Loss: 0.00002030
2024-11-06 14:07:17,681 - INFO - Epoch [154/300], Batch [14/43], Training Loss: 0.00001615
2024-11-06 14:07:17,686 - INFO - Epoch [154/300], Batch [15/43], Training Loss: 0.00000565
2024-11-06 14:07:17,690 - INFO - Epoch [154/300], Batch [16/43], Training Loss: 0.00002676
2024-11-06 14:07:17,695 - INFO - Epoch [154/300], Batch [17/43], Training Loss: 0.00000864
2024-11-06 14:07:17,699 - INFO - Epoch [154/300], Batch [18/43], Training Loss: 0.00001473
2024-11-06 14:07:17,704 - INFO - Epoch [154/300], Batch [19/43], Training Loss: 0.00000918
2024-11-06 14:07:17,709 - INFO - Epoch [154/300], Batch [20/43], Training Loss: 0.00002180
2024-11-06 14:07:17,714 - INFO - Epoch [154/300], Batch [21/43], Training Loss: 0.00000752
2024-11-06 14:07:17,719 - INFO - Epoch [154/300], Batch [22/43], Training Loss: 0.00001202
2024-11-06 14:07:17,723 - INFO - Epoch [154/300], Batch [23/43], Training Loss: 0.00000543
2024-11-06 14:07:17,727 - INFO - Epoch [154/300], Batch [24/43], Training Loss: 0.00001960
2024-11-06 14:07:17,732 - INFO - Epoch [154/300], Batch [25/43], Training Loss: 0.00002191
2024-11-06 14:07:17,736 - INFO - Epoch [154/300], Batch [26/43], Training Loss: 0.00000656
2024-11-06 14:07:17,740 - INFO - Epoch [154/300], Batch [27/43], Training Loss: 0.00000911
2024-11-06 14:07:17,743 - INFO - Epoch [154/300], Batch [28/43], Training Loss: 0.00000787
2024-11-06 14:07:17,746 - INFO - Epoch [154/300], Batch [29/43], Training Loss: 0.00000467
2024-11-06 14:07:17,750 - INFO - Epoch [154/300], Batch [30/43], Training Loss: 0.00002591
2024-11-06 14:07:17,754 - INFO - Epoch [154/300], Batch [31/43], Training Loss: 0.00000911
2024-11-06 14:07:17,759 - INFO - Epoch [154/300], Batch [32/43], Training Loss: 0.00001034
2024-11-06 14:07:17,763 - INFO - Epoch [154/300], Batch [33/43], Training Loss: 0.00001233
2024-11-06 14:07:17,767 - INFO - Epoch [154/300], Batch [34/43], Training Loss: 0.00000445
2024-11-06 14:07:17,770 - INFO - Epoch [154/300], Batch [35/43], Training Loss: 0.00000394
2024-11-06 14:07:17,774 - INFO - Epoch [154/300], Batch [36/43], Training Loss: 0.00000779
2024-11-06 14:07:17,778 - INFO - Epoch [154/300], Batch [37/43], Training Loss: 0.00000539
2024-11-06 14:07:17,781 - INFO - Epoch [154/300], Batch [38/43], Training Loss: 0.00000561
2024-11-06 14:07:17,785 - INFO - Epoch [154/300], Batch [39/43], Training Loss: 0.00001556
2024-11-06 14:07:17,789 - INFO - Epoch [154/300], Batch [40/43], Training Loss: 0.00001217
2024-11-06 14:07:17,792 - INFO - Epoch [154/300], Batch [41/43], Training Loss: 0.00001374
2024-11-06 14:07:17,796 - INFO - Epoch [154/300], Batch [42/43], Training Loss: 0.00001474
2024-11-06 14:07:17,800 - INFO - Epoch [154/300], Batch [43/43], Training Loss: 0.00001265
2024-11-06 14:07:17,811 - INFO - Epoch [154/300], Average Training Loss: 0.00001215, Validation Loss: 0.00002060
2024-11-06 14:07:17,814 - INFO - Epoch [155/300], Batch [1/43], Training Loss: 0.00002006
2024-11-06 14:07:17,818 - INFO - Epoch [155/300], Batch [2/43], Training Loss: 0.00001294
2024-11-06 14:07:17,821 - INFO - Epoch [155/300], Batch [3/43], Training Loss: 0.00002066
2024-11-06 14:07:17,824 - INFO - Epoch [155/300], Batch [4/43], Training Loss: 0.00002779
2024-11-06 14:07:17,827 - INFO - Epoch [155/300], Batch [5/43], Training Loss: 0.00002807
2024-11-06 14:07:17,831 - INFO - Epoch [155/300], Batch [6/43], Training Loss: 0.00002088
2024-11-06 14:07:17,834 - INFO - Epoch [155/300], Batch [7/43], Training Loss: 0.00001744
2024-11-06 14:07:17,838 - INFO - Epoch [155/300], Batch [8/43], Training Loss: 0.00004432
2024-11-06 14:07:17,841 - INFO - Epoch [155/300], Batch [9/43], Training Loss: 0.00001071
2024-11-06 14:07:17,844 - INFO - Epoch [155/300], Batch [10/43], Training Loss: 0.00002045
2024-11-06 14:07:17,847 - INFO - Epoch [155/300], Batch [11/43], Training Loss: 0.00001729
2024-11-06 14:07:17,851 - INFO - Epoch [155/300], Batch [12/43], Training Loss: 0.00000935
2024-11-06 14:07:17,855 - INFO - Epoch [155/300], Batch [13/43], Training Loss: 0.00000734
2024-11-06 14:07:17,860 - INFO - Epoch [155/300], Batch [14/43], Training Loss: 0.00001907
2024-11-06 14:07:17,865 - INFO - Epoch [155/300], Batch [15/43], Training Loss: 0.00000980
2024-11-06 14:07:17,870 - INFO - Epoch [155/300], Batch [16/43], Training Loss: 0.00001089
2024-11-06 14:07:17,874 - INFO - Epoch [155/300], Batch [17/43], Training Loss: 0.00000849
2024-11-06 14:07:17,877 - INFO - Epoch [155/300], Batch [18/43], Training Loss: 0.00002456
2024-11-06 14:07:17,881 - INFO - Epoch [155/300], Batch [19/43], Training Loss: 0.00000784
2024-11-06 14:07:17,885 - INFO - Epoch [155/300], Batch [20/43], Training Loss: 0.00002530
2024-11-06 14:07:17,889 - INFO - Epoch [155/300], Batch [21/43], Training Loss: 0.00002335
2024-11-06 14:07:17,892 - INFO - Epoch [155/300], Batch [22/43], Training Loss: 0.00000714
2024-11-06 14:07:17,895 - INFO - Epoch [155/300], Batch [23/43], Training Loss: 0.00000760
2024-11-06 14:07:17,899 - INFO - Epoch [155/300], Batch [24/43], Training Loss: 0.00000383
2024-11-06 14:07:17,903 - INFO - Epoch [155/300], Batch [25/43], Training Loss: 0.00000737
2024-11-06 14:07:17,908 - INFO - Epoch [155/300], Batch [26/43], Training Loss: 0.00001126
2024-11-06 14:07:17,913 - INFO - Epoch [155/300], Batch [27/43], Training Loss: 0.00000828
2024-11-06 14:07:17,916 - INFO - Epoch [155/300], Batch [28/43], Training Loss: 0.00001287
2024-11-06 14:07:17,920 - INFO - Epoch [155/300], Batch [29/43], Training Loss: 0.00001365
2024-11-06 14:07:17,924 - INFO - Epoch [155/300], Batch [30/43], Training Loss: 0.00000460
2024-11-06 14:07:17,927 - INFO - Epoch [155/300], Batch [31/43], Training Loss: 0.00000977
2024-11-06 14:07:17,931 - INFO - Epoch [155/300], Batch [32/43], Training Loss: 0.00002767
2024-11-06 14:07:17,935 - INFO - Epoch [155/300], Batch [33/43], Training Loss: 0.00001530
2024-11-06 14:07:17,938 - INFO - Epoch [155/300], Batch [34/43], Training Loss: 0.00000639
2024-11-06 14:07:17,942 - INFO - Epoch [155/300], Batch [35/43], Training Loss: 0.00003133
2024-11-06 14:07:17,946 - INFO - Epoch [155/300], Batch [36/43], Training Loss: 0.00001461
2024-11-06 14:07:17,950 - INFO - Epoch [155/300], Batch [37/43], Training Loss: 0.00001115
2024-11-06 14:07:17,954 - INFO - Epoch [155/300], Batch [38/43], Training Loss: 0.00001580
2024-11-06 14:07:17,958 - INFO - Epoch [155/300], Batch [39/43], Training Loss: 0.00001105
2024-11-06 14:07:17,962 - INFO - Epoch [155/300], Batch [40/43], Training Loss: 0.00001547
2024-11-06 14:07:17,966 - INFO - Epoch [155/300], Batch [41/43], Training Loss: 0.00002269
2024-11-06 14:07:17,970 - INFO - Epoch [155/300], Batch [42/43], Training Loss: 0.00001035
2024-11-06 14:07:17,974 - INFO - Epoch [155/300], Batch [43/43], Training Loss: 0.00002330
2024-11-06 14:07:17,984 - INFO - Epoch [155/300], Average Training Loss: 0.00001577, Validation Loss: 0.00001510
2024-11-06 14:07:17,988 - INFO - Epoch [156/300], Batch [1/43], Training Loss: 0.00000708
2024-11-06 14:07:17,991 - INFO - Epoch [156/300], Batch [2/43], Training Loss: 0.00000334
2024-11-06 14:07:17,995 - INFO - Epoch [156/300], Batch [3/43], Training Loss: 0.00000303
2024-11-06 14:07:17,998 - INFO - Epoch [156/300], Batch [4/43], Training Loss: 0.00000900
2024-11-06 14:07:18,002 - INFO - Epoch [156/300], Batch [5/43], Training Loss: 0.00002374
2024-11-06 14:07:18,006 - INFO - Epoch [156/300], Batch [6/43], Training Loss: 0.00001149
2024-11-06 14:07:18,009 - INFO - Epoch [156/300], Batch [7/43], Training Loss: 0.00000372
2024-11-06 14:07:18,013 - INFO - Epoch [156/300], Batch [8/43], Training Loss: 0.00001980
2024-11-06 14:07:18,018 - INFO - Epoch [156/300], Batch [9/43], Training Loss: 0.00000967
2024-11-06 14:07:18,022 - INFO - Epoch [156/300], Batch [10/43], Training Loss: 0.00001303
2024-11-06 14:07:18,026 - INFO - Epoch [156/300], Batch [11/43], Training Loss: 0.00001053
2024-11-06 14:07:18,029 - INFO - Epoch [156/300], Batch [12/43], Training Loss: 0.00000680
2024-11-06 14:07:18,033 - INFO - Epoch [156/300], Batch [13/43], Training Loss: 0.00002015
2024-11-06 14:07:18,037 - INFO - Epoch [156/300], Batch [14/43], Training Loss: 0.00000910
2024-11-06 14:07:18,041 - INFO - Epoch [156/300], Batch [15/43], Training Loss: 0.00000632
2024-11-06 14:07:18,044 - INFO - Epoch [156/300], Batch [16/43], Training Loss: 0.00001055
2024-11-06 14:07:18,048 - INFO - Epoch [156/300], Batch [17/43], Training Loss: 0.00001720
2024-11-06 14:07:18,051 - INFO - Epoch [156/300], Batch [18/43], Training Loss: 0.00001656
2024-11-06 14:07:18,055 - INFO - Epoch [156/300], Batch [19/43], Training Loss: 0.00002561
2024-11-06 14:07:18,058 - INFO - Epoch [156/300], Batch [20/43], Training Loss: 0.00001431
2024-11-06 14:07:18,061 - INFO - Epoch [156/300], Batch [21/43], Training Loss: 0.00001294
2024-11-06 14:07:18,064 - INFO - Epoch [156/300], Batch [22/43], Training Loss: 0.00001418
2024-11-06 14:07:18,068 - INFO - Epoch [156/300], Batch [23/43], Training Loss: 0.00000879
2024-11-06 14:07:18,070 - INFO - Epoch [156/300], Batch [24/43], Training Loss: 0.00001480
2024-11-06 14:07:18,073 - INFO - Epoch [156/300], Batch [25/43], Training Loss: 0.00003368
2024-11-06 14:07:18,076 - INFO - Epoch [156/300], Batch [26/43], Training Loss: 0.00000661
2024-11-06 14:07:18,079 - INFO - Epoch [156/300], Batch [27/43], Training Loss: 0.00000560
2024-11-06 14:07:18,082 - INFO - Epoch [156/300], Batch [28/43], Training Loss: 0.00000917
2024-11-06 14:07:18,085 - INFO - Epoch [156/300], Batch [29/43], Training Loss: 0.00000827
2024-11-06 14:07:18,087 - INFO - Epoch [156/300], Batch [30/43], Training Loss: 0.00001654
2024-11-06 14:07:18,090 - INFO - Epoch [156/300], Batch [31/43], Training Loss: 0.00000560
2024-11-06 14:07:18,093 - INFO - Epoch [156/300], Batch [32/43], Training Loss: 0.00000671
2024-11-06 14:07:18,097 - INFO - Epoch [156/300], Batch [33/43], Training Loss: 0.00000565
2024-11-06 14:07:18,101 - INFO - Epoch [156/300], Batch [34/43], Training Loss: 0.00000301
2024-11-06 14:07:18,104 - INFO - Epoch [156/300], Batch [35/43], Training Loss: 0.00001024
2024-11-06 14:07:18,107 - INFO - Epoch [156/300], Batch [36/43], Training Loss: 0.00000515
2024-11-06 14:07:18,110 - INFO - Epoch [156/300], Batch [37/43], Training Loss: 0.00002630
2024-11-06 14:07:18,113 - INFO - Epoch [156/300], Batch [38/43], Training Loss: 0.00001789
2024-11-06 14:07:18,116 - INFO - Epoch [156/300], Batch [39/43], Training Loss: 0.00000745
2024-11-06 14:07:18,119 - INFO - Epoch [156/300], Batch [40/43], Training Loss: 0.00001942
2024-11-06 14:07:18,122 - INFO - Epoch [156/300], Batch [41/43], Training Loss: 0.00002569
2024-11-06 14:07:18,125 - INFO - Epoch [156/300], Batch [42/43], Training Loss: 0.00001242
2024-11-06 14:07:18,129 - INFO - Epoch [156/300], Batch [43/43], Training Loss: 0.00002142
2024-11-06 14:07:18,141 - INFO - Epoch [156/300], Average Training Loss: 0.00001252, Validation Loss: 0.00002865
2024-11-06 14:07:18,145 - INFO - Epoch [157/300], Batch [1/43], Training Loss: 0.00001977
2024-11-06 14:07:18,149 - INFO - Epoch [157/300], Batch [2/43], Training Loss: 0.00000883
2024-11-06 14:07:18,154 - INFO - Epoch [157/300], Batch [3/43], Training Loss: 0.00000651
2024-11-06 14:07:18,158 - INFO - Epoch [157/300], Batch [4/43], Training Loss: 0.00001707
2024-11-06 14:07:18,162 - INFO - Epoch [157/300], Batch [5/43], Training Loss: 0.00002648
2024-11-06 14:07:18,167 - INFO - Epoch [157/300], Batch [6/43], Training Loss: 0.00001099
2024-11-06 14:07:18,171 - INFO - Epoch [157/300], Batch [7/43], Training Loss: 0.00002960
2024-11-06 14:07:18,175 - INFO - Epoch [157/300], Batch [8/43], Training Loss: 0.00002839
2024-11-06 14:07:18,178 - INFO - Epoch [157/300], Batch [9/43], Training Loss: 0.00001063
2024-11-06 14:07:18,182 - INFO - Epoch [157/300], Batch [10/43], Training Loss: 0.00001225
2024-11-06 14:07:18,186 - INFO - Epoch [157/300], Batch [11/43], Training Loss: 0.00001012
2024-11-06 14:07:18,190 - INFO - Epoch [157/300], Batch [12/43], Training Loss: 0.00001369
2024-11-06 14:07:18,193 - INFO - Epoch [157/300], Batch [13/43], Training Loss: 0.00000777
2024-11-06 14:07:18,197 - INFO - Epoch [157/300], Batch [14/43], Training Loss: 0.00000637
2024-11-06 14:07:18,200 - INFO - Epoch [157/300], Batch [15/43], Training Loss: 0.00001517
2024-11-06 14:07:18,203 - INFO - Epoch [157/300], Batch [16/43], Training Loss: 0.00001049
2024-11-06 14:07:18,207 - INFO - Epoch [157/300], Batch [17/43], Training Loss: 0.00000333
2024-11-06 14:07:18,210 - INFO - Epoch [157/300], Batch [18/43], Training Loss: 0.00001738
2024-11-06 14:07:18,214 - INFO - Epoch [157/300], Batch [19/43], Training Loss: 0.00000499
2024-11-06 14:07:18,217 - INFO - Epoch [157/300], Batch [20/43], Training Loss: 0.00001074
2024-11-06 14:07:18,220 - INFO - Epoch [157/300], Batch [21/43], Training Loss: 0.00001195
2024-11-06 14:07:18,223 - INFO - Epoch [157/300], Batch [22/43], Training Loss: 0.00001173
2024-11-06 14:07:18,226 - INFO - Epoch [157/300], Batch [23/43], Training Loss: 0.00000933
2024-11-06 14:07:18,230 - INFO - Epoch [157/300], Batch [24/43], Training Loss: 0.00001381
2024-11-06 14:07:18,233 - INFO - Epoch [157/300], Batch [25/43], Training Loss: 0.00001064
2024-11-06 14:07:18,236 - INFO - Epoch [157/300], Batch [26/43], Training Loss: 0.00001712
2024-11-06 14:07:18,239 - INFO - Epoch [157/300], Batch [27/43], Training Loss: 0.00000596
2024-11-06 14:07:18,242 - INFO - Epoch [157/300], Batch [28/43], Training Loss: 0.00001066
2024-11-06 14:07:18,245 - INFO - Epoch [157/300], Batch [29/43], Training Loss: 0.00001502
2024-11-06 14:07:18,248 - INFO - Epoch [157/300], Batch [30/43], Training Loss: 0.00002121
2024-11-06 14:07:18,251 - INFO - Epoch [157/300], Batch [31/43], Training Loss: 0.00002965
2024-11-06 14:07:18,254 - INFO - Epoch [157/300], Batch [32/43], Training Loss: 0.00000764
2024-11-06 14:07:18,257 - INFO - Epoch [157/300], Batch [33/43], Training Loss: 0.00002348
2024-11-06 14:07:18,260 - INFO - Epoch [157/300], Batch [34/43], Training Loss: 0.00001637
2024-11-06 14:07:18,263 - INFO - Epoch [157/300], Batch [35/43], Training Loss: 0.00000285
2024-11-06 14:07:18,266 - INFO - Epoch [157/300], Batch [36/43], Training Loss: 0.00002636
2024-11-06 14:07:18,269 - INFO - Epoch [157/300], Batch [37/43], Training Loss: 0.00002367
2024-11-06 14:07:18,272 - INFO - Epoch [157/300], Batch [38/43], Training Loss: 0.00000830
2024-11-06 14:07:18,275 - INFO - Epoch [157/300], Batch [39/43], Training Loss: 0.00000745
2024-11-06 14:07:18,279 - INFO - Epoch [157/300], Batch [40/43], Training Loss: 0.00002827
2024-11-06 14:07:18,283 - INFO - Epoch [157/300], Batch [41/43], Training Loss: 0.00001558
2024-11-06 14:07:18,287 - INFO - Epoch [157/300], Batch [42/43], Training Loss: 0.00001130
2024-11-06 14:07:18,291 - INFO - Epoch [157/300], Batch [43/43], Training Loss: 0.00001099
2024-11-06 14:07:18,301 - INFO - Epoch [157/300], Average Training Loss: 0.00001418, Validation Loss: 0.00002008
2024-11-06 14:07:18,305 - INFO - Epoch [158/300], Batch [1/43], Training Loss: 0.00001741
2024-11-06 14:07:18,309 - INFO - Epoch [158/300], Batch [2/43], Training Loss: 0.00002343
2024-11-06 14:07:18,312 - INFO - Epoch [158/300], Batch [3/43], Training Loss: 0.00001225
2024-11-06 14:07:18,317 - INFO - Epoch [158/300], Batch [4/43], Training Loss: 0.00002248
2024-11-06 14:07:18,321 - INFO - Epoch [158/300], Batch [5/43], Training Loss: 0.00002096
2024-11-06 14:07:18,324 - INFO - Epoch [158/300], Batch [6/43], Training Loss: 0.00001923
2024-11-06 14:07:18,327 - INFO - Epoch [158/300], Batch [7/43], Training Loss: 0.00000730
2024-11-06 14:07:18,331 - INFO - Epoch [158/300], Batch [8/43], Training Loss: 0.00001845
2024-11-06 14:07:18,336 - INFO - Epoch [158/300], Batch [9/43], Training Loss: 0.00002018
2024-11-06 14:07:18,339 - INFO - Epoch [158/300], Batch [10/43], Training Loss: 0.00000485
2024-11-06 14:07:18,343 - INFO - Epoch [158/300], Batch [11/43], Training Loss: 0.00001542
2024-11-06 14:07:18,346 - INFO - Epoch [158/300], Batch [12/43], Training Loss: 0.00002399
2024-11-06 14:07:18,349 - INFO - Epoch [158/300], Batch [13/43], Training Loss: 0.00002033
2024-11-06 14:07:18,353 - INFO - Epoch [158/300], Batch [14/43], Training Loss: 0.00001635
2024-11-06 14:07:18,356 - INFO - Epoch [158/300], Batch [15/43], Training Loss: 0.00000742
2024-11-06 14:07:18,360 - INFO - Epoch [158/300], Batch [16/43], Training Loss: 0.00003599
2024-11-06 14:07:18,363 - INFO - Epoch [158/300], Batch [17/43], Training Loss: 0.00001294
2024-11-06 14:07:18,367 - INFO - Epoch [158/300], Batch [18/43], Training Loss: 0.00001187
2024-11-06 14:07:18,369 - INFO - Epoch [158/300], Batch [19/43], Training Loss: 0.00000697
2024-11-06 14:07:18,372 - INFO - Epoch [158/300], Batch [20/43], Training Loss: 0.00001920
2024-11-06 14:07:18,375 - INFO - Epoch [158/300], Batch [21/43], Training Loss: 0.00002198
2024-11-06 14:07:18,378 - INFO - Epoch [158/300], Batch [22/43], Training Loss: 0.00001409
2024-11-06 14:07:18,382 - INFO - Epoch [158/300], Batch [23/43], Training Loss: 0.00001436
2024-11-06 14:07:18,385 - INFO - Epoch [158/300], Batch [24/43], Training Loss: 0.00002253
2024-11-06 14:07:18,388 - INFO - Epoch [158/300], Batch [25/43], Training Loss: 0.00000345
2024-11-06 14:07:18,391 - INFO - Epoch [158/300], Batch [26/43], Training Loss: 0.00000520
2024-11-06 14:07:18,395 - INFO - Epoch [158/300], Batch [27/43], Training Loss: 0.00001208
2024-11-06 14:07:18,398 - INFO - Epoch [158/300], Batch [28/43], Training Loss: 0.00000831
2024-11-06 14:07:18,402 - INFO - Epoch [158/300], Batch [29/43], Training Loss: 0.00002151
2024-11-06 14:07:18,405 - INFO - Epoch [158/300], Batch [30/43], Training Loss: 0.00001615
2024-11-06 14:07:18,407 - INFO - Epoch [158/300], Batch [31/43], Training Loss: 0.00000911
2024-11-06 14:07:18,411 - INFO - Epoch [158/300], Batch [32/43], Training Loss: 0.00000778
2024-11-06 14:07:18,414 - INFO - Epoch [158/300], Batch [33/43], Training Loss: 0.00001985
2024-11-06 14:07:18,416 - INFO - Epoch [158/300], Batch [34/43], Training Loss: 0.00001012
2024-11-06 14:07:18,420 - INFO - Epoch [158/300], Batch [35/43], Training Loss: 0.00001129
2024-11-06 14:07:18,423 - INFO - Epoch [158/300], Batch [36/43], Training Loss: 0.00001608
2024-11-06 14:07:18,426 - INFO - Epoch [158/300], Batch [37/43], Training Loss: 0.00001303
2024-11-06 14:07:18,429 - INFO - Epoch [158/300], Batch [38/43], Training Loss: 0.00001864
2024-11-06 14:07:18,433 - INFO - Epoch [158/300], Batch [39/43], Training Loss: 0.00001311
2024-11-06 14:07:18,437 - INFO - Epoch [158/300], Batch [40/43], Training Loss: 0.00001259
2024-11-06 14:07:18,441 - INFO - Epoch [158/300], Batch [41/43], Training Loss: 0.00001037
2024-11-06 14:07:18,444 - INFO - Epoch [158/300], Batch [42/43], Training Loss: 0.00001046
2024-11-06 14:07:18,447 - INFO - Epoch [158/300], Batch [43/43], Training Loss: 0.00001290
2024-11-06 14:07:18,460 - INFO - Epoch [158/300], Average Training Loss: 0.00001493, Validation Loss: 0.00002252
2024-11-06 14:07:18,464 - INFO - Epoch [159/300], Batch [1/43], Training Loss: 0.00001322
2024-11-06 14:07:18,468 - INFO - Epoch [159/300], Batch [2/43], Training Loss: 0.00001951
2024-11-06 14:07:18,473 - INFO - Epoch [159/300], Batch [3/43], Training Loss: 0.00001124
2024-11-06 14:07:18,477 - INFO - Epoch [159/300], Batch [4/43], Training Loss: 0.00001552
2024-11-06 14:07:18,481 - INFO - Epoch [159/300], Batch [5/43], Training Loss: 0.00001215
2024-11-06 14:07:18,486 - INFO - Epoch [159/300], Batch [6/43], Training Loss: 0.00002646
2024-11-06 14:07:18,490 - INFO - Epoch [159/300], Batch [7/43], Training Loss: 0.00000485
2024-11-06 14:07:18,494 - INFO - Epoch [159/300], Batch [8/43], Training Loss: 0.00000420
2024-11-06 14:07:18,498 - INFO - Epoch [159/300], Batch [9/43], Training Loss: 0.00001070
2024-11-06 14:07:18,501 - INFO - Epoch [159/300], Batch [10/43], Training Loss: 0.00002059
2024-11-06 14:07:18,505 - INFO - Epoch [159/300], Batch [11/43], Training Loss: 0.00001188
2024-11-06 14:07:18,509 - INFO - Epoch [159/300], Batch [12/43], Training Loss: 0.00000524
2024-11-06 14:07:18,513 - INFO - Epoch [159/300], Batch [13/43], Training Loss: 0.00000673
2024-11-06 14:07:18,517 - INFO - Epoch [159/300], Batch [14/43], Training Loss: 0.00001372
2024-11-06 14:07:18,521 - INFO - Epoch [159/300], Batch [15/43], Training Loss: 0.00001572
2024-11-06 14:07:18,525 - INFO - Epoch [159/300], Batch [16/43], Training Loss: 0.00001298
2024-11-06 14:07:18,529 - INFO - Epoch [159/300], Batch [17/43], Training Loss: 0.00001519
2024-11-06 14:07:18,534 - INFO - Epoch [159/300], Batch [18/43], Training Loss: 0.00000867
2024-11-06 14:07:18,538 - INFO - Epoch [159/300], Batch [19/43], Training Loss: 0.00000409
2024-11-06 14:07:18,542 - INFO - Epoch [159/300], Batch [20/43], Training Loss: 0.00001239
2024-11-06 14:07:18,546 - INFO - Epoch [159/300], Batch [21/43], Training Loss: 0.00001398
2024-11-06 14:07:18,550 - INFO - Epoch [159/300], Batch [22/43], Training Loss: 0.00002844
2024-11-06 14:07:18,554 - INFO - Epoch [159/300], Batch [23/43], Training Loss: 0.00002228
2024-11-06 14:07:18,558 - INFO - Epoch [159/300], Batch [24/43], Training Loss: 0.00001291
2024-11-06 14:07:18,562 - INFO - Epoch [159/300], Batch [25/43], Training Loss: 0.00001156
2024-11-06 14:07:18,566 - INFO - Epoch [159/300], Batch [26/43], Training Loss: 0.00000979
2024-11-06 14:07:18,571 - INFO - Epoch [159/300], Batch [27/43], Training Loss: 0.00001209
2024-11-06 14:07:18,575 - INFO - Epoch [159/300], Batch [28/43], Training Loss: 0.00001447
2024-11-06 14:07:18,579 - INFO - Epoch [159/300], Batch [29/43], Training Loss: 0.00000790
2024-11-06 14:07:18,584 - INFO - Epoch [159/300], Batch [30/43], Training Loss: 0.00001455
2024-11-06 14:07:18,589 - INFO - Epoch [159/300], Batch [31/43], Training Loss: 0.00000651
2024-11-06 14:07:18,594 - INFO - Epoch [159/300], Batch [32/43], Training Loss: 0.00000943
2024-11-06 14:07:18,599 - INFO - Epoch [159/300], Batch [33/43], Training Loss: 0.00001293
2024-11-06 14:07:18,604 - INFO - Epoch [159/300], Batch [34/43], Training Loss: 0.00000911
2024-11-06 14:07:18,608 - INFO - Epoch [159/300], Batch [35/43], Training Loss: 0.00002413
2024-11-06 14:07:18,612 - INFO - Epoch [159/300], Batch [36/43], Training Loss: 0.00000362
2024-11-06 14:07:18,616 - INFO - Epoch [159/300], Batch [37/43], Training Loss: 0.00000869
2024-11-06 14:07:18,621 - INFO - Epoch [159/300], Batch [38/43], Training Loss: 0.00002675
2024-11-06 14:07:18,625 - INFO - Epoch [159/300], Batch [39/43], Training Loss: 0.00001276
2024-11-06 14:07:18,629 - INFO - Epoch [159/300], Batch [40/43], Training Loss: 0.00001389
2024-11-06 14:07:18,633 - INFO - Epoch [159/300], Batch [41/43], Training Loss: 0.00001411
2024-11-06 14:07:18,638 - INFO - Epoch [159/300], Batch [42/43], Training Loss: 0.00001877
2024-11-06 14:07:18,642 - INFO - Epoch [159/300], Batch [43/43], Training Loss: 0.00000388
2024-11-06 14:07:18,655 - INFO - Epoch [159/300], Average Training Loss: 0.00001297, Validation Loss: 0.00002258
2024-11-06 14:07:18,659 - INFO - Epoch [160/300], Batch [1/43], Training Loss: 0.00000711
2024-11-06 14:07:18,664 - INFO - Epoch [160/300], Batch [2/43], Training Loss: 0.00001499
2024-11-06 14:07:18,668 - INFO - Epoch [160/300], Batch [3/43], Training Loss: 0.00000904
2024-11-06 14:07:18,672 - INFO - Epoch [160/300], Batch [4/43], Training Loss: 0.00000789
2024-11-06 14:07:18,676 - INFO - Epoch [160/300], Batch [5/43], Training Loss: 0.00000999
2024-11-06 14:07:18,680 - INFO - Epoch [160/300], Batch [6/43], Training Loss: 0.00001599
2024-11-06 14:07:18,684 - INFO - Epoch [160/300], Batch [7/43], Training Loss: 0.00001597
2024-11-06 14:07:18,688 - INFO - Epoch [160/300], Batch [8/43], Training Loss: 0.00001178
2024-11-06 14:07:18,692 - INFO - Epoch [160/300], Batch [9/43], Training Loss: 0.00002658
2024-11-06 14:07:18,697 - INFO - Epoch [160/300], Batch [10/43], Training Loss: 0.00000756
2024-11-06 14:07:18,701 - INFO - Epoch [160/300], Batch [11/43], Training Loss: 0.00001679
2024-11-06 14:07:18,705 - INFO - Epoch [160/300], Batch [12/43], Training Loss: 0.00001392
2024-11-06 14:07:18,709 - INFO - Epoch [160/300], Batch [13/43], Training Loss: 0.00001828
2024-11-06 14:07:18,713 - INFO - Epoch [160/300], Batch [14/43], Training Loss: 0.00000356
2024-11-06 14:07:18,717 - INFO - Epoch [160/300], Batch [15/43], Training Loss: 0.00000631
2024-11-06 14:07:18,721 - INFO - Epoch [160/300], Batch [16/43], Training Loss: 0.00001056
2024-11-06 14:07:18,726 - INFO - Epoch [160/300], Batch [17/43], Training Loss: 0.00000567
2024-11-06 14:07:18,730 - INFO - Epoch [160/300], Batch [18/43], Training Loss: 0.00001066
2024-11-06 14:07:18,735 - INFO - Epoch [160/300], Batch [19/43], Training Loss: 0.00000996
2024-11-06 14:07:18,740 - INFO - Epoch [160/300], Batch [20/43], Training Loss: 0.00001482
2024-11-06 14:07:18,745 - INFO - Epoch [160/300], Batch [21/43], Training Loss: 0.00000660
2024-11-06 14:07:18,749 - INFO - Epoch [160/300], Batch [22/43], Training Loss: 0.00001202
2024-11-06 14:07:18,753 - INFO - Epoch [160/300], Batch [23/43], Training Loss: 0.00000954
2024-11-06 14:07:18,757 - INFO - Epoch [160/300], Batch [24/43], Training Loss: 0.00001178
2024-11-06 14:07:18,762 - INFO - Epoch [160/300], Batch [25/43], Training Loss: 0.00001010
2024-11-06 14:07:18,766 - INFO - Epoch [160/300], Batch [26/43], Training Loss: 0.00002468
2024-11-06 14:07:18,772 - INFO - Epoch [160/300], Batch [27/43], Training Loss: 0.00000948
2024-11-06 14:07:18,778 - INFO - Epoch [160/300], Batch [28/43], Training Loss: 0.00000978
2024-11-06 14:07:18,783 - INFO - Epoch [160/300], Batch [29/43], Training Loss: 0.00000524
2024-11-06 14:07:18,788 - INFO - Epoch [160/300], Batch [30/43], Training Loss: 0.00001833
2024-11-06 14:07:18,793 - INFO - Epoch [160/300], Batch [31/43], Training Loss: 0.00002267
2024-11-06 14:07:18,797 - INFO - Epoch [160/300], Batch [32/43], Training Loss: 0.00001403
2024-11-06 14:07:18,803 - INFO - Epoch [160/300], Batch [33/43], Training Loss: 0.00000591
2024-11-06 14:07:18,808 - INFO - Epoch [160/300], Batch [34/43], Training Loss: 0.00001205
2024-11-06 14:07:18,813 - INFO - Epoch [160/300], Batch [35/43], Training Loss: 0.00001098
2024-11-06 14:07:18,818 - INFO - Epoch [160/300], Batch [36/43], Training Loss: 0.00000374
2024-11-06 14:07:18,824 - INFO - Epoch [160/300], Batch [37/43], Training Loss: 0.00001458
2024-11-06 14:07:18,828 - INFO - Epoch [160/300], Batch [38/43], Training Loss: 0.00000454
2024-11-06 14:07:18,833 - INFO - Epoch [160/300], Batch [39/43], Training Loss: 0.00000396
2024-11-06 14:07:18,837 - INFO - Epoch [160/300], Batch [40/43], Training Loss: 0.00001491
2024-11-06 14:07:18,841 - INFO - Epoch [160/300], Batch [41/43], Training Loss: 0.00000859
2024-11-06 14:07:18,845 - INFO - Epoch [160/300], Batch [42/43], Training Loss: 0.00000898
2024-11-06 14:07:18,850 - INFO - Epoch [160/300], Batch [43/43], Training Loss: 0.00000842
2024-11-06 14:07:18,863 - INFO - Epoch [160/300], Average Training Loss: 0.00001136, Validation Loss: 0.00001507
2024-11-06 14:07:18,868 - INFO - Epoch [161/300], Batch [1/43], Training Loss: 0.00000930
2024-11-06 14:07:18,872 - INFO - Epoch [161/300], Batch [2/43], Training Loss: 0.00001444
2024-11-06 14:07:18,877 - INFO - Epoch [161/300], Batch [3/43], Training Loss: 0.00001105
2024-11-06 14:07:18,881 - INFO - Epoch [161/300], Batch [4/43], Training Loss: 0.00001413
2024-11-06 14:07:18,885 - INFO - Epoch [161/300], Batch [5/43], Training Loss: 0.00001004
2024-11-06 14:07:18,890 - INFO - Epoch [161/300], Batch [6/43], Training Loss: 0.00002605
2024-11-06 14:07:18,894 - INFO - Epoch [161/300], Batch [7/43], Training Loss: 0.00001439
2024-11-06 14:07:18,898 - INFO - Epoch [161/300], Batch [8/43], Training Loss: 0.00000834
2024-11-06 14:07:18,902 - INFO - Epoch [161/300], Batch [9/43], Training Loss: 0.00000586
2024-11-06 14:07:18,908 - INFO - Epoch [161/300], Batch [10/43], Training Loss: 0.00001364
2024-11-06 14:07:18,913 - INFO - Epoch [161/300], Batch [11/43], Training Loss: 0.00000990
2024-11-06 14:07:18,918 - INFO - Epoch [161/300], Batch [12/43], Training Loss: 0.00000567
2024-11-06 14:07:18,922 - INFO - Epoch [161/300], Batch [13/43], Training Loss: 0.00001614
2024-11-06 14:07:18,925 - INFO - Epoch [161/300], Batch [14/43], Training Loss: 0.00000857
2024-11-06 14:07:18,929 - INFO - Epoch [161/300], Batch [15/43], Training Loss: 0.00001264
2024-11-06 14:07:18,933 - INFO - Epoch [161/300], Batch [16/43], Training Loss: 0.00000983
2024-11-06 14:07:18,936 - INFO - Epoch [161/300], Batch [17/43], Training Loss: 0.00001544
2024-11-06 14:07:18,940 - INFO - Epoch [161/300], Batch [18/43], Training Loss: 0.00000675
2024-11-06 14:07:18,945 - INFO - Epoch [161/300], Batch [19/43], Training Loss: 0.00000492
2024-11-06 14:07:18,949 - INFO - Epoch [161/300], Batch [20/43], Training Loss: 0.00000818
2024-11-06 14:07:18,953 - INFO - Epoch [161/300], Batch [21/43], Training Loss: 0.00001634
2024-11-06 14:07:18,958 - INFO - Epoch [161/300], Batch [22/43], Training Loss: 0.00001730
2024-11-06 14:07:18,963 - INFO - Epoch [161/300], Batch [23/43], Training Loss: 0.00001306
2024-11-06 14:07:18,967 - INFO - Epoch [161/300], Batch [24/43], Training Loss: 0.00002833
2024-11-06 14:07:18,971 - INFO - Epoch [161/300], Batch [25/43], Training Loss: 0.00001333
2024-11-06 14:07:18,975 - INFO - Epoch [161/300], Batch [26/43], Training Loss: 0.00000763
2024-11-06 14:07:18,979 - INFO - Epoch [161/300], Batch [27/43], Training Loss: 0.00001674
2024-11-06 14:07:18,983 - INFO - Epoch [161/300], Batch [28/43], Training Loss: 0.00000507
2024-11-06 14:07:18,988 - INFO - Epoch [161/300], Batch [29/43], Training Loss: 0.00001724
2024-11-06 14:07:18,992 - INFO - Epoch [161/300], Batch [30/43], Training Loss: 0.00001976
2024-11-06 14:07:18,997 - INFO - Epoch [161/300], Batch [31/43], Training Loss: 0.00000800
2024-11-06 14:07:19,001 - INFO - Epoch [161/300], Batch [32/43], Training Loss: 0.00004247
2024-11-06 14:07:19,006 - INFO - Epoch [161/300], Batch [33/43], Training Loss: 0.00000717
2024-11-06 14:07:19,010 - INFO - Epoch [161/300], Batch [34/43], Training Loss: 0.00001099
2024-11-06 14:07:19,014 - INFO - Epoch [161/300], Batch [35/43], Training Loss: 0.00000942
2024-11-06 14:07:19,019 - INFO - Epoch [161/300], Batch [36/43], Training Loss: 0.00000617
2024-11-06 14:07:19,023 - INFO - Epoch [161/300], Batch [37/43], Training Loss: 0.00001038
2024-11-06 14:07:19,027 - INFO - Epoch [161/300], Batch [38/43], Training Loss: 0.00000367
2024-11-06 14:07:19,032 - INFO - Epoch [161/300], Batch [39/43], Training Loss: 0.00000651
2024-11-06 14:07:19,037 - INFO - Epoch [161/300], Batch [40/43], Training Loss: 0.00000750
2024-11-06 14:07:19,042 - INFO - Epoch [161/300], Batch [41/43], Training Loss: 0.00002091
2024-11-06 14:07:19,047 - INFO - Epoch [161/300], Batch [42/43], Training Loss: 0.00001392
2024-11-06 14:07:19,053 - INFO - Epoch [161/300], Batch [43/43], Training Loss: 0.00000426
2024-11-06 14:07:19,068 - INFO - Epoch [161/300], Average Training Loss: 0.00001236, Validation Loss: 0.00001646
2024-11-06 14:07:19,072 - INFO - Epoch [162/300], Batch [1/43], Training Loss: 0.00001434
2024-11-06 14:07:19,078 - INFO - Epoch [162/300], Batch [2/43], Training Loss: 0.00000690
2024-11-06 14:07:19,085 - INFO - Epoch [162/300], Batch [3/43], Training Loss: 0.00000558
2024-11-06 14:07:19,089 - INFO - Epoch [162/300], Batch [4/43], Training Loss: 0.00000585
2024-11-06 14:07:19,094 - INFO - Epoch [162/300], Batch [5/43], Training Loss: 0.00001386
2024-11-06 14:07:19,100 - INFO - Epoch [162/300], Batch [6/43], Training Loss: 0.00001490
2024-11-06 14:07:19,105 - INFO - Epoch [162/300], Batch [7/43], Training Loss: 0.00001360
2024-11-06 14:07:19,110 - INFO - Epoch [162/300], Batch [8/43], Training Loss: 0.00000576
2024-11-06 14:07:19,114 - INFO - Epoch [162/300], Batch [9/43], Training Loss: 0.00001020
2024-11-06 14:07:19,119 - INFO - Epoch [162/300], Batch [10/43], Training Loss: 0.00000554
2024-11-06 14:07:19,123 - INFO - Epoch [162/300], Batch [11/43], Training Loss: 0.00000738
2024-11-06 14:07:19,129 - INFO - Epoch [162/300], Batch [12/43], Training Loss: 0.00000593
2024-11-06 14:07:19,134 - INFO - Epoch [162/300], Batch [13/43], Training Loss: 0.00000606
2024-11-06 14:07:19,138 - INFO - Epoch [162/300], Batch [14/43], Training Loss: 0.00002305
2024-11-06 14:07:19,142 - INFO - Epoch [162/300], Batch [15/43], Training Loss: 0.00002680
2024-11-06 14:07:19,146 - INFO - Epoch [162/300], Batch [16/43], Training Loss: 0.00000685
2024-11-06 14:07:19,151 - INFO - Epoch [162/300], Batch [17/43], Training Loss: 0.00001526
2024-11-06 14:07:19,157 - INFO - Epoch [162/300], Batch [18/43], Training Loss: 0.00000810
2024-11-06 14:07:19,164 - INFO - Epoch [162/300], Batch [19/43], Training Loss: 0.00001662
2024-11-06 14:07:19,169 - INFO - Epoch [162/300], Batch [20/43], Training Loss: 0.00001425
2024-11-06 14:07:19,174 - INFO - Epoch [162/300], Batch [21/43], Training Loss: 0.00001162
2024-11-06 14:07:19,179 - INFO - Epoch [162/300], Batch [22/43], Training Loss: 0.00000893
2024-11-06 14:07:19,184 - INFO - Epoch [162/300], Batch [23/43], Training Loss: 0.00001062
2024-11-06 14:07:19,189 - INFO - Epoch [162/300], Batch [24/43], Training Loss: 0.00001397
2024-11-06 14:07:19,194 - INFO - Epoch [162/300], Batch [25/43], Training Loss: 0.00001092
2024-11-06 14:07:19,199 - INFO - Epoch [162/300], Batch [26/43], Training Loss: 0.00000690
2024-11-06 14:07:19,204 - INFO - Epoch [162/300], Batch [27/43], Training Loss: 0.00002085
2024-11-06 14:07:19,209 - INFO - Epoch [162/300], Batch [28/43], Training Loss: 0.00000938
2024-11-06 14:07:19,214 - INFO - Epoch [162/300], Batch [29/43], Training Loss: 0.00000865
2024-11-06 14:07:19,219 - INFO - Epoch [162/300], Batch [30/43], Training Loss: 0.00002461
2024-11-06 14:07:19,223 - INFO - Epoch [162/300], Batch [31/43], Training Loss: 0.00001104
2024-11-06 14:07:19,228 - INFO - Epoch [162/300], Batch [32/43], Training Loss: 0.00000706
2024-11-06 14:07:19,234 - INFO - Epoch [162/300], Batch [33/43], Training Loss: 0.00002539
2024-11-06 14:07:19,237 - INFO - Epoch [162/300], Batch [34/43], Training Loss: 0.00001102
2024-11-06 14:07:19,242 - INFO - Epoch [162/300], Batch [35/43], Training Loss: 0.00001817
2024-11-06 14:07:19,247 - INFO - Epoch [162/300], Batch [36/43], Training Loss: 0.00001908
2024-11-06 14:07:19,252 - INFO - Epoch [162/300], Batch [37/43], Training Loss: 0.00000644
2024-11-06 14:07:19,257 - INFO - Epoch [162/300], Batch [38/43], Training Loss: 0.00000724
2024-11-06 14:07:19,262 - INFO - Epoch [162/300], Batch [39/43], Training Loss: 0.00000744
2024-11-06 14:07:19,267 - INFO - Epoch [162/300], Batch [40/43], Training Loss: 0.00001508
2024-11-06 14:07:19,271 - INFO - Epoch [162/300], Batch [41/43], Training Loss: 0.00001653
2024-11-06 14:07:19,275 - INFO - Epoch [162/300], Batch [42/43], Training Loss: 0.00001187
2024-11-06 14:07:19,280 - INFO - Epoch [162/300], Batch [43/43], Training Loss: 0.00001406
2024-11-06 14:07:19,293 - INFO - Epoch [162/300], Average Training Loss: 0.00001218, Validation Loss: 0.00001604
2024-11-06 14:07:19,297 - INFO - Epoch [163/300], Batch [1/43], Training Loss: 0.00001266
2024-11-06 14:07:19,302 - INFO - Epoch [163/300], Batch [2/43], Training Loss: 0.00000312
2024-11-06 14:07:19,305 - INFO - Epoch [163/300], Batch [3/43], Training Loss: 0.00000674
2024-11-06 14:07:19,308 - INFO - Epoch [163/300], Batch [4/43], Training Loss: 0.00001118
2024-11-06 14:07:19,312 - INFO - Epoch [163/300], Batch [5/43], Training Loss: 0.00001498
2024-11-06 14:07:19,315 - INFO - Epoch [163/300], Batch [6/43], Training Loss: 0.00001607
2024-11-06 14:07:19,318 - INFO - Epoch [163/300], Batch [7/43], Training Loss: 0.00002078
2024-11-06 14:07:19,322 - INFO - Epoch [163/300], Batch [8/43], Training Loss: 0.00000373
2024-11-06 14:07:19,326 - INFO - Epoch [163/300], Batch [9/43], Training Loss: 0.00000983
2024-11-06 14:07:19,330 - INFO - Epoch [163/300], Batch [10/43], Training Loss: 0.00000789
2024-11-06 14:07:19,333 - INFO - Epoch [163/300], Batch [11/43], Training Loss: 0.00001542
2024-11-06 14:07:19,336 - INFO - Epoch [163/300], Batch [12/43], Training Loss: 0.00000695
2024-11-06 14:07:19,339 - INFO - Epoch [163/300], Batch [13/43], Training Loss: 0.00000696
2024-11-06 14:07:19,342 - INFO - Epoch [163/300], Batch [14/43], Training Loss: 0.00000610
2024-11-06 14:07:19,345 - INFO - Epoch [163/300], Batch [15/43], Training Loss: 0.00000916
2024-11-06 14:07:19,349 - INFO - Epoch [163/300], Batch [16/43], Training Loss: 0.00001503
2024-11-06 14:07:19,353 - INFO - Epoch [163/300], Batch [17/43], Training Loss: 0.00001505
2024-11-06 14:07:19,357 - INFO - Epoch [163/300], Batch [18/43], Training Loss: 0.00001094
2024-11-06 14:07:19,360 - INFO - Epoch [163/300], Batch [19/43], Training Loss: 0.00001784
2024-11-06 14:07:19,364 - INFO - Epoch [163/300], Batch [20/43], Training Loss: 0.00000849
2024-11-06 14:07:19,367 - INFO - Epoch [163/300], Batch [21/43], Training Loss: 0.00002039
2024-11-06 14:07:19,371 - INFO - Epoch [163/300], Batch [22/43], Training Loss: 0.00001830
2024-11-06 14:07:19,375 - INFO - Epoch [163/300], Batch [23/43], Training Loss: 0.00001864
2024-11-06 14:07:19,378 - INFO - Epoch [163/300], Batch [24/43], Training Loss: 0.00000412
2024-11-06 14:07:19,381 - INFO - Epoch [163/300], Batch [25/43], Training Loss: 0.00000462
2024-11-06 14:07:19,385 - INFO - Epoch [163/300], Batch [26/43], Training Loss: 0.00001553
2024-11-06 14:07:19,388 - INFO - Epoch [163/300], Batch [27/43], Training Loss: 0.00001117
2024-11-06 14:07:19,391 - INFO - Epoch [163/300], Batch [28/43], Training Loss: 0.00000848
2024-11-06 14:07:19,394 - INFO - Epoch [163/300], Batch [29/43], Training Loss: 0.00000824
2024-11-06 14:07:19,397 - INFO - Epoch [163/300], Batch [30/43], Training Loss: 0.00000460
2024-11-06 14:07:19,401 - INFO - Epoch [163/300], Batch [31/43], Training Loss: 0.00000757
2024-11-06 14:07:19,404 - INFO - Epoch [163/300], Batch [32/43], Training Loss: 0.00000870
2024-11-06 14:07:19,407 - INFO - Epoch [163/300], Batch [33/43], Training Loss: 0.00000998
2024-11-06 14:07:19,411 - INFO - Epoch [163/300], Batch [34/43], Training Loss: 0.00000231
2024-11-06 14:07:19,413 - INFO - Epoch [163/300], Batch [35/43], Training Loss: 0.00001030
2024-11-06 14:07:19,416 - INFO - Epoch [163/300], Batch [36/43], Training Loss: 0.00000674
2024-11-06 14:07:19,419 - INFO - Epoch [163/300], Batch [37/43], Training Loss: 0.00000690
2024-11-06 14:07:19,423 - INFO - Epoch [163/300], Batch [38/43], Training Loss: 0.00000601
2024-11-06 14:07:19,427 - INFO - Epoch [163/300], Batch [39/43], Training Loss: 0.00002093
2024-11-06 14:07:19,431 - INFO - Epoch [163/300], Batch [40/43], Training Loss: 0.00000976
2024-11-06 14:07:19,435 - INFO - Epoch [163/300], Batch [41/43], Training Loss: 0.00000960
2024-11-06 14:07:19,439 - INFO - Epoch [163/300], Batch [42/43], Training Loss: 0.00001894
2024-11-06 14:07:19,445 - INFO - Epoch [163/300], Batch [43/43], Training Loss: 0.00000476
2024-11-06 14:07:19,458 - INFO - Epoch [163/300], Average Training Loss: 0.00001059, Validation Loss: 0.00001415
2024-11-06 14:07:19,462 - INFO - Epoch [164/300], Batch [1/43], Training Loss: 0.00002074
2024-11-06 14:07:19,466 - INFO - Epoch [164/300], Batch [2/43], Training Loss: 0.00000479
2024-11-06 14:07:19,470 - INFO - Epoch [164/300], Batch [3/43], Training Loss: 0.00001463
2024-11-06 14:07:19,474 - INFO - Epoch [164/300], Batch [4/43], Training Loss: 0.00000299
2024-11-06 14:07:19,477 - INFO - Epoch [164/300], Batch [5/43], Training Loss: 0.00001470
2024-11-06 14:07:19,481 - INFO - Epoch [164/300], Batch [6/43], Training Loss: 0.00000813
2024-11-06 14:07:19,484 - INFO - Epoch [164/300], Batch [7/43], Training Loss: 0.00000654
2024-11-06 14:07:19,488 - INFO - Epoch [164/300], Batch [8/43], Training Loss: 0.00000421
2024-11-06 14:07:19,492 - INFO - Epoch [164/300], Batch [9/43], Training Loss: 0.00001015
2024-11-06 14:07:19,495 - INFO - Epoch [164/300], Batch [10/43], Training Loss: 0.00000601
2024-11-06 14:07:19,498 - INFO - Epoch [164/300], Batch [11/43], Training Loss: 0.00001044
2024-11-06 14:07:19,502 - INFO - Epoch [164/300], Batch [12/43], Training Loss: 0.00003067
2024-11-06 14:07:19,505 - INFO - Epoch [164/300], Batch [13/43], Training Loss: 0.00000979
2024-11-06 14:07:19,508 - INFO - Epoch [164/300], Batch [14/43], Training Loss: 0.00001059
2024-11-06 14:07:19,512 - INFO - Epoch [164/300], Batch [15/43], Training Loss: 0.00002166
2024-11-06 14:07:19,516 - INFO - Epoch [164/300], Batch [16/43], Training Loss: 0.00000659
2024-11-06 14:07:19,518 - INFO - Epoch [164/300], Batch [17/43], Training Loss: 0.00001450
2024-11-06 14:07:19,522 - INFO - Epoch [164/300], Batch [18/43], Training Loss: 0.00000648
2024-11-06 14:07:19,526 - INFO - Epoch [164/300], Batch [19/43], Training Loss: 0.00000684
2024-11-06 14:07:19,529 - INFO - Epoch [164/300], Batch [20/43], Training Loss: 0.00001170
2024-11-06 14:07:19,533 - INFO - Epoch [164/300], Batch [21/43], Training Loss: 0.00001276
2024-11-06 14:07:19,537 - INFO - Epoch [164/300], Batch [22/43], Training Loss: 0.00000861
2024-11-06 14:07:19,541 - INFO - Epoch [164/300], Batch [23/43], Training Loss: 0.00002844
2024-11-06 14:07:19,544 - INFO - Epoch [164/300], Batch [24/43], Training Loss: 0.00000729
2024-11-06 14:07:19,547 - INFO - Epoch [164/300], Batch [25/43], Training Loss: 0.00000954
2024-11-06 14:07:19,551 - INFO - Epoch [164/300], Batch [26/43], Training Loss: 0.00001536
2024-11-06 14:07:19,555 - INFO - Epoch [164/300], Batch [27/43], Training Loss: 0.00000637
2024-11-06 14:07:19,558 - INFO - Epoch [164/300], Batch [28/43], Training Loss: 0.00000941
2024-11-06 14:07:19,561 - INFO - Epoch [164/300], Batch [29/43], Training Loss: 0.00002042
2024-11-06 14:07:19,565 - INFO - Epoch [164/300], Batch [30/43], Training Loss: 0.00001077
2024-11-06 14:07:19,568 - INFO - Epoch [164/300], Batch [31/43], Training Loss: 0.00001153
2024-11-06 14:07:19,572 - INFO - Epoch [164/300], Batch [32/43], Training Loss: 0.00001504
2024-11-06 14:07:19,576 - INFO - Epoch [164/300], Batch [33/43], Training Loss: 0.00000493
2024-11-06 14:07:19,579 - INFO - Epoch [164/300], Batch [34/43], Training Loss: 0.00000999
2024-11-06 14:07:19,583 - INFO - Epoch [164/300], Batch [35/43], Training Loss: 0.00001312
2024-11-06 14:07:19,586 - INFO - Epoch [164/300], Batch [36/43], Training Loss: 0.00001743
2024-11-06 14:07:19,590 - INFO - Epoch [164/300], Batch [37/43], Training Loss: 0.00000510
2024-11-06 14:07:19,593 - INFO - Epoch [164/300], Batch [38/43], Training Loss: 0.00000928
2024-11-06 14:07:19,597 - INFO - Epoch [164/300], Batch [39/43], Training Loss: 0.00000900
2024-11-06 14:07:19,601 - INFO - Epoch [164/300], Batch [40/43], Training Loss: 0.00001038
2024-11-06 14:07:19,603 - INFO - Epoch [164/300], Batch [41/43], Training Loss: 0.00001557
2024-11-06 14:07:19,606 - INFO - Epoch [164/300], Batch [42/43], Training Loss: 0.00003670
2024-11-06 14:07:19,610 - INFO - Epoch [164/300], Batch [43/43], Training Loss: 0.00002053
2024-11-06 14:07:19,620 - INFO - Epoch [164/300], Average Training Loss: 0.00001232, Validation Loss: 0.00001361
2024-11-06 14:07:19,624 - INFO - Epoch [165/300], Batch [1/43], Training Loss: 0.00000394
2024-11-06 14:07:19,628 - INFO - Epoch [165/300], Batch [2/43], Training Loss: 0.00001458
2024-11-06 14:07:19,632 - INFO - Epoch [165/300], Batch [3/43], Training Loss: 0.00000658
2024-11-06 14:07:19,637 - INFO - Epoch [165/300], Batch [4/43], Training Loss: 0.00001272
2024-11-06 14:07:19,641 - INFO - Epoch [165/300], Batch [5/43], Training Loss: 0.00001226
2024-11-06 14:07:19,645 - INFO - Epoch [165/300], Batch [6/43], Training Loss: 0.00000564
2024-11-06 14:07:19,649 - INFO - Epoch [165/300], Batch [7/43], Training Loss: 0.00002364
2024-11-06 14:07:19,653 - INFO - Epoch [165/300], Batch [8/43], Training Loss: 0.00002627
2024-11-06 14:07:19,657 - INFO - Epoch [165/300], Batch [9/43], Training Loss: 0.00000811
2024-11-06 14:07:19,661 - INFO - Epoch [165/300], Batch [10/43], Training Loss: 0.00002638
2024-11-06 14:07:19,665 - INFO - Epoch [165/300], Batch [11/43], Training Loss: 0.00001477
2024-11-06 14:07:19,668 - INFO - Epoch [165/300], Batch [12/43], Training Loss: 0.00000668
2024-11-06 14:07:19,672 - INFO - Epoch [165/300], Batch [13/43], Training Loss: 0.00001856
2024-11-06 14:07:19,675 - INFO - Epoch [165/300], Batch [14/43], Training Loss: 0.00000717
2024-11-06 14:07:19,679 - INFO - Epoch [165/300], Batch [15/43], Training Loss: 0.00002448
2024-11-06 14:07:19,684 - INFO - Epoch [165/300], Batch [16/43], Training Loss: 0.00001056
2024-11-06 14:07:19,688 - INFO - Epoch [165/300], Batch [17/43], Training Loss: 0.00000853
2024-11-06 14:07:19,691 - INFO - Epoch [165/300], Batch [18/43], Training Loss: 0.00000700
2024-11-06 14:07:19,696 - INFO - Epoch [165/300], Batch [19/43], Training Loss: 0.00001538
2024-11-06 14:07:19,700 - INFO - Epoch [165/300], Batch [20/43], Training Loss: 0.00001268
2024-11-06 14:07:19,704 - INFO - Epoch [165/300], Batch [21/43], Training Loss: 0.00000413
2024-11-06 14:07:19,708 - INFO - Epoch [165/300], Batch [22/43], Training Loss: 0.00000971
2024-11-06 14:07:19,712 - INFO - Epoch [165/300], Batch [23/43], Training Loss: 0.00000365
2024-11-06 14:07:19,716 - INFO - Epoch [165/300], Batch [24/43], Training Loss: 0.00001765
2024-11-06 14:07:19,720 - INFO - Epoch [165/300], Batch [25/43], Training Loss: 0.00000423
2024-11-06 14:07:19,724 - INFO - Epoch [165/300], Batch [26/43], Training Loss: 0.00001705
2024-11-06 14:07:19,727 - INFO - Epoch [165/300], Batch [27/43], Training Loss: 0.00000591
2024-11-06 14:07:19,730 - INFO - Epoch [165/300], Batch [28/43], Training Loss: 0.00000972
2024-11-06 14:07:19,733 - INFO - Epoch [165/300], Batch [29/43], Training Loss: 0.00001840
2024-11-06 14:07:19,737 - INFO - Epoch [165/300], Batch [30/43], Training Loss: 0.00001919
2024-11-06 14:07:19,741 - INFO - Epoch [165/300], Batch [31/43], Training Loss: 0.00001518
2024-11-06 14:07:19,745 - INFO - Epoch [165/300], Batch [32/43], Training Loss: 0.00001027
2024-11-06 14:07:19,749 - INFO - Epoch [165/300], Batch [33/43], Training Loss: 0.00000495
2024-11-06 14:07:19,753 - INFO - Epoch [165/300], Batch [34/43], Training Loss: 0.00001042
2024-11-06 14:07:19,756 - INFO - Epoch [165/300], Batch [35/43], Training Loss: 0.00001686
2024-11-06 14:07:19,759 - INFO - Epoch [165/300], Batch [36/43], Training Loss: 0.00000970
2024-11-06 14:07:19,762 - INFO - Epoch [165/300], Batch [37/43], Training Loss: 0.00001063
2024-11-06 14:07:19,765 - INFO - Epoch [165/300], Batch [38/43], Training Loss: 0.00001164
2024-11-06 14:07:19,769 - INFO - Epoch [165/300], Batch [39/43], Training Loss: 0.00002531
2024-11-06 14:07:19,774 - INFO - Epoch [165/300], Batch [40/43], Training Loss: 0.00001224
2024-11-06 14:07:19,777 - INFO - Epoch [165/300], Batch [41/43], Training Loss: 0.00001004
2024-11-06 14:07:19,782 - INFO - Epoch [165/300], Batch [42/43], Training Loss: 0.00000683
2024-11-06 14:07:19,787 - INFO - Epoch [165/300], Batch [43/43], Training Loss: 0.00001130
2024-11-06 14:07:19,799 - INFO - Epoch [165/300], Average Training Loss: 0.00001235, Validation Loss: 0.00001379
2024-11-06 14:07:19,803 - INFO - Epoch [166/300], Batch [1/43], Training Loss: 0.00000428
2024-11-06 14:07:19,807 - INFO - Epoch [166/300], Batch [2/43], Training Loss: 0.00001804
2024-11-06 14:07:19,811 - INFO - Epoch [166/300], Batch [3/43], Training Loss: 0.00000798
2024-11-06 14:07:19,814 - INFO - Epoch [166/300], Batch [4/43], Training Loss: 0.00000304
2024-11-06 14:07:19,819 - INFO - Epoch [166/300], Batch [5/43], Training Loss: 0.00002034
2024-11-06 14:07:19,823 - INFO - Epoch [166/300], Batch [6/43], Training Loss: 0.00000388
2024-11-06 14:07:19,826 - INFO - Epoch [166/300], Batch [7/43], Training Loss: 0.00000950
2024-11-06 14:07:19,830 - INFO - Epoch [166/300], Batch [8/43], Training Loss: 0.00002258
2024-11-06 14:07:19,833 - INFO - Epoch [166/300], Batch [9/43], Training Loss: 0.00000577
2024-11-06 14:07:19,837 - INFO - Epoch [166/300], Batch [10/43], Training Loss: 0.00001509
2024-11-06 14:07:19,841 - INFO - Epoch [166/300], Batch [11/43], Training Loss: 0.00000781
2024-11-06 14:07:19,844 - INFO - Epoch [166/300], Batch [12/43], Training Loss: 0.00000627
2024-11-06 14:07:19,848 - INFO - Epoch [166/300], Batch [13/43], Training Loss: 0.00000743
2024-11-06 14:07:19,852 - INFO - Epoch [166/300], Batch [14/43], Training Loss: 0.00000983
2024-11-06 14:07:19,855 - INFO - Epoch [166/300], Batch [15/43], Training Loss: 0.00001040
2024-11-06 14:07:19,858 - INFO - Epoch [166/300], Batch [16/43], Training Loss: 0.00000329
2024-11-06 14:07:19,861 - INFO - Epoch [166/300], Batch [17/43], Training Loss: 0.00001342
2024-11-06 14:07:19,864 - INFO - Epoch [166/300], Batch [18/43], Training Loss: 0.00001023
2024-11-06 14:07:19,868 - INFO - Epoch [166/300], Batch [19/43], Training Loss: 0.00002906
2024-11-06 14:07:19,871 - INFO - Epoch [166/300], Batch [20/43], Training Loss: 0.00000845
2024-11-06 14:07:19,874 - INFO - Epoch [166/300], Batch [21/43], Training Loss: 0.00000984
2024-11-06 14:07:19,878 - INFO - Epoch [166/300], Batch [22/43], Training Loss: 0.00001664
2024-11-06 14:07:19,881 - INFO - Epoch [166/300], Batch [23/43], Training Loss: 0.00002243
2024-11-06 14:07:19,885 - INFO - Epoch [166/300], Batch [24/43], Training Loss: 0.00000628
2024-11-06 14:07:19,889 - INFO - Epoch [166/300], Batch [25/43], Training Loss: 0.00000636
2024-11-06 14:07:19,891 - INFO - Epoch [166/300], Batch [26/43], Training Loss: 0.00000884
2024-11-06 14:07:19,895 - INFO - Epoch [166/300], Batch [27/43], Training Loss: 0.00001479
2024-11-06 14:07:19,898 - INFO - Epoch [166/300], Batch [28/43], Training Loss: 0.00002224
2024-11-06 14:07:19,902 - INFO - Epoch [166/300], Batch [29/43], Training Loss: 0.00001973
2024-11-06 14:07:19,905 - INFO - Epoch [166/300], Batch [30/43], Training Loss: 0.00003350
2024-11-06 14:07:19,907 - INFO - Epoch [166/300], Batch [31/43], Training Loss: 0.00001780
2024-11-06 14:07:19,913 - INFO - Epoch [166/300], Batch [32/43], Training Loss: 0.00002062
2024-11-06 14:07:19,917 - INFO - Epoch [166/300], Batch [33/43], Training Loss: 0.00000811
2024-11-06 14:07:19,922 - INFO - Epoch [166/300], Batch [34/43], Training Loss: 0.00001314
2024-11-06 14:07:19,926 - INFO - Epoch [166/300], Batch [35/43], Training Loss: 0.00001207
2024-11-06 14:07:19,930 - INFO - Epoch [166/300], Batch [36/43], Training Loss: 0.00000783
2024-11-06 14:07:19,934 - INFO - Epoch [166/300], Batch [37/43], Training Loss: 0.00000976
2024-11-06 14:07:19,938 - INFO - Epoch [166/300], Batch [38/43], Training Loss: 0.00002320
2024-11-06 14:07:19,942 - INFO - Epoch [166/300], Batch [39/43], Training Loss: 0.00000495
2024-11-06 14:07:19,945 - INFO - Epoch [166/300], Batch [40/43], Training Loss: 0.00000621
2024-11-06 14:07:19,949 - INFO - Epoch [166/300], Batch [41/43], Training Loss: 0.00001514
2024-11-06 14:07:19,953 - INFO - Epoch [166/300], Batch [42/43], Training Loss: 0.00001250
2024-11-06 14:07:19,956 - INFO - Epoch [166/300], Batch [43/43], Training Loss: 0.00000878
2024-11-06 14:07:19,968 - INFO - Epoch [166/300], Average Training Loss: 0.00001250, Validation Loss: 0.00001740
2024-11-06 14:07:19,972 - INFO - Epoch [167/300], Batch [1/43], Training Loss: 0.00001612
2024-11-06 14:07:19,976 - INFO - Epoch [167/300], Batch [2/43], Training Loss: 0.00002647
2024-11-06 14:07:19,979 - INFO - Epoch [167/300], Batch [3/43], Training Loss: 0.00001098
2024-11-06 14:07:19,983 - INFO - Epoch [167/300], Batch [4/43], Training Loss: 0.00000841
2024-11-06 14:07:19,987 - INFO - Epoch [167/300], Batch [5/43], Training Loss: 0.00002191
2024-11-06 14:07:19,990 - INFO - Epoch [167/300], Batch [6/43], Training Loss: 0.00001065
2024-11-06 14:07:19,993 - INFO - Epoch [167/300], Batch [7/43], Training Loss: 0.00000665
2024-11-06 14:07:19,996 - INFO - Epoch [167/300], Batch [8/43], Training Loss: 0.00002467
2024-11-06 14:07:19,999 - INFO - Epoch [167/300], Batch [9/43], Training Loss: 0.00001007
2024-11-06 14:07:20,002 - INFO - Epoch [167/300], Batch [10/43], Training Loss: 0.00000571
2024-11-06 14:07:20,005 - INFO - Epoch [167/300], Batch [11/43], Training Loss: 0.00000482
2024-11-06 14:07:20,009 - INFO - Epoch [167/300], Batch [12/43], Training Loss: 0.00001450
2024-11-06 14:07:20,012 - INFO - Epoch [167/300], Batch [13/43], Training Loss: 0.00000381
2024-11-06 14:07:20,015 - INFO - Epoch [167/300], Batch [14/43], Training Loss: 0.00002050
2024-11-06 14:07:20,017 - INFO - Epoch [167/300], Batch [15/43], Training Loss: 0.00000582
2024-11-06 14:07:20,021 - INFO - Epoch [167/300], Batch [16/43], Training Loss: 0.00000730
2024-11-06 14:07:20,024 - INFO - Epoch [167/300], Batch [17/43], Training Loss: 0.00001379
2024-11-06 14:07:20,027 - INFO - Epoch [167/300], Batch [18/43], Training Loss: 0.00000312
2024-11-06 14:07:20,030 - INFO - Epoch [167/300], Batch [19/43], Training Loss: 0.00001425
2024-11-06 14:07:20,033 - INFO - Epoch [167/300], Batch [20/43], Training Loss: 0.00000786
2024-11-06 14:07:20,037 - INFO - Epoch [167/300], Batch [21/43], Training Loss: 0.00001285
2024-11-06 14:07:20,040 - INFO - Epoch [167/300], Batch [22/43], Training Loss: 0.00001018
2024-11-06 14:07:20,043 - INFO - Epoch [167/300], Batch [23/43], Training Loss: 0.00000739
2024-11-06 14:07:20,046 - INFO - Epoch [167/300], Batch [24/43], Training Loss: 0.00000708
2024-11-06 14:07:20,049 - INFO - Epoch [167/300], Batch [25/43], Training Loss: 0.00000572
2024-11-06 14:07:20,053 - INFO - Epoch [167/300], Batch [26/43], Training Loss: 0.00000953
2024-11-06 14:07:20,057 - INFO - Epoch [167/300], Batch [27/43], Training Loss: 0.00001749
2024-11-06 14:07:20,060 - INFO - Epoch [167/300], Batch [28/43], Training Loss: 0.00000151
2024-11-06 14:07:20,065 - INFO - Epoch [167/300], Batch [29/43], Training Loss: 0.00000727
2024-11-06 14:07:20,069 - INFO - Epoch [167/300], Batch [30/43], Training Loss: 0.00000801
2024-11-06 14:07:20,073 - INFO - Epoch [167/300], Batch [31/43], Training Loss: 0.00000887
2024-11-06 14:07:20,077 - INFO - Epoch [167/300], Batch [32/43], Training Loss: 0.00000363
2024-11-06 14:07:20,081 - INFO - Epoch [167/300], Batch [33/43], Training Loss: 0.00000846
2024-11-06 14:07:20,086 - INFO - Epoch [167/300], Batch [34/43], Training Loss: 0.00000903
2024-11-06 14:07:20,090 - INFO - Epoch [167/300], Batch [35/43], Training Loss: 0.00000458
2024-11-06 14:07:20,093 - INFO - Epoch [167/300], Batch [36/43], Training Loss: 0.00000380
2024-11-06 14:07:20,097 - INFO - Epoch [167/300], Batch [37/43], Training Loss: 0.00001668
2024-11-06 14:07:20,101 - INFO - Epoch [167/300], Batch [38/43], Training Loss: 0.00001038
2024-11-06 14:07:20,105 - INFO - Epoch [167/300], Batch [39/43], Training Loss: 0.00001090
2024-11-06 14:07:20,108 - INFO - Epoch [167/300], Batch [40/43], Training Loss: 0.00000589
2024-11-06 14:07:20,111 - INFO - Epoch [167/300], Batch [41/43], Training Loss: 0.00001228
2024-11-06 14:07:20,114 - INFO - Epoch [167/300], Batch [42/43], Training Loss: 0.00001205
2024-11-06 14:07:20,117 - INFO - Epoch [167/300], Batch [43/43], Training Loss: 0.00002109
2024-11-06 14:07:20,129 - INFO - Epoch [167/300], Average Training Loss: 0.00001051, Validation Loss: 0.00001425
2024-11-06 14:07:20,134 - INFO - Epoch [168/300], Batch [1/43], Training Loss: 0.00000732
2024-11-06 14:07:20,138 - INFO - Epoch [168/300], Batch [2/43], Training Loss: 0.00001622
2024-11-06 14:07:20,141 - INFO - Epoch [168/300], Batch [3/43], Training Loss: 0.00000661
2024-11-06 14:07:20,146 - INFO - Epoch [168/300], Batch [4/43], Training Loss: 0.00000910
2024-11-06 14:07:20,150 - INFO - Epoch [168/300], Batch [5/43], Training Loss: 0.00002469
2024-11-06 14:07:20,154 - INFO - Epoch [168/300], Batch [6/43], Training Loss: 0.00001471
2024-11-06 14:07:20,158 - INFO - Epoch [168/300], Batch [7/43], Training Loss: 0.00000825
2024-11-06 14:07:20,162 - INFO - Epoch [168/300], Batch [8/43], Training Loss: 0.00000777
2024-11-06 14:07:20,165 - INFO - Epoch [168/300], Batch [9/43], Training Loss: 0.00001706
2024-11-06 14:07:20,168 - INFO - Epoch [168/300], Batch [10/43], Training Loss: 0.00001166
2024-11-06 14:07:20,173 - INFO - Epoch [168/300], Batch [11/43], Training Loss: 0.00000408
2024-11-06 14:07:20,175 - INFO - Epoch [168/300], Batch [12/43], Training Loss: 0.00000745
2024-11-06 14:07:20,178 - INFO - Epoch [168/300], Batch [13/43], Training Loss: 0.00000912
2024-11-06 14:07:20,181 - INFO - Epoch [168/300], Batch [14/43], Training Loss: 0.00001039
2024-11-06 14:07:20,184 - INFO - Epoch [168/300], Batch [15/43], Training Loss: 0.00000987
2024-11-06 14:07:20,187 - INFO - Epoch [168/300], Batch [16/43], Training Loss: 0.00001157
2024-11-06 14:07:20,189 - INFO - Epoch [168/300], Batch [17/43], Training Loss: 0.00000715
2024-11-06 14:07:20,192 - INFO - Epoch [168/300], Batch [18/43], Training Loss: 0.00001996
2024-11-06 14:07:20,195 - INFO - Epoch [168/300], Batch [19/43], Training Loss: 0.00000551
2024-11-06 14:07:20,200 - INFO - Epoch [168/300], Batch [20/43], Training Loss: 0.00002372
2024-11-06 14:07:20,203 - INFO - Epoch [168/300], Batch [21/43], Training Loss: 0.00002290
2024-11-06 14:07:20,207 - INFO - Epoch [168/300], Batch [22/43], Training Loss: 0.00000445
2024-11-06 14:07:20,211 - INFO - Epoch [168/300], Batch [23/43], Training Loss: 0.00001558
2024-11-06 14:07:20,216 - INFO - Epoch [168/300], Batch [24/43], Training Loss: 0.00003348
2024-11-06 14:07:20,220 - INFO - Epoch [168/300], Batch [25/43], Training Loss: 0.00000931
2024-11-06 14:07:20,223 - INFO - Epoch [168/300], Batch [26/43], Training Loss: 0.00000332
2024-11-06 14:07:20,228 - INFO - Epoch [168/300], Batch [27/43], Training Loss: 0.00002347
2024-11-06 14:07:20,232 - INFO - Epoch [168/300], Batch [28/43], Training Loss: 0.00002055
2024-11-06 14:07:20,236 - INFO - Epoch [168/300], Batch [29/43], Training Loss: 0.00000721
2024-11-06 14:07:20,239 - INFO - Epoch [168/300], Batch [30/43], Training Loss: 0.00001045
2024-11-06 14:07:20,243 - INFO - Epoch [168/300], Batch [31/43], Training Loss: 0.00001096
2024-11-06 14:07:20,246 - INFO - Epoch [168/300], Batch [32/43], Training Loss: 0.00001695
2024-11-06 14:07:20,249 - INFO - Epoch [168/300], Batch [33/43], Training Loss: 0.00000631
2024-11-06 14:07:20,252 - INFO - Epoch [168/300], Batch [34/43], Training Loss: 0.00002405
2024-11-06 14:07:20,255 - INFO - Epoch [168/300], Batch [35/43], Training Loss: 0.00001022
2024-11-06 14:07:20,257 - INFO - Epoch [168/300], Batch [36/43], Training Loss: 0.00000965
2024-11-06 14:07:20,260 - INFO - Epoch [168/300], Batch [37/43], Training Loss: 0.00001737
2024-11-06 14:07:20,263 - INFO - Epoch [168/300], Batch [38/43], Training Loss: 0.00002365
2024-11-06 14:07:20,266 - INFO - Epoch [168/300], Batch [39/43], Training Loss: 0.00001327
2024-11-06 14:07:20,269 - INFO - Epoch [168/300], Batch [40/43], Training Loss: 0.00001139
2024-11-06 14:07:20,272 - INFO - Epoch [168/300], Batch [41/43], Training Loss: 0.00002477
2024-11-06 14:07:20,275 - INFO - Epoch [168/300], Batch [42/43], Training Loss: 0.00000563
2024-11-06 14:07:20,278 - INFO - Epoch [168/300], Batch [43/43], Training Loss: 0.00001272
2024-11-06 14:07:20,290 - INFO - Epoch [168/300], Average Training Loss: 0.00001325, Validation Loss: 0.00002058
2024-11-06 14:07:20,294 - INFO - Epoch [169/300], Batch [1/43], Training Loss: 0.00001095
2024-11-06 14:07:20,297 - INFO - Epoch [169/300], Batch [2/43], Training Loss: 0.00002587
2024-11-06 14:07:20,301 - INFO - Epoch [169/300], Batch [3/43], Training Loss: 0.00000385
2024-11-06 14:07:20,305 - INFO - Epoch [169/300], Batch [4/43], Training Loss: 0.00001028
2024-11-06 14:07:20,308 - INFO - Epoch [169/300], Batch [5/43], Training Loss: 0.00002642
2024-11-06 14:07:20,310 - INFO - Epoch [169/300], Batch [6/43], Training Loss: 0.00000708
2024-11-06 14:07:20,314 - INFO - Epoch [169/300], Batch [7/43], Training Loss: 0.00000433
2024-11-06 14:07:20,317 - INFO - Epoch [169/300], Batch [8/43], Training Loss: 0.00001453
2024-11-06 14:07:20,320 - INFO - Epoch [169/300], Batch [9/43], Training Loss: 0.00001741
2024-11-06 14:07:20,323 - INFO - Epoch [169/300], Batch [10/43], Training Loss: 0.00000457
2024-11-06 14:07:20,326 - INFO - Epoch [169/300], Batch [11/43], Training Loss: 0.00000507
2024-11-06 14:07:20,329 - INFO - Epoch [169/300], Batch [12/43], Training Loss: 0.00001007
2024-11-06 14:07:20,332 - INFO - Epoch [169/300], Batch [13/43], Training Loss: 0.00001146
2024-11-06 14:07:20,335 - INFO - Epoch [169/300], Batch [14/43], Training Loss: 0.00001280
2024-11-06 14:07:20,339 - INFO - Epoch [169/300], Batch [15/43], Training Loss: 0.00000945
2024-11-06 14:07:20,344 - INFO - Epoch [169/300], Batch [16/43], Training Loss: 0.00001505
2024-11-06 14:07:20,349 - INFO - Epoch [169/300], Batch [17/43], Training Loss: 0.00001018
2024-11-06 14:07:20,354 - INFO - Epoch [169/300], Batch [18/43], Training Loss: 0.00001929
2024-11-06 14:07:20,358 - INFO - Epoch [169/300], Batch [19/43], Training Loss: 0.00000377
2024-11-06 14:07:20,362 - INFO - Epoch [169/300], Batch [20/43], Training Loss: 0.00000918
2024-11-06 14:07:20,365 - INFO - Epoch [169/300], Batch [21/43], Training Loss: 0.00000439
2024-11-06 14:07:20,369 - INFO - Epoch [169/300], Batch [22/43], Training Loss: 0.00000389
2024-11-06 14:07:20,373 - INFO - Epoch [169/300], Batch [23/43], Training Loss: 0.00001363
2024-11-06 14:07:20,404 - INFO - Epoch [169/300], Batch [24/43], Training Loss: 0.00001282
2024-11-06 14:07:20,410 - INFO - Epoch [169/300], Batch [25/43], Training Loss: 0.00000625
2024-11-06 14:07:20,418 - INFO - Epoch [169/300], Batch [26/43], Training Loss: 0.00000913
2024-11-06 14:07:20,423 - INFO - Epoch [169/300], Batch [27/43], Training Loss: 0.00000986
2024-11-06 14:07:20,429 - INFO - Epoch [169/300], Batch [28/43], Training Loss: 0.00000833
2024-11-06 14:07:20,437 - INFO - Epoch [169/300], Batch [29/43], Training Loss: 0.00000907
2024-11-06 14:07:20,441 - INFO - Epoch [169/300], Batch [30/43], Training Loss: 0.00000640
2024-11-06 14:07:20,446 - INFO - Epoch [169/300], Batch [31/43], Training Loss: 0.00001224
2024-11-06 14:07:20,450 - INFO - Epoch [169/300], Batch [32/43], Training Loss: 0.00001113
2024-11-06 14:07:20,455 - INFO - Epoch [169/300], Batch [33/43], Training Loss: 0.00000398
2024-11-06 14:07:20,460 - INFO - Epoch [169/300], Batch [34/43], Training Loss: 0.00001964
2024-11-06 14:07:20,463 - INFO - Epoch [169/300], Batch [35/43], Training Loss: 0.00001443
2024-11-06 14:07:20,467 - INFO - Epoch [169/300], Batch [36/43], Training Loss: 0.00001127
2024-11-06 14:07:20,471 - INFO - Epoch [169/300], Batch [37/43], Training Loss: 0.00001791
2024-11-06 14:07:20,474 - INFO - Epoch [169/300], Batch [38/43], Training Loss: 0.00000648
2024-11-06 14:07:20,478 - INFO - Epoch [169/300], Batch [39/43], Training Loss: 0.00000994
2024-11-06 14:07:20,481 - INFO - Epoch [169/300], Batch [40/43], Training Loss: 0.00000620
2024-11-06 14:07:20,485 - INFO - Epoch [169/300], Batch [41/43], Training Loss: 0.00001386
2024-11-06 14:07:20,488 - INFO - Epoch [169/300], Batch [42/43], Training Loss: 0.00001112
2024-11-06 14:07:20,491 - INFO - Epoch [169/300], Batch [43/43], Training Loss: 0.00001226
2024-11-06 14:07:20,501 - INFO - Epoch [169/300], Average Training Loss: 0.00001083, Validation Loss: 0.00001383
2024-11-06 14:07:20,504 - INFO - Epoch [170/300], Batch [1/43], Training Loss: 0.00001398
2024-11-06 14:07:20,507 - INFO - Epoch [170/300], Batch [2/43], Training Loss: 0.00001215
2024-11-06 14:07:20,510 - INFO - Epoch [170/300], Batch [3/43], Training Loss: 0.00000711
2024-11-06 14:07:20,513 - INFO - Epoch [170/300], Batch [4/43], Training Loss: 0.00001860
2024-11-06 14:07:20,516 - INFO - Epoch [170/300], Batch [5/43], Training Loss: 0.00000844
2024-11-06 14:07:20,519 - INFO - Epoch [170/300], Batch [6/43], Training Loss: 0.00000944
2024-11-06 14:07:20,522 - INFO - Epoch [170/300], Batch [7/43], Training Loss: 0.00001270
2024-11-06 14:07:20,525 - INFO - Epoch [170/300], Batch [8/43], Training Loss: 0.00001089
2024-11-06 14:07:20,528 - INFO - Epoch [170/300], Batch [9/43], Training Loss: 0.00000718
2024-11-06 14:07:20,531 - INFO - Epoch [170/300], Batch [10/43], Training Loss: 0.00000310
2024-11-06 14:07:20,534 - INFO - Epoch [170/300], Batch [11/43], Training Loss: 0.00000603
2024-11-06 14:07:20,537 - INFO - Epoch [170/300], Batch [12/43], Training Loss: 0.00002995
2024-11-06 14:07:20,540 - INFO - Epoch [170/300], Batch [13/43], Training Loss: 0.00001886
2024-11-06 14:07:20,544 - INFO - Epoch [170/300], Batch [14/43], Training Loss: 0.00000985
2024-11-06 14:07:20,548 - INFO - Epoch [170/300], Batch [15/43], Training Loss: 0.00000952
2024-11-06 14:07:20,552 - INFO - Epoch [170/300], Batch [16/43], Training Loss: 0.00001561
2024-11-06 14:07:20,557 - INFO - Epoch [170/300], Batch [17/43], Training Loss: 0.00000798
2024-11-06 14:07:20,561 - INFO - Epoch [170/300], Batch [18/43], Training Loss: 0.00000592
2024-11-06 14:07:20,566 - INFO - Epoch [170/300], Batch [19/43], Training Loss: 0.00000444
2024-11-06 14:07:20,569 - INFO - Epoch [170/300], Batch [20/43], Training Loss: 0.00002474
2024-11-06 14:07:20,574 - INFO - Epoch [170/300], Batch [21/43], Training Loss: 0.00001313
2024-11-06 14:07:20,577 - INFO - Epoch [170/300], Batch [22/43], Training Loss: 0.00001543
2024-11-06 14:07:20,581 - INFO - Epoch [170/300], Batch [23/43], Training Loss: 0.00001430
2024-11-06 14:07:20,584 - INFO - Epoch [170/300], Batch [24/43], Training Loss: 0.00000591
2024-11-06 14:07:20,587 - INFO - Epoch [170/300], Batch [25/43], Training Loss: 0.00001271
2024-11-06 14:07:20,590 - INFO - Epoch [170/300], Batch [26/43], Training Loss: 0.00000774
2024-11-06 14:07:20,594 - INFO - Epoch [170/300], Batch [27/43], Training Loss: 0.00000571
2024-11-06 14:07:20,598 - INFO - Epoch [170/300], Batch [28/43], Training Loss: 0.00001048
2024-11-06 14:07:20,601 - INFO - Epoch [170/300], Batch [29/43], Training Loss: 0.00000926
2024-11-06 14:07:20,605 - INFO - Epoch [170/300], Batch [30/43], Training Loss: 0.00002142
2024-11-06 14:07:20,608 - INFO - Epoch [170/300], Batch [31/43], Training Loss: 0.00000258
2024-11-06 14:07:20,612 - INFO - Epoch [170/300], Batch [32/43], Training Loss: 0.00001905
2024-11-06 14:07:20,615 - INFO - Epoch [170/300], Batch [33/43], Training Loss: 0.00000863
2024-11-06 14:07:20,618 - INFO - Epoch [170/300], Batch [34/43], Training Loss: 0.00001472
2024-11-06 14:07:20,621 - INFO - Epoch [170/300], Batch [35/43], Training Loss: 0.00000577
2024-11-06 14:07:20,624 - INFO - Epoch [170/300], Batch [36/43], Training Loss: 0.00000290
2024-11-06 14:07:20,626 - INFO - Epoch [170/300], Batch [37/43], Training Loss: 0.00001480
2024-11-06 14:07:20,629 - INFO - Epoch [170/300], Batch [38/43], Training Loss: 0.00000554
2024-11-06 14:07:20,633 - INFO - Epoch [170/300], Batch [39/43], Training Loss: 0.00001130
2024-11-06 14:07:20,635 - INFO - Epoch [170/300], Batch [40/43], Training Loss: 0.00000797
2024-11-06 14:07:20,639 - INFO - Epoch [170/300], Batch [41/43], Training Loss: 0.00001733
2024-11-06 14:07:20,642 - INFO - Epoch [170/300], Batch [42/43], Training Loss: 0.00000816
2024-11-06 14:07:20,645 - INFO - Epoch [170/300], Batch [43/43], Training Loss: 0.00000676
2024-11-06 14:07:20,657 - INFO - Epoch [170/300], Average Training Loss: 0.00001112, Validation Loss: 0.00001917
2024-11-06 14:07:20,661 - INFO - Epoch [171/300], Batch [1/43], Training Loss: 0.00002312
2024-11-06 14:07:20,663 - INFO - Epoch [171/300], Batch [2/43], Training Loss: 0.00002411
2024-11-06 14:07:20,667 - INFO - Epoch [171/300], Batch [3/43], Training Loss: 0.00002290
2024-11-06 14:07:20,670 - INFO - Epoch [171/300], Batch [4/43], Training Loss: 0.00001600
2024-11-06 14:07:20,674 - INFO - Epoch [171/300], Batch [5/43], Training Loss: 0.00001875
2024-11-06 14:07:20,677 - INFO - Epoch [171/300], Batch [6/43], Training Loss: 0.00002750
2024-11-06 14:07:20,679 - INFO - Epoch [171/300], Batch [7/43], Training Loss: 0.00001398
2024-11-06 14:07:20,683 - INFO - Epoch [171/300], Batch [8/43], Training Loss: 0.00001426
2024-11-06 14:07:20,688 - INFO - Epoch [171/300], Batch [9/43], Training Loss: 0.00002041
2024-11-06 14:07:20,691 - INFO - Epoch [171/300], Batch [10/43], Training Loss: 0.00002484
2024-11-06 14:07:20,696 - INFO - Epoch [171/300], Batch [11/43], Training Loss: 0.00000475
2024-11-06 14:07:20,700 - INFO - Epoch [171/300], Batch [12/43], Training Loss: 0.00001363
2024-11-06 14:07:20,705 - INFO - Epoch [171/300], Batch [13/43], Training Loss: 0.00001921
2024-11-06 14:07:20,710 - INFO - Epoch [171/300], Batch [14/43], Training Loss: 0.00000963
2024-11-06 14:07:20,713 - INFO - Epoch [171/300], Batch [15/43], Training Loss: 0.00001281
2024-11-06 14:07:20,718 - INFO - Epoch [171/300], Batch [16/43], Training Loss: 0.00001168
2024-11-06 14:07:20,722 - INFO - Epoch [171/300], Batch [17/43], Training Loss: 0.00001005
2024-11-06 14:07:20,726 - INFO - Epoch [171/300], Batch [18/43], Training Loss: 0.00001599
2024-11-06 14:07:20,731 - INFO - Epoch [171/300], Batch [19/43], Training Loss: 0.00000773
2024-11-06 14:07:20,735 - INFO - Epoch [171/300], Batch [20/43], Training Loss: 0.00000353
2024-11-06 14:07:20,739 - INFO - Epoch [171/300], Batch [21/43], Training Loss: 0.00000371
2024-11-06 14:07:20,743 - INFO - Epoch [171/300], Batch [22/43], Training Loss: 0.00000475
2024-11-06 14:07:20,746 - INFO - Epoch [171/300], Batch [23/43], Training Loss: 0.00000861
2024-11-06 14:07:20,749 - INFO - Epoch [171/300], Batch [24/43], Training Loss: 0.00002847
2024-11-06 14:07:20,752 - INFO - Epoch [171/300], Batch [25/43], Training Loss: 0.00000807
2024-11-06 14:07:20,756 - INFO - Epoch [171/300], Batch [26/43], Training Loss: 0.00000479
2024-11-06 14:07:20,760 - INFO - Epoch [171/300], Batch [27/43], Training Loss: 0.00002746
2024-11-06 14:07:20,763 - INFO - Epoch [171/300], Batch [28/43], Training Loss: 0.00000892
2024-11-06 14:07:20,767 - INFO - Epoch [171/300], Batch [29/43], Training Loss: 0.00000799
2024-11-06 14:07:20,770 - INFO - Epoch [171/300], Batch [30/43], Training Loss: 0.00000808
2024-11-06 14:07:20,774 - INFO - Epoch [171/300], Batch [31/43], Training Loss: 0.00000920
2024-11-06 14:07:20,778 - INFO - Epoch [171/300], Batch [32/43], Training Loss: 0.00001032
2024-11-06 14:07:20,782 - INFO - Epoch [171/300], Batch [33/43], Training Loss: 0.00000863
2024-11-06 14:07:20,786 - INFO - Epoch [171/300], Batch [34/43], Training Loss: 0.00001376
2024-11-06 14:07:20,790 - INFO - Epoch [171/300], Batch [35/43], Training Loss: 0.00000961
2024-11-06 14:07:20,793 - INFO - Epoch [171/300], Batch [36/43], Training Loss: 0.00000693
2024-11-06 14:07:20,797 - INFO - Epoch [171/300], Batch [37/43], Training Loss: 0.00000764
2024-11-06 14:07:20,801 - INFO - Epoch [171/300], Batch [38/43], Training Loss: 0.00001855
2024-11-06 14:07:20,804 - INFO - Epoch [171/300], Batch [39/43], Training Loss: 0.00001830
2024-11-06 14:07:20,808 - INFO - Epoch [171/300], Batch [40/43], Training Loss: 0.00001040
2024-11-06 14:07:20,811 - INFO - Epoch [171/300], Batch [41/43], Training Loss: 0.00000928
2024-11-06 14:07:20,815 - INFO - Epoch [171/300], Batch [42/43], Training Loss: 0.00001487
2024-11-06 14:07:20,820 - INFO - Epoch [171/300], Batch [43/43], Training Loss: 0.00001080
2024-11-06 14:07:20,832 - INFO - Epoch [171/300], Average Training Loss: 0.00001335, Validation Loss: 0.00001265
2024-11-06 14:07:20,837 - INFO - Epoch [172/300], Batch [1/43], Training Loss: 0.00001580
2024-11-06 14:07:20,841 - INFO - Epoch [172/300], Batch [2/43], Training Loss: 0.00000327
2024-11-06 14:07:20,846 - INFO - Epoch [172/300], Batch [3/43], Training Loss: 0.00000897
2024-11-06 14:07:20,850 - INFO - Epoch [172/300], Batch [4/43], Training Loss: 0.00001605
2024-11-06 14:07:20,854 - INFO - Epoch [172/300], Batch [5/43], Training Loss: 0.00001278
2024-11-06 14:07:20,858 - INFO - Epoch [172/300], Batch [6/43], Training Loss: 0.00001127
2024-11-06 14:07:20,862 - INFO - Epoch [172/300], Batch [7/43], Training Loss: 0.00001405
2024-11-06 14:07:20,866 - INFO - Epoch [172/300], Batch [8/43], Training Loss: 0.00000943
2024-11-06 14:07:20,871 - INFO - Epoch [172/300], Batch [9/43], Training Loss: 0.00001480
2024-11-06 14:07:20,883 - INFO - Epoch [172/300], Batch [10/43], Training Loss: 0.00000986
2024-11-06 14:07:20,887 - INFO - Epoch [172/300], Batch [11/43], Training Loss: 0.00000786
2024-11-06 14:07:20,891 - INFO - Epoch [172/300], Batch [12/43], Training Loss: 0.00000522
2024-11-06 14:07:20,895 - INFO - Epoch [172/300], Batch [13/43], Training Loss: 0.00001372
2024-11-06 14:07:20,899 - INFO - Epoch [172/300], Batch [14/43], Training Loss: 0.00001931
2024-11-06 14:07:20,904 - INFO - Epoch [172/300], Batch [15/43], Training Loss: 0.00000917
2024-11-06 14:07:20,909 - INFO - Epoch [172/300], Batch [16/43], Training Loss: 0.00000424
2024-11-06 14:07:20,913 - INFO - Epoch [172/300], Batch [17/43], Training Loss: 0.00000435
2024-11-06 14:07:20,917 - INFO - Epoch [172/300], Batch [18/43], Training Loss: 0.00000830
2024-11-06 14:07:20,920 - INFO - Epoch [172/300], Batch [19/43], Training Loss: 0.00000678
2024-11-06 14:07:20,923 - INFO - Epoch [172/300], Batch [20/43], Training Loss: 0.00000887
2024-11-06 14:07:20,926 - INFO - Epoch [172/300], Batch [21/43], Training Loss: 0.00000761
2024-11-06 14:07:20,930 - INFO - Epoch [172/300], Batch [22/43], Training Loss: 0.00002670
2024-11-06 14:07:20,934 - INFO - Epoch [172/300], Batch [23/43], Training Loss: 0.00001937
2024-11-06 14:07:20,938 - INFO - Epoch [172/300], Batch [24/43], Training Loss: 0.00001286
2024-11-06 14:07:20,942 - INFO - Epoch [172/300], Batch [25/43], Training Loss: 0.00001551
2024-11-06 14:07:20,946 - INFO - Epoch [172/300], Batch [26/43], Training Loss: 0.00000484
2024-11-06 14:07:20,949 - INFO - Epoch [172/300], Batch [27/43], Training Loss: 0.00000661
2024-11-06 14:07:20,955 - INFO - Epoch [172/300], Batch [28/43], Training Loss: 0.00001132
2024-11-06 14:07:20,958 - INFO - Epoch [172/300], Batch [29/43], Training Loss: 0.00001288
2024-11-06 14:07:20,961 - INFO - Epoch [172/300], Batch [30/43], Training Loss: 0.00001314
2024-11-06 14:07:20,965 - INFO - Epoch [172/300], Batch [31/43], Training Loss: 0.00000720
2024-11-06 14:07:20,969 - INFO - Epoch [172/300], Batch [32/43], Training Loss: 0.00000300
2024-11-06 14:07:20,973 - INFO - Epoch [172/300], Batch [33/43], Training Loss: 0.00001186
2024-11-06 14:07:20,977 - INFO - Epoch [172/300], Batch [34/43], Training Loss: 0.00000334
2024-11-06 14:07:20,981 - INFO - Epoch [172/300], Batch [35/43], Training Loss: 0.00000805
2024-11-06 14:07:20,986 - INFO - Epoch [172/300], Batch [36/43], Training Loss: 0.00001948
2024-11-06 14:07:20,990 - INFO - Epoch [172/300], Batch [37/43], Training Loss: 0.00000488
2024-11-06 14:07:20,994 - INFO - Epoch [172/300], Batch [38/43], Training Loss: 0.00002707
2024-11-06 14:07:20,999 - INFO - Epoch [172/300], Batch [39/43], Training Loss: 0.00000997
2024-11-06 14:07:21,003 - INFO - Epoch [172/300], Batch [40/43], Training Loss: 0.00000568
2024-11-06 14:07:21,007 - INFO - Epoch [172/300], Batch [41/43], Training Loss: 0.00000451
2024-11-06 14:07:21,011 - INFO - Epoch [172/300], Batch [42/43], Training Loss: 0.00000661
2024-11-06 14:07:21,016 - INFO - Epoch [172/300], Batch [43/43], Training Loss: 0.00000287
2024-11-06 14:07:21,028 - INFO - Epoch [172/300], Average Training Loss: 0.00001045, Validation Loss: 0.00001281
2024-11-06 14:07:21,033 - INFO - Epoch [173/300], Batch [1/43], Training Loss: 0.00001086
2024-11-06 14:07:21,037 - INFO - Epoch [173/300], Batch [2/43], Training Loss: 0.00001205
2024-11-06 14:07:21,040 - INFO - Epoch [173/300], Batch [3/43], Training Loss: 0.00000892
2024-11-06 14:07:21,044 - INFO - Epoch [173/300], Batch [4/43], Training Loss: 0.00001276
2024-11-06 14:07:21,048 - INFO - Epoch [173/300], Batch [5/43], Training Loss: 0.00000998
2024-11-06 14:07:21,051 - INFO - Epoch [173/300], Batch [6/43], Training Loss: 0.00000965
2024-11-06 14:07:21,055 - INFO - Epoch [173/300], Batch [7/43], Training Loss: 0.00000974
2024-11-06 14:07:21,058 - INFO - Epoch [173/300], Batch [8/43], Training Loss: 0.00001658
2024-11-06 14:07:21,063 - INFO - Epoch [173/300], Batch [9/43], Training Loss: 0.00000392
2024-11-06 14:07:21,066 - INFO - Epoch [173/300], Batch [10/43], Training Loss: 0.00000836
2024-11-06 14:07:21,070 - INFO - Epoch [173/300], Batch [11/43], Training Loss: 0.00001064
2024-11-06 14:07:21,074 - INFO - Epoch [173/300], Batch [12/43], Training Loss: 0.00000614
2024-11-06 14:07:21,078 - INFO - Epoch [173/300], Batch [13/43], Training Loss: 0.00000419
2024-11-06 14:07:21,081 - INFO - Epoch [173/300], Batch [14/43], Training Loss: 0.00002047
2024-11-06 14:07:21,086 - INFO - Epoch [173/300], Batch [15/43], Training Loss: 0.00000430
2024-11-06 14:07:21,089 - INFO - Epoch [173/300], Batch [16/43], Training Loss: 0.00000575
2024-11-06 14:07:21,094 - INFO - Epoch [173/300], Batch [17/43], Training Loss: 0.00000994
2024-11-06 14:07:21,098 - INFO - Epoch [173/300], Batch [18/43], Training Loss: 0.00000716
2024-11-06 14:07:21,102 - INFO - Epoch [173/300], Batch [19/43], Training Loss: 0.00000486
2024-11-06 14:07:21,105 - INFO - Epoch [173/300], Batch [20/43], Training Loss: 0.00001580
2024-11-06 14:07:21,109 - INFO - Epoch [173/300], Batch [21/43], Training Loss: 0.00000992
2024-11-06 14:07:21,113 - INFO - Epoch [173/300], Batch [22/43], Training Loss: 0.00000878
2024-11-06 14:07:21,116 - INFO - Epoch [173/300], Batch [23/43], Training Loss: 0.00000612
2024-11-06 14:07:21,120 - INFO - Epoch [173/300], Batch [24/43], Training Loss: 0.00000382
2024-11-06 14:07:21,122 - INFO - Epoch [173/300], Batch [25/43], Training Loss: 0.00000805
2024-11-06 14:07:21,126 - INFO - Epoch [173/300], Batch [26/43], Training Loss: 0.00002166
2024-11-06 14:07:21,129 - INFO - Epoch [173/300], Batch [27/43], Training Loss: 0.00001170
2024-11-06 14:07:21,133 - INFO - Epoch [173/300], Batch [28/43], Training Loss: 0.00001004
2024-11-06 14:07:21,136 - INFO - Epoch [173/300], Batch [29/43], Training Loss: 0.00002568
2024-11-06 14:07:21,140 - INFO - Epoch [173/300], Batch [30/43], Training Loss: 0.00000703
2024-11-06 14:07:21,143 - INFO - Epoch [173/300], Batch [31/43], Training Loss: 0.00000788
2024-11-06 14:07:21,147 - INFO - Epoch [173/300], Batch [32/43], Training Loss: 0.00000876
2024-11-06 14:07:21,151 - INFO - Epoch [173/300], Batch [33/43], Training Loss: 0.00001110
2024-11-06 14:07:21,155 - INFO - Epoch [173/300], Batch [34/43], Training Loss: 0.00001262
2024-11-06 14:07:21,158 - INFO - Epoch [173/300], Batch [35/43], Training Loss: 0.00000807
2024-11-06 14:07:21,161 - INFO - Epoch [173/300], Batch [36/43], Training Loss: 0.00001358
2024-11-06 14:07:21,164 - INFO - Epoch [173/300], Batch [37/43], Training Loss: 0.00001539
2024-11-06 14:07:21,167 - INFO - Epoch [173/300], Batch [38/43], Training Loss: 0.00001697
2024-11-06 14:07:21,170 - INFO - Epoch [173/300], Batch [39/43], Training Loss: 0.00000412
2024-11-06 14:07:21,173 - INFO - Epoch [173/300], Batch [40/43], Training Loss: 0.00002232
2024-11-06 14:07:21,175 - INFO - Epoch [173/300], Batch [41/43], Training Loss: 0.00001613
2024-11-06 14:07:21,179 - INFO - Epoch [173/300], Batch [42/43], Training Loss: 0.00001185
2024-11-06 14:07:21,182 - INFO - Epoch [173/300], Batch [43/43], Training Loss: 0.00000267
2024-11-06 14:07:21,193 - INFO - Epoch [173/300], Average Training Loss: 0.00001061, Validation Loss: 0.00001420
2024-11-06 14:07:21,197 - INFO - Epoch [174/300], Batch [1/43], Training Loss: 0.00001166
2024-11-06 14:07:21,201 - INFO - Epoch [174/300], Batch [2/43], Training Loss: 0.00001743
2024-11-06 14:07:21,204 - INFO - Epoch [174/300], Batch [3/43], Training Loss: 0.00001991
2024-11-06 14:07:21,208 - INFO - Epoch [174/300], Batch [4/43], Training Loss: 0.00001846
2024-11-06 14:07:21,213 - INFO - Epoch [174/300], Batch [5/43], Training Loss: 0.00002088
2024-11-06 14:07:21,217 - INFO - Epoch [174/300], Batch [6/43], Training Loss: 0.00001214
2024-11-06 14:07:21,222 - INFO - Epoch [174/300], Batch [7/43], Training Loss: 0.00001493
2024-11-06 14:07:21,226 - INFO - Epoch [174/300], Batch [8/43], Training Loss: 0.00001415
2024-11-06 14:07:21,229 - INFO - Epoch [174/300], Batch [9/43], Training Loss: 0.00001188
2024-11-06 14:07:21,232 - INFO - Epoch [174/300], Batch [10/43], Training Loss: 0.00001056
2024-11-06 14:07:21,236 - INFO - Epoch [174/300], Batch [11/43], Training Loss: 0.00000738
2024-11-06 14:07:21,240 - INFO - Epoch [174/300], Batch [12/43], Training Loss: 0.00000758
2024-11-06 14:07:21,243 - INFO - Epoch [174/300], Batch [13/43], Training Loss: 0.00000765
2024-11-06 14:07:21,246 - INFO - Epoch [174/300], Batch [14/43], Training Loss: 0.00000374
2024-11-06 14:07:21,249 - INFO - Epoch [174/300], Batch [15/43], Training Loss: 0.00001315
2024-11-06 14:07:21,252 - INFO - Epoch [174/300], Batch [16/43], Training Loss: 0.00000529
2024-11-06 14:07:21,256 - INFO - Epoch [174/300], Batch [17/43], Training Loss: 0.00000950
2024-11-06 14:07:21,259 - INFO - Epoch [174/300], Batch [18/43], Training Loss: 0.00000651
2024-11-06 14:07:21,262 - INFO - Epoch [174/300], Batch [19/43], Training Loss: 0.00000347
2024-11-06 14:07:21,265 - INFO - Epoch [174/300], Batch [20/43], Training Loss: 0.00001321
2024-11-06 14:07:21,269 - INFO - Epoch [174/300], Batch [21/43], Training Loss: 0.00001472
2024-11-06 14:07:21,272 - INFO - Epoch [174/300], Batch [22/43], Training Loss: 0.00000622
2024-11-06 14:07:21,275 - INFO - Epoch [174/300], Batch [23/43], Training Loss: 0.00002046
2024-11-06 14:07:21,278 - INFO - Epoch [174/300], Batch [24/43], Training Loss: 0.00001826
2024-11-06 14:07:21,282 - INFO - Epoch [174/300], Batch [25/43], Training Loss: 0.00000682
2024-11-06 14:07:21,286 - INFO - Epoch [174/300], Batch [26/43], Training Loss: 0.00001213
2024-11-06 14:07:21,289 - INFO - Epoch [174/300], Batch [27/43], Training Loss: 0.00000350
2024-11-06 14:07:21,292 - INFO - Epoch [174/300], Batch [28/43], Training Loss: 0.00001307
2024-11-06 14:07:21,295 - INFO - Epoch [174/300], Batch [29/43], Training Loss: 0.00001324
2024-11-06 14:07:21,299 - INFO - Epoch [174/300], Batch [30/43], Training Loss: 0.00000544
2024-11-06 14:07:21,302 - INFO - Epoch [174/300], Batch [31/43], Training Loss: 0.00000924
2024-11-06 14:07:21,306 - INFO - Epoch [174/300], Batch [32/43], Training Loss: 0.00001054
2024-11-06 14:07:21,309 - INFO - Epoch [174/300], Batch [33/43], Training Loss: 0.00000612
2024-11-06 14:07:21,313 - INFO - Epoch [174/300], Batch [34/43], Training Loss: 0.00000588
2024-11-06 14:07:21,316 - INFO - Epoch [174/300], Batch [35/43], Training Loss: 0.00001329
2024-11-06 14:07:21,319 - INFO - Epoch [174/300], Batch [36/43], Training Loss: 0.00000371
2024-11-06 14:07:21,322 - INFO - Epoch [174/300], Batch [37/43], Training Loss: 0.00000754
2024-11-06 14:07:21,325 - INFO - Epoch [174/300], Batch [38/43], Training Loss: 0.00000609
2024-11-06 14:07:21,328 - INFO - Epoch [174/300], Batch [39/43], Training Loss: 0.00000736
2024-11-06 14:07:21,332 - INFO - Epoch [174/300], Batch [40/43], Training Loss: 0.00002367
2024-11-06 14:07:21,336 - INFO - Epoch [174/300], Batch [41/43], Training Loss: 0.00000851
2024-11-06 14:07:21,341 - INFO - Epoch [174/300], Batch [42/43], Training Loss: 0.00001733
2024-11-06 14:07:21,345 - INFO - Epoch [174/300], Batch [43/43], Training Loss: 0.00000456
2024-11-06 14:07:21,356 - INFO - Epoch [174/300], Average Training Loss: 0.00001086, Validation Loss: 0.00001242
2024-11-06 14:07:21,359 - INFO - Epoch [175/300], Batch [1/43], Training Loss: 0.00001408
2024-11-06 14:07:21,363 - INFO - Epoch [175/300], Batch [2/43], Training Loss: 0.00001199
2024-11-06 14:07:21,366 - INFO - Epoch [175/300], Batch [3/43], Training Loss: 0.00001130
2024-11-06 14:07:21,370 - INFO - Epoch [175/300], Batch [4/43], Training Loss: 0.00000679
2024-11-06 14:07:21,374 - INFO - Epoch [175/300], Batch [5/43], Training Loss: 0.00001102
2024-11-06 14:07:21,378 - INFO - Epoch [175/300], Batch [6/43], Training Loss: 0.00001053
2024-11-06 14:07:21,382 - INFO - Epoch [175/300], Batch [7/43], Training Loss: 0.00000577
2024-11-06 14:07:21,386 - INFO - Epoch [175/300], Batch [8/43], Training Loss: 0.00000640
2024-11-06 14:07:21,389 - INFO - Epoch [175/300], Batch [9/43], Training Loss: 0.00000894
2024-11-06 14:07:21,394 - INFO - Epoch [175/300], Batch [10/43], Training Loss: 0.00000786
2024-11-06 14:07:21,399 - INFO - Epoch [175/300], Batch [11/43], Training Loss: 0.00001847
2024-11-06 14:07:21,403 - INFO - Epoch [175/300], Batch [12/43], Training Loss: 0.00000532
2024-11-06 14:07:21,408 - INFO - Epoch [175/300], Batch [13/43], Training Loss: 0.00000504
2024-11-06 14:07:21,413 - INFO - Epoch [175/300], Batch [14/43], Training Loss: 0.00000395
2024-11-06 14:07:21,419 - INFO - Epoch [175/300], Batch [15/43], Training Loss: 0.00000798
2024-11-06 14:07:21,423 - INFO - Epoch [175/300], Batch [16/43], Training Loss: 0.00000871
2024-11-06 14:07:21,428 - INFO - Epoch [175/300], Batch [17/43], Training Loss: 0.00000518
2024-11-06 14:07:21,432 - INFO - Epoch [175/300], Batch [18/43], Training Loss: 0.00000482
2024-11-06 14:07:21,436 - INFO - Epoch [175/300], Batch [19/43], Training Loss: 0.00000662
2024-11-06 14:07:21,440 - INFO - Epoch [175/300], Batch [20/43], Training Loss: 0.00001936
2024-11-06 14:07:21,445 - INFO - Epoch [175/300], Batch [21/43], Training Loss: 0.00002256
2024-11-06 14:07:21,450 - INFO - Epoch [175/300], Batch [22/43], Training Loss: 0.00000631
2024-11-06 14:07:21,454 - INFO - Epoch [175/300], Batch [23/43], Training Loss: 0.00001217
2024-11-06 14:07:21,458 - INFO - Epoch [175/300], Batch [24/43], Training Loss: 0.00002961
2024-11-06 14:07:21,461 - INFO - Epoch [175/300], Batch [25/43], Training Loss: 0.00002149
2024-11-06 14:07:21,465 - INFO - Epoch [175/300], Batch [26/43], Training Loss: 0.00002411
2024-11-06 14:07:21,470 - INFO - Epoch [175/300], Batch [27/43], Training Loss: 0.00001124
2024-11-06 14:07:21,474 - INFO - Epoch [175/300], Batch [28/43], Training Loss: 0.00001050
2024-11-06 14:07:21,478 - INFO - Epoch [175/300], Batch [29/43], Training Loss: 0.00000779
2024-11-06 14:07:21,482 - INFO - Epoch [175/300], Batch [30/43], Training Loss: 0.00000531
2024-11-06 14:07:21,486 - INFO - Epoch [175/300], Batch [31/43], Training Loss: 0.00001374
2024-11-06 14:07:21,491 - INFO - Epoch [175/300], Batch [32/43], Training Loss: 0.00000677
2024-11-06 14:07:21,495 - INFO - Epoch [175/300], Batch [33/43], Training Loss: 0.00000780
2024-11-06 14:07:21,499 - INFO - Epoch [175/300], Batch [34/43], Training Loss: 0.00000485
2024-11-06 14:07:21,503 - INFO - Epoch [175/300], Batch [35/43], Training Loss: 0.00000702
2024-11-06 14:07:21,507 - INFO - Epoch [175/300], Batch [36/43], Training Loss: 0.00000741
2024-11-06 14:07:21,511 - INFO - Epoch [175/300], Batch [37/43], Training Loss: 0.00000525
2024-11-06 14:07:21,516 - INFO - Epoch [175/300], Batch [38/43], Training Loss: 0.00000742
2024-11-06 14:07:21,521 - INFO - Epoch [175/300], Batch [39/43], Training Loss: 0.00000620
2024-11-06 14:07:21,527 - INFO - Epoch [175/300], Batch [40/43], Training Loss: 0.00001124
2024-11-06 14:07:21,532 - INFO - Epoch [175/300], Batch [41/43], Training Loss: 0.00001931
2024-11-06 14:07:21,536 - INFO - Epoch [175/300], Batch [42/43], Training Loss: 0.00000487
2024-11-06 14:07:21,541 - INFO - Epoch [175/300], Batch [43/43], Training Loss: 0.00003916
2024-11-06 14:07:21,554 - INFO - Epoch [175/300], Average Training Loss: 0.00001098, Validation Loss: 0.00001281
2024-11-06 14:07:21,558 - INFO - Epoch [176/300], Batch [1/43], Training Loss: 0.00001049
2024-11-06 14:07:21,563 - INFO - Epoch [176/300], Batch [2/43], Training Loss: 0.00000636
2024-11-06 14:07:21,567 - INFO - Epoch [176/300], Batch [3/43], Training Loss: 0.00001011
2024-11-06 14:07:21,572 - INFO - Epoch [176/300], Batch [4/43], Training Loss: 0.00001238
2024-11-06 14:07:21,575 - INFO - Epoch [176/300], Batch [5/43], Training Loss: 0.00000696
2024-11-06 14:07:21,581 - INFO - Epoch [176/300], Batch [6/43], Training Loss: 0.00000510
2024-11-06 14:07:21,585 - INFO - Epoch [176/300], Batch [7/43], Training Loss: 0.00000591
2024-11-06 14:07:21,589 - INFO - Epoch [176/300], Batch [8/43], Training Loss: 0.00000457
2024-11-06 14:07:21,593 - INFO - Epoch [176/300], Batch [9/43], Training Loss: 0.00001130
2024-11-06 14:07:21,597 - INFO - Epoch [176/300], Batch [10/43], Training Loss: 0.00000399
2024-11-06 14:07:21,602 - INFO - Epoch [176/300], Batch [11/43], Training Loss: 0.00000390
2024-11-06 14:07:21,606 - INFO - Epoch [176/300], Batch [12/43], Training Loss: 0.00000306
2024-11-06 14:07:21,611 - INFO - Epoch [176/300], Batch [13/43], Training Loss: 0.00001432
2024-11-06 14:07:21,615 - INFO - Epoch [176/300], Batch [14/43], Training Loss: 0.00000912
2024-11-06 14:07:21,619 - INFO - Epoch [176/300], Batch [15/43], Training Loss: 0.00000420
2024-11-06 14:07:21,624 - INFO - Epoch [176/300], Batch [16/43], Training Loss: 0.00001495
2024-11-06 14:07:21,629 - INFO - Epoch [176/300], Batch [17/43], Training Loss: 0.00001961
2024-11-06 14:07:21,634 - INFO - Epoch [176/300], Batch [18/43], Training Loss: 0.00002724
2024-11-06 14:07:21,638 - INFO - Epoch [176/300], Batch [19/43], Training Loss: 0.00000589
2024-11-06 14:07:21,643 - INFO - Epoch [176/300], Batch [20/43], Training Loss: 0.00002826
2024-11-06 14:07:21,647 - INFO - Epoch [176/300], Batch [21/43], Training Loss: 0.00001397
2024-11-06 14:07:21,651 - INFO - Epoch [176/300], Batch [22/43], Training Loss: 0.00000789
2024-11-06 14:07:21,656 - INFO - Epoch [176/300], Batch [23/43], Training Loss: 0.00001251
2024-11-06 14:07:21,662 - INFO - Epoch [176/300], Batch [24/43], Training Loss: 0.00002900
2024-11-06 14:07:21,666 - INFO - Epoch [176/300], Batch [25/43], Training Loss: 0.00003915
2024-11-06 14:07:21,672 - INFO - Epoch [176/300], Batch [26/43], Training Loss: 0.00000434
2024-11-06 14:07:21,677 - INFO - Epoch [176/300], Batch [27/43], Training Loss: 0.00003287
2024-11-06 14:07:21,681 - INFO - Epoch [176/300], Batch [28/43], Training Loss: 0.00003625
2024-11-06 14:07:21,687 - INFO - Epoch [176/300], Batch [29/43], Training Loss: 0.00001164
2024-11-06 14:07:21,693 - INFO - Epoch [176/300], Batch [30/43], Training Loss: 0.00002830
2024-11-06 14:07:21,698 - INFO - Epoch [176/300], Batch [31/43], Training Loss: 0.00001641
2024-11-06 14:07:21,704 - INFO - Epoch [176/300], Batch [32/43], Training Loss: 0.00000972
2024-11-06 14:07:21,710 - INFO - Epoch [176/300], Batch [33/43], Training Loss: 0.00000984
2024-11-06 14:07:21,715 - INFO - Epoch [176/300], Batch [34/43], Training Loss: 0.00000405
2024-11-06 14:07:21,719 - INFO - Epoch [176/300], Batch [35/43], Training Loss: 0.00001446
2024-11-06 14:07:21,724 - INFO - Epoch [176/300], Batch [36/43], Training Loss: 0.00001186
2024-11-06 14:07:21,728 - INFO - Epoch [176/300], Batch [37/43], Training Loss: 0.00000805
2024-11-06 14:07:21,733 - INFO - Epoch [176/300], Batch [38/43], Training Loss: 0.00001834
2024-11-06 14:07:21,736 - INFO - Epoch [176/300], Batch [39/43], Training Loss: 0.00001341
2024-11-06 14:07:21,740 - INFO - Epoch [176/300], Batch [40/43], Training Loss: 0.00003139
2024-11-06 14:07:21,745 - INFO - Epoch [176/300], Batch [41/43], Training Loss: 0.00001571
2024-11-06 14:07:21,749 - INFO - Epoch [176/300], Batch [42/43], Training Loss: 0.00002285
2024-11-06 14:07:21,753 - INFO - Epoch [176/300], Batch [43/43], Training Loss: 0.00003192
2024-11-06 14:07:21,767 - INFO - Epoch [176/300], Average Training Loss: 0.00001469, Validation Loss: 0.00002355
2024-11-06 14:07:21,773 - INFO - Epoch [177/300], Batch [1/43], Training Loss: 0.00002015
2024-11-06 14:07:21,778 - INFO - Epoch [177/300], Batch [2/43], Training Loss: 0.00001140
2024-11-06 14:07:21,784 - INFO - Epoch [177/300], Batch [3/43], Training Loss: 0.00001676
2024-11-06 14:07:21,791 - INFO - Epoch [177/300], Batch [4/43], Training Loss: 0.00002377
2024-11-06 14:07:21,796 - INFO - Epoch [177/300], Batch [5/43], Training Loss: 0.00001664
2024-11-06 14:07:21,801 - INFO - Epoch [177/300], Batch [6/43], Training Loss: 0.00000502
2024-11-06 14:07:21,806 - INFO - Epoch [177/300], Batch [7/43], Training Loss: 0.00002569
2024-11-06 14:07:21,811 - INFO - Epoch [177/300], Batch [8/43], Training Loss: 0.00001823
2024-11-06 14:07:21,816 - INFO - Epoch [177/300], Batch [9/43], Training Loss: 0.00001653
2024-11-06 14:07:21,820 - INFO - Epoch [177/300], Batch [10/43], Training Loss: 0.00001822
2024-11-06 14:07:21,826 - INFO - Epoch [177/300], Batch [11/43], Training Loss: 0.00003233
2024-11-06 14:07:21,831 - INFO - Epoch [177/300], Batch [12/43], Training Loss: 0.00000915
2024-11-06 14:07:21,837 - INFO - Epoch [177/300], Batch [13/43], Training Loss: 0.00000309
2024-11-06 14:07:21,842 - INFO - Epoch [177/300], Batch [14/43], Training Loss: 0.00001366
2024-11-06 14:07:21,846 - INFO - Epoch [177/300], Batch [15/43], Training Loss: 0.00003043
2024-11-06 14:07:21,851 - INFO - Epoch [177/300], Batch [16/43], Training Loss: 0.00001422
2024-11-06 14:07:21,856 - INFO - Epoch [177/300], Batch [17/43], Training Loss: 0.00001571
2024-11-06 14:07:21,861 - INFO - Epoch [177/300], Batch [18/43], Training Loss: 0.00001411
2024-11-06 14:07:21,866 - INFO - Epoch [177/300], Batch [19/43], Training Loss: 0.00000948
2024-11-06 14:07:21,871 - INFO - Epoch [177/300], Batch [20/43], Training Loss: 0.00001001
2024-11-06 14:07:21,876 - INFO - Epoch [177/300], Batch [21/43], Training Loss: 0.00000833
2024-11-06 14:07:21,880 - INFO - Epoch [177/300], Batch [22/43], Training Loss: 0.00001123
2024-11-06 14:07:21,886 - INFO - Epoch [177/300], Batch [23/43], Training Loss: 0.00001094
2024-11-06 14:07:21,892 - INFO - Epoch [177/300], Batch [24/43], Training Loss: 0.00000597
2024-11-06 14:07:21,896 - INFO - Epoch [177/300], Batch [25/43], Training Loss: 0.00000288
2024-11-06 14:07:21,900 - INFO - Epoch [177/300], Batch [26/43], Training Loss: 0.00001334
2024-11-06 14:07:21,906 - INFO - Epoch [177/300], Batch [27/43], Training Loss: 0.00001028
2024-11-06 14:07:21,910 - INFO - Epoch [177/300], Batch [28/43], Training Loss: 0.00001252
2024-11-06 14:07:21,914 - INFO - Epoch [177/300], Batch [29/43], Training Loss: 0.00001197
2024-11-06 14:07:21,917 - INFO - Epoch [177/300], Batch [30/43], Training Loss: 0.00000411
2024-11-06 14:07:21,922 - INFO - Epoch [177/300], Batch [31/43], Training Loss: 0.00000829
2024-11-06 14:07:21,926 - INFO - Epoch [177/300], Batch [32/43], Training Loss: 0.00000503
2024-11-06 14:07:21,929 - INFO - Epoch [177/300], Batch [33/43], Training Loss: 0.00001761
2024-11-06 14:07:21,933 - INFO - Epoch [177/300], Batch [34/43], Training Loss: 0.00001431
2024-11-06 14:07:21,938 - INFO - Epoch [177/300], Batch [35/43], Training Loss: 0.00001782
2024-11-06 14:07:21,942 - INFO - Epoch [177/300], Batch [36/43], Training Loss: 0.00001654
2024-11-06 14:07:21,947 - INFO - Epoch [177/300], Batch [37/43], Training Loss: 0.00000902
2024-11-06 14:07:21,951 - INFO - Epoch [177/300], Batch [38/43], Training Loss: 0.00001585
2024-11-06 14:07:21,955 - INFO - Epoch [177/300], Batch [39/43], Training Loss: 0.00001007
2024-11-06 14:07:21,960 - INFO - Epoch [177/300], Batch [40/43], Training Loss: 0.00001954
2024-11-06 14:07:21,965 - INFO - Epoch [177/300], Batch [41/43], Training Loss: 0.00001017
2024-11-06 14:07:21,969 - INFO - Epoch [177/300], Batch [42/43], Training Loss: 0.00003355
2024-11-06 14:07:21,974 - INFO - Epoch [177/300], Batch [43/43], Training Loss: 0.00000923
2024-11-06 14:07:21,987 - INFO - Epoch [177/300], Average Training Loss: 0.00001403, Validation Loss: 0.00001438
2024-11-06 14:07:21,990 - INFO - Epoch [178/300], Batch [1/43], Training Loss: 0.00001483
2024-11-06 14:07:21,995 - INFO - Epoch [178/300], Batch [2/43], Training Loss: 0.00000691
2024-11-06 14:07:21,999 - INFO - Epoch [178/300], Batch [3/43], Training Loss: 0.00000652
2024-11-06 14:07:22,003 - INFO - Epoch [178/300], Batch [4/43], Training Loss: 0.00000840
2024-11-06 14:07:22,008 - INFO - Epoch [178/300], Batch [5/43], Training Loss: 0.00000831
2024-11-06 14:07:22,012 - INFO - Epoch [178/300], Batch [6/43], Training Loss: 0.00000454
2024-11-06 14:07:22,018 - INFO - Epoch [178/300], Batch [7/43], Training Loss: 0.00001776
2024-11-06 14:07:22,022 - INFO - Epoch [178/300], Batch [8/43], Training Loss: 0.00000517
2024-11-06 14:07:22,026 - INFO - Epoch [178/300], Batch [9/43], Training Loss: 0.00000597
2024-11-06 14:07:22,030 - INFO - Epoch [178/300], Batch [10/43], Training Loss: 0.00001979
2024-11-06 14:07:22,033 - INFO - Epoch [178/300], Batch [11/43], Training Loss: 0.00001549
2024-11-06 14:07:22,036 - INFO - Epoch [178/300], Batch [12/43], Training Loss: 0.00001828
2024-11-06 14:07:22,040 - INFO - Epoch [178/300], Batch [13/43], Training Loss: 0.00001411
2024-11-06 14:07:22,043 - INFO - Epoch [178/300], Batch [14/43], Training Loss: 0.00001284
2024-11-06 14:07:22,046 - INFO - Epoch [178/300], Batch [15/43], Training Loss: 0.00000870
2024-11-06 14:07:22,050 - INFO - Epoch [178/300], Batch [16/43], Training Loss: 0.00000660
2024-11-06 14:07:22,053 - INFO - Epoch [178/300], Batch [17/43], Training Loss: 0.00001007
2024-11-06 14:07:22,057 - INFO - Epoch [178/300], Batch [18/43], Training Loss: 0.00000910
2024-11-06 14:07:22,060 - INFO - Epoch [178/300], Batch [19/43], Training Loss: 0.00001259
2024-11-06 14:07:22,064 - INFO - Epoch [178/300], Batch [20/43], Training Loss: 0.00001073
2024-11-06 14:07:22,068 - INFO - Epoch [178/300], Batch [21/43], Training Loss: 0.00001475
2024-11-06 14:07:22,073 - INFO - Epoch [178/300], Batch [22/43], Training Loss: 0.00000680
2024-11-06 14:07:22,076 - INFO - Epoch [178/300], Batch [23/43], Training Loss: 0.00001271
2024-11-06 14:07:22,081 - INFO - Epoch [178/300], Batch [24/43], Training Loss: 0.00001246
2024-11-06 14:07:22,085 - INFO - Epoch [178/300], Batch [25/43], Training Loss: 0.00001800
2024-11-06 14:07:22,089 - INFO - Epoch [178/300], Batch [26/43], Training Loss: 0.00000679
2024-11-06 14:07:22,094 - INFO - Epoch [178/300], Batch [27/43], Training Loss: 0.00000598
2024-11-06 14:07:22,099 - INFO - Epoch [178/300], Batch [28/43], Training Loss: 0.00000446
2024-11-06 14:07:22,104 - INFO - Epoch [178/300], Batch [29/43], Training Loss: 0.00000890
2024-11-06 14:07:22,108 - INFO - Epoch [178/300], Batch [30/43], Training Loss: 0.00000747
2024-11-06 14:07:22,113 - INFO - Epoch [178/300], Batch [31/43], Training Loss: 0.00001554
2024-11-06 14:07:22,117 - INFO - Epoch [178/300], Batch [32/43], Training Loss: 0.00000412
2024-11-06 14:07:22,121 - INFO - Epoch [178/300], Batch [33/43], Training Loss: 0.00001421
2024-11-06 14:07:22,125 - INFO - Epoch [178/300], Batch [34/43], Training Loss: 0.00000585
2024-11-06 14:07:22,130 - INFO - Epoch [178/300], Batch [35/43], Training Loss: 0.00000989
2024-11-06 14:07:22,133 - INFO - Epoch [178/300], Batch [36/43], Training Loss: 0.00000595
2024-11-06 14:07:22,137 - INFO - Epoch [178/300], Batch [37/43], Training Loss: 0.00000338
2024-11-06 14:07:22,142 - INFO - Epoch [178/300], Batch [38/43], Training Loss: 0.00000858
2024-11-06 14:07:22,146 - INFO - Epoch [178/300], Batch [39/43], Training Loss: 0.00000636
2024-11-06 14:07:22,149 - INFO - Epoch [178/300], Batch [40/43], Training Loss: 0.00000783
2024-11-06 14:07:22,153 - INFO - Epoch [178/300], Batch [41/43], Training Loss: 0.00000821
2024-11-06 14:07:22,157 - INFO - Epoch [178/300], Batch [42/43], Training Loss: 0.00001562
2024-11-06 14:07:22,160 - INFO - Epoch [178/300], Batch [43/43], Training Loss: 0.00004373
2024-11-06 14:07:22,173 - INFO - Epoch [178/300], Average Training Loss: 0.00001080, Validation Loss: 0.00001370
2024-11-06 14:07:22,178 - INFO - Epoch [179/300], Batch [1/43], Training Loss: 0.00002271
2024-11-06 14:07:22,184 - INFO - Epoch [179/300], Batch [2/43], Training Loss: 0.00001506
2024-11-06 14:07:22,188 - INFO - Epoch [179/300], Batch [3/43], Training Loss: 0.00000705
2024-11-06 14:07:22,192 - INFO - Epoch [179/300], Batch [4/43], Training Loss: 0.00000641
2024-11-06 14:07:22,196 - INFO - Epoch [179/300], Batch [5/43], Training Loss: 0.00000571
2024-11-06 14:07:22,200 - INFO - Epoch [179/300], Batch [6/43], Training Loss: 0.00000790
2024-11-06 14:07:22,204 - INFO - Epoch [179/300], Batch [7/43], Training Loss: 0.00000893
2024-11-06 14:07:22,207 - INFO - Epoch [179/300], Batch [8/43], Training Loss: 0.00000549
2024-11-06 14:07:22,211 - INFO - Epoch [179/300], Batch [9/43], Training Loss: 0.00001261
2024-11-06 14:07:22,214 - INFO - Epoch [179/300], Batch [10/43], Training Loss: 0.00000686
2024-11-06 14:07:22,218 - INFO - Epoch [179/300], Batch [11/43], Training Loss: 0.00001045
2024-11-06 14:07:22,222 - INFO - Epoch [179/300], Batch [12/43], Training Loss: 0.00001803
2024-11-06 14:07:22,226 - INFO - Epoch [179/300], Batch [13/43], Training Loss: 0.00002162
2024-11-06 14:07:22,231 - INFO - Epoch [179/300], Batch [14/43], Training Loss: 0.00000653
2024-11-06 14:07:22,236 - INFO - Epoch [179/300], Batch [15/43], Training Loss: 0.00001013
2024-11-06 14:07:22,241 - INFO - Epoch [179/300], Batch [16/43], Training Loss: 0.00002407
2024-11-06 14:07:22,245 - INFO - Epoch [179/300], Batch [17/43], Training Loss: 0.00001373
2024-11-06 14:07:22,249 - INFO - Epoch [179/300], Batch [18/43], Training Loss: 0.00000879
2024-11-06 14:07:22,253 - INFO - Epoch [179/300], Batch [19/43], Training Loss: 0.00002062
2024-11-06 14:07:22,257 - INFO - Epoch [179/300], Batch [20/43], Training Loss: 0.00001256
2024-11-06 14:07:22,261 - INFO - Epoch [179/300], Batch [21/43], Training Loss: 0.00000379
2024-11-06 14:07:22,266 - INFO - Epoch [179/300], Batch [22/43], Training Loss: 0.00000477
2024-11-06 14:07:22,270 - INFO - Epoch [179/300], Batch [23/43], Training Loss: 0.00000472
2024-11-06 14:07:22,275 - INFO - Epoch [179/300], Batch [24/43], Training Loss: 0.00000560
2024-11-06 14:07:22,279 - INFO - Epoch [179/300], Batch [25/43], Training Loss: 0.00001346
2024-11-06 14:07:22,284 - INFO - Epoch [179/300], Batch [26/43], Training Loss: 0.00000692
2024-11-06 14:07:22,289 - INFO - Epoch [179/300], Batch [27/43], Training Loss: 0.00001370
2024-11-06 14:07:22,293 - INFO - Epoch [179/300], Batch [28/43], Training Loss: 0.00000571
2024-11-06 14:07:22,297 - INFO - Epoch [179/300], Batch [29/43], Training Loss: 0.00001290
2024-11-06 14:07:22,301 - INFO - Epoch [179/300], Batch [30/43], Training Loss: 0.00000761
2024-11-06 14:07:22,304 - INFO - Epoch [179/300], Batch [31/43], Training Loss: 0.00001230
2024-11-06 14:07:22,309 - INFO - Epoch [179/300], Batch [32/43], Training Loss: 0.00000647
2024-11-06 14:07:22,312 - INFO - Epoch [179/300], Batch [33/43], Training Loss: 0.00000804
2024-11-06 14:07:22,316 - INFO - Epoch [179/300], Batch [34/43], Training Loss: 0.00001853
2024-11-06 14:07:22,319 - INFO - Epoch [179/300], Batch [35/43], Training Loss: 0.00000680
2024-11-06 14:07:22,323 - INFO - Epoch [179/300], Batch [36/43], Training Loss: 0.00000684
2024-11-06 14:07:22,326 - INFO - Epoch [179/300], Batch [37/43], Training Loss: 0.00000830
2024-11-06 14:07:22,331 - INFO - Epoch [179/300], Batch [38/43], Training Loss: 0.00001953
2024-11-06 14:07:22,336 - INFO - Epoch [179/300], Batch [39/43], Training Loss: 0.00000206
2024-11-06 14:07:22,340 - INFO - Epoch [179/300], Batch [40/43], Training Loss: 0.00001737
2024-11-06 14:07:22,343 - INFO - Epoch [179/300], Batch [41/43], Training Loss: 0.00002013
2024-11-06 14:07:22,347 - INFO - Epoch [179/300], Batch [42/43], Training Loss: 0.00000296
2024-11-06 14:07:22,350 - INFO - Epoch [179/300], Batch [43/43], Training Loss: 0.00001295
2024-11-06 14:07:22,362 - INFO - Epoch [179/300], Average Training Loss: 0.00001085, Validation Loss: 0.00001238
2024-11-06 14:07:22,365 - INFO - Epoch [180/300], Batch [1/43], Training Loss: 0.00000529
2024-11-06 14:07:22,369 - INFO - Epoch [180/300], Batch [2/43], Training Loss: 0.00001679
2024-11-06 14:07:22,372 - INFO - Epoch [180/300], Batch [3/43], Training Loss: 0.00001262
2024-11-06 14:07:22,375 - INFO - Epoch [180/300], Batch [4/43], Training Loss: 0.00000631
2024-11-06 14:07:22,378 - INFO - Epoch [180/300], Batch [5/43], Training Loss: 0.00003205
2024-11-06 14:07:22,381 - INFO - Epoch [180/300], Batch [6/43], Training Loss: 0.00000995
2024-11-06 14:07:22,385 - INFO - Epoch [180/300], Batch [7/43], Training Loss: 0.00001440
2024-11-06 14:07:22,389 - INFO - Epoch [180/300], Batch [8/43], Training Loss: 0.00001714
2024-11-06 14:07:22,392 - INFO - Epoch [180/300], Batch [9/43], Training Loss: 0.00001004
2024-11-06 14:07:22,395 - INFO - Epoch [180/300], Batch [10/43], Training Loss: 0.00001630
2024-11-06 14:07:22,398 - INFO - Epoch [180/300], Batch [11/43], Training Loss: 0.00001629
2024-11-06 14:07:22,401 - INFO - Epoch [180/300], Batch [12/43], Training Loss: 0.00003547
2024-11-06 14:07:22,404 - INFO - Epoch [180/300], Batch [13/43], Training Loss: 0.00000919
2024-11-06 14:07:22,407 - INFO - Epoch [180/300], Batch [14/43], Training Loss: 0.00000667
2024-11-06 14:07:22,412 - INFO - Epoch [180/300], Batch [15/43], Training Loss: 0.00000699
2024-11-06 14:07:22,417 - INFO - Epoch [180/300], Batch [16/43], Training Loss: 0.00002057
2024-11-06 14:07:22,423 - INFO - Epoch [180/300], Batch [17/43], Training Loss: 0.00000915
2024-11-06 14:07:22,428 - INFO - Epoch [180/300], Batch [18/43], Training Loss: 0.00000362
2024-11-06 14:07:22,432 - INFO - Epoch [180/300], Batch [19/43], Training Loss: 0.00002337
2024-11-06 14:07:22,436 - INFO - Epoch [180/300], Batch [20/43], Training Loss: 0.00000948
2024-11-06 14:07:22,440 - INFO - Epoch [180/300], Batch [21/43], Training Loss: 0.00000758
2024-11-06 14:07:22,443 - INFO - Epoch [180/300], Batch [22/43], Training Loss: 0.00001170
2024-11-06 14:07:22,447 - INFO - Epoch [180/300], Batch [23/43], Training Loss: 0.00001083
2024-11-06 14:07:22,451 - INFO - Epoch [180/300], Batch [24/43], Training Loss: 0.00000606
2024-11-06 14:07:22,455 - INFO - Epoch [180/300], Batch [25/43], Training Loss: 0.00001386
2024-11-06 14:07:22,459 - INFO - Epoch [180/300], Batch [26/43], Training Loss: 0.00000987
2024-11-06 14:07:22,462 - INFO - Epoch [180/300], Batch [27/43], Training Loss: 0.00001096
2024-11-06 14:07:22,466 - INFO - Epoch [180/300], Batch [28/43], Training Loss: 0.00001107
2024-11-06 14:07:22,470 - INFO - Epoch [180/300], Batch [29/43], Training Loss: 0.00000757
2024-11-06 14:07:22,474 - INFO - Epoch [180/300], Batch [30/43], Training Loss: 0.00000733
2024-11-06 14:07:22,478 - INFO - Epoch [180/300], Batch [31/43], Training Loss: 0.00001102
2024-11-06 14:07:22,481 - INFO - Epoch [180/300], Batch [32/43], Training Loss: 0.00000960
2024-11-06 14:07:22,485 - INFO - Epoch [180/300], Batch [33/43], Training Loss: 0.00001281
2024-11-06 14:07:22,488 - INFO - Epoch [180/300], Batch [34/43], Training Loss: 0.00000858
2024-11-06 14:07:22,492 - INFO - Epoch [180/300], Batch [35/43], Training Loss: 0.00001403
2024-11-06 14:07:22,495 - INFO - Epoch [180/300], Batch [36/43], Training Loss: 0.00000587
2024-11-06 14:07:22,498 - INFO - Epoch [180/300], Batch [37/43], Training Loss: 0.00000393
2024-11-06 14:07:22,501 - INFO - Epoch [180/300], Batch [38/43], Training Loss: 0.00001010
2024-11-06 14:07:22,505 - INFO - Epoch [180/300], Batch [39/43], Training Loss: 0.00000919
2024-11-06 14:07:22,509 - INFO - Epoch [180/300], Batch [40/43], Training Loss: 0.00000341
2024-11-06 14:07:22,512 - INFO - Epoch [180/300], Batch [41/43], Training Loss: 0.00001211
2024-11-06 14:07:22,516 - INFO - Epoch [180/300], Batch [42/43], Training Loss: 0.00001419
2024-11-06 14:07:22,519 - INFO - Epoch [180/300], Batch [43/43], Training Loss: 0.00001414
2024-11-06 14:07:22,531 - INFO - Epoch [180/300], Average Training Loss: 0.00001180, Validation Loss: 0.00001647
2024-11-06 14:07:22,535 - INFO - Epoch [181/300], Batch [1/43], Training Loss: 0.00000568
2024-11-06 14:07:22,538 - INFO - Epoch [181/300], Batch [2/43], Training Loss: 0.00001504
2024-11-06 14:07:22,542 - INFO - Epoch [181/300], Batch [3/43], Training Loss: 0.00000279
2024-11-06 14:07:22,545 - INFO - Epoch [181/300], Batch [4/43], Training Loss: 0.00001011
2024-11-06 14:07:22,548 - INFO - Epoch [181/300], Batch [5/43], Training Loss: 0.00003016
2024-11-06 14:07:22,552 - INFO - Epoch [181/300], Batch [6/43], Training Loss: 0.00001193
2024-11-06 14:07:22,556 - INFO - Epoch [181/300], Batch [7/43], Training Loss: 0.00000663
2024-11-06 14:07:22,559 - INFO - Epoch [181/300], Batch [8/43], Training Loss: 0.00001067
2024-11-06 14:07:22,562 - INFO - Epoch [181/300], Batch [9/43], Training Loss: 0.00000629
2024-11-06 14:07:22,566 - INFO - Epoch [181/300], Batch [10/43], Training Loss: 0.00000469
2024-11-06 14:07:22,569 - INFO - Epoch [181/300], Batch [11/43], Training Loss: 0.00000731
2024-11-06 14:07:22,572 - INFO - Epoch [181/300], Batch [12/43], Training Loss: 0.00001641
2024-11-06 14:07:22,576 - INFO - Epoch [181/300], Batch [13/43], Training Loss: 0.00000604
2024-11-06 14:07:22,580 - INFO - Epoch [181/300], Batch [14/43], Training Loss: 0.00000372
2024-11-06 14:07:22,583 - INFO - Epoch [181/300], Batch [15/43], Training Loss: 0.00001856
2024-11-06 14:07:22,587 - INFO - Epoch [181/300], Batch [16/43], Training Loss: 0.00001518
2024-11-06 14:07:22,590 - INFO - Epoch [181/300], Batch [17/43], Training Loss: 0.00000528
2024-11-06 14:07:22,593 - INFO - Epoch [181/300], Batch [18/43], Training Loss: 0.00000743
2024-11-06 14:07:22,596 - INFO - Epoch [181/300], Batch [19/43], Training Loss: 0.00000506
2024-11-06 14:07:22,600 - INFO - Epoch [181/300], Batch [20/43], Training Loss: 0.00000903
2024-11-06 14:07:22,603 - INFO - Epoch [181/300], Batch [21/43], Training Loss: 0.00001800
2024-11-06 14:07:22,607 - INFO - Epoch [181/300], Batch [22/43], Training Loss: 0.00000957
2024-11-06 14:07:22,610 - INFO - Epoch [181/300], Batch [23/43], Training Loss: 0.00001734
2024-11-06 14:07:22,614 - INFO - Epoch [181/300], Batch [24/43], Training Loss: 0.00000771
2024-11-06 14:07:22,618 - INFO - Epoch [181/300], Batch [25/43], Training Loss: 0.00001007
2024-11-06 14:07:22,622 - INFO - Epoch [181/300], Batch [26/43], Training Loss: 0.00000575
2024-11-06 14:07:22,626 - INFO - Epoch [181/300], Batch [27/43], Training Loss: 0.00001560
2024-11-06 14:07:22,630 - INFO - Epoch [181/300], Batch [28/43], Training Loss: 0.00001120
2024-11-06 14:07:22,633 - INFO - Epoch [181/300], Batch [29/43], Training Loss: 0.00001067
2024-11-06 14:07:22,637 - INFO - Epoch [181/300], Batch [30/43], Training Loss: 0.00000807
2024-11-06 14:07:22,640 - INFO - Epoch [181/300], Batch [31/43], Training Loss: 0.00000906
2024-11-06 14:07:22,643 - INFO - Epoch [181/300], Batch [32/43], Training Loss: 0.00000623
2024-11-06 14:07:22,648 - INFO - Epoch [181/300], Batch [33/43], Training Loss: 0.00000888
2024-11-06 14:07:22,652 - INFO - Epoch [181/300], Batch [34/43], Training Loss: 0.00001357
2024-11-06 14:07:22,656 - INFO - Epoch [181/300], Batch [35/43], Training Loss: 0.00000708
2024-11-06 14:07:22,659 - INFO - Epoch [181/300], Batch [36/43], Training Loss: 0.00000510
2024-11-06 14:07:22,663 - INFO - Epoch [181/300], Batch [37/43], Training Loss: 0.00003248
2024-11-06 14:07:22,668 - INFO - Epoch [181/300], Batch [38/43], Training Loss: 0.00002177
2024-11-06 14:07:22,671 - INFO - Epoch [181/300], Batch [39/43], Training Loss: 0.00000975
2024-11-06 14:07:22,675 - INFO - Epoch [181/300], Batch [40/43], Training Loss: 0.00000607
2024-11-06 14:07:22,678 - INFO - Epoch [181/300], Batch [41/43], Training Loss: 0.00000338
2024-11-06 14:07:22,682 - INFO - Epoch [181/300], Batch [42/43], Training Loss: 0.00001024
2024-11-06 14:07:22,686 - INFO - Epoch [181/300], Batch [43/43], Training Loss: 0.00000797
2024-11-06 14:07:22,697 - INFO - Epoch [181/300], Average Training Loss: 0.00001055, Validation Loss: 0.00001292
2024-11-06 14:07:22,701 - INFO - Epoch [182/300], Batch [1/43], Training Loss: 0.00000433
2024-11-06 14:07:22,705 - INFO - Epoch [182/300], Batch [2/43], Training Loss: 0.00000781
2024-11-06 14:07:22,708 - INFO - Epoch [182/300], Batch [3/43], Training Loss: 0.00000575
2024-11-06 14:07:22,713 - INFO - Epoch [182/300], Batch [4/43], Training Loss: 0.00000616
2024-11-06 14:07:22,716 - INFO - Epoch [182/300], Batch [5/43], Training Loss: 0.00000250
2024-11-06 14:07:22,720 - INFO - Epoch [182/300], Batch [6/43], Training Loss: 0.00001283
2024-11-06 14:07:22,723 - INFO - Epoch [182/300], Batch [7/43], Training Loss: 0.00001454
2024-11-06 14:07:22,727 - INFO - Epoch [182/300], Batch [8/43], Training Loss: 0.00000949
2024-11-06 14:07:22,730 - INFO - Epoch [182/300], Batch [9/43], Training Loss: 0.00000765
2024-11-06 14:07:22,734 - INFO - Epoch [182/300], Batch [10/43], Training Loss: 0.00000607
2024-11-06 14:07:22,739 - INFO - Epoch [182/300], Batch [11/43], Training Loss: 0.00002485
2024-11-06 14:07:22,743 - INFO - Epoch [182/300], Batch [12/43], Training Loss: 0.00001668
2024-11-06 14:07:22,747 - INFO - Epoch [182/300], Batch [13/43], Training Loss: 0.00000995
2024-11-06 14:07:22,751 - INFO - Epoch [182/300], Batch [14/43], Training Loss: 0.00001021
2024-11-06 14:07:22,755 - INFO - Epoch [182/300], Batch [15/43], Training Loss: 0.00000390
2024-11-06 14:07:22,761 - INFO - Epoch [182/300], Batch [16/43], Training Loss: 0.00000920
2024-11-06 14:07:22,765 - INFO - Epoch [182/300], Batch [17/43], Training Loss: 0.00001199
2024-11-06 14:07:22,769 - INFO - Epoch [182/300], Batch [18/43], Training Loss: 0.00001083
2024-11-06 14:07:22,773 - INFO - Epoch [182/300], Batch [19/43], Training Loss: 0.00001375
2024-11-06 14:07:22,777 - INFO - Epoch [182/300], Batch [20/43], Training Loss: 0.00000640
2024-11-06 14:07:22,781 - INFO - Epoch [182/300], Batch [21/43], Training Loss: 0.00000698
2024-11-06 14:07:22,785 - INFO - Epoch [182/300], Batch [22/43], Training Loss: 0.00002327
2024-11-06 14:07:22,789 - INFO - Epoch [182/300], Batch [23/43], Training Loss: 0.00000739
2024-11-06 14:07:22,794 - INFO - Epoch [182/300], Batch [24/43], Training Loss: 0.00000895
2024-11-06 14:07:22,798 - INFO - Epoch [182/300], Batch [25/43], Training Loss: 0.00000922
2024-11-06 14:07:22,803 - INFO - Epoch [182/300], Batch [26/43], Training Loss: 0.00000769
2024-11-06 14:07:22,808 - INFO - Epoch [182/300], Batch [27/43], Training Loss: 0.00000685
2024-11-06 14:07:22,813 - INFO - Epoch [182/300], Batch [28/43], Training Loss: 0.00001536
2024-11-06 14:07:22,818 - INFO - Epoch [182/300], Batch [29/43], Training Loss: 0.00001374
2024-11-06 14:07:22,823 - INFO - Epoch [182/300], Batch [30/43], Training Loss: 0.00000395
2024-11-06 14:07:22,828 - INFO - Epoch [182/300], Batch [31/43], Training Loss: 0.00003102
2024-11-06 14:07:22,833 - INFO - Epoch [182/300], Batch [32/43], Training Loss: 0.00001102
2024-11-06 14:07:22,837 - INFO - Epoch [182/300], Batch [33/43], Training Loss: 0.00000975
2024-11-06 14:07:22,842 - INFO - Epoch [182/300], Batch [34/43], Training Loss: 0.00001499
2024-11-06 14:07:22,847 - INFO - Epoch [182/300], Batch [35/43], Training Loss: 0.00001810
2024-11-06 14:07:22,852 - INFO - Epoch [182/300], Batch [36/43], Training Loss: 0.00002004
2024-11-06 14:07:22,858 - INFO - Epoch [182/300], Batch [37/43], Training Loss: 0.00000684
2024-11-06 14:07:22,863 - INFO - Epoch [182/300], Batch [38/43], Training Loss: 0.00002345
2024-11-06 14:07:22,868 - INFO - Epoch [182/300], Batch [39/43], Training Loss: 0.00000955
2024-11-06 14:07:22,873 - INFO - Epoch [182/300], Batch [40/43], Training Loss: 0.00000734
2024-11-06 14:07:22,877 - INFO - Epoch [182/300], Batch [41/43], Training Loss: 0.00000478
2024-11-06 14:07:22,881 - INFO - Epoch [182/300], Batch [42/43], Training Loss: 0.00001279
2024-11-06 14:07:22,886 - INFO - Epoch [182/300], Batch [43/43], Training Loss: 0.00000642
2024-11-06 14:07:22,899 - INFO - Epoch [182/300], Average Training Loss: 0.00001103, Validation Loss: 0.00001247
2024-11-06 14:07:22,904 - INFO - Epoch [183/300], Batch [1/43], Training Loss: 0.00000865
2024-11-06 14:07:22,908 - INFO - Epoch [183/300], Batch [2/43], Training Loss: 0.00000979
2024-11-06 14:07:22,913 - INFO - Epoch [183/300], Batch [3/43], Training Loss: 0.00001829
2024-11-06 14:07:22,917 - INFO - Epoch [183/300], Batch [4/43], Training Loss: 0.00000777
2024-11-06 14:07:22,921 - INFO - Epoch [183/300], Batch [5/43], Training Loss: 0.00000940
2024-11-06 14:07:22,926 - INFO - Epoch [183/300], Batch [6/43], Training Loss: 0.00000730
2024-11-06 14:07:22,930 - INFO - Epoch [183/300], Batch [7/43], Training Loss: 0.00000769
2024-11-06 14:07:22,934 - INFO - Epoch [183/300], Batch [8/43], Training Loss: 0.00001809
2024-11-06 14:07:22,938 - INFO - Epoch [183/300], Batch [9/43], Training Loss: 0.00000736
2024-11-06 14:07:22,942 - INFO - Epoch [183/300], Batch [10/43], Training Loss: 0.00000218
2024-11-06 14:07:22,947 - INFO - Epoch [183/300], Batch [11/43], Training Loss: 0.00001301
2024-11-06 14:07:22,952 - INFO - Epoch [183/300], Batch [12/43], Training Loss: 0.00000914
2024-11-06 14:07:22,956 - INFO - Epoch [183/300], Batch [13/43], Training Loss: 0.00001810
2024-11-06 14:07:22,961 - INFO - Epoch [183/300], Batch [14/43], Training Loss: 0.00001814
2024-11-06 14:07:22,965 - INFO - Epoch [183/300], Batch [15/43], Training Loss: 0.00000527
2024-11-06 14:07:22,969 - INFO - Epoch [183/300], Batch [16/43], Training Loss: 0.00000435
2024-11-06 14:07:22,974 - INFO - Epoch [183/300], Batch [17/43], Training Loss: 0.00001756
2024-11-06 14:07:22,978 - INFO - Epoch [183/300], Batch [18/43], Training Loss: 0.00000877
2024-11-06 14:07:22,981 - INFO - Epoch [183/300], Batch [19/43], Training Loss: 0.00000882
2024-11-06 14:07:22,986 - INFO - Epoch [183/300], Batch [20/43], Training Loss: 0.00001400
2024-11-06 14:07:22,992 - INFO - Epoch [183/300], Batch [21/43], Training Loss: 0.00001173
2024-11-06 14:07:22,997 - INFO - Epoch [183/300], Batch [22/43], Training Loss: 0.00001136
2024-11-06 14:07:23,001 - INFO - Epoch [183/300], Batch [23/43], Training Loss: 0.00000120
2024-11-06 14:07:23,008 - INFO - Epoch [183/300], Batch [24/43], Training Loss: 0.00001304
2024-11-06 14:07:23,013 - INFO - Epoch [183/300], Batch [25/43], Training Loss: 0.00000440
2024-11-06 14:07:23,017 - INFO - Epoch [183/300], Batch [26/43], Training Loss: 0.00002038
2024-11-06 14:07:23,020 - INFO - Epoch [183/300], Batch [27/43], Training Loss: 0.00001774
2024-11-06 14:07:23,025 - INFO - Epoch [183/300], Batch [28/43], Training Loss: 0.00001148
2024-11-06 14:07:23,030 - INFO - Epoch [183/300], Batch [29/43], Training Loss: 0.00001559
2024-11-06 14:07:23,036 - INFO - Epoch [183/300], Batch [30/43], Training Loss: 0.00000435
2024-11-06 14:07:23,040 - INFO - Epoch [183/300], Batch [31/43], Training Loss: 0.00001922
2024-11-06 14:07:23,044 - INFO - Epoch [183/300], Batch [32/43], Training Loss: 0.00001726
2024-11-06 14:07:23,048 - INFO - Epoch [183/300], Batch [33/43], Training Loss: 0.00000674
2024-11-06 14:07:23,053 - INFO - Epoch [183/300], Batch [34/43], Training Loss: 0.00001885
2024-11-06 14:07:23,057 - INFO - Epoch [183/300], Batch [35/43], Training Loss: 0.00000672
2024-11-06 14:07:23,062 - INFO - Epoch [183/300], Batch [36/43], Training Loss: 0.00000710
2024-11-06 14:07:23,067 - INFO - Epoch [183/300], Batch [37/43], Training Loss: 0.00001187
2024-11-06 14:07:23,071 - INFO - Epoch [183/300], Batch [38/43], Training Loss: 0.00001709
2024-11-06 14:07:23,075 - INFO - Epoch [183/300], Batch [39/43], Training Loss: 0.00000575
2024-11-06 14:07:23,079 - INFO - Epoch [183/300], Batch [40/43], Training Loss: 0.00000492
2024-11-06 14:07:23,083 - INFO - Epoch [183/300], Batch [41/43], Training Loss: 0.00000119
2024-11-06 14:07:23,087 - INFO - Epoch [183/300], Batch [42/43], Training Loss: 0.00000644
2024-11-06 14:07:23,092 - INFO - Epoch [183/300], Batch [43/43], Training Loss: 0.00000331
2024-11-06 14:07:23,106 - INFO - Epoch [183/300], Average Training Loss: 0.00001050, Validation Loss: 0.00001248
2024-11-06 14:07:23,111 - INFO - Epoch [184/300], Batch [1/43], Training Loss: 0.00001237
2024-11-06 14:07:23,117 - INFO - Epoch [184/300], Batch [2/43], Training Loss: 0.00000709
2024-11-06 14:07:23,122 - INFO - Epoch [184/300], Batch [3/43], Training Loss: 0.00001359
2024-11-06 14:07:23,129 - INFO - Epoch [184/300], Batch [4/43], Training Loss: 0.00000522
2024-11-06 14:07:23,134 - INFO - Epoch [184/300], Batch [5/43], Training Loss: 0.00002074
2024-11-06 14:07:23,138 - INFO - Epoch [184/300], Batch [6/43], Training Loss: 0.00001253
2024-11-06 14:07:23,147 - INFO - Epoch [184/300], Batch [7/43], Training Loss: 0.00001011
2024-11-06 14:07:23,154 - INFO - Epoch [184/300], Batch [8/43], Training Loss: 0.00000638
2024-11-06 14:07:23,164 - INFO - Epoch [184/300], Batch [9/43], Training Loss: 0.00000633
2024-11-06 14:07:23,171 - INFO - Epoch [184/300], Batch [10/43], Training Loss: 0.00001525
2024-11-06 14:07:23,178 - INFO - Epoch [184/300], Batch [11/43], Training Loss: 0.00001177
2024-11-06 14:07:23,184 - INFO - Epoch [184/300], Batch [12/43], Training Loss: 0.00001643
2024-11-06 14:07:23,190 - INFO - Epoch [184/300], Batch [13/43], Training Loss: 0.00001320
2024-11-06 14:07:23,195 - INFO - Epoch [184/300], Batch [14/43], Training Loss: 0.00001498
2024-11-06 14:07:23,213 - INFO - Epoch [184/300], Batch [15/43], Training Loss: 0.00001905
2024-11-06 14:07:23,219 - INFO - Epoch [184/300], Batch [16/43], Training Loss: 0.00001412
2024-11-06 14:07:23,223 - INFO - Epoch [184/300], Batch [17/43], Training Loss: 0.00002236
2024-11-06 14:07:23,227 - INFO - Epoch [184/300], Batch [18/43], Training Loss: 0.00000762
2024-11-06 14:07:23,231 - INFO - Epoch [184/300], Batch [19/43], Training Loss: 0.00000585
2024-11-06 14:07:23,235 - INFO - Epoch [184/300], Batch [20/43], Training Loss: 0.00001337
2024-11-06 14:07:23,239 - INFO - Epoch [184/300], Batch [21/43], Training Loss: 0.00000419
2024-11-06 14:07:23,244 - INFO - Epoch [184/300], Batch [22/43], Training Loss: 0.00000655
2024-11-06 14:07:23,248 - INFO - Epoch [184/300], Batch [23/43], Training Loss: 0.00000727
2024-11-06 14:07:23,253 - INFO - Epoch [184/300], Batch [24/43], Training Loss: 0.00000758
2024-11-06 14:07:23,257 - INFO - Epoch [184/300], Batch [25/43], Training Loss: 0.00000236
2024-11-06 14:07:23,262 - INFO - Epoch [184/300], Batch [26/43], Training Loss: 0.00000427
2024-11-06 14:07:23,266 - INFO - Epoch [184/300], Batch [27/43], Training Loss: 0.00000821
2024-11-06 14:07:23,271 - INFO - Epoch [184/300], Batch [28/43], Training Loss: 0.00000577
2024-11-06 14:07:23,276 - INFO - Epoch [184/300], Batch [29/43], Training Loss: 0.00000762
2024-11-06 14:07:23,281 - INFO - Epoch [184/300], Batch [30/43], Training Loss: 0.00003281
2024-11-06 14:07:23,286 - INFO - Epoch [184/300], Batch [31/43], Training Loss: 0.00001113
2024-11-06 14:07:23,291 - INFO - Epoch [184/300], Batch [32/43], Training Loss: 0.00000363
2024-11-06 14:07:23,296 - INFO - Epoch [184/300], Batch [33/43], Training Loss: 0.00000641
2024-11-06 14:07:23,300 - INFO - Epoch [184/300], Batch [34/43], Training Loss: 0.00001212
2024-11-06 14:07:23,306 - INFO - Epoch [184/300], Batch [35/43], Training Loss: 0.00000545
2024-11-06 14:07:23,311 - INFO - Epoch [184/300], Batch [36/43], Training Loss: 0.00001686
2024-11-06 14:07:23,316 - INFO - Epoch [184/300], Batch [37/43], Training Loss: 0.00000515
2024-11-06 14:07:23,321 - INFO - Epoch [184/300], Batch [38/43], Training Loss: 0.00001167
2024-11-06 14:07:23,325 - INFO - Epoch [184/300], Batch [39/43], Training Loss: 0.00000623
2024-11-06 14:07:23,329 - INFO - Epoch [184/300], Batch [40/43], Training Loss: 0.00002689
2024-11-06 14:07:23,334 - INFO - Epoch [184/300], Batch [41/43], Training Loss: 0.00000646
2024-11-06 14:07:23,338 - INFO - Epoch [184/300], Batch [42/43], Training Loss: 0.00001606
2024-11-06 14:07:23,342 - INFO - Epoch [184/300], Batch [43/43], Training Loss: 0.00001328
2024-11-06 14:07:23,354 - INFO - Epoch [184/300], Average Training Loss: 0.00001108, Validation Loss: 0.00001373
2024-11-06 14:07:23,359 - INFO - Epoch [185/300], Batch [1/43], Training Loss: 0.00001500
2024-11-06 14:07:23,364 - INFO - Epoch [185/300], Batch [2/43], Training Loss: 0.00000694
2024-11-06 14:07:23,368 - INFO - Epoch [185/300], Batch [3/43], Training Loss: 0.00000572
2024-11-06 14:07:23,373 - INFO - Epoch [185/300], Batch [4/43], Training Loss: 0.00002808
2024-11-06 14:07:23,378 - INFO - Epoch [185/300], Batch [5/43], Training Loss: 0.00000381
2024-11-06 14:07:23,383 - INFO - Epoch [185/300], Batch [6/43], Training Loss: 0.00000704
2024-11-06 14:07:23,388 - INFO - Epoch [185/300], Batch [7/43], Training Loss: 0.00002078
2024-11-06 14:07:23,392 - INFO - Epoch [185/300], Batch [8/43], Training Loss: 0.00000570
2024-11-06 14:07:23,396 - INFO - Epoch [185/300], Batch [9/43], Training Loss: 0.00001885
2024-11-06 14:07:23,400 - INFO - Epoch [185/300], Batch [10/43], Training Loss: 0.00001080
2024-11-06 14:07:23,404 - INFO - Epoch [185/300], Batch [11/43], Training Loss: 0.00001248
2024-11-06 14:07:23,409 - INFO - Epoch [185/300], Batch [12/43], Training Loss: 0.00000584
2024-11-06 14:07:23,413 - INFO - Epoch [185/300], Batch [13/43], Training Loss: 0.00000536
2024-11-06 14:07:23,417 - INFO - Epoch [185/300], Batch [14/43], Training Loss: 0.00000801
2024-11-06 14:07:23,421 - INFO - Epoch [185/300], Batch [15/43], Training Loss: 0.00000628
2024-11-06 14:07:23,424 - INFO - Epoch [185/300], Batch [16/43], Training Loss: 0.00000881
2024-11-06 14:07:23,427 - INFO - Epoch [185/300], Batch [17/43], Training Loss: 0.00001704
2024-11-06 14:07:23,431 - INFO - Epoch [185/300], Batch [18/43], Training Loss: 0.00000595
2024-11-06 14:07:23,435 - INFO - Epoch [185/300], Batch [19/43], Training Loss: 0.00001934
2024-11-06 14:07:23,439 - INFO - Epoch [185/300], Batch [20/43], Training Loss: 0.00001346
2024-11-06 14:07:23,443 - INFO - Epoch [185/300], Batch [21/43], Training Loss: 0.00000511
2024-11-06 14:07:23,446 - INFO - Epoch [185/300], Batch [22/43], Training Loss: 0.00001420
2024-11-06 14:07:23,451 - INFO - Epoch [185/300], Batch [23/43], Training Loss: 0.00001020
2024-11-06 14:07:23,454 - INFO - Epoch [185/300], Batch [24/43], Training Loss: 0.00001729
2024-11-06 14:07:23,458 - INFO - Epoch [185/300], Batch [25/43], Training Loss: 0.00001757
2024-11-06 14:07:23,462 - INFO - Epoch [185/300], Batch [26/43], Training Loss: 0.00000476
2024-11-06 14:07:23,466 - INFO - Epoch [185/300], Batch [27/43], Training Loss: 0.00000977
2024-11-06 14:07:23,470 - INFO - Epoch [185/300], Batch [28/43], Training Loss: 0.00001056
2024-11-06 14:07:23,473 - INFO - Epoch [185/300], Batch [29/43], Training Loss: 0.00001072
2024-11-06 14:07:23,476 - INFO - Epoch [185/300], Batch [30/43], Training Loss: 0.00000435
2024-11-06 14:07:23,479 - INFO - Epoch [185/300], Batch [31/43], Training Loss: 0.00002106
2024-11-06 14:07:23,483 - INFO - Epoch [185/300], Batch [32/43], Training Loss: 0.00001141
2024-11-06 14:07:23,486 - INFO - Epoch [185/300], Batch [33/43], Training Loss: 0.00002499
2024-11-06 14:07:23,489 - INFO - Epoch [185/300], Batch [34/43], Training Loss: 0.00000563
2024-11-06 14:07:23,493 - INFO - Epoch [185/300], Batch [35/43], Training Loss: 0.00000932
2024-11-06 14:07:23,498 - INFO - Epoch [185/300], Batch [36/43], Training Loss: 0.00000762
2024-11-06 14:07:23,502 - INFO - Epoch [185/300], Batch [37/43], Training Loss: 0.00001006
2024-11-06 14:07:23,505 - INFO - Epoch [185/300], Batch [38/43], Training Loss: 0.00000577
2024-11-06 14:07:23,510 - INFO - Epoch [185/300], Batch [39/43], Training Loss: 0.00001379
2024-11-06 14:07:23,514 - INFO - Epoch [185/300], Batch [40/43], Training Loss: 0.00000936
2024-11-06 14:07:23,518 - INFO - Epoch [185/300], Batch [41/43], Training Loss: 0.00000658
2024-11-06 14:07:23,522 - INFO - Epoch [185/300], Batch [42/43], Training Loss: 0.00001019
2024-11-06 14:07:23,527 - INFO - Epoch [185/300], Batch [43/43], Training Loss: 0.00000395
2024-11-06 14:07:23,539 - INFO - Epoch [185/300], Average Training Loss: 0.00001092, Validation Loss: 0.00001284
2024-11-06 14:07:23,543 - INFO - Epoch [186/300], Batch [1/43], Training Loss: 0.00001039
2024-11-06 14:07:23,547 - INFO - Epoch [186/300], Batch [2/43], Training Loss: 0.00001306
2024-11-06 14:07:23,550 - INFO - Epoch [186/300], Batch [3/43], Training Loss: 0.00000610
2024-11-06 14:07:23,554 - INFO - Epoch [186/300], Batch [4/43], Training Loss: 0.00001469
2024-11-06 14:07:23,558 - INFO - Epoch [186/300], Batch [5/43], Training Loss: 0.00000889
2024-11-06 14:07:23,561 - INFO - Epoch [186/300], Batch [6/43], Training Loss: 0.00000873
2024-11-06 14:07:23,565 - INFO - Epoch [186/300], Batch [7/43], Training Loss: 0.00000719
2024-11-06 14:07:23,569 - INFO - Epoch [186/300], Batch [8/43], Training Loss: 0.00000367
2024-11-06 14:07:23,572 - INFO - Epoch [186/300], Batch [9/43], Training Loss: 0.00000708
2024-11-06 14:07:23,575 - INFO - Epoch [186/300], Batch [10/43], Training Loss: 0.00001105
2024-11-06 14:07:23,578 - INFO - Epoch [186/300], Batch [11/43], Training Loss: 0.00001889
2024-11-06 14:07:23,582 - INFO - Epoch [186/300], Batch [12/43], Training Loss: 0.00002335
2024-11-06 14:07:23,586 - INFO - Epoch [186/300], Batch [13/43], Training Loss: 0.00001313
2024-11-06 14:07:23,589 - INFO - Epoch [186/300], Batch [14/43], Training Loss: 0.00000602
2024-11-06 14:07:23,593 - INFO - Epoch [186/300], Batch [15/43], Training Loss: 0.00001557
2024-11-06 14:07:23,596 - INFO - Epoch [186/300], Batch [16/43], Training Loss: 0.00001108
2024-11-06 14:07:23,599 - INFO - Epoch [186/300], Batch [17/43], Training Loss: 0.00000861
2024-11-06 14:07:23,602 - INFO - Epoch [186/300], Batch [18/43], Training Loss: 0.00001292
2024-11-06 14:07:23,606 - INFO - Epoch [186/300], Batch [19/43], Training Loss: 0.00000843
2024-11-06 14:07:23,609 - INFO - Epoch [186/300], Batch [20/43], Training Loss: 0.00000456
2024-11-06 14:07:23,612 - INFO - Epoch [186/300], Batch [21/43], Training Loss: 0.00000153
2024-11-06 14:07:23,615 - INFO - Epoch [186/300], Batch [22/43], Training Loss: 0.00000707
2024-11-06 14:07:23,618 - INFO - Epoch [186/300], Batch [23/43], Training Loss: 0.00000820
2024-11-06 14:07:23,621 - INFO - Epoch [186/300], Batch [24/43], Training Loss: 0.00000800
2024-11-06 14:07:23,624 - INFO - Epoch [186/300], Batch [25/43], Training Loss: 0.00002084
2024-11-06 14:07:23,628 - INFO - Epoch [186/300], Batch [26/43], Training Loss: 0.00002247
2024-11-06 14:07:23,632 - INFO - Epoch [186/300], Batch [27/43], Training Loss: 0.00000535
2024-11-06 14:07:23,635 - INFO - Epoch [186/300], Batch [28/43], Training Loss: 0.00001343
2024-11-06 14:07:23,639 - INFO - Epoch [186/300], Batch [29/43], Training Loss: 0.00000913
2024-11-06 14:07:23,643 - INFO - Epoch [186/300], Batch [30/43], Training Loss: 0.00001048
2024-11-06 14:07:23,646 - INFO - Epoch [186/300], Batch [31/43], Training Loss: 0.00001125
2024-11-06 14:07:23,649 - INFO - Epoch [186/300], Batch [32/43], Training Loss: 0.00001542
2024-11-06 14:07:23,652 - INFO - Epoch [186/300], Batch [33/43], Training Loss: 0.00000657
2024-11-06 14:07:23,656 - INFO - Epoch [186/300], Batch [34/43], Training Loss: 0.00000540
2024-11-06 14:07:23,659 - INFO - Epoch [186/300], Batch [35/43], Training Loss: 0.00000890
2024-11-06 14:07:23,662 - INFO - Epoch [186/300], Batch [36/43], Training Loss: 0.00001309
2024-11-06 14:07:23,666 - INFO - Epoch [186/300], Batch [37/43], Training Loss: 0.00000408
2024-11-06 14:07:23,670 - INFO - Epoch [186/300], Batch [38/43], Training Loss: 0.00000661
2024-11-06 14:07:23,673 - INFO - Epoch [186/300], Batch [39/43], Training Loss: 0.00001062
2024-11-06 14:07:23,676 - INFO - Epoch [186/300], Batch [40/43], Training Loss: 0.00000760
2024-11-06 14:07:23,680 - INFO - Epoch [186/300], Batch [41/43], Training Loss: 0.00000718
2024-11-06 14:07:23,684 - INFO - Epoch [186/300], Batch [42/43], Training Loss: 0.00002710
2024-11-06 14:07:23,689 - INFO - Epoch [186/300], Batch [43/43], Training Loss: 0.00000390
2024-11-06 14:07:23,702 - INFO - Epoch [186/300], Average Training Loss: 0.00001041, Validation Loss: 0.00001287
2024-11-06 14:07:23,706 - INFO - Epoch [187/300], Batch [1/43], Training Loss: 0.00000757
2024-11-06 14:07:23,711 - INFO - Epoch [187/300], Batch [2/43], Training Loss: 0.00000894
2024-11-06 14:07:23,716 - INFO - Epoch [187/300], Batch [3/43], Training Loss: 0.00001637
2024-11-06 14:07:23,721 - INFO - Epoch [187/300], Batch [4/43], Training Loss: 0.00000992
2024-11-06 14:07:23,726 - INFO - Epoch [187/300], Batch [5/43], Training Loss: 0.00000689
2024-11-06 14:07:23,730 - INFO - Epoch [187/300], Batch [6/43], Training Loss: 0.00002095
2024-11-06 14:07:23,734 - INFO - Epoch [187/300], Batch [7/43], Training Loss: 0.00001145
2024-11-06 14:07:23,739 - INFO - Epoch [187/300], Batch [8/43], Training Loss: 0.00001394
2024-11-06 14:07:23,744 - INFO - Epoch [187/300], Batch [9/43], Training Loss: 0.00000494
2024-11-06 14:07:23,747 - INFO - Epoch [187/300], Batch [10/43], Training Loss: 0.00000527
2024-11-06 14:07:23,751 - INFO - Epoch [187/300], Batch [11/43], Training Loss: 0.00000662
2024-11-06 14:07:23,754 - INFO - Epoch [187/300], Batch [12/43], Training Loss: 0.00001074
2024-11-06 14:07:23,758 - INFO - Epoch [187/300], Batch [13/43], Training Loss: 0.00001006
2024-11-06 14:07:23,762 - INFO - Epoch [187/300], Batch [14/43], Training Loss: 0.00001251
2024-11-06 14:07:23,765 - INFO - Epoch [187/300], Batch [15/43], Training Loss: 0.00000642
2024-11-06 14:07:23,768 - INFO - Epoch [187/300], Batch [16/43], Training Loss: 0.00001068
2024-11-06 14:07:23,771 - INFO - Epoch [187/300], Batch [17/43], Training Loss: 0.00000684
2024-11-06 14:07:23,775 - INFO - Epoch [187/300], Batch [18/43], Training Loss: 0.00001897
2024-11-06 14:07:23,779 - INFO - Epoch [187/300], Batch [19/43], Training Loss: 0.00000957
2024-11-06 14:07:23,783 - INFO - Epoch [187/300], Batch [20/43], Training Loss: 0.00001008
2024-11-06 14:07:23,788 - INFO - Epoch [187/300], Batch [21/43], Training Loss: 0.00001181
2024-11-06 14:07:23,791 - INFO - Epoch [187/300], Batch [22/43], Training Loss: 0.00000402
2024-11-06 14:07:23,795 - INFO - Epoch [187/300], Batch [23/43], Training Loss: 0.00001102
2024-11-06 14:07:23,799 - INFO - Epoch [187/300], Batch [24/43], Training Loss: 0.00000644
2024-11-06 14:07:23,802 - INFO - Epoch [187/300], Batch [25/43], Training Loss: 0.00000935
2024-11-06 14:07:23,807 - INFO - Epoch [187/300], Batch [26/43], Training Loss: 0.00000658
2024-11-06 14:07:23,811 - INFO - Epoch [187/300], Batch [27/43], Training Loss: 0.00002682
2024-11-06 14:07:23,816 - INFO - Epoch [187/300], Batch [28/43], Training Loss: 0.00000889
2024-11-06 14:07:23,820 - INFO - Epoch [187/300], Batch [29/43], Training Loss: 0.00000893
2024-11-06 14:07:23,824 - INFO - Epoch [187/300], Batch [30/43], Training Loss: 0.00000662
2024-11-06 14:07:23,828 - INFO - Epoch [187/300], Batch [31/43], Training Loss: 0.00000828
2024-11-06 14:07:23,831 - INFO - Epoch [187/300], Batch [32/43], Training Loss: 0.00001175
2024-11-06 14:07:23,835 - INFO - Epoch [187/300], Batch [33/43], Training Loss: 0.00000279
2024-11-06 14:07:23,839 - INFO - Epoch [187/300], Batch [34/43], Training Loss: 0.00000668
2024-11-06 14:07:23,843 - INFO - Epoch [187/300], Batch [35/43], Training Loss: 0.00001611
2024-11-06 14:07:23,847 - INFO - Epoch [187/300], Batch [36/43], Training Loss: 0.00000597
2024-11-06 14:07:23,851 - INFO - Epoch [187/300], Batch [37/43], Training Loss: 0.00002666
2024-11-06 14:07:23,855 - INFO - Epoch [187/300], Batch [38/43], Training Loss: 0.00000727
2024-11-06 14:07:23,860 - INFO - Epoch [187/300], Batch [39/43], Training Loss: 0.00001282
2024-11-06 14:07:23,864 - INFO - Epoch [187/300], Batch [40/43], Training Loss: 0.00002140
2024-11-06 14:07:23,869 - INFO - Epoch [187/300], Batch [41/43], Training Loss: 0.00000162
2024-11-06 14:07:23,875 - INFO - Epoch [187/300], Batch [42/43], Training Loss: 0.00000631
2024-11-06 14:07:23,880 - INFO - Epoch [187/300], Batch [43/43], Training Loss: 0.00002052
2024-11-06 14:07:23,893 - INFO - Epoch [187/300], Average Training Loss: 0.00001064, Validation Loss: 0.00001209
2024-11-06 14:07:23,898 - INFO - Epoch [188/300], Batch [1/43], Training Loss: 0.00001432
2024-11-06 14:07:23,901 - INFO - Epoch [188/300], Batch [2/43], Training Loss: 0.00000467
2024-11-06 14:07:23,905 - INFO - Epoch [188/300], Batch [3/43], Training Loss: 0.00000671
2024-11-06 14:07:23,908 - INFO - Epoch [188/300], Batch [4/43], Training Loss: 0.00000979
2024-11-06 14:07:23,913 - INFO - Epoch [188/300], Batch [5/43], Training Loss: 0.00000808
2024-11-06 14:07:23,916 - INFO - Epoch [188/300], Batch [6/43], Training Loss: 0.00001701
2024-11-06 14:07:23,921 - INFO - Epoch [188/300], Batch [7/43], Training Loss: 0.00000517
2024-11-06 14:07:23,924 - INFO - Epoch [188/300], Batch [8/43], Training Loss: 0.00001234
2024-11-06 14:07:23,928 - INFO - Epoch [188/300], Batch [9/43], Training Loss: 0.00001987
2024-11-06 14:07:23,932 - INFO - Epoch [188/300], Batch [10/43], Training Loss: 0.00001237
2024-11-06 14:07:23,937 - INFO - Epoch [188/300], Batch [11/43], Training Loss: 0.00000304
2024-11-06 14:07:23,941 - INFO - Epoch [188/300], Batch [12/43], Training Loss: 0.00000558
2024-11-06 14:07:23,945 - INFO - Epoch [188/300], Batch [13/43], Training Loss: 0.00000785
2024-11-06 14:07:23,949 - INFO - Epoch [188/300], Batch [14/43], Training Loss: 0.00000685
2024-11-06 14:07:23,953 - INFO - Epoch [188/300], Batch [15/43], Training Loss: 0.00001071
2024-11-06 14:07:23,956 - INFO - Epoch [188/300], Batch [16/43], Training Loss: 0.00000883
2024-11-06 14:07:23,960 - INFO - Epoch [188/300], Batch [17/43], Training Loss: 0.00000383
2024-11-06 14:07:23,964 - INFO - Epoch [188/300], Batch [18/43], Training Loss: 0.00000682
2024-11-06 14:07:23,968 - INFO - Epoch [188/300], Batch [19/43], Training Loss: 0.00003828
2024-11-06 14:07:23,971 - INFO - Epoch [188/300], Batch [20/43], Training Loss: 0.00000830
2024-11-06 14:07:23,974 - INFO - Epoch [188/300], Batch [21/43], Training Loss: 0.00001443
2024-11-06 14:07:23,977 - INFO - Epoch [188/300], Batch [22/43], Training Loss: 0.00001065
2024-11-06 14:07:23,981 - INFO - Epoch [188/300], Batch [23/43], Training Loss: 0.00001695
2024-11-06 14:07:23,984 - INFO - Epoch [188/300], Batch [24/43], Training Loss: 0.00001610
2024-11-06 14:07:23,989 - INFO - Epoch [188/300], Batch [25/43], Training Loss: 0.00000938
2024-11-06 14:07:23,993 - INFO - Epoch [188/300], Batch [26/43], Training Loss: 0.00000704
2024-11-06 14:07:23,997 - INFO - Epoch [188/300], Batch [27/43], Training Loss: 0.00000542
2024-11-06 14:07:24,001 - INFO - Epoch [188/300], Batch [28/43], Training Loss: 0.00002621
2024-11-06 14:07:24,005 - INFO - Epoch [188/300], Batch [29/43], Training Loss: 0.00001408
2024-11-06 14:07:24,008 - INFO - Epoch [188/300], Batch [30/43], Training Loss: 0.00000492
2024-11-06 14:07:24,013 - INFO - Epoch [188/300], Batch [31/43], Training Loss: 0.00001281
2024-11-06 14:07:24,016 - INFO - Epoch [188/300], Batch [32/43], Training Loss: 0.00000575
2024-11-06 14:07:24,019 - INFO - Epoch [188/300], Batch [33/43], Training Loss: 0.00001710
2024-11-06 14:07:24,023 - INFO - Epoch [188/300], Batch [34/43], Training Loss: 0.00000758
2024-11-06 14:07:24,027 - INFO - Epoch [188/300], Batch [35/43], Training Loss: 0.00000725
2024-11-06 14:07:24,031 - INFO - Epoch [188/300], Batch [36/43], Training Loss: 0.00001713
2024-11-06 14:07:24,035 - INFO - Epoch [188/300], Batch [37/43], Training Loss: 0.00000444
2024-11-06 14:07:24,039 - INFO - Epoch [188/300], Batch [38/43], Training Loss: 0.00001926
2024-11-06 14:07:24,043 - INFO - Epoch [188/300], Batch [39/43], Training Loss: 0.00002221
2024-11-06 14:07:24,047 - INFO - Epoch [188/300], Batch [40/43], Training Loss: 0.00000785
2024-11-06 14:07:24,052 - INFO - Epoch [188/300], Batch [41/43], Training Loss: 0.00000623
2024-11-06 14:07:24,057 - INFO - Epoch [188/300], Batch [42/43], Training Loss: 0.00001307
2024-11-06 14:07:24,063 - INFO - Epoch [188/300], Batch [43/43], Training Loss: 0.00001396
2024-11-06 14:07:24,075 - INFO - Epoch [188/300], Average Training Loss: 0.00001140, Validation Loss: 0.00001443
2024-11-06 14:07:24,080 - INFO - Epoch [189/300], Batch [1/43], Training Loss: 0.00001724
2024-11-06 14:07:24,085 - INFO - Epoch [189/300], Batch [2/43], Training Loss: 0.00000750
2024-11-06 14:07:24,089 - INFO - Epoch [189/300], Batch [3/43], Training Loss: 0.00001068
2024-11-06 14:07:24,093 - INFO - Epoch [189/300], Batch [4/43], Training Loss: 0.00003145
2024-11-06 14:07:24,097 - INFO - Epoch [189/300], Batch [5/43], Training Loss: 0.00001066
2024-11-06 14:07:24,101 - INFO - Epoch [189/300], Batch [6/43], Training Loss: 0.00001656
2024-11-06 14:07:24,106 - INFO - Epoch [189/300], Batch [7/43], Training Loss: 0.00003945
2024-11-06 14:07:24,110 - INFO - Epoch [189/300], Batch [8/43], Training Loss: 0.00000946
2024-11-06 14:07:24,114 - INFO - Epoch [189/300], Batch [9/43], Training Loss: 0.00001042
2024-11-06 14:07:24,119 - INFO - Epoch [189/300], Batch [10/43], Training Loss: 0.00000501
2024-11-06 14:07:24,123 - INFO - Epoch [189/300], Batch [11/43], Training Loss: 0.00001928
2024-11-06 14:07:24,126 - INFO - Epoch [189/300], Batch [12/43], Training Loss: 0.00002668
2024-11-06 14:07:24,130 - INFO - Epoch [189/300], Batch [13/43], Training Loss: 0.00001262
2024-11-06 14:07:24,134 - INFO - Epoch [189/300], Batch [14/43], Training Loss: 0.00001011
2024-11-06 14:07:24,138 - INFO - Epoch [189/300], Batch [15/43], Training Loss: 0.00001512
2024-11-06 14:07:24,141 - INFO - Epoch [189/300], Batch [16/43], Training Loss: 0.00002205
2024-11-06 14:07:24,146 - INFO - Epoch [189/300], Batch [17/43], Training Loss: 0.00000820
2024-11-06 14:07:24,150 - INFO - Epoch [189/300], Batch [18/43], Training Loss: 0.00001497
2024-11-06 14:07:24,154 - INFO - Epoch [189/300], Batch [19/43], Training Loss: 0.00000916
2024-11-06 14:07:24,158 - INFO - Epoch [189/300], Batch [20/43], Training Loss: 0.00000962
2024-11-06 14:07:24,162 - INFO - Epoch [189/300], Batch [21/43], Training Loss: 0.00000969
2024-11-06 14:07:24,165 - INFO - Epoch [189/300], Batch [22/43], Training Loss: 0.00000893
2024-11-06 14:07:24,169 - INFO - Epoch [189/300], Batch [23/43], Training Loss: 0.00000910
2024-11-06 14:07:24,173 - INFO - Epoch [189/300], Batch [24/43], Training Loss: 0.00001239
2024-11-06 14:07:24,177 - INFO - Epoch [189/300], Batch [25/43], Training Loss: 0.00000972
2024-11-06 14:07:24,181 - INFO - Epoch [189/300], Batch [26/43], Training Loss: 0.00000720
2024-11-06 14:07:24,185 - INFO - Epoch [189/300], Batch [27/43], Training Loss: 0.00000803
2024-11-06 14:07:24,188 - INFO - Epoch [189/300], Batch [28/43], Training Loss: 0.00001528
2024-11-06 14:07:24,192 - INFO - Epoch [189/300], Batch [29/43], Training Loss: 0.00000532
2024-11-06 14:07:24,196 - INFO - Epoch [189/300], Batch [30/43], Training Loss: 0.00001429
2024-11-06 14:07:24,200 - INFO - Epoch [189/300], Batch [31/43], Training Loss: 0.00000880
2024-11-06 14:07:24,204 - INFO - Epoch [189/300], Batch [32/43], Training Loss: 0.00001204
2024-11-06 14:07:24,207 - INFO - Epoch [189/300], Batch [33/43], Training Loss: 0.00000664
2024-11-06 14:07:24,212 - INFO - Epoch [189/300], Batch [34/43], Training Loss: 0.00000276
2024-11-06 14:07:24,216 - INFO - Epoch [189/300], Batch [35/43], Training Loss: 0.00000648
2024-11-06 14:07:24,221 - INFO - Epoch [189/300], Batch [36/43], Training Loss: 0.00000384
2024-11-06 14:07:24,225 - INFO - Epoch [189/300], Batch [37/43], Training Loss: 0.00001338
2024-11-06 14:07:24,229 - INFO - Epoch [189/300], Batch [38/43], Training Loss: 0.00000378
2024-11-06 14:07:24,232 - INFO - Epoch [189/300], Batch [39/43], Training Loss: 0.00001051
2024-11-06 14:07:24,236 - INFO - Epoch [189/300], Batch [40/43], Training Loss: 0.00000992
2024-11-06 14:07:24,239 - INFO - Epoch [189/300], Batch [41/43], Training Loss: 0.00000758
2024-11-06 14:07:24,244 - INFO - Epoch [189/300], Batch [42/43], Training Loss: 0.00000419
2024-11-06 14:07:24,248 - INFO - Epoch [189/300], Batch [43/43], Training Loss: 0.00000713
2024-11-06 14:07:24,262 - INFO - Epoch [189/300], Average Training Loss: 0.00001170, Validation Loss: 0.00001334
2024-11-06 14:07:24,267 - INFO - Epoch [190/300], Batch [1/43], Training Loss: 0.00001372
2024-11-06 14:07:24,271 - INFO - Epoch [190/300], Batch [2/43], Training Loss: 0.00000341
2024-11-06 14:07:24,275 - INFO - Epoch [190/300], Batch [3/43], Training Loss: 0.00000448
2024-11-06 14:07:24,279 - INFO - Epoch [190/300], Batch [4/43], Training Loss: 0.00000552
2024-11-06 14:07:24,283 - INFO - Epoch [190/300], Batch [5/43], Training Loss: 0.00000289
2024-11-06 14:07:24,287 - INFO - Epoch [190/300], Batch [6/43], Training Loss: 0.00001090
2024-11-06 14:07:24,291 - INFO - Epoch [190/300], Batch [7/43], Training Loss: 0.00001022
2024-11-06 14:07:24,295 - INFO - Epoch [190/300], Batch [8/43], Training Loss: 0.00000893
2024-11-06 14:07:24,298 - INFO - Epoch [190/300], Batch [9/43], Training Loss: 0.00001503
2024-11-06 14:07:24,302 - INFO - Epoch [190/300], Batch [10/43], Training Loss: 0.00001487
2024-11-06 14:07:24,305 - INFO - Epoch [190/300], Batch [11/43], Training Loss: 0.00000673
2024-11-06 14:07:24,309 - INFO - Epoch [190/300], Batch [12/43], Training Loss: 0.00001096
2024-11-06 14:07:24,314 - INFO - Epoch [190/300], Batch [13/43], Training Loss: 0.00000974
2024-11-06 14:07:24,318 - INFO - Epoch [190/300], Batch [14/43], Training Loss: 0.00001264
2024-11-06 14:07:24,322 - INFO - Epoch [190/300], Batch [15/43], Training Loss: 0.00000677
2024-11-06 14:07:24,326 - INFO - Epoch [190/300], Batch [16/43], Training Loss: 0.00000716
2024-11-06 14:07:24,330 - INFO - Epoch [190/300], Batch [17/43], Training Loss: 0.00000680
2024-11-06 14:07:24,334 - INFO - Epoch [190/300], Batch [18/43], Training Loss: 0.00001294
2024-11-06 14:07:24,339 - INFO - Epoch [190/300], Batch [19/43], Training Loss: 0.00001181
2024-11-06 14:07:24,344 - INFO - Epoch [190/300], Batch [20/43], Training Loss: 0.00000724
2024-11-06 14:07:24,348 - INFO - Epoch [190/300], Batch [21/43], Training Loss: 0.00000730
2024-11-06 14:07:24,351 - INFO - Epoch [190/300], Batch [22/43], Training Loss: 0.00000375
2024-11-06 14:07:24,355 - INFO - Epoch [190/300], Batch [23/43], Training Loss: 0.00001592
2024-11-06 14:07:24,359 - INFO - Epoch [190/300], Batch [24/43], Training Loss: 0.00002044
2024-11-06 14:07:24,362 - INFO - Epoch [190/300], Batch [25/43], Training Loss: 0.00002703
2024-11-06 14:07:24,366 - INFO - Epoch [190/300], Batch [26/43], Training Loss: 0.00001108
2024-11-06 14:07:24,371 - INFO - Epoch [190/300], Batch [27/43], Training Loss: 0.00000823
2024-11-06 14:07:24,377 - INFO - Epoch [190/300], Batch [28/43], Training Loss: 0.00002027
2024-11-06 14:07:24,381 - INFO - Epoch [190/300], Batch [29/43], Training Loss: 0.00001024
2024-11-06 14:07:24,384 - INFO - Epoch [190/300], Batch [30/43], Training Loss: 0.00000655
2024-11-06 14:07:24,388 - INFO - Epoch [190/300], Batch [31/43], Training Loss: 0.00002900
2024-11-06 14:07:24,392 - INFO - Epoch [190/300], Batch [32/43], Training Loss: 0.00001407
2024-11-06 14:07:24,395 - INFO - Epoch [190/300], Batch [33/43], Training Loss: 0.00001263
2024-11-06 14:07:24,400 - INFO - Epoch [190/300], Batch [34/43], Training Loss: 0.00000705
2024-11-06 14:07:24,403 - INFO - Epoch [190/300], Batch [35/43], Training Loss: 0.00001071
2024-11-06 14:07:24,407 - INFO - Epoch [190/300], Batch [36/43], Training Loss: 0.00002570
2024-11-06 14:07:24,411 - INFO - Epoch [190/300], Batch [37/43], Training Loss: 0.00000774
2024-11-06 14:07:24,414 - INFO - Epoch [190/300], Batch [38/43], Training Loss: 0.00001203
2024-11-06 14:07:24,417 - INFO - Epoch [190/300], Batch [39/43], Training Loss: 0.00000698
2024-11-06 14:07:24,421 - INFO - Epoch [190/300], Batch [40/43], Training Loss: 0.00001007
2024-11-06 14:07:24,425 - INFO - Epoch [190/300], Batch [41/43], Training Loss: 0.00000437
2024-11-06 14:07:24,428 - INFO - Epoch [190/300], Batch [42/43], Training Loss: 0.00000985
2024-11-06 14:07:24,433 - INFO - Epoch [190/300], Batch [43/43], Training Loss: 0.00001467
2024-11-06 14:07:24,445 - INFO - Epoch [190/300], Average Training Loss: 0.00001113, Validation Loss: 0.00001600
2024-11-06 14:07:24,450 - INFO - Epoch [191/300], Batch [1/43], Training Loss: 0.00002074
2024-11-06 14:07:24,456 - INFO - Epoch [191/300], Batch [2/43], Training Loss: 0.00002273
2024-11-06 14:07:24,460 - INFO - Epoch [191/300], Batch [3/43], Training Loss: 0.00000928
2024-11-06 14:07:24,466 - INFO - Epoch [191/300], Batch [4/43], Training Loss: 0.00001055
2024-11-06 14:07:24,506 - INFO - Epoch [191/300], Batch [5/43], Training Loss: 0.00000226
2024-11-06 14:07:24,534 - INFO - Epoch [191/300], Batch [6/43], Training Loss: 0.00000992
2024-11-06 14:07:24,538 - INFO - Epoch [191/300], Batch [7/43], Training Loss: 0.00000419
2024-11-06 14:07:24,543 - INFO - Epoch [191/300], Batch [8/43], Training Loss: 0.00000813
2024-11-06 14:07:24,553 - INFO - Epoch [191/300], Batch [9/43], Training Loss: 0.00000398
2024-11-06 14:07:24,560 - INFO - Epoch [191/300], Batch [10/43], Training Loss: 0.00001089
2024-11-06 14:07:24,565 - INFO - Epoch [191/300], Batch [11/43], Training Loss: 0.00002410
2024-11-06 14:07:24,569 - INFO - Epoch [191/300], Batch [12/43], Training Loss: 0.00000544
2024-11-06 14:07:24,574 - INFO - Epoch [191/300], Batch [13/43], Training Loss: 0.00000576
2024-11-06 14:07:24,577 - INFO - Epoch [191/300], Batch [14/43], Training Loss: 0.00000842
2024-11-06 14:07:24,581 - INFO - Epoch [191/300], Batch [15/43], Training Loss: 0.00000463
2024-11-06 14:07:24,585 - INFO - Epoch [191/300], Batch [16/43], Training Loss: 0.00001119
2024-11-06 14:07:24,589 - INFO - Epoch [191/300], Batch [17/43], Training Loss: 0.00000671
2024-11-06 14:07:24,593 - INFO - Epoch [191/300], Batch [18/43], Training Loss: 0.00000889
2024-11-06 14:07:24,596 - INFO - Epoch [191/300], Batch [19/43], Training Loss: 0.00000495
2024-11-06 14:07:24,600 - INFO - Epoch [191/300], Batch [20/43], Training Loss: 0.00001333
2024-11-06 14:07:24,604 - INFO - Epoch [191/300], Batch [21/43], Training Loss: 0.00000180
2024-11-06 14:07:24,608 - INFO - Epoch [191/300], Batch [22/43], Training Loss: 0.00000776
2024-11-06 14:07:24,613 - INFO - Epoch [191/300], Batch [23/43], Training Loss: 0.00001617
2024-11-06 14:07:24,617 - INFO - Epoch [191/300], Batch [24/43], Training Loss: 0.00000668
2024-11-06 14:07:24,621 - INFO - Epoch [191/300], Batch [25/43], Training Loss: 0.00001038
2024-11-06 14:07:24,625 - INFO - Epoch [191/300], Batch [26/43], Training Loss: 0.00001429
2024-11-06 14:07:24,629 - INFO - Epoch [191/300], Batch [27/43], Training Loss: 0.00000843
2024-11-06 14:07:24,633 - INFO - Epoch [191/300], Batch [28/43], Training Loss: 0.00001322
2024-11-06 14:07:24,637 - INFO - Epoch [191/300], Batch [29/43], Training Loss: 0.00000260
2024-11-06 14:07:24,640 - INFO - Epoch [191/300], Batch [30/43], Training Loss: 0.00001737
2024-11-06 14:07:24,644 - INFO - Epoch [191/300], Batch [31/43], Training Loss: 0.00000491
2024-11-06 14:07:24,647 - INFO - Epoch [191/300], Batch [32/43], Training Loss: 0.00000930
2024-11-06 14:07:24,650 - INFO - Epoch [191/300], Batch [33/43], Training Loss: 0.00000381
2024-11-06 14:07:24,654 - INFO - Epoch [191/300], Batch [34/43], Training Loss: 0.00000869
2024-11-06 14:07:24,657 - INFO - Epoch [191/300], Batch [35/43], Training Loss: 0.00000871
2024-11-06 14:07:24,660 - INFO - Epoch [191/300], Batch [36/43], Training Loss: 0.00001338
2024-11-06 14:07:24,663 - INFO - Epoch [191/300], Batch [37/43], Training Loss: 0.00000451
2024-11-06 14:07:24,667 - INFO - Epoch [191/300], Batch [38/43], Training Loss: 0.00000876
2024-11-06 14:07:24,670 - INFO - Epoch [191/300], Batch [39/43], Training Loss: 0.00001203
2024-11-06 14:07:24,674 - INFO - Epoch [191/300], Batch [40/43], Training Loss: 0.00001494
2024-11-06 14:07:24,677 - INFO - Epoch [191/300], Batch [41/43], Training Loss: 0.00002001
2024-11-06 14:07:24,681 - INFO - Epoch [191/300], Batch [42/43], Training Loss: 0.00001189
2024-11-06 14:07:24,684 - INFO - Epoch [191/300], Batch [43/43], Training Loss: 0.00001214
2024-11-06 14:07:24,694 - INFO - Epoch [191/300], Average Training Loss: 0.00000995, Validation Loss: 0.00001866
2024-11-06 14:07:24,698 - INFO - Epoch [192/300], Batch [1/43], Training Loss: 0.00001535
2024-11-06 14:07:24,701 - INFO - Epoch [192/300], Batch [2/43], Training Loss: 0.00000289
2024-11-06 14:07:24,705 - INFO - Epoch [192/300], Batch [3/43], Training Loss: 0.00001634
2024-11-06 14:07:24,708 - INFO - Epoch [192/300], Batch [4/43], Training Loss: 0.00002049
2024-11-06 14:07:24,712 - INFO - Epoch [192/300], Batch [5/43], Training Loss: 0.00001930
2024-11-06 14:07:24,717 - INFO - Epoch [192/300], Batch [6/43], Training Loss: 0.00004345
2024-11-06 14:07:24,720 - INFO - Epoch [192/300], Batch [7/43], Training Loss: 0.00001773
2024-11-06 14:07:24,722 - INFO - Epoch [192/300], Batch [8/43], Training Loss: 0.00000955
2024-11-06 14:07:24,725 - INFO - Epoch [192/300], Batch [9/43], Training Loss: 0.00002003
2024-11-06 14:07:24,729 - INFO - Epoch [192/300], Batch [10/43], Training Loss: 0.00001007
2024-11-06 14:07:24,733 - INFO - Epoch [192/300], Batch [11/43], Training Loss: 0.00001212
2024-11-06 14:07:24,737 - INFO - Epoch [192/300], Batch [12/43], Training Loss: 0.00000344
2024-11-06 14:07:24,740 - INFO - Epoch [192/300], Batch [13/43], Training Loss: 0.00001134
2024-11-06 14:07:24,744 - INFO - Epoch [192/300], Batch [14/43], Training Loss: 0.00000820
2024-11-06 14:07:24,748 - INFO - Epoch [192/300], Batch [15/43], Training Loss: 0.00000615
2024-11-06 14:07:24,752 - INFO - Epoch [192/300], Batch [16/43], Training Loss: 0.00001405
2024-11-06 14:07:24,756 - INFO - Epoch [192/300], Batch [17/43], Training Loss: 0.00000751
2024-11-06 14:07:24,760 - INFO - Epoch [192/300], Batch [18/43], Training Loss: 0.00001809
2024-11-06 14:07:24,764 - INFO - Epoch [192/300], Batch [19/43], Training Loss: 0.00000856
2024-11-06 14:07:24,769 - INFO - Epoch [192/300], Batch [20/43], Training Loss: 0.00000576
2024-11-06 14:07:24,773 - INFO - Epoch [192/300], Batch [21/43], Training Loss: 0.00001387
2024-11-06 14:07:24,776 - INFO - Epoch [192/300], Batch [22/43], Training Loss: 0.00000686
2024-11-06 14:07:24,780 - INFO - Epoch [192/300], Batch [23/43], Training Loss: 0.00000525
2024-11-06 14:07:24,783 - INFO - Epoch [192/300], Batch [24/43], Training Loss: 0.00000657
2024-11-06 14:07:24,787 - INFO - Epoch [192/300], Batch [25/43], Training Loss: 0.00001835
2024-11-06 14:07:24,791 - INFO - Epoch [192/300], Batch [26/43], Training Loss: 0.00000405
2024-11-06 14:07:24,795 - INFO - Epoch [192/300], Batch [27/43], Training Loss: 0.00000442
2024-11-06 14:07:24,798 - INFO - Epoch [192/300], Batch [28/43], Training Loss: 0.00002847
2024-11-06 14:07:24,801 - INFO - Epoch [192/300], Batch [29/43], Training Loss: 0.00000482
2024-11-06 14:07:24,804 - INFO - Epoch [192/300], Batch [30/43], Training Loss: 0.00000879
2024-11-06 14:07:24,807 - INFO - Epoch [192/300], Batch [31/43], Training Loss: 0.00001857
2024-11-06 14:07:24,810 - INFO - Epoch [192/300], Batch [32/43], Training Loss: 0.00000671
2024-11-06 14:07:24,813 - INFO - Epoch [192/300], Batch [33/43], Training Loss: 0.00000935
2024-11-06 14:07:24,817 - INFO - Epoch [192/300], Batch [34/43], Training Loss: 0.00001189
2024-11-06 14:07:24,820 - INFO - Epoch [192/300], Batch [35/43], Training Loss: 0.00000763
2024-11-06 14:07:24,824 - INFO - Epoch [192/300], Batch [36/43], Training Loss: 0.00000856
2024-11-06 14:07:24,827 - INFO - Epoch [192/300], Batch [37/43], Training Loss: 0.00000744
2024-11-06 14:07:24,830 - INFO - Epoch [192/300], Batch [38/43], Training Loss: 0.00000658
2024-11-06 14:07:24,833 - INFO - Epoch [192/300], Batch [39/43], Training Loss: 0.00001661
2024-11-06 14:07:24,836 - INFO - Epoch [192/300], Batch [40/43], Training Loss: 0.00002975
2024-11-06 14:07:24,840 - INFO - Epoch [192/300], Batch [41/43], Training Loss: 0.00001809
2024-11-06 14:07:24,844 - INFO - Epoch [192/300], Batch [42/43], Training Loss: 0.00003352
2024-11-06 14:07:24,848 - INFO - Epoch [192/300], Batch [43/43], Training Loss: 0.00001165
2024-11-06 14:07:24,857 - INFO - Epoch [192/300], Average Training Loss: 0.00001298, Validation Loss: 0.00001208
2024-11-06 14:07:24,860 - INFO - Epoch [193/300], Batch [1/43], Training Loss: 0.00001227
2024-11-06 14:07:24,863 - INFO - Epoch [193/300], Batch [2/43], Training Loss: 0.00003188
2024-11-06 14:07:24,866 - INFO - Epoch [193/300], Batch [3/43], Training Loss: 0.00002571
2024-11-06 14:07:24,869 - INFO - Epoch [193/300], Batch [4/43], Training Loss: 0.00000980
2024-11-06 14:07:24,873 - INFO - Epoch [193/300], Batch [5/43], Training Loss: 0.00001901
2024-11-06 14:07:24,876 - INFO - Epoch [193/300], Batch [6/43], Training Loss: 0.00001443
2024-11-06 14:07:24,879 - INFO - Epoch [193/300], Batch [7/43], Training Loss: 0.00000249
2024-11-06 14:07:24,881 - INFO - Epoch [193/300], Batch [8/43], Training Loss: 0.00001367
2024-11-06 14:07:24,884 - INFO - Epoch [193/300], Batch [9/43], Training Loss: 0.00001992
2024-11-06 14:07:24,888 - INFO - Epoch [193/300], Batch [10/43], Training Loss: 0.00001129
2024-11-06 14:07:24,892 - INFO - Epoch [193/300], Batch [11/43], Training Loss: 0.00001450
2024-11-06 14:07:24,896 - INFO - Epoch [193/300], Batch [12/43], Training Loss: 0.00002336
2024-11-06 14:07:24,900 - INFO - Epoch [193/300], Batch [13/43], Training Loss: 0.00000678
2024-11-06 14:07:24,904 - INFO - Epoch [193/300], Batch [14/43], Training Loss: 0.00000949
2024-11-06 14:07:24,910 - INFO - Epoch [193/300], Batch [15/43], Training Loss: 0.00001502
2024-11-06 14:07:24,914 - INFO - Epoch [193/300], Batch [16/43], Training Loss: 0.00001680
2024-11-06 14:07:24,918 - INFO - Epoch [193/300], Batch [17/43], Training Loss: 0.00000776
2024-11-06 14:07:24,921 - INFO - Epoch [193/300], Batch [18/43], Training Loss: 0.00002443
2024-11-06 14:07:24,925 - INFO - Epoch [193/300], Batch [19/43], Training Loss: 0.00002784
2024-11-06 14:07:24,929 - INFO - Epoch [193/300], Batch [20/43], Training Loss: 0.00000530
2024-11-06 14:07:24,933 - INFO - Epoch [193/300], Batch [21/43], Training Loss: 0.00001682
2024-11-06 14:07:24,937 - INFO - Epoch [193/300], Batch [22/43], Training Loss: 0.00000364
2024-11-06 14:07:24,941 - INFO - Epoch [193/300], Batch [23/43], Training Loss: 0.00001710
2024-11-06 14:07:24,945 - INFO - Epoch [193/300], Batch [24/43], Training Loss: 0.00001043
2024-11-06 14:07:24,948 - INFO - Epoch [193/300], Batch [25/43], Training Loss: 0.00000859
2024-11-06 14:07:24,951 - INFO - Epoch [193/300], Batch [26/43], Training Loss: 0.00001936
2024-11-06 14:07:24,954 - INFO - Epoch [193/300], Batch [27/43], Training Loss: 0.00001075
2024-11-06 14:07:24,958 - INFO - Epoch [193/300], Batch [28/43], Training Loss: 0.00000293
2024-11-06 14:07:24,962 - INFO - Epoch [193/300], Batch [29/43], Training Loss: 0.00001354
2024-11-06 14:07:24,966 - INFO - Epoch [193/300], Batch [30/43], Training Loss: 0.00000794
2024-11-06 14:07:24,969 - INFO - Epoch [193/300], Batch [31/43], Training Loss: 0.00000890
2024-11-06 14:07:24,972 - INFO - Epoch [193/300], Batch [32/43], Training Loss: 0.00001104
2024-11-06 14:07:24,975 - INFO - Epoch [193/300], Batch [33/43], Training Loss: 0.00001278
2024-11-06 14:07:24,978 - INFO - Epoch [193/300], Batch [34/43], Training Loss: 0.00000480
2024-11-06 14:07:24,982 - INFO - Epoch [193/300], Batch [35/43], Training Loss: 0.00000460
2024-11-06 14:07:24,987 - INFO - Epoch [193/300], Batch [36/43], Training Loss: 0.00000636
2024-11-06 14:07:24,990 - INFO - Epoch [193/300], Batch [37/43], Training Loss: 0.00001469
2024-11-06 14:07:24,994 - INFO - Epoch [193/300], Batch [38/43], Training Loss: 0.00000542
2024-11-06 14:07:24,998 - INFO - Epoch [193/300], Batch [39/43], Training Loss: 0.00000484
2024-11-06 14:07:25,001 - INFO - Epoch [193/300], Batch [40/43], Training Loss: 0.00001130
2024-11-06 14:07:25,005 - INFO - Epoch [193/300], Batch [41/43], Training Loss: 0.00000536
2024-11-06 14:07:25,009 - INFO - Epoch [193/300], Batch [42/43], Training Loss: 0.00000308
2024-11-06 14:07:25,014 - INFO - Epoch [193/300], Batch [43/43], Training Loss: 0.00000281
2024-11-06 14:07:25,027 - INFO - Epoch [193/300], Average Training Loss: 0.00001207, Validation Loss: 0.00001200
2024-11-06 14:07:25,033 - INFO - Epoch [194/300], Batch [1/43], Training Loss: 0.00001060
2024-11-06 14:07:25,038 - INFO - Epoch [194/300], Batch [2/43], Training Loss: 0.00000478
2024-11-06 14:07:25,042 - INFO - Epoch [194/300], Batch [3/43], Training Loss: 0.00000550
2024-11-06 14:07:25,047 - INFO - Epoch [194/300], Batch [4/43], Training Loss: 0.00000225
2024-11-06 14:07:25,052 - INFO - Epoch [194/300], Batch [5/43], Training Loss: 0.00001309
2024-11-06 14:07:25,056 - INFO - Epoch [194/300], Batch [6/43], Training Loss: 0.00000253
2024-11-06 14:07:25,060 - INFO - Epoch [194/300], Batch [7/43], Training Loss: 0.00000281
2024-11-06 14:07:25,065 - INFO - Epoch [194/300], Batch [8/43], Training Loss: 0.00000734
2024-11-06 14:07:25,068 - INFO - Epoch [194/300], Batch [9/43], Training Loss: 0.00002537
2024-11-06 14:07:25,072 - INFO - Epoch [194/300], Batch [10/43], Training Loss: 0.00001085
2024-11-06 14:07:25,076 - INFO - Epoch [194/300], Batch [11/43], Training Loss: 0.00000794
2024-11-06 14:07:25,080 - INFO - Epoch [194/300], Batch [12/43], Training Loss: 0.00000520
2024-11-06 14:07:25,085 - INFO - Epoch [194/300], Batch [13/43], Training Loss: 0.00002433
2024-11-06 14:07:25,089 - INFO - Epoch [194/300], Batch [14/43], Training Loss: 0.00000765
2024-11-06 14:07:25,093 - INFO - Epoch [194/300], Batch [15/43], Training Loss: 0.00001689
2024-11-06 14:07:25,098 - INFO - Epoch [194/300], Batch [16/43], Training Loss: 0.00001519
2024-11-06 14:07:25,103 - INFO - Epoch [194/300], Batch [17/43], Training Loss: 0.00000829
2024-11-06 14:07:25,109 - INFO - Epoch [194/300], Batch [18/43], Training Loss: 0.00000340
2024-11-06 14:07:25,114 - INFO - Epoch [194/300], Batch [19/43], Training Loss: 0.00000365
2024-11-06 14:07:25,120 - INFO - Epoch [194/300], Batch [20/43], Training Loss: 0.00000876
2024-11-06 14:07:25,126 - INFO - Epoch [194/300], Batch [21/43], Training Loss: 0.00000328
2024-11-06 14:07:25,130 - INFO - Epoch [194/300], Batch [22/43], Training Loss: 0.00001035
2024-11-06 14:07:25,135 - INFO - Epoch [194/300], Batch [23/43], Training Loss: 0.00000623
2024-11-06 14:07:25,138 - INFO - Epoch [194/300], Batch [24/43], Training Loss: 0.00000343
2024-11-06 14:07:25,141 - INFO - Epoch [194/300], Batch [25/43], Training Loss: 0.00001524
2024-11-06 14:07:25,145 - INFO - Epoch [194/300], Batch [26/43], Training Loss: 0.00001213
2024-11-06 14:07:25,149 - INFO - Epoch [194/300], Batch [27/43], Training Loss: 0.00000550
2024-11-06 14:07:25,153 - INFO - Epoch [194/300], Batch [28/43], Training Loss: 0.00002970
2024-11-06 14:07:25,157 - INFO - Epoch [194/300], Batch [29/43], Training Loss: 0.00002254
2024-11-06 14:07:25,161 - INFO - Epoch [194/300], Batch [30/43], Training Loss: 0.00000865
2024-11-06 14:07:25,166 - INFO - Epoch [194/300], Batch [31/43], Training Loss: 0.00000649
2024-11-06 14:07:25,171 - INFO - Epoch [194/300], Batch [32/43], Training Loss: 0.00003279
2024-11-06 14:07:25,175 - INFO - Epoch [194/300], Batch [33/43], Training Loss: 0.00000376
2024-11-06 14:07:25,180 - INFO - Epoch [194/300], Batch [34/43], Training Loss: 0.00000964
2024-11-06 14:07:25,185 - INFO - Epoch [194/300], Batch [35/43], Training Loss: 0.00002221
2024-11-06 14:07:25,190 - INFO - Epoch [194/300], Batch [36/43], Training Loss: 0.00000787
2024-11-06 14:07:25,194 - INFO - Epoch [194/300], Batch [37/43], Training Loss: 0.00001259
2024-11-06 14:07:25,199 - INFO - Epoch [194/300], Batch [38/43], Training Loss: 0.00001194
2024-11-06 14:07:25,203 - INFO - Epoch [194/300], Batch [39/43], Training Loss: 0.00001534
2024-11-06 14:07:25,207 - INFO - Epoch [194/300], Batch [40/43], Training Loss: 0.00000448
2024-11-06 14:07:25,213 - INFO - Epoch [194/300], Batch [41/43], Training Loss: 0.00000743
2024-11-06 14:07:25,219 - INFO - Epoch [194/300], Batch [42/43], Training Loss: 0.00001858
2024-11-06 14:07:25,224 - INFO - Epoch [194/300], Batch [43/43], Training Loss: 0.00001825
2024-11-06 14:07:25,239 - INFO - Epoch [194/300], Average Training Loss: 0.00001104, Validation Loss: 0.00001206
2024-11-06 14:07:25,245 - INFO - Epoch [195/300], Batch [1/43], Training Loss: 0.00000570
2024-11-06 14:07:25,249 - INFO - Epoch [195/300], Batch [2/43], Training Loss: 0.00001114
2024-11-06 14:07:25,253 - INFO - Epoch [195/300], Batch [3/43], Training Loss: 0.00003177
2024-11-06 14:07:25,257 - INFO - Epoch [195/300], Batch [4/43], Training Loss: 0.00000539
2024-11-06 14:07:25,262 - INFO - Epoch [195/300], Batch [5/43], Training Loss: 0.00001037
2024-11-06 14:07:25,268 - INFO - Epoch [195/300], Batch [6/43], Training Loss: 0.00000576
2024-11-06 14:07:25,273 - INFO - Epoch [195/300], Batch [7/43], Training Loss: 0.00001371
2024-11-06 14:07:25,279 - INFO - Epoch [195/300], Batch [8/43], Training Loss: 0.00000559
2024-11-06 14:07:25,282 - INFO - Epoch [195/300], Batch [9/43], Training Loss: 0.00000695
2024-11-06 14:07:25,287 - INFO - Epoch [195/300], Batch [10/43], Training Loss: 0.00001264
2024-11-06 14:07:25,292 - INFO - Epoch [195/300], Batch [11/43], Training Loss: 0.00001330
2024-11-06 14:07:25,296 - INFO - Epoch [195/300], Batch [12/43], Training Loss: 0.00000518
2024-11-06 14:07:25,300 - INFO - Epoch [195/300], Batch [13/43], Training Loss: 0.00002019
2024-11-06 14:07:25,304 - INFO - Epoch [195/300], Batch [14/43], Training Loss: 0.00000986
2024-11-06 14:07:25,309 - INFO - Epoch [195/300], Batch [15/43], Training Loss: 0.00001165
2024-11-06 14:07:25,313 - INFO - Epoch [195/300], Batch [16/43], Training Loss: 0.00001014
2024-11-06 14:07:25,317 - INFO - Epoch [195/300], Batch [17/43], Training Loss: 0.00000842
2024-11-06 14:07:25,322 - INFO - Epoch [195/300], Batch [18/43], Training Loss: 0.00000851
2024-11-06 14:07:25,326 - INFO - Epoch [195/300], Batch [19/43], Training Loss: 0.00001399
2024-11-06 14:07:25,331 - INFO - Epoch [195/300], Batch [20/43], Training Loss: 0.00001368
2024-11-06 14:07:25,336 - INFO - Epoch [195/300], Batch [21/43], Training Loss: 0.00001104
2024-11-06 14:07:25,340 - INFO - Epoch [195/300], Batch [22/43], Training Loss: 0.00001568
2024-11-06 14:07:25,344 - INFO - Epoch [195/300], Batch [23/43], Training Loss: 0.00001128
2024-11-06 14:07:25,348 - INFO - Epoch [195/300], Batch [24/43], Training Loss: 0.00001121
2024-11-06 14:07:25,352 - INFO - Epoch [195/300], Batch [25/43], Training Loss: 0.00001421
2024-11-06 14:07:25,356 - INFO - Epoch [195/300], Batch [26/43], Training Loss: 0.00000441
2024-11-06 14:07:25,360 - INFO - Epoch [195/300], Batch [27/43], Training Loss: 0.00000367
2024-11-06 14:07:25,365 - INFO - Epoch [195/300], Batch [28/43], Training Loss: 0.00000621
2024-11-06 14:07:25,370 - INFO - Epoch [195/300], Batch [29/43], Training Loss: 0.00000708
2024-11-06 14:07:25,375 - INFO - Epoch [195/300], Batch [30/43], Training Loss: 0.00000491
2024-11-06 14:07:25,379 - INFO - Epoch [195/300], Batch [31/43], Training Loss: 0.00002258
2024-11-06 14:07:25,384 - INFO - Epoch [195/300], Batch [32/43], Training Loss: 0.00000313
2024-11-06 14:07:25,390 - INFO - Epoch [195/300], Batch [33/43], Training Loss: 0.00001204
2024-11-06 14:07:25,395 - INFO - Epoch [195/300], Batch [34/43], Training Loss: 0.00001039
2024-11-06 14:07:25,402 - INFO - Epoch [195/300], Batch [35/43], Training Loss: 0.00001410
2024-11-06 14:07:25,407 - INFO - Epoch [195/300], Batch [36/43], Training Loss: 0.00001817
2024-11-06 14:07:25,412 - INFO - Epoch [195/300], Batch [37/43], Training Loss: 0.00000989
2024-11-06 14:07:25,416 - INFO - Epoch [195/300], Batch [38/43], Training Loss: 0.00000850
2024-11-06 14:07:25,421 - INFO - Epoch [195/300], Batch [39/43], Training Loss: 0.00001376
2024-11-06 14:07:25,425 - INFO - Epoch [195/300], Batch [40/43], Training Loss: 0.00001502
2024-11-06 14:07:25,430 - INFO - Epoch [195/300], Batch [41/43], Training Loss: 0.00000278
2024-11-06 14:07:25,434 - INFO - Epoch [195/300], Batch [42/43], Training Loss: 0.00000814
2024-11-06 14:07:25,439 - INFO - Epoch [195/300], Batch [43/43], Training Loss: 0.00000793
2024-11-06 14:07:25,452 - INFO - Epoch [195/300], Average Training Loss: 0.00001070, Validation Loss: 0.00001278
2024-11-06 14:07:25,456 - INFO - Epoch [196/300], Batch [1/43], Training Loss: 0.00001457
2024-11-06 14:07:25,459 - INFO - Epoch [196/300], Batch [2/43], Training Loss: 0.00001426
2024-11-06 14:07:25,462 - INFO - Epoch [196/300], Batch [3/43], Training Loss: 0.00000660
2024-11-06 14:07:25,466 - INFO - Epoch [196/300], Batch [4/43], Training Loss: 0.00001146
2024-11-06 14:07:25,470 - INFO - Epoch [196/300], Batch [5/43], Training Loss: 0.00000258
2024-11-06 14:07:25,473 - INFO - Epoch [196/300], Batch [6/43], Training Loss: 0.00000411
2024-11-06 14:07:25,476 - INFO - Epoch [196/300], Batch [7/43], Training Loss: 0.00000245
2024-11-06 14:07:25,481 - INFO - Epoch [196/300], Batch [8/43], Training Loss: 0.00000588
2024-11-06 14:07:25,485 - INFO - Epoch [196/300], Batch [9/43], Training Loss: 0.00001810
2024-11-06 14:07:25,489 - INFO - Epoch [196/300], Batch [10/43], Training Loss: 0.00001917
2024-11-06 14:07:25,493 - INFO - Epoch [196/300], Batch [11/43], Training Loss: 0.00000233
2024-11-06 14:07:25,497 - INFO - Epoch [196/300], Batch [12/43], Training Loss: 0.00001060
2024-11-06 14:07:25,501 - INFO - Epoch [196/300], Batch [13/43], Training Loss: 0.00002358
2024-11-06 14:07:25,506 - INFO - Epoch [196/300], Batch [14/43], Training Loss: 0.00000474
2024-11-06 14:07:25,510 - INFO - Epoch [196/300], Batch [15/43], Training Loss: 0.00003098
2024-11-06 14:07:25,514 - INFO - Epoch [196/300], Batch [16/43], Training Loss: 0.00000547
2024-11-06 14:07:25,517 - INFO - Epoch [196/300], Batch [17/43], Training Loss: 0.00000985
2024-11-06 14:07:25,521 - INFO - Epoch [196/300], Batch [18/43], Training Loss: 0.00001858
2024-11-06 14:07:25,525 - INFO - Epoch [196/300], Batch [19/43], Training Loss: 0.00001413
2024-11-06 14:07:25,530 - INFO - Epoch [196/300], Batch [20/43], Training Loss: 0.00000557
2024-11-06 14:07:25,534 - INFO - Epoch [196/300], Batch [21/43], Training Loss: 0.00002357
2024-11-06 14:07:25,539 - INFO - Epoch [196/300], Batch [22/43], Training Loss: 0.00001051
2024-11-06 14:07:25,544 - INFO - Epoch [196/300], Batch [23/43], Training Loss: 0.00001626
2024-11-06 14:07:25,549 - INFO - Epoch [196/300], Batch [24/43], Training Loss: 0.00001162
2024-11-06 14:07:25,553 - INFO - Epoch [196/300], Batch [25/43], Training Loss: 0.00002426
2024-11-06 14:07:25,558 - INFO - Epoch [196/300], Batch [26/43], Training Loss: 0.00002670
2024-11-06 14:07:25,562 - INFO - Epoch [196/300], Batch [27/43], Training Loss: 0.00001314
2024-11-06 14:07:25,567 - INFO - Epoch [196/300], Batch [28/43], Training Loss: 0.00000823
2024-11-06 14:07:25,571 - INFO - Epoch [196/300], Batch [29/43], Training Loss: 0.00001570
2024-11-06 14:07:25,575 - INFO - Epoch [196/300], Batch [30/43], Training Loss: 0.00002491
2024-11-06 14:07:25,579 - INFO - Epoch [196/300], Batch [31/43], Training Loss: 0.00000706
2024-11-06 14:07:25,584 - INFO - Epoch [196/300], Batch [32/43], Training Loss: 0.00001075
2024-11-06 14:07:25,588 - INFO - Epoch [196/300], Batch [33/43], Training Loss: 0.00001358
2024-11-06 14:07:25,592 - INFO - Epoch [196/300], Batch [34/43], Training Loss: 0.00001489
2024-11-06 14:07:25,596 - INFO - Epoch [196/300], Batch [35/43], Training Loss: 0.00000303
2024-11-06 14:07:25,600 - INFO - Epoch [196/300], Batch [36/43], Training Loss: 0.00001117
2024-11-06 14:07:25,605 - INFO - Epoch [196/300], Batch [37/43], Training Loss: 0.00001073
2024-11-06 14:07:25,609 - INFO - Epoch [196/300], Batch [38/43], Training Loss: 0.00000988
2024-11-06 14:07:25,613 - INFO - Epoch [196/300], Batch [39/43], Training Loss: 0.00002013
2024-11-06 14:07:25,617 - INFO - Epoch [196/300], Batch [40/43], Training Loss: 0.00000657
2024-11-06 14:07:25,620 - INFO - Epoch [196/300], Batch [41/43], Training Loss: 0.00001249
2024-11-06 14:07:25,624 - INFO - Epoch [196/300], Batch [42/43], Training Loss: 0.00000971
2024-11-06 14:07:25,628 - INFO - Epoch [196/300], Batch [43/43], Training Loss: 0.00001057
2024-11-06 14:07:25,641 - INFO - Epoch [196/300], Average Training Loss: 0.00001257, Validation Loss: 0.00001476
2024-11-06 14:07:25,645 - INFO - Epoch [197/300], Batch [1/43], Training Loss: 0.00001407
2024-11-06 14:07:25,649 - INFO - Epoch [197/300], Batch [2/43], Training Loss: 0.00000829
2024-11-06 14:07:25,653 - INFO - Epoch [197/300], Batch [3/43], Training Loss: 0.00000722
2024-11-06 14:07:25,657 - INFO - Epoch [197/300], Batch [4/43], Training Loss: 0.00001327
2024-11-06 14:07:25,660 - INFO - Epoch [197/300], Batch [5/43], Training Loss: 0.00001505
2024-11-06 14:07:25,664 - INFO - Epoch [197/300], Batch [6/43], Training Loss: 0.00002341
2024-11-06 14:07:25,668 - INFO - Epoch [197/300], Batch [7/43], Training Loss: 0.00000620
2024-11-06 14:07:25,672 - INFO - Epoch [197/300], Batch [8/43], Training Loss: 0.00001719
2024-11-06 14:07:25,676 - INFO - Epoch [197/300], Batch [9/43], Training Loss: 0.00000608
2024-11-06 14:07:25,679 - INFO - Epoch [197/300], Batch [10/43], Training Loss: 0.00001665
2024-11-06 14:07:25,685 - INFO - Epoch [197/300], Batch [11/43], Training Loss: 0.00000660
2024-11-06 14:07:25,689 - INFO - Epoch [197/300], Batch [12/43], Training Loss: 0.00002199
2024-11-06 14:07:25,693 - INFO - Epoch [197/300], Batch [13/43], Training Loss: 0.00001131
2024-11-06 14:07:25,699 - INFO - Epoch [197/300], Batch [14/43], Training Loss: 0.00000674
2024-11-06 14:07:25,704 - INFO - Epoch [197/300], Batch [15/43], Training Loss: 0.00001836
2024-11-06 14:07:25,708 - INFO - Epoch [197/300], Batch [16/43], Training Loss: 0.00001892
2024-11-06 14:07:25,713 - INFO - Epoch [197/300], Batch [17/43], Training Loss: 0.00001155
2024-11-06 14:07:25,717 - INFO - Epoch [197/300], Batch [18/43], Training Loss: 0.00001305
2024-11-06 14:07:25,722 - INFO - Epoch [197/300], Batch [19/43], Training Loss: 0.00000438
2024-11-06 14:07:25,726 - INFO - Epoch [197/300], Batch [20/43], Training Loss: 0.00000286
2024-11-06 14:07:25,730 - INFO - Epoch [197/300], Batch [21/43], Training Loss: 0.00000439
2024-11-06 14:07:25,735 - INFO - Epoch [197/300], Batch [22/43], Training Loss: 0.00001387
2024-11-06 14:07:25,741 - INFO - Epoch [197/300], Batch [23/43], Training Loss: 0.00001062
2024-11-06 14:07:25,745 - INFO - Epoch [197/300], Batch [24/43], Training Loss: 0.00000400
2024-11-06 14:07:25,749 - INFO - Epoch [197/300], Batch [25/43], Training Loss: 0.00000815
2024-11-06 14:07:25,753 - INFO - Epoch [197/300], Batch [26/43], Training Loss: 0.00000680
2024-11-06 14:07:25,758 - INFO - Epoch [197/300], Batch [27/43], Training Loss: 0.00000404
2024-11-06 14:07:25,762 - INFO - Epoch [197/300], Batch [28/43], Training Loss: 0.00000725
2024-11-06 14:07:25,766 - INFO - Epoch [197/300], Batch [29/43], Training Loss: 0.00001278
2024-11-06 14:07:25,772 - INFO - Epoch [197/300], Batch [30/43], Training Loss: 0.00001218
2024-11-06 14:07:25,776 - INFO - Epoch [197/300], Batch [31/43], Training Loss: 0.00002012
2024-11-06 14:07:25,782 - INFO - Epoch [197/300], Batch [32/43], Training Loss: 0.00002760
2024-11-06 14:07:25,787 - INFO - Epoch [197/300], Batch [33/43], Training Loss: 0.00000607
2024-11-06 14:07:25,791 - INFO - Epoch [197/300], Batch [34/43], Training Loss: 0.00001295
2024-11-06 14:07:25,796 - INFO - Epoch [197/300], Batch [35/43], Training Loss: 0.00000855
2024-11-06 14:07:25,800 - INFO - Epoch [197/300], Batch [36/43], Training Loss: 0.00001382
2024-11-06 14:07:25,805 - INFO - Epoch [197/300], Batch [37/43], Training Loss: 0.00001070
2024-11-06 14:07:25,810 - INFO - Epoch [197/300], Batch [38/43], Training Loss: 0.00000978
2024-11-06 14:07:25,815 - INFO - Epoch [197/300], Batch [39/43], Training Loss: 0.00001203
2024-11-06 14:07:25,819 - INFO - Epoch [197/300], Batch [40/43], Training Loss: 0.00000356
2024-11-06 14:07:25,824 - INFO - Epoch [197/300], Batch [41/43], Training Loss: 0.00001369
2024-11-06 14:07:25,829 - INFO - Epoch [197/300], Batch [42/43], Training Loss: 0.00000694
2024-11-06 14:07:25,834 - INFO - Epoch [197/300], Batch [43/43], Training Loss: 0.00000720
2024-11-06 14:07:25,848 - INFO - Epoch [197/300], Average Training Loss: 0.00001117, Validation Loss: 0.00001218
2024-11-06 14:07:25,852 - INFO - Epoch [198/300], Batch [1/43], Training Loss: 0.00000557
2024-11-06 14:07:25,857 - INFO - Epoch [198/300], Batch [2/43], Training Loss: 0.00000453
2024-11-06 14:07:25,861 - INFO - Epoch [198/300], Batch [3/43], Training Loss: 0.00001165
2024-11-06 14:07:25,867 - INFO - Epoch [198/300], Batch [4/43], Training Loss: 0.00001302
2024-11-06 14:07:25,871 - INFO - Epoch [198/300], Batch [5/43], Training Loss: 0.00001140
2024-11-06 14:07:25,875 - INFO - Epoch [198/300], Batch [6/43], Training Loss: 0.00001031
2024-11-06 14:07:25,880 - INFO - Epoch [198/300], Batch [7/43], Training Loss: 0.00001793
2024-11-06 14:07:25,883 - INFO - Epoch [198/300], Batch [8/43], Training Loss: 0.00000592
2024-11-06 14:07:25,888 - INFO - Epoch [198/300], Batch [9/43], Training Loss: 0.00001161
2024-11-06 14:07:25,892 - INFO - Epoch [198/300], Batch [10/43], Training Loss: 0.00000654
2024-11-06 14:07:25,896 - INFO - Epoch [198/300], Batch [11/43], Training Loss: 0.00000416
2024-11-06 14:07:25,899 - INFO - Epoch [198/300], Batch [12/43], Training Loss: 0.00000350
2024-11-06 14:07:25,904 - INFO - Epoch [198/300], Batch [13/43], Training Loss: 0.00001069
2024-11-06 14:07:25,908 - INFO - Epoch [198/300], Batch [14/43], Training Loss: 0.00001916
2024-11-06 14:07:25,912 - INFO - Epoch [198/300], Batch [15/43], Training Loss: 0.00000742
2024-11-06 14:07:25,916 - INFO - Epoch [198/300], Batch [16/43], Training Loss: 0.00001849
2024-11-06 14:07:25,920 - INFO - Epoch [198/300], Batch [17/43], Training Loss: 0.00000576
2024-11-06 14:07:25,925 - INFO - Epoch [198/300], Batch [18/43], Training Loss: 0.00001631
2024-11-06 14:07:25,930 - INFO - Epoch [198/300], Batch [19/43], Training Loss: 0.00000283
2024-11-06 14:07:25,936 - INFO - Epoch [198/300], Batch [20/43], Training Loss: 0.00000622
2024-11-06 14:07:25,940 - INFO - Epoch [198/300], Batch [21/43], Training Loss: 0.00000988
2024-11-06 14:07:25,945 - INFO - Epoch [198/300], Batch [22/43], Training Loss: 0.00000821
2024-11-06 14:07:25,949 - INFO - Epoch [198/300], Batch [23/43], Training Loss: 0.00002405
2024-11-06 14:07:25,954 - INFO - Epoch [198/300], Batch [24/43], Training Loss: 0.00001884
2024-11-06 14:07:25,960 - INFO - Epoch [198/300], Batch [25/43], Training Loss: 0.00003108
2024-11-06 14:07:25,964 - INFO - Epoch [198/300], Batch [26/43], Training Loss: 0.00001760
2024-11-06 14:07:25,969 - INFO - Epoch [198/300], Batch [27/43], Training Loss: 0.00000909
2024-11-06 14:07:25,972 - INFO - Epoch [198/300], Batch [28/43], Training Loss: 0.00002036
2024-11-06 14:07:25,975 - INFO - Epoch [198/300], Batch [29/43], Training Loss: 0.00001748
2024-11-06 14:07:25,980 - INFO - Epoch [198/300], Batch [30/43], Training Loss: 0.00000829
2024-11-06 14:07:25,984 - INFO - Epoch [198/300], Batch [31/43], Training Loss: 0.00000689
2024-11-06 14:07:25,987 - INFO - Epoch [198/300], Batch [32/43], Training Loss: 0.00000845
2024-11-06 14:07:25,990 - INFO - Epoch [198/300], Batch [33/43], Training Loss: 0.00002488
2024-11-06 14:07:25,995 - INFO - Epoch [198/300], Batch [34/43], Training Loss: 0.00000727
2024-11-06 14:07:25,998 - INFO - Epoch [198/300], Batch [35/43], Training Loss: 0.00001021
2024-11-06 14:07:26,001 - INFO - Epoch [198/300], Batch [36/43], Training Loss: 0.00005246
2024-11-06 14:07:26,005 - INFO - Epoch [198/300], Batch [37/43], Training Loss: 0.00000384
2024-11-06 14:07:26,009 - INFO - Epoch [198/300], Batch [38/43], Training Loss: 0.00002252
2024-11-06 14:07:26,013 - INFO - Epoch [198/300], Batch [39/43], Training Loss: 0.00003216
2024-11-06 14:07:26,016 - INFO - Epoch [198/300], Batch [40/43], Training Loss: 0.00000255
2024-11-06 14:07:26,020 - INFO - Epoch [198/300], Batch [41/43], Training Loss: 0.00000853
2024-11-06 14:07:26,023 - INFO - Epoch [198/300], Batch [42/43], Training Loss: 0.00001193
2024-11-06 14:07:26,028 - INFO - Epoch [198/300], Batch [43/43], Training Loss: 0.00001347
2024-11-06 14:07:26,041 - INFO - Epoch [198/300], Average Training Loss: 0.00001309, Validation Loss: 0.00001237
2024-11-06 14:07:26,045 - INFO - Epoch [199/300], Batch [1/43], Training Loss: 0.00002559
2024-11-06 14:07:26,049 - INFO - Epoch [199/300], Batch [2/43], Training Loss: 0.00000972
2024-11-06 14:07:26,053 - INFO - Epoch [199/300], Batch [3/43], Training Loss: 0.00001278
2024-11-06 14:07:26,057 - INFO - Epoch [199/300], Batch [4/43], Training Loss: 0.00000505
2024-11-06 14:07:26,060 - INFO - Epoch [199/300], Batch [5/43], Training Loss: 0.00000235
2024-11-06 14:07:26,063 - INFO - Epoch [199/300], Batch [6/43], Training Loss: 0.00001031
2024-11-06 14:07:26,067 - INFO - Epoch [199/300], Batch [7/43], Training Loss: 0.00000866
2024-11-06 14:07:26,071 - INFO - Epoch [199/300], Batch [8/43], Training Loss: 0.00000678
2024-11-06 14:07:26,075 - INFO - Epoch [199/300], Batch [9/43], Training Loss: 0.00000603
2024-11-06 14:07:26,080 - INFO - Epoch [199/300], Batch [10/43], Training Loss: 0.00000327
2024-11-06 14:07:26,085 - INFO - Epoch [199/300], Batch [11/43], Training Loss: 0.00000751
2024-11-06 14:07:26,090 - INFO - Epoch [199/300], Batch [12/43], Training Loss: 0.00001486
2024-11-06 14:07:26,094 - INFO - Epoch [199/300], Batch [13/43], Training Loss: 0.00001404
2024-11-06 14:07:26,099 - INFO - Epoch [199/300], Batch [14/43], Training Loss: 0.00001122
2024-11-06 14:07:26,103 - INFO - Epoch [199/300], Batch [15/43], Training Loss: 0.00000515
2024-11-06 14:07:26,107 - INFO - Epoch [199/300], Batch [16/43], Training Loss: 0.00001709
2024-11-06 14:07:26,112 - INFO - Epoch [199/300], Batch [17/43], Training Loss: 0.00001410
2024-11-06 14:07:26,115 - INFO - Epoch [199/300], Batch [18/43], Training Loss: 0.00000457
2024-11-06 14:07:26,120 - INFO - Epoch [199/300], Batch [19/43], Training Loss: 0.00000745
2024-11-06 14:07:26,124 - INFO - Epoch [199/300], Batch [20/43], Training Loss: 0.00001531
2024-11-06 14:07:26,127 - INFO - Epoch [199/300], Batch [21/43], Training Loss: 0.00000582
2024-11-06 14:07:26,130 - INFO - Epoch [199/300], Batch [22/43], Training Loss: 0.00001666
2024-11-06 14:07:26,135 - INFO - Epoch [199/300], Batch [23/43], Training Loss: 0.00000890
2024-11-06 14:07:26,138 - INFO - Epoch [199/300], Batch [24/43], Training Loss: 0.00000788
2024-11-06 14:07:26,142 - INFO - Epoch [199/300], Batch [25/43], Training Loss: 0.00000770
2024-11-06 14:07:26,145 - INFO - Epoch [199/300], Batch [26/43], Training Loss: 0.00001668
2024-11-06 14:07:26,149 - INFO - Epoch [199/300], Batch [27/43], Training Loss: 0.00000709
2024-11-06 14:07:26,153 - INFO - Epoch [199/300], Batch [28/43], Training Loss: 0.00000738
2024-11-06 14:07:26,156 - INFO - Epoch [199/300], Batch [29/43], Training Loss: 0.00001229
2024-11-06 14:07:26,159 - INFO - Epoch [199/300], Batch [30/43], Training Loss: 0.00001474
2024-11-06 14:07:26,163 - INFO - Epoch [199/300], Batch [31/43], Training Loss: 0.00001571
2024-11-06 14:07:26,167 - INFO - Epoch [199/300], Batch [32/43], Training Loss: 0.00001253
2024-11-06 14:07:26,170 - INFO - Epoch [199/300], Batch [33/43], Training Loss: 0.00000603
2024-11-06 14:07:26,174 - INFO - Epoch [199/300], Batch [34/43], Training Loss: 0.00000531
2024-11-06 14:07:26,177 - INFO - Epoch [199/300], Batch [35/43], Training Loss: 0.00000496
2024-11-06 14:07:26,181 - INFO - Epoch [199/300], Batch [36/43], Training Loss: 0.00000546
2024-11-06 14:07:26,185 - INFO - Epoch [199/300], Batch [37/43], Training Loss: 0.00001137
2024-11-06 14:07:26,189 - INFO - Epoch [199/300], Batch [38/43], Training Loss: 0.00000554
2024-11-06 14:07:26,193 - INFO - Epoch [199/300], Batch [39/43], Training Loss: 0.00001683
2024-11-06 14:07:26,197 - INFO - Epoch [199/300], Batch [40/43], Training Loss: 0.00002323
2024-11-06 14:07:26,201 - INFO - Epoch [199/300], Batch [41/43], Training Loss: 0.00000277
2024-11-06 14:07:26,205 - INFO - Epoch [199/300], Batch [42/43], Training Loss: 0.00001719
2024-11-06 14:07:26,209 - INFO - Epoch [199/300], Batch [43/43], Training Loss: 0.00002770
2024-11-06 14:07:26,220 - INFO - Epoch [199/300], Average Training Loss: 0.00001074, Validation Loss: 0.00001268
2024-11-06 14:07:26,224 - INFO - Epoch [200/300], Batch [1/43], Training Loss: 0.00000776
2024-11-06 14:07:26,228 - INFO - Epoch [200/300], Batch [2/43], Training Loss: 0.00000676
2024-11-06 14:07:26,232 - INFO - Epoch [200/300], Batch [3/43], Training Loss: 0.00001345
2024-11-06 14:07:26,235 - INFO - Epoch [200/300], Batch [4/43], Training Loss: 0.00001278
2024-11-06 14:07:26,238 - INFO - Epoch [200/300], Batch [5/43], Training Loss: 0.00000205
2024-11-06 14:07:26,242 - INFO - Epoch [200/300], Batch [6/43], Training Loss: 0.00000640
2024-11-06 14:07:26,246 - INFO - Epoch [200/300], Batch [7/43], Training Loss: 0.00001186
2024-11-06 14:07:26,250 - INFO - Epoch [200/300], Batch [8/43], Training Loss: 0.00001917
2024-11-06 14:07:26,254 - INFO - Epoch [200/300], Batch [9/43], Training Loss: 0.00000577
2024-11-06 14:07:26,258 - INFO - Epoch [200/300], Batch [10/43], Training Loss: 0.00000581
2024-11-06 14:07:26,263 - INFO - Epoch [200/300], Batch [11/43], Training Loss: 0.00002827
2024-11-06 14:07:26,267 - INFO - Epoch [200/300], Batch [12/43], Training Loss: 0.00000811
2024-11-06 14:07:26,271 - INFO - Epoch [200/300], Batch [13/43], Training Loss: 0.00003066
2024-11-06 14:07:26,276 - INFO - Epoch [200/300], Batch [14/43], Training Loss: 0.00000848
2024-11-06 14:07:26,280 - INFO - Epoch [200/300], Batch [15/43], Training Loss: 0.00001848
2024-11-06 14:07:26,285 - INFO - Epoch [200/300], Batch [16/43], Training Loss: 0.00000614
2024-11-06 14:07:26,289 - INFO - Epoch [200/300], Batch [17/43], Training Loss: 0.00000915
2024-11-06 14:07:26,293 - INFO - Epoch [200/300], Batch [18/43], Training Loss: 0.00001315
2024-11-06 14:07:26,297 - INFO - Epoch [200/300], Batch [19/43], Training Loss: 0.00000536
2024-11-06 14:07:26,301 - INFO - Epoch [200/300], Batch [20/43], Training Loss: 0.00001816
2024-11-06 14:07:26,306 - INFO - Epoch [200/300], Batch [21/43], Training Loss: 0.00001716
2024-11-06 14:07:26,310 - INFO - Epoch [200/300], Batch [22/43], Training Loss: 0.00002458
2024-11-06 14:07:26,315 - INFO - Epoch [200/300], Batch [23/43], Training Loss: 0.00000736
2024-11-06 14:07:26,318 - INFO - Epoch [200/300], Batch [24/43], Training Loss: 0.00002603
2024-11-06 14:07:26,322 - INFO - Epoch [200/300], Batch [25/43], Training Loss: 0.00001426
2024-11-06 14:07:26,326 - INFO - Epoch [200/300], Batch [26/43], Training Loss: 0.00000493
2024-11-06 14:07:26,330 - INFO - Epoch [200/300], Batch [27/43], Training Loss: 0.00000733
2024-11-06 14:07:26,334 - INFO - Epoch [200/300], Batch [28/43], Training Loss: 0.00000388
2024-11-06 14:07:26,338 - INFO - Epoch [200/300], Batch [29/43], Training Loss: 0.00001556
2024-11-06 14:07:26,342 - INFO - Epoch [200/300], Batch [30/43], Training Loss: 0.00000919
2024-11-06 14:07:26,346 - INFO - Epoch [200/300], Batch [31/43], Training Loss: 0.00001591
2024-11-06 14:07:26,349 - INFO - Epoch [200/300], Batch [32/43], Training Loss: 0.00000942
2024-11-06 14:07:26,353 - INFO - Epoch [200/300], Batch [33/43], Training Loss: 0.00000408
2024-11-06 14:07:26,357 - INFO - Epoch [200/300], Batch [34/43], Training Loss: 0.00002045
2024-11-06 14:07:26,360 - INFO - Epoch [200/300], Batch [35/43], Training Loss: 0.00001595
2024-11-06 14:07:26,364 - INFO - Epoch [200/300], Batch [36/43], Training Loss: 0.00000590
2024-11-06 14:07:26,368 - INFO - Epoch [200/300], Batch [37/43], Training Loss: 0.00001841
2024-11-06 14:07:26,372 - INFO - Epoch [200/300], Batch [38/43], Training Loss: 0.00001752
2024-11-06 14:07:26,376 - INFO - Epoch [200/300], Batch [39/43], Training Loss: 0.00000746
2024-11-06 14:07:26,380 - INFO - Epoch [200/300], Batch [40/43], Training Loss: 0.00001402
2024-11-06 14:07:26,384 - INFO - Epoch [200/300], Batch [41/43], Training Loss: 0.00000895
2024-11-06 14:07:26,388 - INFO - Epoch [200/300], Batch [42/43], Training Loss: 0.00001398
2024-11-06 14:07:26,391 - INFO - Epoch [200/300], Batch [43/43], Training Loss: 0.00000466
2024-11-06 14:07:26,403 - INFO - Epoch [200/300], Average Training Loss: 0.00001220, Validation Loss: 0.00001277
2024-11-06 14:07:26,407 - INFO - Epoch [201/300], Batch [1/43], Training Loss: 0.00002420
2024-11-06 14:07:26,412 - INFO - Epoch [201/300], Batch [2/43], Training Loss: 0.00000500
2024-11-06 14:07:26,417 - INFO - Epoch [201/300], Batch [3/43], Training Loss: 0.00001047
2024-11-06 14:07:26,421 - INFO - Epoch [201/300], Batch [4/43], Training Loss: 0.00000680
2024-11-06 14:07:26,426 - INFO - Epoch [201/300], Batch [5/43], Training Loss: 0.00000173
2024-11-06 14:07:26,432 - INFO - Epoch [201/300], Batch [6/43], Training Loss: 0.00001493
2024-11-06 14:07:26,437 - INFO - Epoch [201/300], Batch [7/43], Training Loss: 0.00001153
2024-11-06 14:07:26,441 - INFO - Epoch [201/300], Batch [8/43], Training Loss: 0.00000764
2024-11-06 14:07:26,447 - INFO - Epoch [201/300], Batch [9/43], Training Loss: 0.00001337
2024-11-06 14:07:26,451 - INFO - Epoch [201/300], Batch [10/43], Training Loss: 0.00001103
2024-11-06 14:07:26,455 - INFO - Epoch [201/300], Batch [11/43], Training Loss: 0.00000500
2024-11-06 14:07:26,458 - INFO - Epoch [201/300], Batch [12/43], Training Loss: 0.00000602
2024-11-06 14:07:26,463 - INFO - Epoch [201/300], Batch [13/43], Training Loss: 0.00001368
2024-11-06 14:07:26,467 - INFO - Epoch [201/300], Batch [14/43], Training Loss: 0.00000657
2024-11-06 14:07:26,472 - INFO - Epoch [201/300], Batch [15/43], Training Loss: 0.00000242
2024-11-06 14:07:26,476 - INFO - Epoch [201/300], Batch [16/43], Training Loss: 0.00001206
2024-11-06 14:07:26,480 - INFO - Epoch [201/300], Batch [17/43], Training Loss: 0.00000686
2024-11-06 14:07:26,484 - INFO - Epoch [201/300], Batch [18/43], Training Loss: 0.00000697
2024-11-06 14:07:26,488 - INFO - Epoch [201/300], Batch [19/43], Training Loss: 0.00001992
2024-11-06 14:07:26,492 - INFO - Epoch [201/300], Batch [20/43], Training Loss: 0.00000527
2024-11-06 14:07:26,497 - INFO - Epoch [201/300], Batch [21/43], Training Loss: 0.00000563
2024-11-06 14:07:26,501 - INFO - Epoch [201/300], Batch [22/43], Training Loss: 0.00000770
2024-11-06 14:07:26,505 - INFO - Epoch [201/300], Batch [23/43], Training Loss: 0.00001068
2024-11-06 14:07:26,509 - INFO - Epoch [201/300], Batch [24/43], Training Loss: 0.00000860
2024-11-06 14:07:26,513 - INFO - Epoch [201/300], Batch [25/43], Training Loss: 0.00000704
2024-11-06 14:07:26,517 - INFO - Epoch [201/300], Batch [26/43], Training Loss: 0.00000852
2024-11-06 14:07:26,521 - INFO - Epoch [201/300], Batch [27/43], Training Loss: 0.00001671
2024-11-06 14:07:26,525 - INFO - Epoch [201/300], Batch [28/43], Training Loss: 0.00001481
2024-11-06 14:07:26,528 - INFO - Epoch [201/300], Batch [29/43], Training Loss: 0.00001357
2024-11-06 14:07:26,532 - INFO - Epoch [201/300], Batch [30/43], Training Loss: 0.00000539
2024-11-06 14:07:26,536 - INFO - Epoch [201/300], Batch [31/43], Training Loss: 0.00002589
2024-11-06 14:07:26,541 - INFO - Epoch [201/300], Batch [32/43], Training Loss: 0.00000677
2024-11-06 14:07:26,545 - INFO - Epoch [201/300], Batch [33/43], Training Loss: 0.00002719
2024-11-06 14:07:26,549 - INFO - Epoch [201/300], Batch [34/43], Training Loss: 0.00000855
2024-11-06 14:07:26,553 - INFO - Epoch [201/300], Batch [35/43], Training Loss: 0.00001861
2024-11-06 14:07:26,558 - INFO - Epoch [201/300], Batch [36/43], Training Loss: 0.00001457
2024-11-06 14:07:26,562 - INFO - Epoch [201/300], Batch [37/43], Training Loss: 0.00001334
2024-11-06 14:07:26,566 - INFO - Epoch [201/300], Batch [38/43], Training Loss: 0.00003858
2024-11-06 14:07:26,570 - INFO - Epoch [201/300], Batch [39/43], Training Loss: 0.00000759
2024-11-06 14:07:26,575 - INFO - Epoch [201/300], Batch [40/43], Training Loss: 0.00000548
2024-11-06 14:07:26,578 - INFO - Epoch [201/300], Batch [41/43], Training Loss: 0.00002015
2024-11-06 14:07:26,582 - INFO - Epoch [201/300], Batch [42/43], Training Loss: 0.00001200
2024-11-06 14:07:26,587 - INFO - Epoch [201/300], Batch [43/43], Training Loss: 0.00001331
2024-11-06 14:07:26,600 - INFO - Epoch [201/300], Average Training Loss: 0.00001168, Validation Loss: 0.00001427
2024-11-06 14:07:26,605 - INFO - Epoch [202/300], Batch [1/43], Training Loss: 0.00002723
2024-11-06 14:07:26,609 - INFO - Epoch [202/300], Batch [2/43], Training Loss: 0.00000859
2024-11-06 14:07:26,617 - INFO - Epoch [202/300], Batch [3/43], Training Loss: 0.00000655
2024-11-06 14:07:26,622 - INFO - Epoch [202/300], Batch [4/43], Training Loss: 0.00000298
2024-11-06 14:07:26,627 - INFO - Epoch [202/300], Batch [5/43], Training Loss: 0.00001233
2024-11-06 14:07:26,631 - INFO - Epoch [202/300], Batch [6/43], Training Loss: 0.00001165
2024-11-06 14:07:26,636 - INFO - Epoch [202/300], Batch [7/43], Training Loss: 0.00000591
2024-11-06 14:07:26,641 - INFO - Epoch [202/300], Batch [8/43], Training Loss: 0.00000943
2024-11-06 14:07:26,646 - INFO - Epoch [202/300], Batch [9/43], Training Loss: 0.00001311
2024-11-06 14:07:26,650 - INFO - Epoch [202/300], Batch [10/43], Training Loss: 0.00001868
2024-11-06 14:07:26,655 - INFO - Epoch [202/300], Batch [11/43], Training Loss: 0.00001088
2024-11-06 14:07:26,659 - INFO - Epoch [202/300], Batch [12/43], Training Loss: 0.00000419
2024-11-06 14:07:26,664 - INFO - Epoch [202/300], Batch [13/43], Training Loss: 0.00000520
2024-11-06 14:07:26,668 - INFO - Epoch [202/300], Batch [14/43], Training Loss: 0.00003280
2024-11-06 14:07:26,673 - INFO - Epoch [202/300], Batch [15/43], Training Loss: 0.00000924
2024-11-06 14:07:26,678 - INFO - Epoch [202/300], Batch [16/43], Training Loss: 0.00001941
2024-11-06 14:07:26,682 - INFO - Epoch [202/300], Batch [17/43], Training Loss: 0.00001330
2024-11-06 14:07:26,687 - INFO - Epoch [202/300], Batch [18/43], Training Loss: 0.00000442
2024-11-06 14:07:26,690 - INFO - Epoch [202/300], Batch [19/43], Training Loss: 0.00000450
2024-11-06 14:07:26,695 - INFO - Epoch [202/300], Batch [20/43], Training Loss: 0.00001009
2024-11-06 14:07:26,699 - INFO - Epoch [202/300], Batch [21/43], Training Loss: 0.00000585
2024-11-06 14:07:26,703 - INFO - Epoch [202/300], Batch [22/43], Training Loss: 0.00001033
2024-11-06 14:07:26,707 - INFO - Epoch [202/300], Batch [23/43], Training Loss: 0.00001036
2024-11-06 14:07:26,712 - INFO - Epoch [202/300], Batch [24/43], Training Loss: 0.00002024
2024-11-06 14:07:26,717 - INFO - Epoch [202/300], Batch [25/43], Training Loss: 0.00000162
2024-11-06 14:07:26,721 - INFO - Epoch [202/300], Batch [26/43], Training Loss: 0.00000374
2024-11-06 14:07:26,725 - INFO - Epoch [202/300], Batch [27/43], Training Loss: 0.00001847
2024-11-06 14:07:26,729 - INFO - Epoch [202/300], Batch [28/43], Training Loss: 0.00001335
2024-11-06 14:07:26,734 - INFO - Epoch [202/300], Batch [29/43], Training Loss: 0.00000421
2024-11-06 14:07:26,738 - INFO - Epoch [202/300], Batch [30/43], Training Loss: 0.00000418
2024-11-06 14:07:26,742 - INFO - Epoch [202/300], Batch [31/43], Training Loss: 0.00001395
2024-11-06 14:07:26,747 - INFO - Epoch [202/300], Batch [32/43], Training Loss: 0.00001086
2024-11-06 14:07:26,751 - INFO - Epoch [202/300], Batch [33/43], Training Loss: 0.00000790
2024-11-06 14:07:26,755 - INFO - Epoch [202/300], Batch [34/43], Training Loss: 0.00000971
2024-11-06 14:07:26,758 - INFO - Epoch [202/300], Batch [35/43], Training Loss: 0.00001529
2024-11-06 14:07:26,763 - INFO - Epoch [202/300], Batch [36/43], Training Loss: 0.00001297
2024-11-06 14:07:26,766 - INFO - Epoch [202/300], Batch [37/43], Training Loss: 0.00001381
2024-11-06 14:07:26,769 - INFO - Epoch [202/300], Batch [38/43], Training Loss: 0.00000587
2024-11-06 14:07:26,772 - INFO - Epoch [202/300], Batch [39/43], Training Loss: 0.00001194
2024-11-06 14:07:26,776 - INFO - Epoch [202/300], Batch [40/43], Training Loss: 0.00000601
2024-11-06 14:07:26,779 - INFO - Epoch [202/300], Batch [41/43], Training Loss: 0.00000447
2024-11-06 14:07:26,784 - INFO - Epoch [202/300], Batch [42/43], Training Loss: 0.00000857
2024-11-06 14:07:26,790 - INFO - Epoch [202/300], Batch [43/43], Training Loss: 0.00000868
2024-11-06 14:07:26,803 - INFO - Epoch [202/300], Average Training Loss: 0.00001053, Validation Loss: 0.00001280
2024-11-06 14:07:26,808 - INFO - Epoch [203/300], Batch [1/43], Training Loss: 0.00000680
2024-11-06 14:07:26,812 - INFO - Epoch [203/300], Batch [2/43], Training Loss: 0.00002505
2024-11-06 14:07:26,817 - INFO - Epoch [203/300], Batch [3/43], Training Loss: 0.00001148
2024-11-06 14:07:26,822 - INFO - Epoch [203/300], Batch [4/43], Training Loss: 0.00000986
2024-11-06 14:07:26,826 - INFO - Epoch [203/300], Batch [5/43], Training Loss: 0.00003271
2024-11-06 14:07:26,830 - INFO - Epoch [203/300], Batch [6/43], Training Loss: 0.00000564
2024-11-06 14:07:26,834 - INFO - Epoch [203/300], Batch [7/43], Training Loss: 0.00000614
2024-11-06 14:07:26,838 - INFO - Epoch [203/300], Batch [8/43], Training Loss: 0.00000531
2024-11-06 14:07:26,845 - INFO - Epoch [203/300], Batch [9/43], Training Loss: 0.00001237
2024-11-06 14:07:26,851 - INFO - Epoch [203/300], Batch [10/43], Training Loss: 0.00000452
2024-11-06 14:07:26,856 - INFO - Epoch [203/300], Batch [11/43], Training Loss: 0.00001211
2024-11-06 14:07:26,861 - INFO - Epoch [203/300], Batch [12/43], Training Loss: 0.00001921
2024-11-06 14:07:26,866 - INFO - Epoch [203/300], Batch [13/43], Training Loss: 0.00001230
2024-11-06 14:07:26,870 - INFO - Epoch [203/300], Batch [14/43], Training Loss: 0.00000513
2024-11-06 14:07:26,875 - INFO - Epoch [203/300], Batch [15/43], Training Loss: 0.00001388
2024-11-06 14:07:26,880 - INFO - Epoch [203/300], Batch [16/43], Training Loss: 0.00001634
2024-11-06 14:07:26,885 - INFO - Epoch [203/300], Batch [17/43], Training Loss: 0.00000873
2024-11-06 14:07:26,889 - INFO - Epoch [203/300], Batch [18/43], Training Loss: 0.00000581
2024-11-06 14:07:26,892 - INFO - Epoch [203/300], Batch [19/43], Training Loss: 0.00000852
2024-11-06 14:07:26,896 - INFO - Epoch [203/300], Batch [20/43], Training Loss: 0.00002564
2024-11-06 14:07:26,900 - INFO - Epoch [203/300], Batch [21/43], Training Loss: 0.00000584
2024-11-06 14:07:26,905 - INFO - Epoch [203/300], Batch [22/43], Training Loss: 0.00001713
2024-11-06 14:07:26,908 - INFO - Epoch [203/300], Batch [23/43], Training Loss: 0.00000815
2024-11-06 14:07:26,912 - INFO - Epoch [203/300], Batch [24/43], Training Loss: 0.00000392
2024-11-06 14:07:26,915 - INFO - Epoch [203/300], Batch [25/43], Training Loss: 0.00000349
2024-11-06 14:07:26,919 - INFO - Epoch [203/300], Batch [26/43], Training Loss: 0.00000521
2024-11-06 14:07:26,923 - INFO - Epoch [203/300], Batch [27/43], Training Loss: 0.00000776
2024-11-06 14:07:26,926 - INFO - Epoch [203/300], Batch [28/43], Training Loss: 0.00000314
2024-11-06 14:07:26,930 - INFO - Epoch [203/300], Batch [29/43], Training Loss: 0.00000489
2024-11-06 14:07:26,934 - INFO - Epoch [203/300], Batch [30/43], Training Loss: 0.00001072
2024-11-06 14:07:26,938 - INFO - Epoch [203/300], Batch [31/43], Training Loss: 0.00000590
2024-11-06 14:07:26,942 - INFO - Epoch [203/300], Batch [32/43], Training Loss: 0.00001726
2024-11-06 14:07:26,946 - INFO - Epoch [203/300], Batch [33/43], Training Loss: 0.00000995
2024-11-06 14:07:26,950 - INFO - Epoch [203/300], Batch [34/43], Training Loss: 0.00000894
2024-11-06 14:07:26,954 - INFO - Epoch [203/300], Batch [35/43], Training Loss: 0.00001016
2024-11-06 14:07:26,958 - INFO - Epoch [203/300], Batch [36/43], Training Loss: 0.00001019
2024-11-06 14:07:26,962 - INFO - Epoch [203/300], Batch [37/43], Training Loss: 0.00001176
2024-11-06 14:07:26,966 - INFO - Epoch [203/300], Batch [38/43], Training Loss: 0.00000515
2024-11-06 14:07:26,970 - INFO - Epoch [203/300], Batch [39/43], Training Loss: 0.00000978
2024-11-06 14:07:26,974 - INFO - Epoch [203/300], Batch [40/43], Training Loss: 0.00000504
2024-11-06 14:07:26,979 - INFO - Epoch [203/300], Batch [41/43], Training Loss: 0.00001234
2024-11-06 14:07:26,982 - INFO - Epoch [203/300], Batch [42/43], Training Loss: 0.00001718
2024-11-06 14:07:26,987 - INFO - Epoch [203/300], Batch [43/43], Training Loss: 0.00000654
2024-11-06 14:07:26,998 - INFO - Epoch [203/300], Average Training Loss: 0.00001042, Validation Loss: 0.00001313
2024-11-06 14:07:27,004 - INFO - Epoch [204/300], Batch [1/43], Training Loss: 0.00000888
2024-11-06 14:07:27,008 - INFO - Epoch [204/300], Batch [2/43], Training Loss: 0.00000414
2024-11-06 14:07:27,013 - INFO - Epoch [204/300], Batch [3/43], Training Loss: 0.00001118
2024-11-06 14:07:27,017 - INFO - Epoch [204/300], Batch [4/43], Training Loss: 0.00001139
2024-11-06 14:07:27,020 - INFO - Epoch [204/300], Batch [5/43], Training Loss: 0.00000500
2024-11-06 14:07:27,023 - INFO - Epoch [204/300], Batch [6/43], Training Loss: 0.00001053
2024-11-06 14:07:27,027 - INFO - Epoch [204/300], Batch [7/43], Training Loss: 0.00000315
2024-11-06 14:07:27,031 - INFO - Epoch [204/300], Batch [8/43], Training Loss: 0.00001082
2024-11-06 14:07:27,035 - INFO - Epoch [204/300], Batch [9/43], Training Loss: 0.00000208
2024-11-06 14:07:27,039 - INFO - Epoch [204/300], Batch [10/43], Training Loss: 0.00001179
2024-11-06 14:07:27,042 - INFO - Epoch [204/300], Batch [11/43], Training Loss: 0.00000466
2024-11-06 14:07:27,047 - INFO - Epoch [204/300], Batch [12/43], Training Loss: 0.00000934
2024-11-06 14:07:27,051 - INFO - Epoch [204/300], Batch [13/43], Training Loss: 0.00000873
2024-11-06 14:07:27,055 - INFO - Epoch [204/300], Batch [14/43], Training Loss: 0.00000922
2024-11-06 14:07:27,059 - INFO - Epoch [204/300], Batch [15/43], Training Loss: 0.00000786
2024-11-06 14:07:27,063 - INFO - Epoch [204/300], Batch [16/43], Training Loss: 0.00000843
2024-11-06 14:07:27,067 - INFO - Epoch [204/300], Batch [17/43], Training Loss: 0.00000691
2024-11-06 14:07:27,071 - INFO - Epoch [204/300], Batch [18/43], Training Loss: 0.00001112
2024-11-06 14:07:27,076 - INFO - Epoch [204/300], Batch [19/43], Training Loss: 0.00001538
2024-11-06 14:07:27,080 - INFO - Epoch [204/300], Batch [20/43], Training Loss: 0.00002590
2024-11-06 14:07:27,084 - INFO - Epoch [204/300], Batch [21/43], Training Loss: 0.00000747
2024-11-06 14:07:27,087 - INFO - Epoch [204/300], Batch [22/43], Training Loss: 0.00001024
2024-11-06 14:07:27,091 - INFO - Epoch [204/300], Batch [23/43], Training Loss: 0.00000778
2024-11-06 14:07:27,096 - INFO - Epoch [204/300], Batch [24/43], Training Loss: 0.00002458
2024-11-06 14:07:27,101 - INFO - Epoch [204/300], Batch [25/43], Training Loss: 0.00000958
2024-11-06 14:07:27,105 - INFO - Epoch [204/300], Batch [26/43], Training Loss: 0.00000980
2024-11-06 14:07:27,109 - INFO - Epoch [204/300], Batch [27/43], Training Loss: 0.00000617
2024-11-06 14:07:27,114 - INFO - Epoch [204/300], Batch [28/43], Training Loss: 0.00000647
2024-11-06 14:07:27,118 - INFO - Epoch [204/300], Batch [29/43], Training Loss: 0.00000869
2024-11-06 14:07:27,123 - INFO - Epoch [204/300], Batch [30/43], Training Loss: 0.00002768
2024-11-06 14:07:27,127 - INFO - Epoch [204/300], Batch [31/43], Training Loss: 0.00000664
2024-11-06 14:07:27,132 - INFO - Epoch [204/300], Batch [32/43], Training Loss: 0.00000941
2024-11-06 14:07:27,136 - INFO - Epoch [204/300], Batch [33/43], Training Loss: 0.00001073
2024-11-06 14:07:27,141 - INFO - Epoch [204/300], Batch [34/43], Training Loss: 0.00000127
2024-11-06 14:07:27,145 - INFO - Epoch [204/300], Batch [35/43], Training Loss: 0.00001939
2024-11-06 14:07:27,149 - INFO - Epoch [204/300], Batch [36/43], Training Loss: 0.00002199
2024-11-06 14:07:27,155 - INFO - Epoch [204/300], Batch [37/43], Training Loss: 0.00001004
2024-11-06 14:07:27,160 - INFO - Epoch [204/300], Batch [38/43], Training Loss: 0.00001111
2024-11-06 14:07:27,169 - INFO - Epoch [204/300], Batch [39/43], Training Loss: 0.00000866
2024-11-06 14:07:27,174 - INFO - Epoch [204/300], Batch [40/43], Training Loss: 0.00000729
2024-11-06 14:07:27,179 - INFO - Epoch [204/300], Batch [41/43], Training Loss: 0.00000534
2024-11-06 14:07:27,183 - INFO - Epoch [204/300], Batch [42/43], Training Loss: 0.00000625
2024-11-06 14:07:27,188 - INFO - Epoch [204/300], Batch [43/43], Training Loss: 0.00000467
2024-11-06 14:07:27,203 - INFO - Epoch [204/300], Average Training Loss: 0.00000995, Validation Loss: 0.00001414
2024-11-06 14:07:27,208 - INFO - Epoch [205/300], Batch [1/43], Training Loss: 0.00001958
2024-11-06 14:07:27,212 - INFO - Epoch [205/300], Batch [2/43], Training Loss: 0.00001124
2024-11-06 14:07:27,217 - INFO - Epoch [205/300], Batch [3/43], Training Loss: 0.00001025
2024-11-06 14:07:27,222 - INFO - Epoch [205/300], Batch [4/43], Training Loss: 0.00000792
2024-11-06 14:07:27,227 - INFO - Epoch [205/300], Batch [5/43], Training Loss: 0.00000493
2024-11-06 14:07:27,232 - INFO - Epoch [205/300], Batch [6/43], Training Loss: 0.00001192
2024-11-06 14:07:27,237 - INFO - Epoch [205/300], Batch [7/43], Training Loss: 0.00000501
2024-11-06 14:07:27,242 - INFO - Epoch [205/300], Batch [8/43], Training Loss: 0.00000753
2024-11-06 14:07:27,247 - INFO - Epoch [205/300], Batch [9/43], Training Loss: 0.00000409
2024-11-06 14:07:27,253 - INFO - Epoch [205/300], Batch [10/43], Training Loss: 0.00000396
2024-11-06 14:07:27,258 - INFO - Epoch [205/300], Batch [11/43], Training Loss: 0.00000431
2024-11-06 14:07:27,264 - INFO - Epoch [205/300], Batch [12/43], Training Loss: 0.00000757
2024-11-06 14:07:27,269 - INFO - Epoch [205/300], Batch [13/43], Training Loss: 0.00000347
2024-11-06 14:07:27,273 - INFO - Epoch [205/300], Batch [14/43], Training Loss: 0.00001988
2024-11-06 14:07:27,277 - INFO - Epoch [205/300], Batch [15/43], Training Loss: 0.00001732
2024-11-06 14:07:27,282 - INFO - Epoch [205/300], Batch [16/43], Training Loss: 0.00001715
2024-11-06 14:07:27,286 - INFO - Epoch [205/300], Batch [17/43], Training Loss: 0.00000775
2024-11-06 14:07:27,290 - INFO - Epoch [205/300], Batch [18/43], Training Loss: 0.00000850
2024-11-06 14:07:27,294 - INFO - Epoch [205/300], Batch [19/43], Training Loss: 0.00000873
2024-11-06 14:07:27,298 - INFO - Epoch [205/300], Batch [20/43], Training Loss: 0.00000666
2024-11-06 14:07:27,302 - INFO - Epoch [205/300], Batch [21/43], Training Loss: 0.00000542
2024-11-06 14:07:27,306 - INFO - Epoch [205/300], Batch [22/43], Training Loss: 0.00001394
2024-11-06 14:07:27,311 - INFO - Epoch [205/300], Batch [23/43], Training Loss: 0.00001641
2024-11-06 14:07:27,315 - INFO - Epoch [205/300], Batch [24/43], Training Loss: 0.00002253
2024-11-06 14:07:27,319 - INFO - Epoch [205/300], Batch [25/43], Training Loss: 0.00001601
2024-11-06 14:07:27,323 - INFO - Epoch [205/300], Batch [26/43], Training Loss: 0.00001355
2024-11-06 14:07:27,327 - INFO - Epoch [205/300], Batch [27/43], Training Loss: 0.00001952
2024-11-06 14:07:27,332 - INFO - Epoch [205/300], Batch [28/43], Training Loss: 0.00000457
2024-11-06 14:07:27,336 - INFO - Epoch [205/300], Batch [29/43], Training Loss: 0.00001019
2024-11-06 14:07:27,340 - INFO - Epoch [205/300], Batch [30/43], Training Loss: 0.00001891
2024-11-06 14:07:27,346 - INFO - Epoch [205/300], Batch [31/43], Training Loss: 0.00001019
2024-11-06 14:07:27,351 - INFO - Epoch [205/300], Batch [32/43], Training Loss: 0.00000648
2024-11-06 14:07:27,355 - INFO - Epoch [205/300], Batch [33/43], Training Loss: 0.00001453
2024-11-06 14:07:27,360 - INFO - Epoch [205/300], Batch [34/43], Training Loss: 0.00001226
2024-11-06 14:07:27,364 - INFO - Epoch [205/300], Batch [35/43], Training Loss: 0.00001912
2024-11-06 14:07:27,368 - INFO - Epoch [205/300], Batch [36/43], Training Loss: 0.00000279
2024-11-06 14:07:27,371 - INFO - Epoch [205/300], Batch [37/43], Training Loss: 0.00001781
2024-11-06 14:07:27,375 - INFO - Epoch [205/300], Batch [38/43], Training Loss: 0.00001587
2024-11-06 14:07:27,379 - INFO - Epoch [205/300], Batch [39/43], Training Loss: 0.00001064
2024-11-06 14:07:27,383 - INFO - Epoch [205/300], Batch [40/43], Training Loss: 0.00000786
2024-11-06 14:07:27,391 - INFO - Epoch [205/300], Batch [41/43], Training Loss: 0.00003066
2024-11-06 14:07:27,396 - INFO - Epoch [205/300], Batch [42/43], Training Loss: 0.00000763
2024-11-06 14:07:27,403 - INFO - Epoch [205/300], Batch [43/43], Training Loss: 0.00001724
2024-11-06 14:07:27,418 - INFO - Epoch [205/300], Average Training Loss: 0.00001167, Validation Loss: 0.00002439
2024-11-06 14:07:27,423 - INFO - Epoch [206/300], Batch [1/43], Training Loss: 0.00002869
2024-11-06 14:07:27,427 - INFO - Epoch [206/300], Batch [2/43], Training Loss: 0.00002121
2024-11-06 14:07:27,432 - INFO - Epoch [206/300], Batch [3/43], Training Loss: 0.00000818
2024-11-06 14:07:27,438 - INFO - Epoch [206/300], Batch [4/43], Training Loss: 0.00001776
2024-11-06 14:07:27,442 - INFO - Epoch [206/300], Batch [5/43], Training Loss: 0.00002884
2024-11-06 14:07:27,447 - INFO - Epoch [206/300], Batch [6/43], Training Loss: 0.00000676
2024-11-06 14:07:27,452 - INFO - Epoch [206/300], Batch [7/43], Training Loss: 0.00001478
2024-11-06 14:07:27,458 - INFO - Epoch [206/300], Batch [8/43], Training Loss: 0.00003052
2024-11-06 14:07:27,462 - INFO - Epoch [206/300], Batch [9/43], Training Loss: 0.00000366
2024-11-06 14:07:27,469 - INFO - Epoch [206/300], Batch [10/43], Training Loss: 0.00001557
2024-11-06 14:07:27,476 - INFO - Epoch [206/300], Batch [11/43], Training Loss: 0.00000999
2024-11-06 14:07:27,483 - INFO - Epoch [206/300], Batch [12/43], Training Loss: 0.00000338
2024-11-06 14:07:27,488 - INFO - Epoch [206/300], Batch [13/43], Training Loss: 0.00000391
2024-11-06 14:07:27,492 - INFO - Epoch [206/300], Batch [14/43], Training Loss: 0.00000995
2024-11-06 14:07:27,497 - INFO - Epoch [206/300], Batch [15/43], Training Loss: 0.00002972
2024-11-06 14:07:27,501 - INFO - Epoch [206/300], Batch [16/43], Training Loss: 0.00001627
2024-11-06 14:07:27,507 - INFO - Epoch [206/300], Batch [17/43], Training Loss: 0.00001823
2024-11-06 14:07:27,511 - INFO - Epoch [206/300], Batch [18/43], Training Loss: 0.00001217
2024-11-06 14:07:27,516 - INFO - Epoch [206/300], Batch [19/43], Training Loss: 0.00000832
2024-11-06 14:07:27,523 - INFO - Epoch [206/300], Batch [20/43], Training Loss: 0.00001167
2024-11-06 14:07:27,529 - INFO - Epoch [206/300], Batch [21/43], Training Loss: 0.00002588
2024-11-06 14:07:27,535 - INFO - Epoch [206/300], Batch [22/43], Training Loss: 0.00000415
2024-11-06 14:07:27,540 - INFO - Epoch [206/300], Batch [23/43], Training Loss: 0.00001059
2024-11-06 14:07:27,545 - INFO - Epoch [206/300], Batch [24/43], Training Loss: 0.00002142
2024-11-06 14:07:27,549 - INFO - Epoch [206/300], Batch [25/43], Training Loss: 0.00000338
2024-11-06 14:07:27,554 - INFO - Epoch [206/300], Batch [26/43], Training Loss: 0.00001097
2024-11-06 14:07:27,558 - INFO - Epoch [206/300], Batch [27/43], Training Loss: 0.00000924
2024-11-06 14:07:27,562 - INFO - Epoch [206/300], Batch [28/43], Training Loss: 0.00000902
2024-11-06 14:07:27,567 - INFO - Epoch [206/300], Batch [29/43], Training Loss: 0.00000632
2024-11-06 14:07:27,571 - INFO - Epoch [206/300], Batch [30/43], Training Loss: 0.00002019
2024-11-06 14:07:27,575 - INFO - Epoch [206/300], Batch [31/43], Training Loss: 0.00001203
2024-11-06 14:07:27,579 - INFO - Epoch [206/300], Batch [32/43], Training Loss: 0.00000413
2024-11-06 14:07:27,583 - INFO - Epoch [206/300], Batch [33/43], Training Loss: 0.00000618
2024-11-06 14:07:27,587 - INFO - Epoch [206/300], Batch [34/43], Training Loss: 0.00001044
2024-11-06 14:07:27,591 - INFO - Epoch [206/300], Batch [35/43], Training Loss: 0.00000567
2024-11-06 14:07:27,596 - INFO - Epoch [206/300], Batch [36/43], Training Loss: 0.00000923
2024-11-06 14:07:27,601 - INFO - Epoch [206/300], Batch [37/43], Training Loss: 0.00001161
2024-11-06 14:07:27,607 - INFO - Epoch [206/300], Batch [38/43], Training Loss: 0.00000207
2024-11-06 14:07:27,612 - INFO - Epoch [206/300], Batch [39/43], Training Loss: 0.00000572
2024-11-06 14:07:27,617 - INFO - Epoch [206/300], Batch [40/43], Training Loss: 0.00000845
2024-11-06 14:07:27,621 - INFO - Epoch [206/300], Batch [41/43], Training Loss: 0.00000662
2024-11-06 14:07:27,626 - INFO - Epoch [206/300], Batch [42/43], Training Loss: 0.00000515
2024-11-06 14:07:27,630 - INFO - Epoch [206/300], Batch [43/43], Training Loss: 0.00000191
2024-11-06 14:07:27,644 - INFO - Epoch [206/300], Average Training Loss: 0.00001186, Validation Loss: 0.00001208
2024-11-06 14:07:27,649 - INFO - Epoch [207/300], Batch [1/43], Training Loss: 0.00000900
2024-11-06 14:07:27,653 - INFO - Epoch [207/300], Batch [2/43], Training Loss: 0.00000440
2024-11-06 14:07:27,657 - INFO - Epoch [207/300], Batch [3/43], Training Loss: 0.00000989
2024-11-06 14:07:27,661 - INFO - Epoch [207/300], Batch [4/43], Training Loss: 0.00000252
2024-11-06 14:07:27,665 - INFO - Epoch [207/300], Batch [5/43], Training Loss: 0.00001299
2024-11-06 14:07:27,671 - INFO - Epoch [207/300], Batch [6/43], Training Loss: 0.00000816
2024-11-06 14:07:27,675 - INFO - Epoch [207/300], Batch [7/43], Training Loss: 0.00001040
2024-11-06 14:07:27,678 - INFO - Epoch [207/300], Batch [8/43], Training Loss: 0.00000796
2024-11-06 14:07:27,681 - INFO - Epoch [207/300], Batch [9/43], Training Loss: 0.00000643
2024-11-06 14:07:27,685 - INFO - Epoch [207/300], Batch [10/43], Training Loss: 0.00001067
2024-11-06 14:07:27,689 - INFO - Epoch [207/300], Batch [11/43], Training Loss: 0.00000431
2024-11-06 14:07:27,693 - INFO - Epoch [207/300], Batch [12/43], Training Loss: 0.00000727
2024-11-06 14:07:27,697 - INFO - Epoch [207/300], Batch [13/43], Training Loss: 0.00002312
2024-11-06 14:07:27,702 - INFO - Epoch [207/300], Batch [14/43], Training Loss: 0.00001211
2024-11-06 14:07:27,706 - INFO - Epoch [207/300], Batch [15/43], Training Loss: 0.00000367
2024-11-06 14:07:27,710 - INFO - Epoch [207/300], Batch [16/43], Training Loss: 0.00000579
2024-11-06 14:07:27,715 - INFO - Epoch [207/300], Batch [17/43], Training Loss: 0.00000602
2024-11-06 14:07:27,720 - INFO - Epoch [207/300], Batch [18/43], Training Loss: 0.00000972
2024-11-06 14:07:27,725 - INFO - Epoch [207/300], Batch [19/43], Training Loss: 0.00000744
2024-11-06 14:07:27,731 - INFO - Epoch [207/300], Batch [20/43], Training Loss: 0.00001145
2024-11-06 14:07:27,735 - INFO - Epoch [207/300], Batch [21/43], Training Loss: 0.00001642
2024-11-06 14:07:27,740 - INFO - Epoch [207/300], Batch [22/43], Training Loss: 0.00001267
2024-11-06 14:07:27,745 - INFO - Epoch [207/300], Batch [23/43], Training Loss: 0.00000688
2024-11-06 14:07:27,749 - INFO - Epoch [207/300], Batch [24/43], Training Loss: 0.00003070
2024-11-06 14:07:27,753 - INFO - Epoch [207/300], Batch [25/43], Training Loss: 0.00000627
2024-11-06 14:07:27,758 - INFO - Epoch [207/300], Batch [26/43], Training Loss: 0.00000565
2024-11-06 14:07:27,763 - INFO - Epoch [207/300], Batch [27/43], Training Loss: 0.00002736
2024-11-06 14:07:27,767 - INFO - Epoch [207/300], Batch [28/43], Training Loss: 0.00001274
2024-11-06 14:07:27,771 - INFO - Epoch [207/300], Batch [29/43], Training Loss: 0.00000702
2024-11-06 14:07:27,776 - INFO - Epoch [207/300], Batch [30/43], Training Loss: 0.00001437
2024-11-06 14:07:27,781 - INFO - Epoch [207/300], Batch [31/43], Training Loss: 0.00001537
2024-11-06 14:07:27,785 - INFO - Epoch [207/300], Batch [32/43], Training Loss: 0.00000771
2024-11-06 14:07:27,790 - INFO - Epoch [207/300], Batch [33/43], Training Loss: 0.00001677
2024-11-06 14:07:27,793 - INFO - Epoch [207/300], Batch [34/43], Training Loss: 0.00000874
2024-11-06 14:07:27,797 - INFO - Epoch [207/300], Batch [35/43], Training Loss: 0.00000475
2024-11-06 14:07:27,800 - INFO - Epoch [207/300], Batch [36/43], Training Loss: 0.00000335
2024-11-06 14:07:27,803 - INFO - Epoch [207/300], Batch [37/43], Training Loss: 0.00001794
2024-11-06 14:07:27,806 - INFO - Epoch [207/300], Batch [38/43], Training Loss: 0.00001072
2024-11-06 14:07:27,810 - INFO - Epoch [207/300], Batch [39/43], Training Loss: 0.00000946
2024-11-06 14:07:27,814 - INFO - Epoch [207/300], Batch [40/43], Training Loss: 0.00000458
2024-11-06 14:07:27,818 - INFO - Epoch [207/300], Batch [41/43], Training Loss: 0.00000770
2024-11-06 14:07:27,821 - INFO - Epoch [207/300], Batch [42/43], Training Loss: 0.00001581
2024-11-06 14:07:27,825 - INFO - Epoch [207/300], Batch [43/43], Training Loss: 0.00001825
2024-11-06 14:07:27,837 - INFO - Epoch [207/300], Average Training Loss: 0.00001057, Validation Loss: 0.00001272
2024-11-06 14:07:27,842 - INFO - Epoch [208/300], Batch [1/43], Training Loss: 0.00000536
2024-11-06 14:07:27,846 - INFO - Epoch [208/300], Batch [2/43], Training Loss: 0.00002230
2024-11-06 14:07:27,850 - INFO - Epoch [208/300], Batch [3/43], Training Loss: 0.00000555
2024-11-06 14:07:27,853 - INFO - Epoch [208/300], Batch [4/43], Training Loss: 0.00002096
2024-11-06 14:07:27,857 - INFO - Epoch [208/300], Batch [5/43], Training Loss: 0.00001305
2024-11-06 14:07:27,861 - INFO - Epoch [208/300], Batch [6/43], Training Loss: 0.00000465
2024-11-06 14:07:27,864 - INFO - Epoch [208/300], Batch [7/43], Training Loss: 0.00001674
2024-11-06 14:07:27,867 - INFO - Epoch [208/300], Batch [8/43], Training Loss: 0.00001076
2024-11-06 14:07:27,872 - INFO - Epoch [208/300], Batch [9/43], Training Loss: 0.00000751
2024-11-06 14:07:27,877 - INFO - Epoch [208/300], Batch [10/43], Training Loss: 0.00001087
2024-11-06 14:07:27,880 - INFO - Epoch [208/300], Batch [11/43], Training Loss: 0.00001435
2024-11-06 14:07:27,885 - INFO - Epoch [208/300], Batch [12/43], Training Loss: 0.00000819
2024-11-06 14:07:27,889 - INFO - Epoch [208/300], Batch [13/43], Training Loss: 0.00000997
2024-11-06 14:07:27,894 - INFO - Epoch [208/300], Batch [14/43], Training Loss: 0.00001006
2024-11-06 14:07:27,898 - INFO - Epoch [208/300], Batch [15/43], Training Loss: 0.00000682
2024-11-06 14:07:27,902 - INFO - Epoch [208/300], Batch [16/43], Training Loss: 0.00001418
2024-11-06 14:07:27,907 - INFO - Epoch [208/300], Batch [17/43], Training Loss: 0.00000360
2024-11-06 14:07:27,911 - INFO - Epoch [208/300], Batch [18/43], Training Loss: 0.00002657
2024-11-06 14:07:27,915 - INFO - Epoch [208/300], Batch [19/43], Training Loss: 0.00001838
2024-11-06 14:07:27,919 - INFO - Epoch [208/300], Batch [20/43], Training Loss: 0.00000969
2024-11-06 14:07:27,922 - INFO - Epoch [208/300], Batch [21/43], Training Loss: 0.00001026
2024-11-06 14:07:27,926 - INFO - Epoch [208/300], Batch [22/43], Training Loss: 0.00000885
2024-11-06 14:07:27,929 - INFO - Epoch [208/300], Batch [23/43], Training Loss: 0.00001085
2024-11-06 14:07:27,932 - INFO - Epoch [208/300], Batch [24/43], Training Loss: 0.00001115
2024-11-06 14:07:27,936 - INFO - Epoch [208/300], Batch [25/43], Training Loss: 0.00001765
2024-11-06 14:07:27,940 - INFO - Epoch [208/300], Batch [26/43], Training Loss: 0.00001702
2024-11-06 14:07:27,943 - INFO - Epoch [208/300], Batch [27/43], Training Loss: 0.00001070
2024-11-06 14:07:27,946 - INFO - Epoch [208/300], Batch [28/43], Training Loss: 0.00000357
2024-11-06 14:07:27,950 - INFO - Epoch [208/300], Batch [29/43], Training Loss: 0.00000716
2024-11-06 14:07:27,954 - INFO - Epoch [208/300], Batch [30/43], Training Loss: 0.00001642
2024-11-06 14:07:27,958 - INFO - Epoch [208/300], Batch [31/43], Training Loss: 0.00001079
2024-11-06 14:07:27,961 - INFO - Epoch [208/300], Batch [32/43], Training Loss: 0.00000355
2024-11-06 14:07:27,965 - INFO - Epoch [208/300], Batch [33/43], Training Loss: 0.00000463
2024-11-06 14:07:27,968 - INFO - Epoch [208/300], Batch [34/43], Training Loss: 0.00000923
2024-11-06 14:07:27,972 - INFO - Epoch [208/300], Batch [35/43], Training Loss: 0.00000785
2024-11-06 14:07:27,975 - INFO - Epoch [208/300], Batch [36/43], Training Loss: 0.00000814
2024-11-06 14:07:27,978 - INFO - Epoch [208/300], Batch [37/43], Training Loss: 0.00000789
2024-11-06 14:07:27,980 - INFO - Epoch [208/300], Batch [38/43], Training Loss: 0.00001512
2024-11-06 14:07:27,984 - INFO - Epoch [208/300], Batch [39/43], Training Loss: 0.00002319
2024-11-06 14:07:27,987 - INFO - Epoch [208/300], Batch [40/43], Training Loss: 0.00000459
2024-11-06 14:07:27,990 - INFO - Epoch [208/300], Batch [41/43], Training Loss: 0.00001330
2024-11-06 14:07:27,993 - INFO - Epoch [208/300], Batch [42/43], Training Loss: 0.00000644
2024-11-06 14:07:27,996 - INFO - Epoch [208/300], Batch [43/43], Training Loss: 0.00003532
2024-11-06 14:07:28,008 - INFO - Epoch [208/300], Average Training Loss: 0.00001170, Validation Loss: 0.00001281
2024-11-06 14:07:28,012 - INFO - Epoch [209/300], Batch [1/43], Training Loss: 0.00000770
2024-11-06 14:07:28,015 - INFO - Epoch [209/300], Batch [2/43], Training Loss: 0.00000408
2024-11-06 14:07:28,018 - INFO - Epoch [209/300], Batch [3/43], Training Loss: 0.00001497
2024-11-06 14:07:28,021 - INFO - Epoch [209/300], Batch [4/43], Training Loss: 0.00002300
2024-11-06 14:07:28,024 - INFO - Epoch [209/300], Batch [5/43], Training Loss: 0.00002349
2024-11-06 14:07:28,028 - INFO - Epoch [209/300], Batch [6/43], Training Loss: 0.00001464
2024-11-06 14:07:28,032 - INFO - Epoch [209/300], Batch [7/43], Training Loss: 0.00000663
2024-11-06 14:07:28,035 - INFO - Epoch [209/300], Batch [8/43], Training Loss: 0.00000867
2024-11-06 14:07:28,038 - INFO - Epoch [209/300], Batch [9/43], Training Loss: 0.00000852
2024-11-06 14:07:28,042 - INFO - Epoch [209/300], Batch [10/43], Training Loss: 0.00000924
2024-11-06 14:07:28,046 - INFO - Epoch [209/300], Batch [11/43], Training Loss: 0.00002763
2024-11-06 14:07:28,050 - INFO - Epoch [209/300], Batch [12/43], Training Loss: 0.00000319
2024-11-06 14:07:28,055 - INFO - Epoch [209/300], Batch [13/43], Training Loss: 0.00000279
2024-11-06 14:07:28,059 - INFO - Epoch [209/300], Batch [14/43], Training Loss: 0.00000522
2024-11-06 14:07:28,063 - INFO - Epoch [209/300], Batch [15/43], Training Loss: 0.00000516
2024-11-06 14:07:28,067 - INFO - Epoch [209/300], Batch [16/43], Training Loss: 0.00000837
2024-11-06 14:07:28,070 - INFO - Epoch [209/300], Batch [17/43], Training Loss: 0.00000753
2024-11-06 14:07:28,073 - INFO - Epoch [209/300], Batch [18/43], Training Loss: 0.00000367
2024-11-06 14:07:28,076 - INFO - Epoch [209/300], Batch [19/43], Training Loss: 0.00001043
2024-11-06 14:07:28,114 - INFO - Epoch [209/300], Batch [20/43], Training Loss: 0.00000327
2024-11-06 14:07:28,126 - INFO - Epoch [209/300], Batch [21/43], Training Loss: 0.00000659
2024-11-06 14:07:28,133 - INFO - Epoch [209/300], Batch [22/43], Training Loss: 0.00000408
2024-11-06 14:07:28,141 - INFO - Epoch [209/300], Batch [23/43], Training Loss: 0.00000901
2024-11-06 14:07:28,145 - INFO - Epoch [209/300], Batch [24/43], Training Loss: 0.00001283
2024-11-06 14:07:28,150 - INFO - Epoch [209/300], Batch [25/43], Training Loss: 0.00001603
2024-11-06 14:07:28,155 - INFO - Epoch [209/300], Batch [26/43], Training Loss: 0.00001031
2024-11-06 14:07:28,160 - INFO - Epoch [209/300], Batch [27/43], Training Loss: 0.00000489
2024-11-06 14:07:28,164 - INFO - Epoch [209/300], Batch [28/43], Training Loss: 0.00000772
2024-11-06 14:07:28,169 - INFO - Epoch [209/300], Batch [29/43], Training Loss: 0.00001444
2024-11-06 14:07:28,173 - INFO - Epoch [209/300], Batch [30/43], Training Loss: 0.00000663
2024-11-06 14:07:28,177 - INFO - Epoch [209/300], Batch [31/43], Training Loss: 0.00000980
2024-11-06 14:07:28,182 - INFO - Epoch [209/300], Batch [32/43], Training Loss: 0.00001571
2024-11-06 14:07:28,187 - INFO - Epoch [209/300], Batch [33/43], Training Loss: 0.00001084
2024-11-06 14:07:28,191 - INFO - Epoch [209/300], Batch [34/43], Training Loss: 0.00000685
2024-11-06 14:07:28,195 - INFO - Epoch [209/300], Batch [35/43], Training Loss: 0.00000418
2024-11-06 14:07:28,200 - INFO - Epoch [209/300], Batch [36/43], Training Loss: 0.00001164
2024-11-06 14:07:28,205 - INFO - Epoch [209/300], Batch [37/43], Training Loss: 0.00000977
2024-11-06 14:07:28,209 - INFO - Epoch [209/300], Batch [38/43], Training Loss: 0.00000567
2024-11-06 14:07:28,213 - INFO - Epoch [209/300], Batch [39/43], Training Loss: 0.00001466
2024-11-06 14:07:28,217 - INFO - Epoch [209/300], Batch [40/43], Training Loss: 0.00000933
2024-11-06 14:07:28,221 - INFO - Epoch [209/300], Batch [41/43], Training Loss: 0.00000539
2024-11-06 14:07:28,225 - INFO - Epoch [209/300], Batch [42/43], Training Loss: 0.00001248
2024-11-06 14:07:28,229 - INFO - Epoch [209/300], Batch [43/43], Training Loss: 0.00002137
2024-11-06 14:07:28,242 - INFO - Epoch [209/300], Average Training Loss: 0.00000996, Validation Loss: 0.00001290
2024-11-06 14:07:28,246 - INFO - Epoch [210/300], Batch [1/43], Training Loss: 0.00000633
2024-11-06 14:07:28,251 - INFO - Epoch [210/300], Batch [2/43], Training Loss: 0.00000936
2024-11-06 14:07:28,256 - INFO - Epoch [210/300], Batch [3/43], Training Loss: 0.00001947
2024-11-06 14:07:28,260 - INFO - Epoch [210/300], Batch [4/43], Training Loss: 0.00001258
2024-11-06 14:07:28,267 - INFO - Epoch [210/300], Batch [5/43], Training Loss: 0.00000685
2024-11-06 14:07:28,271 - INFO - Epoch [210/300], Batch [6/43], Training Loss: 0.00000712
2024-11-06 14:07:28,276 - INFO - Epoch [210/300], Batch [7/43], Training Loss: 0.00001374
2024-11-06 14:07:28,280 - INFO - Epoch [210/300], Batch [8/43], Training Loss: 0.00001119
2024-11-06 14:07:28,284 - INFO - Epoch [210/300], Batch [9/43], Training Loss: 0.00001409
2024-11-06 14:07:28,288 - INFO - Epoch [210/300], Batch [10/43], Training Loss: 0.00001953
2024-11-06 14:07:28,292 - INFO - Epoch [210/300], Batch [11/43], Training Loss: 0.00000343
2024-11-06 14:07:28,297 - INFO - Epoch [210/300], Batch [12/43], Training Loss: 0.00001286
2024-11-06 14:07:28,300 - INFO - Epoch [210/300], Batch [13/43], Training Loss: 0.00001319
2024-11-06 14:07:28,304 - INFO - Epoch [210/300], Batch [14/43], Training Loss: 0.00000182
2024-11-06 14:07:28,307 - INFO - Epoch [210/300], Batch [15/43], Training Loss: 0.00000822
2024-11-06 14:07:28,310 - INFO - Epoch [210/300], Batch [16/43], Training Loss: 0.00000657
2024-11-06 14:07:28,314 - INFO - Epoch [210/300], Batch [17/43], Training Loss: 0.00000654
2024-11-06 14:07:28,317 - INFO - Epoch [210/300], Batch [18/43], Training Loss: 0.00000320
2024-11-06 14:07:28,320 - INFO - Epoch [210/300], Batch [19/43], Training Loss: 0.00001545
2024-11-06 14:07:28,324 - INFO - Epoch [210/300], Batch [20/43], Training Loss: 0.00000603
2024-11-06 14:07:28,328 - INFO - Epoch [210/300], Batch [21/43], Training Loss: 0.00000473
2024-11-06 14:07:28,331 - INFO - Epoch [210/300], Batch [22/43], Training Loss: 0.00000893
2024-11-06 14:07:28,334 - INFO - Epoch [210/300], Batch [23/43], Training Loss: 0.00000855
2024-11-06 14:07:28,337 - INFO - Epoch [210/300], Batch [24/43], Training Loss: 0.00000644
2024-11-06 14:07:28,341 - INFO - Epoch [210/300], Batch [25/43], Training Loss: 0.00001289
2024-11-06 14:07:28,344 - INFO - Epoch [210/300], Batch [26/43], Training Loss: 0.00001613
2024-11-06 14:07:28,348 - INFO - Epoch [210/300], Batch [27/43], Training Loss: 0.00000974
2024-11-06 14:07:28,351 - INFO - Epoch [210/300], Batch [28/43], Training Loss: 0.00000669
2024-11-06 14:07:28,355 - INFO - Epoch [210/300], Batch [29/43], Training Loss: 0.00000729
2024-11-06 14:07:28,359 - INFO - Epoch [210/300], Batch [30/43], Training Loss: 0.00000986
2024-11-06 14:07:28,363 - INFO - Epoch [210/300], Batch [31/43], Training Loss: 0.00001904
2024-11-06 14:07:28,366 - INFO - Epoch [210/300], Batch [32/43], Training Loss: 0.00002092
2024-11-06 14:07:28,370 - INFO - Epoch [210/300], Batch [33/43], Training Loss: 0.00001142
2024-11-06 14:07:28,373 - INFO - Epoch [210/300], Batch [34/43], Training Loss: 0.00001990
2024-11-06 14:07:28,376 - INFO - Epoch [210/300], Batch [35/43], Training Loss: 0.00000197
2024-11-06 14:07:28,380 - INFO - Epoch [210/300], Batch [36/43], Training Loss: 0.00000935
2024-11-06 14:07:28,383 - INFO - Epoch [210/300], Batch [37/43], Training Loss: 0.00001307
2024-11-06 14:07:28,387 - INFO - Epoch [210/300], Batch [38/43], Training Loss: 0.00000416
2024-11-06 14:07:28,391 - INFO - Epoch [210/300], Batch [39/43], Training Loss: 0.00000802
2024-11-06 14:07:28,396 - INFO - Epoch [210/300], Batch [40/43], Training Loss: 0.00001422
2024-11-06 14:07:28,400 - INFO - Epoch [210/300], Batch [41/43], Training Loss: 0.00001216
2024-11-06 14:07:28,404 - INFO - Epoch [210/300], Batch [42/43], Training Loss: 0.00000857
2024-11-06 14:07:28,408 - INFO - Epoch [210/300], Batch [43/43], Training Loss: 0.00001392
2024-11-06 14:07:28,420 - INFO - Epoch [210/300], Average Training Loss: 0.00001036, Validation Loss: 0.00001493
2024-11-06 14:07:28,424 - INFO - Epoch [211/300], Batch [1/43], Training Loss: 0.00003160
2024-11-06 14:07:28,428 - INFO - Epoch [211/300], Batch [2/43], Training Loss: 0.00000693
2024-11-06 14:07:28,432 - INFO - Epoch [211/300], Batch [3/43], Training Loss: 0.00001284
2024-11-06 14:07:28,436 - INFO - Epoch [211/300], Batch [4/43], Training Loss: 0.00001807
2024-11-06 14:07:28,439 - INFO - Epoch [211/300], Batch [5/43], Training Loss: 0.00000903
2024-11-06 14:07:28,442 - INFO - Epoch [211/300], Batch [6/43], Training Loss: 0.00001241
2024-11-06 14:07:28,446 - INFO - Epoch [211/300], Batch [7/43], Training Loss: 0.00001451
2024-11-06 14:07:28,449 - INFO - Epoch [211/300], Batch [8/43], Training Loss: 0.00000379
2024-11-06 14:07:28,453 - INFO - Epoch [211/300], Batch [9/43], Training Loss: 0.00000419
2024-11-06 14:07:28,457 - INFO - Epoch [211/300], Batch [10/43], Training Loss: 0.00000652
2024-11-06 14:07:28,462 - INFO - Epoch [211/300], Batch [11/43], Training Loss: 0.00000684
2024-11-06 14:07:28,467 - INFO - Epoch [211/300], Batch [12/43], Training Loss: 0.00002482
2024-11-06 14:07:28,472 - INFO - Epoch [211/300], Batch [13/43], Training Loss: 0.00001633
2024-11-06 14:07:28,477 - INFO - Epoch [211/300], Batch [14/43], Training Loss: 0.00001007
2024-11-06 14:07:28,481 - INFO - Epoch [211/300], Batch [15/43], Training Loss: 0.00000702
2024-11-06 14:07:28,485 - INFO - Epoch [211/300], Batch [16/43], Training Loss: 0.00001127
2024-11-06 14:07:28,488 - INFO - Epoch [211/300], Batch [17/43], Training Loss: 0.00001076
2024-11-06 14:07:28,493 - INFO - Epoch [211/300], Batch [18/43], Training Loss: 0.00002016
2024-11-06 14:07:28,497 - INFO - Epoch [211/300], Batch [19/43], Training Loss: 0.00000685
2024-11-06 14:07:28,501 - INFO - Epoch [211/300], Batch [20/43], Training Loss: 0.00000263
2024-11-06 14:07:28,506 - INFO - Epoch [211/300], Batch [21/43], Training Loss: 0.00000998
2024-11-06 14:07:28,510 - INFO - Epoch [211/300], Batch [22/43], Training Loss: 0.00001902
2024-11-06 14:07:28,514 - INFO - Epoch [211/300], Batch [23/43], Training Loss: 0.00000471
2024-11-06 14:07:28,518 - INFO - Epoch [211/300], Batch [24/43], Training Loss: 0.00000326
2024-11-06 14:07:28,522 - INFO - Epoch [211/300], Batch [25/43], Training Loss: 0.00000782
2024-11-06 14:07:28,525 - INFO - Epoch [211/300], Batch [26/43], Training Loss: 0.00000587
2024-11-06 14:07:28,529 - INFO - Epoch [211/300], Batch [27/43], Training Loss: 0.00001996
2024-11-06 14:07:28,532 - INFO - Epoch [211/300], Batch [28/43], Training Loss: 0.00000600
2024-11-06 14:07:28,536 - INFO - Epoch [211/300], Batch [29/43], Training Loss: 0.00000710
2024-11-06 14:07:28,539 - INFO - Epoch [211/300], Batch [30/43], Training Loss: 0.00000745
2024-11-06 14:07:28,542 - INFO - Epoch [211/300], Batch [31/43], Training Loss: 0.00001349
2024-11-06 14:07:28,546 - INFO - Epoch [211/300], Batch [32/43], Training Loss: 0.00001245
2024-11-06 14:07:28,550 - INFO - Epoch [211/300], Batch [33/43], Training Loss: 0.00001452
2024-11-06 14:07:28,553 - INFO - Epoch [211/300], Batch [34/43], Training Loss: 0.00000478
2024-11-06 14:07:28,557 - INFO - Epoch [211/300], Batch [35/43], Training Loss: 0.00001108
2024-11-06 14:07:28,561 - INFO - Epoch [211/300], Batch [36/43], Training Loss: 0.00000910
2024-11-06 14:07:28,565 - INFO - Epoch [211/300], Batch [37/43], Training Loss: 0.00000866
2024-11-06 14:07:28,568 - INFO - Epoch [211/300], Batch [38/43], Training Loss: 0.00000396
2024-11-06 14:07:28,572 - INFO - Epoch [211/300], Batch [39/43], Training Loss: 0.00001017
2024-11-06 14:07:28,576 - INFO - Epoch [211/300], Batch [40/43], Training Loss: 0.00001381
2024-11-06 14:07:28,580 - INFO - Epoch [211/300], Batch [41/43], Training Loss: 0.00001533
2024-11-06 14:07:28,584 - INFO - Epoch [211/300], Batch [42/43], Training Loss: 0.00000941
2024-11-06 14:07:28,588 - INFO - Epoch [211/300], Batch [43/43], Training Loss: 0.00001841
2024-11-06 14:07:28,600 - INFO - Epoch [211/300], Average Training Loss: 0.00001100, Validation Loss: 0.00001275
2024-11-06 14:07:28,604 - INFO - Epoch [212/300], Batch [1/43], Training Loss: 0.00000544
2024-11-06 14:07:28,607 - INFO - Epoch [212/300], Batch [2/43], Training Loss: 0.00002072
2024-11-06 14:07:28,612 - INFO - Epoch [212/300], Batch [3/43], Training Loss: 0.00000677
2024-11-06 14:07:28,616 - INFO - Epoch [212/300], Batch [4/43], Training Loss: 0.00001090
2024-11-06 14:07:28,620 - INFO - Epoch [212/300], Batch [5/43], Training Loss: 0.00000691
2024-11-06 14:07:28,623 - INFO - Epoch [212/300], Batch [6/43], Training Loss: 0.00000485
2024-11-06 14:07:28,627 - INFO - Epoch [212/300], Batch [7/43], Training Loss: 0.00000954
2024-11-06 14:07:28,631 - INFO - Epoch [212/300], Batch [8/43], Training Loss: 0.00000580
2024-11-06 14:07:28,635 - INFO - Epoch [212/300], Batch [9/43], Training Loss: 0.00000347
2024-11-06 14:07:28,639 - INFO - Epoch [212/300], Batch [10/43], Training Loss: 0.00000790
2024-11-06 14:07:28,642 - INFO - Epoch [212/300], Batch [11/43], Training Loss: 0.00000616
2024-11-06 14:07:28,646 - INFO - Epoch [212/300], Batch [12/43], Training Loss: 0.00000922
2024-11-06 14:07:28,650 - INFO - Epoch [212/300], Batch [13/43], Training Loss: 0.00000629
2024-11-06 14:07:28,654 - INFO - Epoch [212/300], Batch [14/43], Training Loss: 0.00001285
2024-11-06 14:07:28,657 - INFO - Epoch [212/300], Batch [15/43], Training Loss: 0.00000309
2024-11-06 14:07:28,660 - INFO - Epoch [212/300], Batch [16/43], Training Loss: 0.00001086
2024-11-06 14:07:28,663 - INFO - Epoch [212/300], Batch [17/43], Training Loss: 0.00001062
2024-11-06 14:07:28,667 - INFO - Epoch [212/300], Batch [18/43], Training Loss: 0.00000366
2024-11-06 14:07:28,671 - INFO - Epoch [212/300], Batch [19/43], Training Loss: 0.00000716
2024-11-06 14:07:28,674 - INFO - Epoch [212/300], Batch [20/43], Training Loss: 0.00000609
2024-11-06 14:07:28,676 - INFO - Epoch [212/300], Batch [21/43], Training Loss: 0.00000993
2024-11-06 14:07:28,679 - INFO - Epoch [212/300], Batch [22/43], Training Loss: 0.00001020
2024-11-06 14:07:28,682 - INFO - Epoch [212/300], Batch [23/43], Training Loss: 0.00000521
2024-11-06 14:07:28,685 - INFO - Epoch [212/300], Batch [24/43], Training Loss: 0.00000609
2024-11-06 14:07:28,688 - INFO - Epoch [212/300], Batch [25/43], Training Loss: 0.00000353
2024-11-06 14:07:28,691 - INFO - Epoch [212/300], Batch [26/43], Training Loss: 0.00001100
2024-11-06 14:07:28,694 - INFO - Epoch [212/300], Batch [27/43], Training Loss: 0.00000628
2024-11-06 14:07:28,697 - INFO - Epoch [212/300], Batch [28/43], Training Loss: 0.00000746
2024-11-06 14:07:28,700 - INFO - Epoch [212/300], Batch [29/43], Training Loss: 0.00000474
2024-11-06 14:07:28,704 - INFO - Epoch [212/300], Batch [30/43], Training Loss: 0.00000617
2024-11-06 14:07:28,707 - INFO - Epoch [212/300], Batch [31/43], Training Loss: 0.00001649
2024-11-06 14:07:28,711 - INFO - Epoch [212/300], Batch [32/43], Training Loss: 0.00001782
2024-11-06 14:07:28,714 - INFO - Epoch [212/300], Batch [33/43], Training Loss: 0.00001312
2024-11-06 14:07:28,718 - INFO - Epoch [212/300], Batch [34/43], Training Loss: 0.00001289
2024-11-06 14:07:28,721 - INFO - Epoch [212/300], Batch [35/43], Training Loss: 0.00000825
2024-11-06 14:07:28,724 - INFO - Epoch [212/300], Batch [36/43], Training Loss: 0.00001969
2024-11-06 14:07:28,727 - INFO - Epoch [212/300], Batch [37/43], Training Loss: 0.00003089
2024-11-06 14:07:28,730 - INFO - Epoch [212/300], Batch [38/43], Training Loss: 0.00001757
2024-11-06 14:07:28,734 - INFO - Epoch [212/300], Batch [39/43], Training Loss: 0.00000676
2024-11-06 14:07:28,737 - INFO - Epoch [212/300], Batch [40/43], Training Loss: 0.00000726
2024-11-06 14:07:28,741 - INFO - Epoch [212/300], Batch [41/43], Training Loss: 0.00001893
2024-11-06 14:07:28,744 - INFO - Epoch [212/300], Batch [42/43], Training Loss: 0.00000792
2024-11-06 14:07:28,748 - INFO - Epoch [212/300], Batch [43/43], Training Loss: 0.00000614
2024-11-06 14:07:28,762 - INFO - Epoch [212/300], Average Training Loss: 0.00000960, Validation Loss: 0.00001385
2024-11-06 14:07:28,766 - INFO - Epoch [213/300], Batch [1/43], Training Loss: 0.00000567
2024-11-06 14:07:28,770 - INFO - Epoch [213/300], Batch [2/43], Training Loss: 0.00002014
2024-11-06 14:07:28,773 - INFO - Epoch [213/300], Batch [3/43], Training Loss: 0.00001304
2024-11-06 14:07:28,777 - INFO - Epoch [213/300], Batch [4/43], Training Loss: 0.00001881
2024-11-06 14:07:28,780 - INFO - Epoch [213/300], Batch [5/43], Training Loss: 0.00000539
2024-11-06 14:07:28,784 - INFO - Epoch [213/300], Batch [6/43], Training Loss: 0.00001358
2024-11-06 14:07:28,787 - INFO - Epoch [213/300], Batch [7/43], Training Loss: 0.00001275
2024-11-06 14:07:28,790 - INFO - Epoch [213/300], Batch [8/43], Training Loss: 0.00000856
2024-11-06 14:07:28,794 - INFO - Epoch [213/300], Batch [9/43], Training Loss: 0.00000747
2024-11-06 14:07:28,797 - INFO - Epoch [213/300], Batch [10/43], Training Loss: 0.00000920
2024-11-06 14:07:28,800 - INFO - Epoch [213/300], Batch [11/43], Training Loss: 0.00004530
2024-11-06 14:07:28,803 - INFO - Epoch [213/300], Batch [12/43], Training Loss: 0.00002453
2024-11-06 14:07:28,806 - INFO - Epoch [213/300], Batch [13/43], Training Loss: 0.00000702
2024-11-06 14:07:28,810 - INFO - Epoch [213/300], Batch [14/43], Training Loss: 0.00000154
2024-11-06 14:07:28,813 - INFO - Epoch [213/300], Batch [15/43], Training Loss: 0.00000900
2024-11-06 14:07:28,816 - INFO - Epoch [213/300], Batch [16/43], Training Loss: 0.00000880
2024-11-06 14:07:28,819 - INFO - Epoch [213/300], Batch [17/43], Training Loss: 0.00000628
2024-11-06 14:07:28,821 - INFO - Epoch [213/300], Batch [18/43], Training Loss: 0.00001562
2024-11-06 14:07:28,825 - INFO - Epoch [213/300], Batch [19/43], Training Loss: 0.00000465
2024-11-06 14:07:28,828 - INFO - Epoch [213/300], Batch [20/43], Training Loss: 0.00000738
2024-11-06 14:07:28,831 - INFO - Epoch [213/300], Batch [21/43], Training Loss: 0.00001650
2024-11-06 14:07:28,834 - INFO - Epoch [213/300], Batch [22/43], Training Loss: 0.00000797
2024-11-06 14:07:28,837 - INFO - Epoch [213/300], Batch [23/43], Training Loss: 0.00000272
2024-11-06 14:07:28,841 - INFO - Epoch [213/300], Batch [24/43], Training Loss: 0.00000593
2024-11-06 14:07:28,844 - INFO - Epoch [213/300], Batch [25/43], Training Loss: 0.00001260
2024-11-06 14:07:28,847 - INFO - Epoch [213/300], Batch [26/43], Training Loss: 0.00002364
2024-11-06 14:07:28,850 - INFO - Epoch [213/300], Batch [27/43], Training Loss: 0.00002075
2024-11-06 14:07:28,852 - INFO - Epoch [213/300], Batch [28/43], Training Loss: 0.00001308
2024-11-06 14:07:28,855 - INFO - Epoch [213/300], Batch [29/43], Training Loss: 0.00000694
2024-11-06 14:07:28,859 - INFO - Epoch [213/300], Batch [30/43], Training Loss: 0.00000988
2024-11-06 14:07:28,862 - INFO - Epoch [213/300], Batch [31/43], Training Loss: 0.00001745
2024-11-06 14:07:28,865 - INFO - Epoch [213/300], Batch [32/43], Training Loss: 0.00000545
2024-11-06 14:07:28,868 - INFO - Epoch [213/300], Batch [33/43], Training Loss: 0.00000614
2024-11-06 14:07:28,872 - INFO - Epoch [213/300], Batch [34/43], Training Loss: 0.00001019
2024-11-06 14:07:28,875 - INFO - Epoch [213/300], Batch [35/43], Training Loss: 0.00000243
2024-11-06 14:07:28,878 - INFO - Epoch [213/300], Batch [36/43], Training Loss: 0.00000373
2024-11-06 14:07:28,882 - INFO - Epoch [213/300], Batch [37/43], Training Loss: 0.00000266
2024-11-06 14:07:28,887 - INFO - Epoch [213/300], Batch [38/43], Training Loss: 0.00000524
2024-11-06 14:07:28,891 - INFO - Epoch [213/300], Batch [39/43], Training Loss: 0.00001387
2024-11-06 14:07:28,894 - INFO - Epoch [213/300], Batch [40/43], Training Loss: 0.00001359
2024-11-06 14:07:28,898 - INFO - Epoch [213/300], Batch [41/43], Training Loss: 0.00001018
2024-11-06 14:07:28,902 - INFO - Epoch [213/300], Batch [42/43], Training Loss: 0.00000486
2024-11-06 14:07:28,907 - INFO - Epoch [213/300], Batch [43/43], Training Loss: 0.00001827
2024-11-06 14:07:28,919 - INFO - Epoch [213/300], Average Training Loss: 0.00001113, Validation Loss: 0.00001596
2024-11-06 14:07:28,924 - INFO - Epoch [214/300], Batch [1/43], Training Loss: 0.00002392
2024-11-06 14:07:28,929 - INFO - Epoch [214/300], Batch [2/43], Training Loss: 0.00001302
2024-11-06 14:07:28,932 - INFO - Epoch [214/300], Batch [3/43], Training Loss: 0.00001867
2024-11-06 14:07:28,936 - INFO - Epoch [214/300], Batch [4/43], Training Loss: 0.00000825
2024-11-06 14:07:28,940 - INFO - Epoch [214/300], Batch [5/43], Training Loss: 0.00000304
2024-11-06 14:07:28,943 - INFO - Epoch [214/300], Batch [6/43], Training Loss: 0.00001162
2024-11-06 14:07:28,947 - INFO - Epoch [214/300], Batch [7/43], Training Loss: 0.00002178
2024-11-06 14:07:28,950 - INFO - Epoch [214/300], Batch [8/43], Training Loss: 0.00000757
2024-11-06 14:07:28,954 - INFO - Epoch [214/300], Batch [9/43], Training Loss: 0.00001685
2024-11-06 14:07:28,958 - INFO - Epoch [214/300], Batch [10/43], Training Loss: 0.00003510
2024-11-06 14:07:28,963 - INFO - Epoch [214/300], Batch [11/43], Training Loss: 0.00000630
2024-11-06 14:07:28,968 - INFO - Epoch [214/300], Batch [12/43], Training Loss: 0.00001507
2024-11-06 14:07:28,971 - INFO - Epoch [214/300], Batch [13/43], Training Loss: 0.00000759
2024-11-06 14:07:28,975 - INFO - Epoch [214/300], Batch [14/43], Training Loss: 0.00001734
2024-11-06 14:07:28,978 - INFO - Epoch [214/300], Batch [15/43], Training Loss: 0.00000633
2024-11-06 14:07:28,982 - INFO - Epoch [214/300], Batch [16/43], Training Loss: 0.00000772
2024-11-06 14:07:28,985 - INFO - Epoch [214/300], Batch [17/43], Training Loss: 0.00001859
2024-11-06 14:07:28,989 - INFO - Epoch [214/300], Batch [18/43], Training Loss: 0.00000845
2024-11-06 14:07:28,992 - INFO - Epoch [214/300], Batch [19/43], Training Loss: 0.00000508
2024-11-06 14:07:28,996 - INFO - Epoch [214/300], Batch [20/43], Training Loss: 0.00000986
2024-11-06 14:07:29,001 - INFO - Epoch [214/300], Batch [21/43], Training Loss: 0.00000455
2024-11-06 14:07:29,005 - INFO - Epoch [214/300], Batch [22/43], Training Loss: 0.00001086
2024-11-06 14:07:29,009 - INFO - Epoch [214/300], Batch [23/43], Training Loss: 0.00000510
2024-11-06 14:07:29,012 - INFO - Epoch [214/300], Batch [24/43], Training Loss: 0.00002966
2024-11-06 14:07:29,015 - INFO - Epoch [214/300], Batch [25/43], Training Loss: 0.00000579
2024-11-06 14:07:29,018 - INFO - Epoch [214/300], Batch [26/43], Training Loss: 0.00000264
2024-11-06 14:07:29,021 - INFO - Epoch [214/300], Batch [27/43], Training Loss: 0.00000563
2024-11-06 14:07:29,025 - INFO - Epoch [214/300], Batch [28/43], Training Loss: 0.00001326
2024-11-06 14:07:29,029 - INFO - Epoch [214/300], Batch [29/43], Training Loss: 0.00001020
2024-11-06 14:07:29,034 - INFO - Epoch [214/300], Batch [30/43], Training Loss: 0.00000651
2024-11-06 14:07:29,038 - INFO - Epoch [214/300], Batch [31/43], Training Loss: 0.00000340
2024-11-06 14:07:29,042 - INFO - Epoch [214/300], Batch [32/43], Training Loss: 0.00000852
2024-11-06 14:07:29,047 - INFO - Epoch [214/300], Batch [33/43], Training Loss: 0.00000874
2024-11-06 14:07:29,052 - INFO - Epoch [214/300], Batch [34/43], Training Loss: 0.00001761
2024-11-06 14:07:29,056 - INFO - Epoch [214/300], Batch [35/43], Training Loss: 0.00000887
2024-11-06 14:07:29,060 - INFO - Epoch [214/300], Batch [36/43], Training Loss: 0.00002065
2024-11-06 14:07:29,064 - INFO - Epoch [214/300], Batch [37/43], Training Loss: 0.00001599
2024-11-06 14:07:29,068 - INFO - Epoch [214/300], Batch [38/43], Training Loss: 0.00000673
2024-11-06 14:07:29,073 - INFO - Epoch [214/300], Batch [39/43], Training Loss: 0.00002160
2024-11-06 14:07:29,076 - INFO - Epoch [214/300], Batch [40/43], Training Loss: 0.00004169
2024-11-06 14:07:29,080 - INFO - Epoch [214/300], Batch [41/43], Training Loss: 0.00000725
2024-11-06 14:07:29,085 - INFO - Epoch [214/300], Batch [42/43], Training Loss: 0.00002789
2024-11-06 14:07:29,089 - INFO - Epoch [214/300], Batch [43/43], Training Loss: 0.00002898
2024-11-06 14:07:29,101 - INFO - Epoch [214/300], Average Training Loss: 0.00001336, Validation Loss: 0.00001946
2024-11-06 14:07:29,105 - INFO - Epoch [215/300], Batch [1/43], Training Loss: 0.00000935
2024-11-06 14:07:29,109 - INFO - Epoch [215/300], Batch [2/43], Training Loss: 0.00002322
2024-11-06 14:07:29,113 - INFO - Epoch [215/300], Batch [3/43], Training Loss: 0.00001670
2024-11-06 14:07:29,117 - INFO - Epoch [215/300], Batch [4/43], Training Loss: 0.00001802
2024-11-06 14:07:29,121 - INFO - Epoch [215/300], Batch [5/43], Training Loss: 0.00000909
2024-11-06 14:07:29,125 - INFO - Epoch [215/300], Batch [6/43], Training Loss: 0.00000802
2024-11-06 14:07:29,129 - INFO - Epoch [215/300], Batch [7/43], Training Loss: 0.00000967
2024-11-06 14:07:29,133 - INFO - Epoch [215/300], Batch [8/43], Training Loss: 0.00001910
2024-11-06 14:07:29,137 - INFO - Epoch [215/300], Batch [9/43], Training Loss: 0.00000809
2024-11-06 14:07:29,142 - INFO - Epoch [215/300], Batch [10/43], Training Loss: 0.00000732
2024-11-06 14:07:29,145 - INFO - Epoch [215/300], Batch [11/43], Training Loss: 0.00002034
2024-11-06 14:07:29,148 - INFO - Epoch [215/300], Batch [12/43], Training Loss: 0.00001491
2024-11-06 14:07:29,152 - INFO - Epoch [215/300], Batch [13/43], Training Loss: 0.00000738
2024-11-06 14:07:29,155 - INFO - Epoch [215/300], Batch [14/43], Training Loss: 0.00001274
2024-11-06 14:07:29,159 - INFO - Epoch [215/300], Batch [15/43], Training Loss: 0.00002205
2024-11-06 14:07:29,162 - INFO - Epoch [215/300], Batch [16/43], Training Loss: 0.00000561
2024-11-06 14:07:29,166 - INFO - Epoch [215/300], Batch [17/43], Training Loss: 0.00000450
2024-11-06 14:07:29,169 - INFO - Epoch [215/300], Batch [18/43], Training Loss: 0.00001731
2024-11-06 14:07:29,173 - INFO - Epoch [215/300], Batch [19/43], Training Loss: 0.00001947
2024-11-06 14:07:29,176 - INFO - Epoch [215/300], Batch [20/43], Training Loss: 0.00000353
2024-11-06 14:07:29,180 - INFO - Epoch [215/300], Batch [21/43], Training Loss: 0.00002367
2024-11-06 14:07:29,183 - INFO - Epoch [215/300], Batch [22/43], Training Loss: 0.00001185
2024-11-06 14:07:29,188 - INFO - Epoch [215/300], Batch [23/43], Training Loss: 0.00000936
2024-11-06 14:07:29,192 - INFO - Epoch [215/300], Batch [24/43], Training Loss: 0.00001312
2024-11-06 14:07:29,197 - INFO - Epoch [215/300], Batch [25/43], Training Loss: 0.00001677
2024-11-06 14:07:29,202 - INFO - Epoch [215/300], Batch [26/43], Training Loss: 0.00000275
2024-11-06 14:07:29,206 - INFO - Epoch [215/300], Batch [27/43], Training Loss: 0.00001105
2024-11-06 14:07:29,211 - INFO - Epoch [215/300], Batch [28/43], Training Loss: 0.00002211
2024-11-06 14:07:29,214 - INFO - Epoch [215/300], Batch [29/43], Training Loss: 0.00001280
2024-11-06 14:07:29,218 - INFO - Epoch [215/300], Batch [30/43], Training Loss: 0.00001708
2024-11-06 14:07:29,222 - INFO - Epoch [215/300], Batch [31/43], Training Loss: 0.00000884
2024-11-06 14:07:29,226 - INFO - Epoch [215/300], Batch [32/43], Training Loss: 0.00001350
2024-11-06 14:07:29,230 - INFO - Epoch [215/300], Batch [33/43], Training Loss: 0.00000640
2024-11-06 14:07:29,234 - INFO - Epoch [215/300], Batch [34/43], Training Loss: 0.00000684
2024-11-06 14:07:29,239 - INFO - Epoch [215/300], Batch [35/43], Training Loss: 0.00001562
2024-11-06 14:07:29,243 - INFO - Epoch [215/300], Batch [36/43], Training Loss: 0.00001814
2024-11-06 14:07:29,246 - INFO - Epoch [215/300], Batch [37/43], Training Loss: 0.00000683
2024-11-06 14:07:29,249 - INFO - Epoch [215/300], Batch [38/43], Training Loss: 0.00001392
2024-11-06 14:07:29,252 - INFO - Epoch [215/300], Batch [39/43], Training Loss: 0.00000798
2024-11-06 14:07:29,256 - INFO - Epoch [215/300], Batch [40/43], Training Loss: 0.00000801
2024-11-06 14:07:29,258 - INFO - Epoch [215/300], Batch [41/43], Training Loss: 0.00000959
2024-11-06 14:07:29,263 - INFO - Epoch [215/300], Batch [42/43], Training Loss: 0.00000574
2024-11-06 14:07:29,268 - INFO - Epoch [215/300], Batch [43/43], Training Loss: 0.00001453
2024-11-06 14:07:29,279 - INFO - Epoch [215/300], Average Training Loss: 0.00001239, Validation Loss: 0.00001384
2024-11-06 14:07:29,282 - INFO - Epoch [216/300], Batch [1/43], Training Loss: 0.00000490
2024-11-06 14:07:29,286 - INFO - Epoch [216/300], Batch [2/43], Training Loss: 0.00000788
2024-11-06 14:07:29,290 - INFO - Epoch [216/300], Batch [3/43], Training Loss: 0.00000886
2024-11-06 14:07:29,293 - INFO - Epoch [216/300], Batch [4/43], Training Loss: 0.00001056
2024-11-06 14:07:29,297 - INFO - Epoch [216/300], Batch [5/43], Training Loss: 0.00000867
2024-11-06 14:07:29,301 - INFO - Epoch [216/300], Batch [6/43], Training Loss: 0.00000198
2024-11-06 14:07:29,305 - INFO - Epoch [216/300], Batch [7/43], Training Loss: 0.00000709
2024-11-06 14:07:29,309 - INFO - Epoch [216/300], Batch [8/43], Training Loss: 0.00000735
2024-11-06 14:07:29,313 - INFO - Epoch [216/300], Batch [9/43], Training Loss: 0.00000313
2024-11-06 14:07:29,317 - INFO - Epoch [216/300], Batch [10/43], Training Loss: 0.00001654
2024-11-06 14:07:29,321 - INFO - Epoch [216/300], Batch [11/43], Training Loss: 0.00000804
2024-11-06 14:07:29,325 - INFO - Epoch [216/300], Batch [12/43], Training Loss: 0.00002294
2024-11-06 14:07:29,330 - INFO - Epoch [216/300], Batch [13/43], Training Loss: 0.00000531
2024-11-06 14:07:29,335 - INFO - Epoch [216/300], Batch [14/43], Training Loss: 0.00000689
2024-11-06 14:07:29,339 - INFO - Epoch [216/300], Batch [15/43], Training Loss: 0.00000794
2024-11-06 14:07:29,343 - INFO - Epoch [216/300], Batch [16/43], Training Loss: 0.00000294
2024-11-06 14:07:29,347 - INFO - Epoch [216/300], Batch [17/43], Training Loss: 0.00001117
2024-11-06 14:07:29,351 - INFO - Epoch [216/300], Batch [18/43], Training Loss: 0.00000523
2024-11-06 14:07:29,357 - INFO - Epoch [216/300], Batch [19/43], Training Loss: 0.00001001
2024-11-06 14:07:29,361 - INFO - Epoch [216/300], Batch [20/43], Training Loss: 0.00000836
2024-11-06 14:07:29,365 - INFO - Epoch [216/300], Batch [21/43], Training Loss: 0.00000455
2024-11-06 14:07:29,369 - INFO - Epoch [216/300], Batch [22/43], Training Loss: 0.00000926
2024-11-06 14:07:29,373 - INFO - Epoch [216/300], Batch [23/43], Training Loss: 0.00000393
2024-11-06 14:07:29,377 - INFO - Epoch [216/300], Batch [24/43], Training Loss: 0.00000796
2024-11-06 14:07:29,381 - INFO - Epoch [216/300], Batch [25/43], Training Loss: 0.00001505
2024-11-06 14:07:29,384 - INFO - Epoch [216/300], Batch [26/43], Training Loss: 0.00002105
2024-11-06 14:07:29,388 - INFO - Epoch [216/300], Batch [27/43], Training Loss: 0.00003066
2024-11-06 14:07:29,392 - INFO - Epoch [216/300], Batch [28/43], Training Loss: 0.00002190
2024-11-06 14:07:29,396 - INFO - Epoch [216/300], Batch [29/43], Training Loss: 0.00001111
2024-11-06 14:07:29,400 - INFO - Epoch [216/300], Batch [30/43], Training Loss: 0.00003089
2024-11-06 14:07:29,403 - INFO - Epoch [216/300], Batch [31/43], Training Loss: 0.00001246
2024-11-06 14:07:29,407 - INFO - Epoch [216/300], Batch [32/43], Training Loss: 0.00002670
2024-11-06 14:07:29,410 - INFO - Epoch [216/300], Batch [33/43], Training Loss: 0.00001162
2024-11-06 14:07:29,414 - INFO - Epoch [216/300], Batch [34/43], Training Loss: 0.00001192
2024-11-06 14:07:29,418 - INFO - Epoch [216/300], Batch [35/43], Training Loss: 0.00000941
2024-11-06 14:07:29,422 - INFO - Epoch [216/300], Batch [36/43], Training Loss: 0.00001120
2024-11-06 14:07:29,427 - INFO - Epoch [216/300], Batch [37/43], Training Loss: 0.00000807
2024-11-06 14:07:29,431 - INFO - Epoch [216/300], Batch [38/43], Training Loss: 0.00001684
2024-11-06 14:07:29,435 - INFO - Epoch [216/300], Batch [39/43], Training Loss: 0.00000978
2024-11-06 14:07:29,439 - INFO - Epoch [216/300], Batch [40/43], Training Loss: 0.00000592
2024-11-06 14:07:29,442 - INFO - Epoch [216/300], Batch [41/43], Training Loss: 0.00002586
2024-11-06 14:07:29,447 - INFO - Epoch [216/300], Batch [42/43], Training Loss: 0.00001596
2024-11-06 14:07:29,452 - INFO - Epoch [216/300], Batch [43/43], Training Loss: 0.00000961
2024-11-06 14:07:29,463 - INFO - Epoch [216/300], Average Training Loss: 0.00001157, Validation Loss: 0.00001237
2024-11-06 14:07:29,469 - INFO - Epoch [217/300], Batch [1/43], Training Loss: 0.00001584
2024-11-06 14:07:29,473 - INFO - Epoch [217/300], Batch [2/43], Training Loss: 0.00000559
2024-11-06 14:07:29,478 - INFO - Epoch [217/300], Batch [3/43], Training Loss: 0.00000787
2024-11-06 14:07:29,482 - INFO - Epoch [217/300], Batch [4/43], Training Loss: 0.00000332
2024-11-06 14:07:29,486 - INFO - Epoch [217/300], Batch [5/43], Training Loss: 0.00000597
2024-11-06 14:07:29,491 - INFO - Epoch [217/300], Batch [6/43], Training Loss: 0.00001062
2024-11-06 14:07:29,496 - INFO - Epoch [217/300], Batch [7/43], Training Loss: 0.00000341
2024-11-06 14:07:29,500 - INFO - Epoch [217/300], Batch [8/43], Training Loss: 0.00001548
2024-11-06 14:07:29,504 - INFO - Epoch [217/300], Batch [9/43], Training Loss: 0.00000627
2024-11-06 14:07:29,508 - INFO - Epoch [217/300], Batch [10/43], Training Loss: 0.00001529
2024-11-06 14:07:29,513 - INFO - Epoch [217/300], Batch [11/43], Training Loss: 0.00002832
2024-11-06 14:07:29,517 - INFO - Epoch [217/300], Batch [12/43], Training Loss: 0.00000431
2024-11-06 14:07:29,521 - INFO - Epoch [217/300], Batch [13/43], Training Loss: 0.00000471
2024-11-06 14:07:29,524 - INFO - Epoch [217/300], Batch [14/43], Training Loss: 0.00001158
2024-11-06 14:07:29,529 - INFO - Epoch [217/300], Batch [15/43], Training Loss: 0.00001963
2024-11-06 14:07:29,533 - INFO - Epoch [217/300], Batch [16/43], Training Loss: 0.00000275
2024-11-06 14:07:29,537 - INFO - Epoch [217/300], Batch [17/43], Training Loss: 0.00000853
2024-11-06 14:07:29,541 - INFO - Epoch [217/300], Batch [18/43], Training Loss: 0.00001909
2024-11-06 14:07:29,547 - INFO - Epoch [217/300], Batch [19/43], Training Loss: 0.00001193
2024-11-06 14:07:29,552 - INFO - Epoch [217/300], Batch [20/43], Training Loss: 0.00000704
2024-11-06 14:07:29,557 - INFO - Epoch [217/300], Batch [21/43], Training Loss: 0.00001615
2024-11-06 14:07:29,561 - INFO - Epoch [217/300], Batch [22/43], Training Loss: 0.00000517
2024-11-06 14:07:29,565 - INFO - Epoch [217/300], Batch [23/43], Training Loss: 0.00002389
2024-11-06 14:07:29,569 - INFO - Epoch [217/300], Batch [24/43], Training Loss: 0.00002854
2024-11-06 14:07:29,573 - INFO - Epoch [217/300], Batch [25/43], Training Loss: 0.00000542
2024-11-06 14:07:29,577 - INFO - Epoch [217/300], Batch [26/43], Training Loss: 0.00000581
2024-11-06 14:07:29,580 - INFO - Epoch [217/300], Batch [27/43], Training Loss: 0.00000646
2024-11-06 14:07:29,583 - INFO - Epoch [217/300], Batch [28/43], Training Loss: 0.00000797
2024-11-06 14:07:29,588 - INFO - Epoch [217/300], Batch [29/43], Training Loss: 0.00000719
2024-11-06 14:07:29,593 - INFO - Epoch [217/300], Batch [30/43], Training Loss: 0.00001296
2024-11-06 14:07:29,597 - INFO - Epoch [217/300], Batch [31/43], Training Loss: 0.00001149
2024-11-06 14:07:29,602 - INFO - Epoch [217/300], Batch [32/43], Training Loss: 0.00002670
2024-11-06 14:07:29,607 - INFO - Epoch [217/300], Batch [33/43], Training Loss: 0.00000855
2024-11-06 14:07:29,612 - INFO - Epoch [217/300], Batch [34/43], Training Loss: 0.00000505
2024-11-06 14:07:29,617 - INFO - Epoch [217/300], Batch [35/43], Training Loss: 0.00001351
2024-11-06 14:07:29,621 - INFO - Epoch [217/300], Batch [36/43], Training Loss: 0.00001919
2024-11-06 14:07:29,628 - INFO - Epoch [217/300], Batch [37/43], Training Loss: 0.00000442
2024-11-06 14:07:29,633 - INFO - Epoch [217/300], Batch [38/43], Training Loss: 0.00000662
2024-11-06 14:07:29,640 - INFO - Epoch [217/300], Batch [39/43], Training Loss: 0.00000746
2024-11-06 14:07:29,644 - INFO - Epoch [217/300], Batch [40/43], Training Loss: 0.00000566
2024-11-06 14:07:29,650 - INFO - Epoch [217/300], Batch [41/43], Training Loss: 0.00000901
2024-11-06 14:07:29,655 - INFO - Epoch [217/300], Batch [42/43], Training Loss: 0.00000505
2024-11-06 14:07:29,661 - INFO - Epoch [217/300], Batch [43/43], Training Loss: 0.00000379
2024-11-06 14:07:29,678 - INFO - Epoch [217/300], Average Training Loss: 0.00001055, Validation Loss: 0.00001178
2024-11-06 14:07:29,683 - INFO - Epoch [218/300], Batch [1/43], Training Loss: 0.00000374
2024-11-06 14:07:29,689 - INFO - Epoch [218/300], Batch [2/43], Training Loss: 0.00000559
2024-11-06 14:07:29,693 - INFO - Epoch [218/300], Batch [3/43], Training Loss: 0.00001109
2024-11-06 14:07:29,698 - INFO - Epoch [218/300], Batch [4/43], Training Loss: 0.00001327
2024-11-06 14:07:29,704 - INFO - Epoch [218/300], Batch [5/43], Training Loss: 0.00000923
2024-11-06 14:07:29,709 - INFO - Epoch [218/300], Batch [6/43], Training Loss: 0.00000790
2024-11-06 14:07:29,713 - INFO - Epoch [218/300], Batch [7/43], Training Loss: 0.00002250
2024-11-06 14:07:29,718 - INFO - Epoch [218/300], Batch [8/43], Training Loss: 0.00001453
2024-11-06 14:07:29,722 - INFO - Epoch [218/300], Batch [9/43], Training Loss: 0.00000974
2024-11-06 14:07:29,726 - INFO - Epoch [218/300], Batch [10/43], Training Loss: 0.00001637
2024-11-06 14:07:29,731 - INFO - Epoch [218/300], Batch [11/43], Training Loss: 0.00001641
2024-11-06 14:07:29,738 - INFO - Epoch [218/300], Batch [12/43], Training Loss: 0.00000850
2024-11-06 14:07:29,743 - INFO - Epoch [218/300], Batch [13/43], Training Loss: 0.00001207
2024-11-06 14:07:29,748 - INFO - Epoch [218/300], Batch [14/43], Training Loss: 0.00001735
2024-11-06 14:07:29,752 - INFO - Epoch [218/300], Batch [15/43], Training Loss: 0.00001093
2024-11-06 14:07:29,756 - INFO - Epoch [218/300], Batch [16/43], Training Loss: 0.00000266
2024-11-06 14:07:29,761 - INFO - Epoch [218/300], Batch [17/43], Training Loss: 0.00001029
2024-11-06 14:07:29,765 - INFO - Epoch [218/300], Batch [18/43], Training Loss: 0.00001676
2024-11-06 14:07:29,769 - INFO - Epoch [218/300], Batch [19/43], Training Loss: 0.00000330
2024-11-06 14:07:29,774 - INFO - Epoch [218/300], Batch [20/43], Training Loss: 0.00001117
2024-11-06 14:07:29,778 - INFO - Epoch [218/300], Batch [21/43], Training Loss: 0.00001866
2024-11-06 14:07:29,782 - INFO - Epoch [218/300], Batch [22/43], Training Loss: 0.00001381
2024-11-06 14:07:29,787 - INFO - Epoch [218/300], Batch [23/43], Training Loss: 0.00001947
2024-11-06 14:07:29,791 - INFO - Epoch [218/300], Batch [24/43], Training Loss: 0.00001962
2024-11-06 14:07:29,795 - INFO - Epoch [218/300], Batch [25/43], Training Loss: 0.00002076
2024-11-06 14:07:29,800 - INFO - Epoch [218/300], Batch [26/43], Training Loss: 0.00001130
2024-11-06 14:07:29,806 - INFO - Epoch [218/300], Batch [27/43], Training Loss: 0.00001294
2024-11-06 14:07:29,810 - INFO - Epoch [218/300], Batch [28/43], Training Loss: 0.00003184
2024-11-06 14:07:29,814 - INFO - Epoch [218/300], Batch [29/43], Training Loss: 0.00002090
2024-11-06 14:07:29,818 - INFO - Epoch [218/300], Batch [30/43], Training Loss: 0.00001250
2024-11-06 14:07:29,821 - INFO - Epoch [218/300], Batch [31/43], Training Loss: 0.00001467
2024-11-06 14:07:29,826 - INFO - Epoch [218/300], Batch [32/43], Training Loss: 0.00002705
2024-11-06 14:07:29,830 - INFO - Epoch [218/300], Batch [33/43], Training Loss: 0.00000444
2024-11-06 14:07:29,835 - INFO - Epoch [218/300], Batch [34/43], Training Loss: 0.00000640
2024-11-06 14:07:29,839 - INFO - Epoch [218/300], Batch [35/43], Training Loss: 0.00000836
2024-11-06 14:07:29,843 - INFO - Epoch [218/300], Batch [36/43], Training Loss: 0.00001623
2024-11-06 14:07:29,847 - INFO - Epoch [218/300], Batch [37/43], Training Loss: 0.00000439
2024-11-06 14:07:29,851 - INFO - Epoch [218/300], Batch [38/43], Training Loss: 0.00000539
2024-11-06 14:07:29,854 - INFO - Epoch [218/300], Batch [39/43], Training Loss: 0.00000961
2024-11-06 14:07:29,858 - INFO - Epoch [218/300], Batch [40/43], Training Loss: 0.00000981
2024-11-06 14:07:29,862 - INFO - Epoch [218/300], Batch [41/43], Training Loss: 0.00000626
2024-11-06 14:07:29,867 - INFO - Epoch [218/300], Batch [42/43], Training Loss: 0.00002120
2024-11-06 14:07:29,871 - INFO - Epoch [218/300], Batch [43/43], Training Loss: 0.00001862
2024-11-06 14:07:29,884 - INFO - Epoch [218/300], Average Training Loss: 0.00001297, Validation Loss: 0.00001274
2024-11-06 14:07:29,889 - INFO - Epoch [219/300], Batch [1/43], Training Loss: 0.00001194
2024-11-06 14:07:29,893 - INFO - Epoch [219/300], Batch [2/43], Training Loss: 0.00001363
2024-11-06 14:07:29,897 - INFO - Epoch [219/300], Batch [3/43], Training Loss: 0.00001218
2024-11-06 14:07:29,902 - INFO - Epoch [219/300], Batch [4/43], Training Loss: 0.00000865
2024-11-06 14:07:29,905 - INFO - Epoch [219/300], Batch [5/43], Training Loss: 0.00001453
2024-11-06 14:07:29,909 - INFO - Epoch [219/300], Batch [6/43], Training Loss: 0.00000925
2024-11-06 14:07:29,914 - INFO - Epoch [219/300], Batch [7/43], Training Loss: 0.00002281
2024-11-06 14:07:29,919 - INFO - Epoch [219/300], Batch [8/43], Training Loss: 0.00000824
2024-11-06 14:07:29,922 - INFO - Epoch [219/300], Batch [9/43], Training Loss: 0.00000267
2024-11-06 14:07:29,926 - INFO - Epoch [219/300], Batch [10/43], Training Loss: 0.00000964
2024-11-06 14:07:29,930 - INFO - Epoch [219/300], Batch [11/43], Training Loss: 0.00000778
2024-11-06 14:07:29,934 - INFO - Epoch [219/300], Batch [12/43], Training Loss: 0.00001071
2024-11-06 14:07:29,938 - INFO - Epoch [219/300], Batch [13/43], Training Loss: 0.00001379
2024-11-06 14:07:29,941 - INFO - Epoch [219/300], Batch [14/43], Training Loss: 0.00000712
2024-11-06 14:07:29,944 - INFO - Epoch [219/300], Batch [15/43], Training Loss: 0.00000927
2024-11-06 14:07:29,949 - INFO - Epoch [219/300], Batch [16/43], Training Loss: 0.00000349
2024-11-06 14:07:29,953 - INFO - Epoch [219/300], Batch [17/43], Training Loss: 0.00001828
2024-11-06 14:07:29,956 - INFO - Epoch [219/300], Batch [18/43], Training Loss: 0.00000973
2024-11-06 14:07:29,960 - INFO - Epoch [219/300], Batch [19/43], Training Loss: 0.00001133
2024-11-06 14:07:29,963 - INFO - Epoch [219/300], Batch [20/43], Training Loss: 0.00000980
2024-11-06 14:07:29,967 - INFO - Epoch [219/300], Batch [21/43], Training Loss: 0.00000710
2024-11-06 14:07:29,970 - INFO - Epoch [219/300], Batch [22/43], Training Loss: 0.00000955
2024-11-06 14:07:29,973 - INFO - Epoch [219/300], Batch [23/43], Training Loss: 0.00000426
2024-11-06 14:07:29,977 - INFO - Epoch [219/300], Batch [24/43], Training Loss: 0.00000273
2024-11-06 14:07:29,981 - INFO - Epoch [219/300], Batch [25/43], Training Loss: 0.00000922
2024-11-06 14:07:29,984 - INFO - Epoch [219/300], Batch [26/43], Training Loss: 0.00001692
2024-11-06 14:07:29,988 - INFO - Epoch [219/300], Batch [27/43], Training Loss: 0.00000628
2024-11-06 14:07:29,992 - INFO - Epoch [219/300], Batch [28/43], Training Loss: 0.00000964
2024-11-06 14:07:29,995 - INFO - Epoch [219/300], Batch [29/43], Training Loss: 0.00001275
2024-11-06 14:07:30,000 - INFO - Epoch [219/300], Batch [30/43], Training Loss: 0.00000701
2024-11-06 14:07:30,004 - INFO - Epoch [219/300], Batch [31/43], Training Loss: 0.00001756
2024-11-06 14:07:30,008 - INFO - Epoch [219/300], Batch [32/43], Training Loss: 0.00000845
2024-11-06 14:07:30,012 - INFO - Epoch [219/300], Batch [33/43], Training Loss: 0.00001559
2024-11-06 14:07:30,016 - INFO - Epoch [219/300], Batch [34/43], Training Loss: 0.00000536
2024-11-06 14:07:30,020 - INFO - Epoch [219/300], Batch [35/43], Training Loss: 0.00000300
2024-11-06 14:07:30,024 - INFO - Epoch [219/300], Batch [36/43], Training Loss: 0.00000805
2024-11-06 14:07:30,027 - INFO - Epoch [219/300], Batch [37/43], Training Loss: 0.00001764
2024-11-06 14:07:30,032 - INFO - Epoch [219/300], Batch [38/43], Training Loss: 0.00002196
2024-11-06 14:07:30,037 - INFO - Epoch [219/300], Batch [39/43], Training Loss: 0.00000353
2024-11-06 14:07:30,041 - INFO - Epoch [219/300], Batch [40/43], Training Loss: 0.00001085
2024-11-06 14:07:30,044 - INFO - Epoch [219/300], Batch [41/43], Training Loss: 0.00000617
2024-11-06 14:07:30,048 - INFO - Epoch [219/300], Batch [42/43], Training Loss: 0.00000747
2024-11-06 14:07:30,052 - INFO - Epoch [219/300], Batch [43/43], Training Loss: 0.00000511
2024-11-06 14:07:30,064 - INFO - Epoch [219/300], Average Training Loss: 0.00001002, Validation Loss: 0.00001335
2024-11-06 14:07:30,068 - INFO - Epoch [220/300], Batch [1/43], Training Loss: 0.00000586
2024-11-06 14:07:30,072 - INFO - Epoch [220/300], Batch [2/43], Training Loss: 0.00001429
2024-11-06 14:07:30,076 - INFO - Epoch [220/300], Batch [3/43], Training Loss: 0.00000770
2024-11-06 14:07:30,079 - INFO - Epoch [220/300], Batch [4/43], Training Loss: 0.00001204
2024-11-06 14:07:30,082 - INFO - Epoch [220/300], Batch [5/43], Training Loss: 0.00001583
2024-11-06 14:07:30,085 - INFO - Epoch [220/300], Batch [6/43], Training Loss: 0.00000978
2024-11-06 14:07:30,089 - INFO - Epoch [220/300], Batch [7/43], Training Loss: 0.00000855
2024-11-06 14:07:30,092 - INFO - Epoch [220/300], Batch [8/43], Training Loss: 0.00000626
2024-11-06 14:07:30,096 - INFO - Epoch [220/300], Batch [9/43], Training Loss: 0.00000998
2024-11-06 14:07:30,099 - INFO - Epoch [220/300], Batch [10/43], Training Loss: 0.00000978
2024-11-06 14:07:30,103 - INFO - Epoch [220/300], Batch [11/43], Training Loss: 0.00000859
2024-11-06 14:07:30,107 - INFO - Epoch [220/300], Batch [12/43], Training Loss: 0.00000418
2024-11-06 14:07:30,110 - INFO - Epoch [220/300], Batch [13/43], Training Loss: 0.00001952
2024-11-06 14:07:30,114 - INFO - Epoch [220/300], Batch [14/43], Training Loss: 0.00001052
2024-11-06 14:07:30,117 - INFO - Epoch [220/300], Batch [15/43], Training Loss: 0.00000598
2024-11-06 14:07:30,120 - INFO - Epoch [220/300], Batch [16/43], Training Loss: 0.00000988
2024-11-06 14:07:30,123 - INFO - Epoch [220/300], Batch [17/43], Training Loss: 0.00001936
2024-11-06 14:07:30,126 - INFO - Epoch [220/300], Batch [18/43], Training Loss: 0.00001213
2024-11-06 14:07:30,129 - INFO - Epoch [220/300], Batch [19/43], Training Loss: 0.00000620
2024-11-06 14:07:30,132 - INFO - Epoch [220/300], Batch [20/43], Training Loss: 0.00000319
2024-11-06 14:07:30,135 - INFO - Epoch [220/300], Batch [21/43], Training Loss: 0.00001045
2024-11-06 14:07:30,138 - INFO - Epoch [220/300], Batch [22/43], Training Loss: 0.00000741
2024-11-06 14:07:30,142 - INFO - Epoch [220/300], Batch [23/43], Training Loss: 0.00001297
2024-11-06 14:07:30,145 - INFO - Epoch [220/300], Batch [24/43], Training Loss: 0.00001101
2024-11-06 14:07:30,149 - INFO - Epoch [220/300], Batch [25/43], Training Loss: 0.00000563
2024-11-06 14:07:30,153 - INFO - Epoch [220/300], Batch [26/43], Training Loss: 0.00000590
2024-11-06 14:07:30,156 - INFO - Epoch [220/300], Batch [27/43], Training Loss: 0.00000535
2024-11-06 14:07:30,159 - INFO - Epoch [220/300], Batch [28/43], Training Loss: 0.00001499
2024-11-06 14:07:30,163 - INFO - Epoch [220/300], Batch [29/43], Training Loss: 0.00000837
2024-11-06 14:07:30,166 - INFO - Epoch [220/300], Batch [30/43], Training Loss: 0.00000698
2024-11-06 14:07:30,170 - INFO - Epoch [220/300], Batch [31/43], Training Loss: 0.00000708
2024-11-06 14:07:30,173 - INFO - Epoch [220/300], Batch [32/43], Training Loss: 0.00001252
2024-11-06 14:07:30,176 - INFO - Epoch [220/300], Batch [33/43], Training Loss: 0.00000812
2024-11-06 14:07:30,181 - INFO - Epoch [220/300], Batch [34/43], Training Loss: 0.00001619
2024-11-06 14:07:30,184 - INFO - Epoch [220/300], Batch [35/43], Training Loss: 0.00002934
2024-11-06 14:07:30,188 - INFO - Epoch [220/300], Batch [36/43], Training Loss: 0.00002857
2024-11-06 14:07:30,192 - INFO - Epoch [220/300], Batch [37/43], Training Loss: 0.00000337
2024-11-06 14:07:30,196 - INFO - Epoch [220/300], Batch [38/43], Training Loss: 0.00000877
2024-11-06 14:07:30,201 - INFO - Epoch [220/300], Batch [39/43], Training Loss: 0.00001204
2024-11-06 14:07:30,205 - INFO - Epoch [220/300], Batch [40/43], Training Loss: 0.00001368
2024-11-06 14:07:30,209 - INFO - Epoch [220/300], Batch [41/43], Training Loss: 0.00001712
2024-11-06 14:07:30,212 - INFO - Epoch [220/300], Batch [42/43], Training Loss: 0.00001211
2024-11-06 14:07:30,216 - INFO - Epoch [220/300], Batch [43/43], Training Loss: 0.00001077
2024-11-06 14:07:30,229 - INFO - Epoch [220/300], Average Training Loss: 0.00001089, Validation Loss: 0.00001960
2024-11-06 14:07:30,233 - INFO - Epoch [221/300], Batch [1/43], Training Loss: 0.00002208
2024-11-06 14:07:30,237 - INFO - Epoch [221/300], Batch [2/43], Training Loss: 0.00001293
2024-11-06 14:07:30,240 - INFO - Epoch [221/300], Batch [3/43], Training Loss: 0.00001038
2024-11-06 14:07:30,244 - INFO - Epoch [221/300], Batch [4/43], Training Loss: 0.00002339
2024-11-06 14:07:30,247 - INFO - Epoch [221/300], Batch [5/43], Training Loss: 0.00000769
2024-11-06 14:07:30,251 - INFO - Epoch [221/300], Batch [6/43], Training Loss: 0.00002303
2024-11-06 14:07:30,255 - INFO - Epoch [221/300], Batch [7/43], Training Loss: 0.00001507
2024-11-06 14:07:30,258 - INFO - Epoch [221/300], Batch [8/43], Training Loss: 0.00000682
2024-11-06 14:07:30,261 - INFO - Epoch [221/300], Batch [9/43], Training Loss: 0.00000999
2024-11-06 14:07:30,264 - INFO - Epoch [221/300], Batch [10/43], Training Loss: 0.00000797
2024-11-06 14:07:30,267 - INFO - Epoch [221/300], Batch [11/43], Training Loss: 0.00001900
2024-11-06 14:07:30,271 - INFO - Epoch [221/300], Batch [12/43], Training Loss: 0.00001754
2024-11-06 14:07:30,275 - INFO - Epoch [221/300], Batch [13/43], Training Loss: 0.00001054
2024-11-06 14:07:30,278 - INFO - Epoch [221/300], Batch [14/43], Training Loss: 0.00000951
2024-11-06 14:07:30,281 - INFO - Epoch [221/300], Batch [15/43], Training Loss: 0.00000779
2024-11-06 14:07:30,284 - INFO - Epoch [221/300], Batch [16/43], Training Loss: 0.00002396
2024-11-06 14:07:30,287 - INFO - Epoch [221/300], Batch [17/43], Training Loss: 0.00000334
2024-11-06 14:07:30,291 - INFO - Epoch [221/300], Batch [18/43], Training Loss: 0.00003425
2024-11-06 14:07:30,294 - INFO - Epoch [221/300], Batch [19/43], Training Loss: 0.00002976
2024-11-06 14:07:30,297 - INFO - Epoch [221/300], Batch [20/43], Training Loss: 0.00001279
2024-11-06 14:07:30,300 - INFO - Epoch [221/300], Batch [21/43], Training Loss: 0.00001348
2024-11-06 14:07:30,303 - INFO - Epoch [221/300], Batch [22/43], Training Loss: 0.00001617
2024-11-06 14:07:30,307 - INFO - Epoch [221/300], Batch [23/43], Training Loss: 0.00001641
2024-11-06 14:07:30,310 - INFO - Epoch [221/300], Batch [24/43], Training Loss: 0.00001140
2024-11-06 14:07:30,313 - INFO - Epoch [221/300], Batch [25/43], Training Loss: 0.00001772
2024-11-06 14:07:30,316 - INFO - Epoch [221/300], Batch [26/43], Training Loss: 0.00000765
2024-11-06 14:07:30,319 - INFO - Epoch [221/300], Batch [27/43], Training Loss: 0.00000814
2024-11-06 14:07:30,322 - INFO - Epoch [221/300], Batch [28/43], Training Loss: 0.00000425
2024-11-06 14:07:30,326 - INFO - Epoch [221/300], Batch [29/43], Training Loss: 0.00001514
2024-11-06 14:07:30,330 - INFO - Epoch [221/300], Batch [30/43], Training Loss: 0.00000502
2024-11-06 14:07:30,335 - INFO - Epoch [221/300], Batch [31/43], Training Loss: 0.00000796
2024-11-06 14:07:30,339 - INFO - Epoch [221/300], Batch [32/43], Training Loss: 0.00000905
2024-11-06 14:07:30,344 - INFO - Epoch [221/300], Batch [33/43], Training Loss: 0.00001695
2024-11-06 14:07:30,347 - INFO - Epoch [221/300], Batch [34/43], Training Loss: 0.00000235
2024-11-06 14:07:30,350 - INFO - Epoch [221/300], Batch [35/43], Training Loss: 0.00001217
2024-11-06 14:07:30,355 - INFO - Epoch [221/300], Batch [36/43], Training Loss: 0.00000810
2024-11-06 14:07:30,359 - INFO - Epoch [221/300], Batch [37/43], Training Loss: 0.00001089
2024-11-06 14:07:30,362 - INFO - Epoch [221/300], Batch [38/43], Training Loss: 0.00000733
2024-11-06 14:07:30,366 - INFO - Epoch [221/300], Batch [39/43], Training Loss: 0.00000586
2024-11-06 14:07:30,370 - INFO - Epoch [221/300], Batch [40/43], Training Loss: 0.00000396
2024-11-06 14:07:30,373 - INFO - Epoch [221/300], Batch [41/43], Training Loss: 0.00001836
2024-11-06 14:07:30,377 - INFO - Epoch [221/300], Batch [42/43], Training Loss: 0.00001723
2024-11-06 14:07:30,380 - INFO - Epoch [221/300], Batch [43/43], Training Loss: 0.00000569
2024-11-06 14:07:30,390 - INFO - Epoch [221/300], Average Training Loss: 0.00001277, Validation Loss: 0.00001239
2024-11-06 14:07:30,395 - INFO - Epoch [222/300], Batch [1/43], Training Loss: 0.00000705
2024-11-06 14:07:30,398 - INFO - Epoch [222/300], Batch [2/43], Training Loss: 0.00000532
2024-11-06 14:07:30,401 - INFO - Epoch [222/300], Batch [3/43], Training Loss: 0.00001064
2024-11-06 14:07:30,405 - INFO - Epoch [222/300], Batch [4/43], Training Loss: 0.00000957
2024-11-06 14:07:30,409 - INFO - Epoch [222/300], Batch [5/43], Training Loss: 0.00000351
2024-11-06 14:07:30,413 - INFO - Epoch [222/300], Batch [6/43], Training Loss: 0.00000855
2024-11-06 14:07:30,416 - INFO - Epoch [222/300], Batch [7/43], Training Loss: 0.00000433
2024-11-06 14:07:30,419 - INFO - Epoch [222/300], Batch [8/43], Training Loss: 0.00001046
2024-11-06 14:07:30,422 - INFO - Epoch [222/300], Batch [9/43], Training Loss: 0.00000562
2024-11-06 14:07:30,424 - INFO - Epoch [222/300], Batch [10/43], Training Loss: 0.00000812
2024-11-06 14:07:30,428 - INFO - Epoch [222/300], Batch [11/43], Training Loss: 0.00001612
2024-11-06 14:07:30,432 - INFO - Epoch [222/300], Batch [12/43], Training Loss: 0.00000919
2024-11-06 14:07:30,436 - INFO - Epoch [222/300], Batch [13/43], Training Loss: 0.00000961
2024-11-06 14:07:30,440 - INFO - Epoch [222/300], Batch [14/43], Training Loss: 0.00000338
2024-11-06 14:07:30,443 - INFO - Epoch [222/300], Batch [15/43], Training Loss: 0.00001780
2024-11-06 14:07:30,447 - INFO - Epoch [222/300], Batch [16/43], Training Loss: 0.00000548
2024-11-06 14:07:30,451 - INFO - Epoch [222/300], Batch [17/43], Training Loss: 0.00001727
2024-11-06 14:07:30,454 - INFO - Epoch [222/300], Batch [18/43], Training Loss: 0.00000740
2024-11-06 14:07:30,458 - INFO - Epoch [222/300], Batch [19/43], Training Loss: 0.00001717
2024-11-06 14:07:30,462 - INFO - Epoch [222/300], Batch [20/43], Training Loss: 0.00000689
2024-11-06 14:07:30,466 - INFO - Epoch [222/300], Batch [21/43], Training Loss: 0.00001012
2024-11-06 14:07:30,470 - INFO - Epoch [222/300], Batch [22/43], Training Loss: 0.00000900
2024-11-06 14:07:30,473 - INFO - Epoch [222/300], Batch [23/43], Training Loss: 0.00000517
2024-11-06 14:07:30,478 - INFO - Epoch [222/300], Batch [24/43], Training Loss: 0.00003076
2024-11-06 14:07:30,484 - INFO - Epoch [222/300], Batch [25/43], Training Loss: 0.00000695
2024-11-06 14:07:30,489 - INFO - Epoch [222/300], Batch [26/43], Training Loss: 0.00001188
2024-11-06 14:07:30,493 - INFO - Epoch [222/300], Batch [27/43], Training Loss: 0.00000565
2024-11-06 14:07:30,496 - INFO - Epoch [222/300], Batch [28/43], Training Loss: 0.00000817
2024-11-06 14:07:30,500 - INFO - Epoch [222/300], Batch [29/43], Training Loss: 0.00001155
2024-11-06 14:07:30,504 - INFO - Epoch [222/300], Batch [30/43], Training Loss: 0.00001608
2024-11-06 14:07:30,508 - INFO - Epoch [222/300], Batch [31/43], Training Loss: 0.00000547
2024-11-06 14:07:30,511 - INFO - Epoch [222/300], Batch [32/43], Training Loss: 0.00000877
2024-11-06 14:07:30,515 - INFO - Epoch [222/300], Batch [33/43], Training Loss: 0.00000304
2024-11-06 14:07:30,518 - INFO - Epoch [222/300], Batch [34/43], Training Loss: 0.00000791
2024-11-06 14:07:30,521 - INFO - Epoch [222/300], Batch [35/43], Training Loss: 0.00001992
2024-11-06 14:07:30,524 - INFO - Epoch [222/300], Batch [36/43], Training Loss: 0.00000782
2024-11-06 14:07:30,527 - INFO - Epoch [222/300], Batch [37/43], Training Loss: 0.00000432
2024-11-06 14:07:30,530 - INFO - Epoch [222/300], Batch [38/43], Training Loss: 0.00001856
2024-11-06 14:07:30,534 - INFO - Epoch [222/300], Batch [39/43], Training Loss: 0.00000240
2024-11-06 14:07:30,538 - INFO - Epoch [222/300], Batch [40/43], Training Loss: 0.00000627
2024-11-06 14:07:30,541 - INFO - Epoch [222/300], Batch [41/43], Training Loss: 0.00001513
2024-11-06 14:07:30,546 - INFO - Epoch [222/300], Batch [42/43], Training Loss: 0.00002521
2024-11-06 14:07:30,550 - INFO - Epoch [222/300], Batch [43/43], Training Loss: 0.00001313
2024-11-06 14:07:30,560 - INFO - Epoch [222/300], Average Training Loss: 0.00001016, Validation Loss: 0.00001333
2024-11-06 14:07:30,563 - INFO - Epoch [223/300], Batch [1/43], Training Loss: 0.00002503
2024-11-06 14:07:30,566 - INFO - Epoch [223/300], Batch [2/43], Training Loss: 0.00001613
2024-11-06 14:07:30,569 - INFO - Epoch [223/300], Batch [3/43], Training Loss: 0.00001050
2024-11-06 14:07:30,572 - INFO - Epoch [223/300], Batch [4/43], Training Loss: 0.00000556
2024-11-06 14:07:30,575 - INFO - Epoch [223/300], Batch [5/43], Training Loss: 0.00000940
2024-11-06 14:07:30,578 - INFO - Epoch [223/300], Batch [6/43], Training Loss: 0.00000953
2024-11-06 14:07:30,581 - INFO - Epoch [223/300], Batch [7/43], Training Loss: 0.00002030
2024-11-06 14:07:30,584 - INFO - Epoch [223/300], Batch [8/43], Training Loss: 0.00001241
2024-11-06 14:07:30,586 - INFO - Epoch [223/300], Batch [9/43], Training Loss: 0.00001520
2024-11-06 14:07:30,589 - INFO - Epoch [223/300], Batch [10/43], Training Loss: 0.00000693
2024-11-06 14:07:30,592 - INFO - Epoch [223/300], Batch [11/43], Training Loss: 0.00001188
2024-11-06 14:07:30,595 - INFO - Epoch [223/300], Batch [12/43], Training Loss: 0.00001155
2024-11-06 14:07:30,599 - INFO - Epoch [223/300], Batch [13/43], Training Loss: 0.00001360
2024-11-06 14:07:30,602 - INFO - Epoch [223/300], Batch [14/43], Training Loss: 0.00001080
2024-11-06 14:07:30,606 - INFO - Epoch [223/300], Batch [15/43], Training Loss: 0.00001441
2024-11-06 14:07:30,610 - INFO - Epoch [223/300], Batch [16/43], Training Loss: 0.00000631
2024-11-06 14:07:30,613 - INFO - Epoch [223/300], Batch [17/43], Training Loss: 0.00000738
2024-11-06 14:07:30,617 - INFO - Epoch [223/300], Batch [18/43], Training Loss: 0.00000524
2024-11-06 14:07:30,621 - INFO - Epoch [223/300], Batch [19/43], Training Loss: 0.00001146
2024-11-06 14:07:30,625 - INFO - Epoch [223/300], Batch [20/43], Training Loss: 0.00000871
2024-11-06 14:07:30,629 - INFO - Epoch [223/300], Batch [21/43], Training Loss: 0.00001325
2024-11-06 14:07:30,633 - INFO - Epoch [223/300], Batch [22/43], Training Loss: 0.00001696
2024-11-06 14:07:30,637 - INFO - Epoch [223/300], Batch [23/43], Training Loss: 0.00000420
2024-11-06 14:07:30,640 - INFO - Epoch [223/300], Batch [24/43], Training Loss: 0.00001467
2024-11-06 14:07:30,645 - INFO - Epoch [223/300], Batch [25/43], Training Loss: 0.00001380
2024-11-06 14:07:30,648 - INFO - Epoch [223/300], Batch [26/43], Training Loss: 0.00002240
2024-11-06 14:07:30,651 - INFO - Epoch [223/300], Batch [27/43], Training Loss: 0.00001491
2024-11-06 14:07:30,655 - INFO - Epoch [223/300], Batch [28/43], Training Loss: 0.00001799
2024-11-06 14:07:30,658 - INFO - Epoch [223/300], Batch [29/43], Training Loss: 0.00000519
2024-11-06 14:07:30,661 - INFO - Epoch [223/300], Batch [30/43], Training Loss: 0.00001158
2024-11-06 14:07:30,664 - INFO - Epoch [223/300], Batch [31/43], Training Loss: 0.00001251
2024-11-06 14:07:30,668 - INFO - Epoch [223/300], Batch [32/43], Training Loss: 0.00000997
2024-11-06 14:07:30,673 - INFO - Epoch [223/300], Batch [33/43], Training Loss: 0.00000395
2024-11-06 14:07:30,676 - INFO - Epoch [223/300], Batch [34/43], Training Loss: 0.00001033
2024-11-06 14:07:30,679 - INFO - Epoch [223/300], Batch [35/43], Training Loss: 0.00000827
2024-11-06 14:07:30,683 - INFO - Epoch [223/300], Batch [36/43], Training Loss: 0.00000947
2024-11-06 14:07:30,686 - INFO - Epoch [223/300], Batch [37/43], Training Loss: 0.00000279
2024-11-06 14:07:30,690 - INFO - Epoch [223/300], Batch [38/43], Training Loss: 0.00000603
2024-11-06 14:07:30,692 - INFO - Epoch [223/300], Batch [39/43], Training Loss: 0.00000524
2024-11-06 14:07:30,695 - INFO - Epoch [223/300], Batch [40/43], Training Loss: 0.00002235
2024-11-06 14:07:30,698 - INFO - Epoch [223/300], Batch [41/43], Training Loss: 0.00000758
2024-11-06 14:07:30,701 - INFO - Epoch [223/300], Batch [42/43], Training Loss: 0.00000791
2024-11-06 14:07:30,704 - INFO - Epoch [223/300], Batch [43/43], Training Loss: 0.00002057
2024-11-06 14:07:30,714 - INFO - Epoch [223/300], Average Training Loss: 0.00001149, Validation Loss: 0.00001261
2024-11-06 14:07:30,718 - INFO - Epoch [224/300], Batch [1/43], Training Loss: 0.00000715
2024-11-06 14:07:30,722 - INFO - Epoch [224/300], Batch [2/43], Training Loss: 0.00001146
2024-11-06 14:07:30,726 - INFO - Epoch [224/300], Batch [3/43], Training Loss: 0.00000631
2024-11-06 14:07:30,730 - INFO - Epoch [224/300], Batch [4/43], Training Loss: 0.00001306
2024-11-06 14:07:30,734 - INFO - Epoch [224/300], Batch [5/43], Training Loss: 0.00001384
2024-11-06 14:07:30,738 - INFO - Epoch [224/300], Batch [6/43], Training Loss: 0.00000737
2024-11-06 14:07:30,741 - INFO - Epoch [224/300], Batch [7/43], Training Loss: 0.00000776
2024-11-06 14:07:30,745 - INFO - Epoch [224/300], Batch [8/43], Training Loss: 0.00001177
2024-11-06 14:07:30,749 - INFO - Epoch [224/300], Batch [9/43], Training Loss: 0.00001070
2024-11-06 14:07:30,752 - INFO - Epoch [224/300], Batch [10/43], Training Loss: 0.00000200
2024-11-06 14:07:30,756 - INFO - Epoch [224/300], Batch [11/43], Training Loss: 0.00000510
2024-11-06 14:07:30,760 - INFO - Epoch [224/300], Batch [12/43], Training Loss: 0.00002153
2024-11-06 14:07:30,764 - INFO - Epoch [224/300], Batch [13/43], Training Loss: 0.00000387
2024-11-06 14:07:30,768 - INFO - Epoch [224/300], Batch [14/43], Training Loss: 0.00000678
2024-11-06 14:07:30,772 - INFO - Epoch [224/300], Batch [15/43], Training Loss: 0.00000716
2024-11-06 14:07:30,775 - INFO - Epoch [224/300], Batch [16/43], Training Loss: 0.00000445
2024-11-06 14:07:30,780 - INFO - Epoch [224/300], Batch [17/43], Training Loss: 0.00000706
2024-11-06 14:07:30,783 - INFO - Epoch [224/300], Batch [18/43], Training Loss: 0.00001675
2024-11-06 14:07:30,786 - INFO - Epoch [224/300], Batch [19/43], Training Loss: 0.00000524
2024-11-06 14:07:30,788 - INFO - Epoch [224/300], Batch [20/43], Training Loss: 0.00000476
2024-11-06 14:07:30,792 - INFO - Epoch [224/300], Batch [21/43], Training Loss: 0.00001191
2024-11-06 14:07:30,795 - INFO - Epoch [224/300], Batch [22/43], Training Loss: 0.00001565
2024-11-06 14:07:30,798 - INFO - Epoch [224/300], Batch [23/43], Training Loss: 0.00002227
2024-11-06 14:07:30,801 - INFO - Epoch [224/300], Batch [24/43], Training Loss: 0.00000368
2024-11-06 14:07:30,804 - INFO - Epoch [224/300], Batch [25/43], Training Loss: 0.00002056
2024-11-06 14:07:30,806 - INFO - Epoch [224/300], Batch [26/43], Training Loss: 0.00001864
2024-11-06 14:07:30,810 - INFO - Epoch [224/300], Batch [27/43], Training Loss: 0.00001017
2024-11-06 14:07:30,814 - INFO - Epoch [224/300], Batch [28/43], Training Loss: 0.00001596
2024-11-06 14:07:30,817 - INFO - Epoch [224/300], Batch [29/43], Training Loss: 0.00003098
2024-11-06 14:07:30,820 - INFO - Epoch [224/300], Batch [30/43], Training Loss: 0.00001623
2024-11-06 14:07:30,824 - INFO - Epoch [224/300], Batch [31/43], Training Loss: 0.00000938
2024-11-06 14:07:30,828 - INFO - Epoch [224/300], Batch [32/43], Training Loss: 0.00000587
2024-11-06 14:07:30,831 - INFO - Epoch [224/300], Batch [33/43], Training Loss: 0.00001595
2024-11-06 14:07:30,834 - INFO - Epoch [224/300], Batch [34/43], Training Loss: 0.00000693
2024-11-06 14:07:30,838 - INFO - Epoch [224/300], Batch [35/43], Training Loss: 0.00002471
2024-11-06 14:07:30,842 - INFO - Epoch [224/300], Batch [36/43], Training Loss: 0.00000700
2024-11-06 14:07:30,846 - INFO - Epoch [224/300], Batch [37/43], Training Loss: 0.00000951
2024-11-06 14:07:30,850 - INFO - Epoch [224/300], Batch [38/43], Training Loss: 0.00001033
2024-11-06 14:07:30,854 - INFO - Epoch [224/300], Batch [39/43], Training Loss: 0.00000594
2024-11-06 14:07:30,858 - INFO - Epoch [224/300], Batch [40/43], Training Loss: 0.00001291
2024-11-06 14:07:30,862 - INFO - Epoch [224/300], Batch [41/43], Training Loss: 0.00000189
2024-11-06 14:07:30,865 - INFO - Epoch [224/300], Batch [42/43], Training Loss: 0.00001124
2024-11-06 14:07:30,870 - INFO - Epoch [224/300], Batch [43/43], Training Loss: 0.00000836
2024-11-06 14:07:30,881 - INFO - Epoch [224/300], Average Training Loss: 0.00001093, Validation Loss: 0.00001240
2024-11-06 14:07:30,885 - INFO - Epoch [225/300], Batch [1/43], Training Loss: 0.00000923
2024-11-06 14:07:30,888 - INFO - Epoch [225/300], Batch [2/43], Training Loss: 0.00000737
2024-11-06 14:07:30,891 - INFO - Epoch [225/300], Batch [3/43], Training Loss: 0.00001607
2024-11-06 14:07:30,895 - INFO - Epoch [225/300], Batch [4/43], Training Loss: 0.00000610
2024-11-06 14:07:30,899 - INFO - Epoch [225/300], Batch [5/43], Training Loss: 0.00000790
2024-11-06 14:07:30,903 - INFO - Epoch [225/300], Batch [6/43], Training Loss: 0.00000837
2024-11-06 14:07:30,908 - INFO - Epoch [225/300], Batch [7/43], Training Loss: 0.00000959
2024-11-06 14:07:30,912 - INFO - Epoch [225/300], Batch [8/43], Training Loss: 0.00000353
2024-11-06 14:07:30,915 - INFO - Epoch [225/300], Batch [9/43], Training Loss: 0.00000785
2024-11-06 14:07:30,919 - INFO - Epoch [225/300], Batch [10/43], Training Loss: 0.00001074
2024-11-06 14:07:30,924 - INFO - Epoch [225/300], Batch [11/43], Training Loss: 0.00000970
2024-11-06 14:07:30,927 - INFO - Epoch [225/300], Batch [12/43], Training Loss: 0.00000768
2024-11-06 14:07:30,930 - INFO - Epoch [225/300], Batch [13/43], Training Loss: 0.00001156
2024-11-06 14:07:30,934 - INFO - Epoch [225/300], Batch [14/43], Training Loss: 0.00002431
2024-11-06 14:07:30,937 - INFO - Epoch [225/300], Batch [15/43], Training Loss: 0.00000476
2024-11-06 14:07:30,941 - INFO - Epoch [225/300], Batch [16/43], Training Loss: 0.00001182
2024-11-06 14:07:30,944 - INFO - Epoch [225/300], Batch [17/43], Training Loss: 0.00001245
2024-11-06 14:07:30,948 - INFO - Epoch [225/300], Batch [18/43], Training Loss: 0.00000891
2024-11-06 14:07:30,951 - INFO - Epoch [225/300], Batch [19/43], Training Loss: 0.00001709
2024-11-06 14:07:30,955 - INFO - Epoch [225/300], Batch [20/43], Training Loss: 0.00001002
2024-11-06 14:07:30,958 - INFO - Epoch [225/300], Batch [21/43], Training Loss: 0.00002816
2024-11-06 14:07:30,961 - INFO - Epoch [225/300], Batch [22/43], Training Loss: 0.00001307
2024-11-06 14:07:30,964 - INFO - Epoch [225/300], Batch [23/43], Training Loss: 0.00000777
2024-11-06 14:07:30,967 - INFO - Epoch [225/300], Batch [24/43], Training Loss: 0.00001346
2024-11-06 14:07:30,971 - INFO - Epoch [225/300], Batch [25/43], Training Loss: 0.00001336
2024-11-06 14:07:30,974 - INFO - Epoch [225/300], Batch [26/43], Training Loss: 0.00000909
2024-11-06 14:07:30,977 - INFO - Epoch [225/300], Batch [27/43], Training Loss: 0.00001645
2024-11-06 14:07:30,980 - INFO - Epoch [225/300], Batch [28/43], Training Loss: 0.00000345
2024-11-06 14:07:30,982 - INFO - Epoch [225/300], Batch [29/43], Training Loss: 0.00001095
2024-11-06 14:07:30,985 - INFO - Epoch [225/300], Batch [30/43], Training Loss: 0.00001139
2024-11-06 14:07:30,988 - INFO - Epoch [225/300], Batch [31/43], Training Loss: 0.00001390
2024-11-06 14:07:30,991 - INFO - Epoch [225/300], Batch [32/43], Training Loss: 0.00001449
2024-11-06 14:07:30,995 - INFO - Epoch [225/300], Batch [33/43], Training Loss: 0.00001123
2024-11-06 14:07:30,999 - INFO - Epoch [225/300], Batch [34/43], Training Loss: 0.00001439
2024-11-06 14:07:31,002 - INFO - Epoch [225/300], Batch [35/43], Training Loss: 0.00001089
2024-11-06 14:07:31,005 - INFO - Epoch [225/300], Batch [36/43], Training Loss: 0.00000624
2024-11-06 14:07:31,008 - INFO - Epoch [225/300], Batch [37/43], Training Loss: 0.00002158
2024-11-06 14:07:31,011 - INFO - Epoch [225/300], Batch [38/43], Training Loss: 0.00000913
2024-11-06 14:07:31,014 - INFO - Epoch [225/300], Batch [39/43], Training Loss: 0.00000819
2024-11-06 14:07:31,017 - INFO - Epoch [225/300], Batch [40/43], Training Loss: 0.00003709
2024-11-06 14:07:31,020 - INFO - Epoch [225/300], Batch [41/43], Training Loss: 0.00000423
2024-11-06 14:07:31,023 - INFO - Epoch [225/300], Batch [42/43], Training Loss: 0.00001603
2024-11-06 14:07:31,027 - INFO - Epoch [225/300], Batch [43/43], Training Loss: 0.00001498
2024-11-06 14:07:31,039 - INFO - Epoch [225/300], Average Training Loss: 0.00001197, Validation Loss: 0.00001696
2024-11-06 14:07:31,043 - INFO - Epoch [226/300], Batch [1/43], Training Loss: 0.00001121
2024-11-06 14:07:31,048 - INFO - Epoch [226/300], Batch [2/43], Training Loss: 0.00000915
2024-11-06 14:07:31,052 - INFO - Epoch [226/300], Batch [3/43], Training Loss: 0.00000401
2024-11-06 14:07:31,055 - INFO - Epoch [226/300], Batch [4/43], Training Loss: 0.00001030
2024-11-06 14:07:31,059 - INFO - Epoch [226/300], Batch [5/43], Training Loss: 0.00001007
2024-11-06 14:07:31,063 - INFO - Epoch [226/300], Batch [6/43], Training Loss: 0.00000817
2024-11-06 14:07:31,067 - INFO - Epoch [226/300], Batch [7/43], Training Loss: 0.00000279
2024-11-06 14:07:31,071 - INFO - Epoch [226/300], Batch [8/43], Training Loss: 0.00001618
2024-11-06 14:07:31,074 - INFO - Epoch [226/300], Batch [9/43], Training Loss: 0.00000364
2024-11-06 14:07:31,078 - INFO - Epoch [226/300], Batch [10/43], Training Loss: 0.00001612
2024-11-06 14:07:31,080 - INFO - Epoch [226/300], Batch [11/43], Training Loss: 0.00000359
2024-11-06 14:07:31,084 - INFO - Epoch [226/300], Batch [12/43], Training Loss: 0.00001423
2024-11-06 14:07:31,087 - INFO - Epoch [226/300], Batch [13/43], Training Loss: 0.00000504
2024-11-06 14:07:31,091 - INFO - Epoch [226/300], Batch [14/43], Training Loss: 0.00001042
2024-11-06 14:07:31,094 - INFO - Epoch [226/300], Batch [15/43], Training Loss: 0.00001360
2024-11-06 14:07:31,098 - INFO - Epoch [226/300], Batch [16/43], Training Loss: 0.00001691
2024-11-06 14:07:31,101 - INFO - Epoch [226/300], Batch [17/43], Training Loss: 0.00002247
2024-11-06 14:07:31,104 - INFO - Epoch [226/300], Batch [18/43], Training Loss: 0.00000661
2024-11-06 14:07:31,108 - INFO - Epoch [226/300], Batch [19/43], Training Loss: 0.00001113
2024-11-06 14:07:31,112 - INFO - Epoch [226/300], Batch [20/43], Training Loss: 0.00003292
2024-11-06 14:07:31,115 - INFO - Epoch [226/300], Batch [21/43], Training Loss: 0.00000453
2024-11-06 14:07:31,118 - INFO - Epoch [226/300], Batch [22/43], Training Loss: 0.00000621
2024-11-06 14:07:31,121 - INFO - Epoch [226/300], Batch [23/43], Training Loss: 0.00002081
2024-11-06 14:07:31,124 - INFO - Epoch [226/300], Batch [24/43], Training Loss: 0.00000699
2024-11-06 14:07:31,127 - INFO - Epoch [226/300], Batch [25/43], Training Loss: 0.00000517
2024-11-06 14:07:31,130 - INFO - Epoch [226/300], Batch [26/43], Training Loss: 0.00001715
2024-11-06 14:07:31,134 - INFO - Epoch [226/300], Batch [27/43], Training Loss: 0.00000888
2024-11-06 14:07:31,138 - INFO - Epoch [226/300], Batch [28/43], Training Loss: 0.00003169
2024-11-06 14:07:31,142 - INFO - Epoch [226/300], Batch [29/43], Training Loss: 0.00001087
2024-11-06 14:07:31,146 - INFO - Epoch [226/300], Batch [30/43], Training Loss: 0.00000921
2024-11-06 14:07:31,150 - INFO - Epoch [226/300], Batch [31/43], Training Loss: 0.00000653
2024-11-06 14:07:31,153 - INFO - Epoch [226/300], Batch [32/43], Training Loss: 0.00000754
2024-11-06 14:07:31,157 - INFO - Epoch [226/300], Batch [33/43], Training Loss: 0.00000755
2024-11-06 14:07:31,160 - INFO - Epoch [226/300], Batch [34/43], Training Loss: 0.00000543
2024-11-06 14:07:31,162 - INFO - Epoch [226/300], Batch [35/43], Training Loss: 0.00000371
2024-11-06 14:07:31,165 - INFO - Epoch [226/300], Batch [36/43], Training Loss: 0.00000852
2024-11-06 14:07:31,169 - INFO - Epoch [226/300], Batch [37/43], Training Loss: 0.00001476
2024-11-06 14:07:31,174 - INFO - Epoch [226/300], Batch [38/43], Training Loss: 0.00001218
2024-11-06 14:07:31,177 - INFO - Epoch [226/300], Batch [39/43], Training Loss: 0.00001618
2024-11-06 14:07:31,181 - INFO - Epoch [226/300], Batch [40/43], Training Loss: 0.00001343
2024-11-06 14:07:31,184 - INFO - Epoch [226/300], Batch [41/43], Training Loss: 0.00000701
2024-11-06 14:07:31,188 - INFO - Epoch [226/300], Batch [42/43], Training Loss: 0.00001010
2024-11-06 14:07:31,192 - INFO - Epoch [226/300], Batch [43/43], Training Loss: 0.00001058
2024-11-06 14:07:31,204 - INFO - Epoch [226/300], Average Training Loss: 0.00001101, Validation Loss: 0.00001247
2024-11-06 14:07:31,208 - INFO - Epoch [227/300], Batch [1/43], Training Loss: 0.00001368
2024-11-06 14:07:31,212 - INFO - Epoch [227/300], Batch [2/43], Training Loss: 0.00001187
2024-11-06 14:07:31,215 - INFO - Epoch [227/300], Batch [3/43], Training Loss: 0.00001204
2024-11-06 14:07:31,218 - INFO - Epoch [227/300], Batch [4/43], Training Loss: 0.00000561
2024-11-06 14:07:31,222 - INFO - Epoch [227/300], Batch [5/43], Training Loss: 0.00000634
2024-11-06 14:07:31,226 - INFO - Epoch [227/300], Batch [6/43], Training Loss: 0.00000299
2024-11-06 14:07:31,229 - INFO - Epoch [227/300], Batch [7/43], Training Loss: 0.00000288
2024-11-06 14:07:31,233 - INFO - Epoch [227/300], Batch [8/43], Training Loss: 0.00001144
2024-11-06 14:07:31,237 - INFO - Epoch [227/300], Batch [9/43], Training Loss: 0.00000408
2024-11-06 14:07:31,239 - INFO - Epoch [227/300], Batch [10/43], Training Loss: 0.00001490
2024-11-06 14:07:31,242 - INFO - Epoch [227/300], Batch [11/43], Training Loss: 0.00001751
2024-11-06 14:07:31,245 - INFO - Epoch [227/300], Batch [12/43], Training Loss: 0.00000305
2024-11-06 14:07:31,249 - INFO - Epoch [227/300], Batch [13/43], Training Loss: 0.00000230
2024-11-06 14:07:31,252 - INFO - Epoch [227/300], Batch [14/43], Training Loss: 0.00000929
2024-11-06 14:07:31,256 - INFO - Epoch [227/300], Batch [15/43], Training Loss: 0.00001651
2024-11-06 14:07:31,259 - INFO - Epoch [227/300], Batch [16/43], Training Loss: 0.00000514
2024-11-06 14:07:31,263 - INFO - Epoch [227/300], Batch [17/43], Training Loss: 0.00000367
2024-11-06 14:07:31,266 - INFO - Epoch [227/300], Batch [18/43], Training Loss: 0.00000693
2024-11-06 14:07:31,270 - INFO - Epoch [227/300], Batch [19/43], Training Loss: 0.00002772
2024-11-06 14:07:31,274 - INFO - Epoch [227/300], Batch [20/43], Training Loss: 0.00001955
2024-11-06 14:07:31,277 - INFO - Epoch [227/300], Batch [21/43], Training Loss: 0.00000893
2024-11-06 14:07:31,280 - INFO - Epoch [227/300], Batch [22/43], Training Loss: 0.00000604
2024-11-06 14:07:31,283 - INFO - Epoch [227/300], Batch [23/43], Training Loss: 0.00000628
2024-11-06 14:07:31,286 - INFO - Epoch [227/300], Batch [24/43], Training Loss: 0.00000540
2024-11-06 14:07:31,289 - INFO - Epoch [227/300], Batch [25/43], Training Loss: 0.00000401
2024-11-06 14:07:31,292 - INFO - Epoch [227/300], Batch [26/43], Training Loss: 0.00000427
2024-11-06 14:07:31,296 - INFO - Epoch [227/300], Batch [27/43], Training Loss: 0.00000758
2024-11-06 14:07:31,300 - INFO - Epoch [227/300], Batch [28/43], Training Loss: 0.00001229
2024-11-06 14:07:31,303 - INFO - Epoch [227/300], Batch [29/43], Training Loss: 0.00000566
2024-11-06 14:07:31,306 - INFO - Epoch [227/300], Batch [30/43], Training Loss: 0.00000676
2024-11-06 14:07:31,309 - INFO - Epoch [227/300], Batch [31/43], Training Loss: 0.00001511
2024-11-06 14:07:31,312 - INFO - Epoch [227/300], Batch [32/43], Training Loss: 0.00000632
2024-11-06 14:07:31,315 - INFO - Epoch [227/300], Batch [33/43], Training Loss: 0.00002418
2024-11-06 14:07:31,318 - INFO - Epoch [227/300], Batch [34/43], Training Loss: 0.00000848
2024-11-06 14:07:31,320 - INFO - Epoch [227/300], Batch [35/43], Training Loss: 0.00000980
2024-11-06 14:07:31,323 - INFO - Epoch [227/300], Batch [36/43], Training Loss: 0.00001055
2024-11-06 14:07:31,326 - INFO - Epoch [227/300], Batch [37/43], Training Loss: 0.00002107
2024-11-06 14:07:31,330 - INFO - Epoch [227/300], Batch [38/43], Training Loss: 0.00000951
2024-11-06 14:07:31,333 - INFO - Epoch [227/300], Batch [39/43], Training Loss: 0.00001276
2024-11-06 14:07:31,336 - INFO - Epoch [227/300], Batch [40/43], Training Loss: 0.00000840
2024-11-06 14:07:31,340 - INFO - Epoch [227/300], Batch [41/43], Training Loss: 0.00000747
2024-11-06 14:07:31,343 - INFO - Epoch [227/300], Batch [42/43], Training Loss: 0.00000362
2024-11-06 14:07:31,346 - INFO - Epoch [227/300], Batch [43/43], Training Loss: 0.00000711
2024-11-06 14:07:31,357 - INFO - Epoch [227/300], Average Training Loss: 0.00000951, Validation Loss: 0.00001381
2024-11-06 14:07:31,360 - INFO - Epoch [228/300], Batch [1/43], Training Loss: 0.00001416
2024-11-06 14:07:31,363 - INFO - Epoch [228/300], Batch [2/43], Training Loss: 0.00001328
2024-11-06 14:07:31,366 - INFO - Epoch [228/300], Batch [3/43], Training Loss: 0.00000795
2024-11-06 14:07:31,369 - INFO - Epoch [228/300], Batch [4/43], Training Loss: 0.00001046
2024-11-06 14:07:31,372 - INFO - Epoch [228/300], Batch [5/43], Training Loss: 0.00000543
2024-11-06 14:07:31,377 - INFO - Epoch [228/300], Batch [6/43], Training Loss: 0.00002827
2024-11-06 14:07:31,381 - INFO - Epoch [228/300], Batch [7/43], Training Loss: 0.00001121
2024-11-06 14:07:31,386 - INFO - Epoch [228/300], Batch [8/43], Training Loss: 0.00001066
2024-11-06 14:07:31,391 - INFO - Epoch [228/300], Batch [9/43], Training Loss: 0.00001894
2024-11-06 14:07:31,395 - INFO - Epoch [228/300], Batch [10/43], Training Loss: 0.00001390
2024-11-06 14:07:31,399 - INFO - Epoch [228/300], Batch [11/43], Training Loss: 0.00000395
2024-11-06 14:07:31,403 - INFO - Epoch [228/300], Batch [12/43], Training Loss: 0.00000798
2024-11-06 14:07:31,407 - INFO - Epoch [228/300], Batch [13/43], Training Loss: 0.00001723
2024-11-06 14:07:31,411 - INFO - Epoch [228/300], Batch [14/43], Training Loss: 0.00000422
2024-11-06 14:07:31,414 - INFO - Epoch [228/300], Batch [15/43], Training Loss: 0.00000386
2024-11-06 14:07:31,418 - INFO - Epoch [228/300], Batch [16/43], Training Loss: 0.00000859
2024-11-06 14:07:31,421 - INFO - Epoch [228/300], Batch [17/43], Training Loss: 0.00000679
2024-11-06 14:07:31,425 - INFO - Epoch [228/300], Batch [18/43], Training Loss: 0.00001305
2024-11-06 14:07:31,428 - INFO - Epoch [228/300], Batch [19/43], Training Loss: 0.00000613
2024-11-06 14:07:31,432 - INFO - Epoch [228/300], Batch [20/43], Training Loss: 0.00002375
2024-11-06 14:07:31,436 - INFO - Epoch [228/300], Batch [21/43], Training Loss: 0.00001909
2024-11-06 14:07:31,439 - INFO - Epoch [228/300], Batch [22/43], Training Loss: 0.00001143
2024-11-06 14:07:31,443 - INFO - Epoch [228/300], Batch [23/43], Training Loss: 0.00001596
2024-11-06 14:07:31,447 - INFO - Epoch [228/300], Batch [24/43], Training Loss: 0.00001390
2024-11-06 14:07:31,450 - INFO - Epoch [228/300], Batch [25/43], Training Loss: 0.00002374
2024-11-06 14:07:31,453 - INFO - Epoch [228/300], Batch [26/43], Training Loss: 0.00000732
2024-11-06 14:07:31,456 - INFO - Epoch [228/300], Batch [27/43], Training Loss: 0.00000299
2024-11-06 14:07:31,459 - INFO - Epoch [228/300], Batch [28/43], Training Loss: 0.00000812
2024-11-06 14:07:31,462 - INFO - Epoch [228/300], Batch [29/43], Training Loss: 0.00001922
2024-11-06 14:07:31,464 - INFO - Epoch [228/300], Batch [30/43], Training Loss: 0.00001397
2024-11-06 14:07:31,468 - INFO - Epoch [228/300], Batch [31/43], Training Loss: 0.00001409
2024-11-06 14:07:31,473 - INFO - Epoch [228/300], Batch [32/43], Training Loss: 0.00001091
2024-11-06 14:07:31,476 - INFO - Epoch [228/300], Batch [33/43], Training Loss: 0.00001185
2024-11-06 14:07:31,479 - INFO - Epoch [228/300], Batch [34/43], Training Loss: 0.00000511
2024-11-06 14:07:31,482 - INFO - Epoch [228/300], Batch [35/43], Training Loss: 0.00001893
2024-11-06 14:07:31,486 - INFO - Epoch [228/300], Batch [36/43], Training Loss: 0.00001167
2024-11-06 14:07:31,489 - INFO - Epoch [228/300], Batch [37/43], Training Loss: 0.00000698
2024-11-06 14:07:31,493 - INFO - Epoch [228/300], Batch [38/43], Training Loss: 0.00000323
2024-11-06 14:07:31,497 - INFO - Epoch [228/300], Batch [39/43], Training Loss: 0.00000628
2024-11-06 14:07:31,500 - INFO - Epoch [228/300], Batch [40/43], Training Loss: 0.00000860
2024-11-06 14:07:31,504 - INFO - Epoch [228/300], Batch [41/43], Training Loss: 0.00001400
2024-11-06 14:07:31,507 - INFO - Epoch [228/300], Batch [42/43], Training Loss: 0.00000932
2024-11-06 14:07:31,512 - INFO - Epoch [228/300], Batch [43/43], Training Loss: 0.00000421
2024-11-06 14:07:31,523 - INFO - Epoch [228/300], Average Training Loss: 0.00001141, Validation Loss: 0.00001465
2024-11-06 14:07:31,527 - INFO - Epoch [229/300], Batch [1/43], Training Loss: 0.00000289
2024-11-06 14:07:31,531 - INFO - Epoch [229/300], Batch [2/43], Training Loss: 0.00000801
2024-11-06 14:07:31,534 - INFO - Epoch [229/300], Batch [3/43], Training Loss: 0.00002676
2024-11-06 14:07:31,538 - INFO - Epoch [229/300], Batch [4/43], Training Loss: 0.00002199
2024-11-06 14:07:31,542 - INFO - Epoch [229/300], Batch [5/43], Training Loss: 0.00001216
2024-11-06 14:07:31,545 - INFO - Epoch [229/300], Batch [6/43], Training Loss: 0.00002840
2024-11-06 14:07:31,547 - INFO - Epoch [229/300], Batch [7/43], Training Loss: 0.00000647
2024-11-06 14:07:31,550 - INFO - Epoch [229/300], Batch [8/43], Training Loss: 0.00000732
2024-11-06 14:07:31,553 - INFO - Epoch [229/300], Batch [9/43], Training Loss: 0.00000787
2024-11-06 14:07:31,556 - INFO - Epoch [229/300], Batch [10/43], Training Loss: 0.00000393
2024-11-06 14:07:31,558 - INFO - Epoch [229/300], Batch [11/43], Training Loss: 0.00001356
2024-11-06 14:07:31,562 - INFO - Epoch [229/300], Batch [12/43], Training Loss: 0.00000456
2024-11-06 14:07:31,565 - INFO - Epoch [229/300], Batch [13/43], Training Loss: 0.00000419
2024-11-06 14:07:31,568 - INFO - Epoch [229/300], Batch [14/43], Training Loss: 0.00001244
2024-11-06 14:07:31,571 - INFO - Epoch [229/300], Batch [15/43], Training Loss: 0.00000421
2024-11-06 14:07:31,574 - INFO - Epoch [229/300], Batch [16/43], Training Loss: 0.00001664
2024-11-06 14:07:31,578 - INFO - Epoch [229/300], Batch [17/43], Training Loss: 0.00000560
2024-11-06 14:07:31,583 - INFO - Epoch [229/300], Batch [18/43], Training Loss: 0.00001219
2024-11-06 14:07:31,587 - INFO - Epoch [229/300], Batch [19/43], Training Loss: 0.00000949
2024-11-06 14:07:31,591 - INFO - Epoch [229/300], Batch [20/43], Training Loss: 0.00000530
2024-11-06 14:07:31,595 - INFO - Epoch [229/300], Batch [21/43], Training Loss: 0.00000738
2024-11-06 14:07:31,599 - INFO - Epoch [229/300], Batch [22/43], Training Loss: 0.00000905
2024-11-06 14:07:31,603 - INFO - Epoch [229/300], Batch [23/43], Training Loss: 0.00000874
2024-11-06 14:07:31,607 - INFO - Epoch [229/300], Batch [24/43], Training Loss: 0.00002695
2024-11-06 14:07:31,610 - INFO - Epoch [229/300], Batch [25/43], Training Loss: 0.00000798
2024-11-06 14:07:31,614 - INFO - Epoch [229/300], Batch [26/43], Training Loss: 0.00000587
2024-11-06 14:07:31,618 - INFO - Epoch [229/300], Batch [27/43], Training Loss: 0.00001136
2024-11-06 14:07:31,622 - INFO - Epoch [229/300], Batch [28/43], Training Loss: 0.00000873
2024-11-06 14:07:31,626 - INFO - Epoch [229/300], Batch [29/43], Training Loss: 0.00000188
2024-11-06 14:07:31,630 - INFO - Epoch [229/300], Batch [30/43], Training Loss: 0.00001089
2024-11-06 14:07:31,634 - INFO - Epoch [229/300], Batch [31/43], Training Loss: 0.00001453
2024-11-06 14:07:31,638 - INFO - Epoch [229/300], Batch [32/43], Training Loss: 0.00001030
2024-11-06 14:07:31,642 - INFO - Epoch [229/300], Batch [33/43], Training Loss: 0.00000837
2024-11-06 14:07:31,645 - INFO - Epoch [229/300], Batch [34/43], Training Loss: 0.00002843
2024-11-06 14:07:31,648 - INFO - Epoch [229/300], Batch [35/43], Training Loss: 0.00000809
2024-11-06 14:07:31,652 - INFO - Epoch [229/300], Batch [36/43], Training Loss: 0.00001052
2024-11-06 14:07:31,655 - INFO - Epoch [229/300], Batch [37/43], Training Loss: 0.00000307
2024-11-06 14:07:31,659 - INFO - Epoch [229/300], Batch [38/43], Training Loss: 0.00000664
2024-11-06 14:07:31,663 - INFO - Epoch [229/300], Batch [39/43], Training Loss: 0.00001233
2024-11-06 14:07:31,667 - INFO - Epoch [229/300], Batch [40/43], Training Loss: 0.00001782
2024-11-06 14:07:31,672 - INFO - Epoch [229/300], Batch [41/43], Training Loss: 0.00001093
2024-11-06 14:07:31,675 - INFO - Epoch [229/300], Batch [42/43], Training Loss: 0.00000597
2024-11-06 14:07:31,680 - INFO - Epoch [229/300], Batch [43/43], Training Loss: 0.00000889
2024-11-06 14:07:31,691 - INFO - Epoch [229/300], Average Training Loss: 0.00001067, Validation Loss: 0.00001235
2024-11-06 14:07:31,694 - INFO - Epoch [230/300], Batch [1/43], Training Loss: 0.00000264
2024-11-06 14:07:31,698 - INFO - Epoch [230/300], Batch [2/43], Training Loss: 0.00000809
2024-11-06 14:07:31,701 - INFO - Epoch [230/300], Batch [3/43], Training Loss: 0.00001250
2024-11-06 14:07:31,705 - INFO - Epoch [230/300], Batch [4/43], Training Loss: 0.00002617
2024-11-06 14:07:31,708 - INFO - Epoch [230/300], Batch [5/43], Training Loss: 0.00002399
2024-11-06 14:07:31,711 - INFO - Epoch [230/300], Batch [6/43], Training Loss: 0.00002213
2024-11-06 14:07:31,714 - INFO - Epoch [230/300], Batch [7/43], Training Loss: 0.00000457
2024-11-06 14:07:31,717 - INFO - Epoch [230/300], Batch [8/43], Training Loss: 0.00000384
2024-11-06 14:07:31,721 - INFO - Epoch [230/300], Batch [9/43], Training Loss: 0.00000591
2024-11-06 14:07:31,724 - INFO - Epoch [230/300], Batch [10/43], Training Loss: 0.00000382
2024-11-06 14:07:31,727 - INFO - Epoch [230/300], Batch [11/43], Training Loss: 0.00001209
2024-11-06 14:07:31,731 - INFO - Epoch [230/300], Batch [12/43], Training Loss: 0.00001057
2024-11-06 14:07:31,736 - INFO - Epoch [230/300], Batch [13/43], Training Loss: 0.00001344
2024-11-06 14:07:31,740 - INFO - Epoch [230/300], Batch [14/43], Training Loss: 0.00000416
2024-11-06 14:07:31,744 - INFO - Epoch [230/300], Batch [15/43], Training Loss: 0.00003005
2024-11-06 14:07:31,748 - INFO - Epoch [230/300], Batch [16/43], Training Loss: 0.00000415
2024-11-06 14:07:31,754 - INFO - Epoch [230/300], Batch [17/43], Training Loss: 0.00001176
2024-11-06 14:07:31,759 - INFO - Epoch [230/300], Batch [18/43], Training Loss: 0.00003080
2024-11-06 14:07:31,763 - INFO - Epoch [230/300], Batch [19/43], Training Loss: 0.00000544
2024-11-06 14:07:31,767 - INFO - Epoch [230/300], Batch [20/43], Training Loss: 0.00001337
2024-11-06 14:07:31,771 - INFO - Epoch [230/300], Batch [21/43], Training Loss: 0.00002351
2024-11-06 14:07:31,774 - INFO - Epoch [230/300], Batch [22/43], Training Loss: 0.00000790
2024-11-06 14:07:31,777 - INFO - Epoch [230/300], Batch [23/43], Training Loss: 0.00000712
2024-11-06 14:07:31,781 - INFO - Epoch [230/300], Batch [24/43], Training Loss: 0.00001107
2024-11-06 14:07:31,785 - INFO - Epoch [230/300], Batch [25/43], Training Loss: 0.00001123
2024-11-06 14:07:31,789 - INFO - Epoch [230/300], Batch [26/43], Training Loss: 0.00000993
2024-11-06 14:07:31,792 - INFO - Epoch [230/300], Batch [27/43], Training Loss: 0.00000524
2024-11-06 14:07:31,796 - INFO - Epoch [230/300], Batch [28/43], Training Loss: 0.00002068
2024-11-06 14:07:31,800 - INFO - Epoch [230/300], Batch [29/43], Training Loss: 0.00001038
2024-11-06 14:07:31,804 - INFO - Epoch [230/300], Batch [30/43], Training Loss: 0.00000871
2024-11-06 14:07:31,807 - INFO - Epoch [230/300], Batch [31/43], Training Loss: 0.00000825
2024-11-06 14:07:31,811 - INFO - Epoch [230/300], Batch [32/43], Training Loss: 0.00001004
2024-11-06 14:07:31,815 - INFO - Epoch [230/300], Batch [33/43], Training Loss: 0.00000585
2024-11-06 14:07:31,818 - INFO - Epoch [230/300], Batch [34/43], Training Loss: 0.00000548
2024-11-06 14:07:31,821 - INFO - Epoch [230/300], Batch [35/43], Training Loss: 0.00001249
2024-11-06 14:07:31,825 - INFO - Epoch [230/300], Batch [36/43], Training Loss: 0.00002019
2024-11-06 14:07:31,828 - INFO - Epoch [230/300], Batch [37/43], Training Loss: 0.00001106
2024-11-06 14:07:31,832 - INFO - Epoch [230/300], Batch [38/43], Training Loss: 0.00000436
2024-11-06 14:07:31,835 - INFO - Epoch [230/300], Batch [39/43], Training Loss: 0.00000708
2024-11-06 14:07:31,838 - INFO - Epoch [230/300], Batch [40/43], Training Loss: 0.00000605
2024-11-06 14:07:31,841 - INFO - Epoch [230/300], Batch [41/43], Training Loss: 0.00000660
2024-11-06 14:07:31,845 - INFO - Epoch [230/300], Batch [42/43], Training Loss: 0.00001037
2024-11-06 14:07:31,849 - INFO - Epoch [230/300], Batch [43/43], Training Loss: 0.00001417
2024-11-06 14:07:31,862 - INFO - Epoch [230/300], Average Training Loss: 0.00001133, Validation Loss: 0.00001410
2024-11-06 14:07:31,866 - INFO - Epoch [231/300], Batch [1/43], Training Loss: 0.00001530
2024-11-06 14:07:31,870 - INFO - Epoch [231/300], Batch [2/43], Training Loss: 0.00000834
2024-11-06 14:07:31,874 - INFO - Epoch [231/300], Batch [3/43], Training Loss: 0.00003538
2024-11-06 14:07:31,879 - INFO - Epoch [231/300], Batch [4/43], Training Loss: 0.00000620
2024-11-06 14:07:31,882 - INFO - Epoch [231/300], Batch [5/43], Training Loss: 0.00001032
2024-11-06 14:07:31,886 - INFO - Epoch [231/300], Batch [6/43], Training Loss: 0.00001216
2024-11-06 14:07:31,891 - INFO - Epoch [231/300], Batch [7/43], Training Loss: 0.00001645
2024-11-06 14:07:31,896 - INFO - Epoch [231/300], Batch [8/43], Training Loss: 0.00001174
2024-11-06 14:07:31,902 - INFO - Epoch [231/300], Batch [9/43], Training Loss: 0.00001585
2024-11-06 14:07:31,906 - INFO - Epoch [231/300], Batch [10/43], Training Loss: 0.00000796
2024-11-06 14:07:31,909 - INFO - Epoch [231/300], Batch [11/43], Training Loss: 0.00000979
2024-11-06 14:07:31,913 - INFO - Epoch [231/300], Batch [12/43], Training Loss: 0.00000644
2024-11-06 14:07:31,917 - INFO - Epoch [231/300], Batch [13/43], Training Loss: 0.00000525
2024-11-06 14:07:31,922 - INFO - Epoch [231/300], Batch [14/43], Training Loss: 0.00001796
2024-11-06 14:07:31,927 - INFO - Epoch [231/300], Batch [15/43], Training Loss: 0.00000743
2024-11-06 14:07:31,931 - INFO - Epoch [231/300], Batch [16/43], Training Loss: 0.00000388
2024-11-06 14:07:31,936 - INFO - Epoch [231/300], Batch [17/43], Training Loss: 0.00000360
2024-11-06 14:07:31,942 - INFO - Epoch [231/300], Batch [18/43], Training Loss: 0.00001764
2024-11-06 14:07:31,949 - INFO - Epoch [231/300], Batch [19/43], Training Loss: 0.00003039
2024-11-06 14:07:31,955 - INFO - Epoch [231/300], Batch [20/43], Training Loss: 0.00000981
2024-11-06 14:07:31,960 - INFO - Epoch [231/300], Batch [21/43], Training Loss: 0.00000804
2024-11-06 14:07:31,964 - INFO - Epoch [231/300], Batch [22/43], Training Loss: 0.00000614
2024-11-06 14:07:31,968 - INFO - Epoch [231/300], Batch [23/43], Training Loss: 0.00000692
2024-11-06 14:07:31,973 - INFO - Epoch [231/300], Batch [24/43], Training Loss: 0.00001046
2024-11-06 14:07:31,977 - INFO - Epoch [231/300], Batch [25/43], Training Loss: 0.00001464
2024-11-06 14:07:31,980 - INFO - Epoch [231/300], Batch [26/43], Training Loss: 0.00001251
2024-11-06 14:07:31,984 - INFO - Epoch [231/300], Batch [27/43], Training Loss: 0.00000698
2024-11-06 14:07:31,989 - INFO - Epoch [231/300], Batch [28/43], Training Loss: 0.00001268
2024-11-06 14:07:31,993 - INFO - Epoch [231/300], Batch [29/43], Training Loss: 0.00001565
2024-11-06 14:07:31,997 - INFO - Epoch [231/300], Batch [30/43], Training Loss: 0.00001703
2024-11-06 14:07:32,001 - INFO - Epoch [231/300], Batch [31/43], Training Loss: 0.00000677
2024-11-06 14:07:32,005 - INFO - Epoch [231/300], Batch [32/43], Training Loss: 0.00000722
2024-11-06 14:07:32,009 - INFO - Epoch [231/300], Batch [33/43], Training Loss: 0.00001483
2024-11-06 14:07:32,014 - INFO - Epoch [231/300], Batch [34/43], Training Loss: 0.00000507
2024-11-06 14:07:32,018 - INFO - Epoch [231/300], Batch [35/43], Training Loss: 0.00001387
2024-11-06 14:07:32,022 - INFO - Epoch [231/300], Batch [36/43], Training Loss: 0.00001004
2024-11-06 14:07:32,026 - INFO - Epoch [231/300], Batch [37/43], Training Loss: 0.00000709
2024-11-06 14:07:32,030 - INFO - Epoch [231/300], Batch [38/43], Training Loss: 0.00000704
2024-11-06 14:07:32,034 - INFO - Epoch [231/300], Batch [39/43], Training Loss: 0.00001069
2024-11-06 14:07:32,037 - INFO - Epoch [231/300], Batch [40/43], Training Loss: 0.00000627
2024-11-06 14:07:32,040 - INFO - Epoch [231/300], Batch [41/43], Training Loss: 0.00001049
2024-11-06 14:07:32,044 - INFO - Epoch [231/300], Batch [42/43], Training Loss: 0.00000600
2024-11-06 14:07:32,048 - INFO - Epoch [231/300], Batch [43/43], Training Loss: 0.00000981
2024-11-06 14:07:32,061 - INFO - Epoch [231/300], Average Training Loss: 0.00001112, Validation Loss: 0.00001390
2024-11-06 14:07:32,065 - INFO - Epoch [232/300], Batch [1/43], Training Loss: 0.00000977
2024-11-06 14:07:32,068 - INFO - Epoch [232/300], Batch [2/43], Training Loss: 0.00001271
2024-11-06 14:07:32,071 - INFO - Epoch [232/300], Batch [3/43], Training Loss: 0.00000778
2024-11-06 14:07:32,074 - INFO - Epoch [232/300], Batch [4/43], Training Loss: 0.00000849
2024-11-06 14:07:32,078 - INFO - Epoch [232/300], Batch [5/43], Training Loss: 0.00000630
2024-11-06 14:07:32,118 - INFO - Epoch [232/300], Batch [6/43], Training Loss: 0.00000536
2024-11-06 14:07:32,126 - INFO - Epoch [232/300], Batch [7/43], Training Loss: 0.00000921
2024-11-06 14:07:32,134 - INFO - Epoch [232/300], Batch [8/43], Training Loss: 0.00000453
2024-11-06 14:07:32,144 - INFO - Epoch [232/300], Batch [9/43], Training Loss: 0.00001050
2024-11-06 14:07:32,150 - INFO - Epoch [232/300], Batch [10/43], Training Loss: 0.00000483
2024-11-06 14:07:32,154 - INFO - Epoch [232/300], Batch [11/43], Training Loss: 0.00000174
2024-11-06 14:07:32,160 - INFO - Epoch [232/300], Batch [12/43], Training Loss: 0.00001034
2024-11-06 14:07:32,163 - INFO - Epoch [232/300], Batch [13/43], Training Loss: 0.00001794
2024-11-06 14:07:32,167 - INFO - Epoch [232/300], Batch [14/43], Training Loss: 0.00000813
2024-11-06 14:07:32,171 - INFO - Epoch [232/300], Batch [15/43], Training Loss: 0.00001672
2024-11-06 14:07:32,176 - INFO - Epoch [232/300], Batch [16/43], Training Loss: 0.00001097
2024-11-06 14:07:32,181 - INFO - Epoch [232/300], Batch [17/43], Training Loss: 0.00000969
2024-11-06 14:07:32,185 - INFO - Epoch [232/300], Batch [18/43], Training Loss: 0.00000735
2024-11-06 14:07:32,189 - INFO - Epoch [232/300], Batch [19/43], Training Loss: 0.00000757
2024-11-06 14:07:32,192 - INFO - Epoch [232/300], Batch [20/43], Training Loss: 0.00000240
2024-11-06 14:07:32,197 - INFO - Epoch [232/300], Batch [21/43], Training Loss: 0.00000646
2024-11-06 14:07:32,200 - INFO - Epoch [232/300], Batch [22/43], Training Loss: 0.00000959
2024-11-06 14:07:32,203 - INFO - Epoch [232/300], Batch [23/43], Training Loss: 0.00000471
2024-11-06 14:07:32,207 - INFO - Epoch [232/300], Batch [24/43], Training Loss: 0.00000852
2024-11-06 14:07:32,211 - INFO - Epoch [232/300], Batch [25/43], Training Loss: 0.00004200
2024-11-06 14:07:32,215 - INFO - Epoch [232/300], Batch [26/43], Training Loss: 0.00000880
2024-11-06 14:07:32,218 - INFO - Epoch [232/300], Batch [27/43], Training Loss: 0.00001365
2024-11-06 14:07:32,221 - INFO - Epoch [232/300], Batch [28/43], Training Loss: 0.00003925
2024-11-06 14:07:32,225 - INFO - Epoch [232/300], Batch [29/43], Training Loss: 0.00000760
2024-11-06 14:07:32,228 - INFO - Epoch [232/300], Batch [30/43], Training Loss: 0.00001000
2024-11-06 14:07:32,231 - INFO - Epoch [232/300], Batch [31/43], Training Loss: 0.00002533
2024-11-06 14:07:32,234 - INFO - Epoch [232/300], Batch [32/43], Training Loss: 0.00001376
2024-11-06 14:07:32,237 - INFO - Epoch [232/300], Batch [33/43], Training Loss: 0.00000464
2024-11-06 14:07:32,241 - INFO - Epoch [232/300], Batch [34/43], Training Loss: 0.00002623
2024-11-06 14:07:32,245 - INFO - Epoch [232/300], Batch [35/43], Training Loss: 0.00000962
2024-11-06 14:07:32,249 - INFO - Epoch [232/300], Batch [36/43], Training Loss: 0.00001195
2024-11-06 14:07:32,254 - INFO - Epoch [232/300], Batch [37/43], Training Loss: 0.00000921
2024-11-06 14:07:32,258 - INFO - Epoch [232/300], Batch [38/43], Training Loss: 0.00001193
2024-11-06 14:07:32,262 - INFO - Epoch [232/300], Batch [39/43], Training Loss: 0.00001412
2024-11-06 14:07:32,267 - INFO - Epoch [232/300], Batch [40/43], Training Loss: 0.00001213
2024-11-06 14:07:32,270 - INFO - Epoch [232/300], Batch [41/43], Training Loss: 0.00001067
2024-11-06 14:07:32,273 - INFO - Epoch [232/300], Batch [42/43], Training Loss: 0.00001220
2024-11-06 14:07:32,277 - INFO - Epoch [232/300], Batch [43/43], Training Loss: 0.00003019
2024-11-06 14:07:32,289 - INFO - Epoch [232/300], Average Training Loss: 0.00001197, Validation Loss: 0.00001246
2024-11-06 14:07:32,293 - INFO - Epoch [233/300], Batch [1/43], Training Loss: 0.00000882
2024-11-06 14:07:32,296 - INFO - Epoch [233/300], Batch [2/43], Training Loss: 0.00000450
2024-11-06 14:07:32,300 - INFO - Epoch [233/300], Batch [3/43], Training Loss: 0.00000471
2024-11-06 14:07:32,303 - INFO - Epoch [233/300], Batch [4/43], Training Loss: 0.00001566
2024-11-06 14:07:32,306 - INFO - Epoch [233/300], Batch [5/43], Training Loss: 0.00000644
2024-11-06 14:07:32,309 - INFO - Epoch [233/300], Batch [6/43], Training Loss: 0.00000920
2024-11-06 14:07:32,313 - INFO - Epoch [233/300], Batch [7/43], Training Loss: 0.00001698
2024-11-06 14:07:32,317 - INFO - Epoch [233/300], Batch [8/43], Training Loss: 0.00001028
2024-11-06 14:07:32,320 - INFO - Epoch [233/300], Batch [9/43], Training Loss: 0.00000956
2024-11-06 14:07:32,324 - INFO - Epoch [233/300], Batch [10/43], Training Loss: 0.00000728
2024-11-06 14:07:32,327 - INFO - Epoch [233/300], Batch [11/43], Training Loss: 0.00001045
2024-11-06 14:07:32,332 - INFO - Epoch [233/300], Batch [12/43], Training Loss: 0.00001132
2024-11-06 14:07:32,336 - INFO - Epoch [233/300], Batch [13/43], Training Loss: 0.00000788
2024-11-06 14:07:32,340 - INFO - Epoch [233/300], Batch [14/43], Training Loss: 0.00001656
2024-11-06 14:07:32,344 - INFO - Epoch [233/300], Batch [15/43], Training Loss: 0.00000957
2024-11-06 14:07:32,348 - INFO - Epoch [233/300], Batch [16/43], Training Loss: 0.00000684
2024-11-06 14:07:32,352 - INFO - Epoch [233/300], Batch [17/43], Training Loss: 0.00000923
2024-11-06 14:07:32,356 - INFO - Epoch [233/300], Batch [18/43], Training Loss: 0.00003267
2024-11-06 14:07:32,360 - INFO - Epoch [233/300], Batch [19/43], Training Loss: 0.00000899
2024-11-06 14:07:32,363 - INFO - Epoch [233/300], Batch [20/43], Training Loss: 0.00001263
2024-11-06 14:07:32,368 - INFO - Epoch [233/300], Batch [21/43], Training Loss: 0.00000623
2024-11-06 14:07:32,371 - INFO - Epoch [233/300], Batch [22/43], Training Loss: 0.00001611
2024-11-06 14:07:32,375 - INFO - Epoch [233/300], Batch [23/43], Training Loss: 0.00002866
2024-11-06 14:07:32,380 - INFO - Epoch [233/300], Batch [24/43], Training Loss: 0.00002989
2024-11-06 14:07:32,383 - INFO - Epoch [233/300], Batch [25/43], Training Loss: 0.00001045
2024-11-06 14:07:32,387 - INFO - Epoch [233/300], Batch [26/43], Training Loss: 0.00001065
2024-11-06 14:07:32,390 - INFO - Epoch [233/300], Batch [27/43], Training Loss: 0.00001290
2024-11-06 14:07:32,395 - INFO - Epoch [233/300], Batch [28/43], Training Loss: 0.00000466
2024-11-06 14:07:32,399 - INFO - Epoch [233/300], Batch [29/43], Training Loss: 0.00002324
2024-11-06 14:07:32,404 - INFO - Epoch [233/300], Batch [30/43], Training Loss: 0.00000681
2024-11-06 14:07:32,408 - INFO - Epoch [233/300], Batch [31/43], Training Loss: 0.00001417
2024-11-06 14:07:32,412 - INFO - Epoch [233/300], Batch [32/43], Training Loss: 0.00001662
2024-11-06 14:07:32,415 - INFO - Epoch [233/300], Batch [33/43], Training Loss: 0.00000511
2024-11-06 14:07:32,419 - INFO - Epoch [233/300], Batch [34/43], Training Loss: 0.00002220
2024-11-06 14:07:32,423 - INFO - Epoch [233/300], Batch [35/43], Training Loss: 0.00001676
2024-11-06 14:07:32,427 - INFO - Epoch [233/300], Batch [36/43], Training Loss: 0.00002049
2024-11-06 14:07:32,430 - INFO - Epoch [233/300], Batch [37/43], Training Loss: 0.00000923
2024-11-06 14:07:32,435 - INFO - Epoch [233/300], Batch [38/43], Training Loss: 0.00001320
2024-11-06 14:07:32,438 - INFO - Epoch [233/300], Batch [39/43], Training Loss: 0.00001001
2024-11-06 14:07:32,442 - INFO - Epoch [233/300], Batch [40/43], Training Loss: 0.00001223
2024-11-06 14:07:32,446 - INFO - Epoch [233/300], Batch [41/43], Training Loss: 0.00000700
2024-11-06 14:07:32,451 - INFO - Epoch [233/300], Batch [42/43], Training Loss: 0.00000559
2024-11-06 14:07:32,455 - INFO - Epoch [233/300], Batch [43/43], Training Loss: 0.00000930
2024-11-06 14:07:32,466 - INFO - Epoch [233/300], Average Training Loss: 0.00001235, Validation Loss: 0.00001290
2024-11-06 14:07:32,469 - INFO - Epoch [234/300], Batch [1/43], Training Loss: 0.00001139
2024-11-06 14:07:32,472 - INFO - Epoch [234/300], Batch [2/43], Training Loss: 0.00001655
2024-11-06 14:07:32,475 - INFO - Epoch [234/300], Batch [3/43], Training Loss: 0.00000661
2024-11-06 14:07:32,478 - INFO - Epoch [234/300], Batch [4/43], Training Loss: 0.00000308
2024-11-06 14:07:32,481 - INFO - Epoch [234/300], Batch [5/43], Training Loss: 0.00001129
2024-11-06 14:07:32,484 - INFO - Epoch [234/300], Batch [6/43], Training Loss: 0.00000478
2024-11-06 14:07:32,488 - INFO - Epoch [234/300], Batch [7/43], Training Loss: 0.00000583
2024-11-06 14:07:32,491 - INFO - Epoch [234/300], Batch [8/43], Training Loss: 0.00000953
2024-11-06 14:07:32,495 - INFO - Epoch [234/300], Batch [9/43], Training Loss: 0.00000471
2024-11-06 14:07:32,498 - INFO - Epoch [234/300], Batch [10/43], Training Loss: 0.00001101
2024-11-06 14:07:32,501 - INFO - Epoch [234/300], Batch [11/43], Training Loss: 0.00000717
2024-11-06 14:07:32,504 - INFO - Epoch [234/300], Batch [12/43], Training Loss: 0.00000977
2024-11-06 14:07:32,508 - INFO - Epoch [234/300], Batch [13/43], Training Loss: 0.00000928
2024-11-06 14:07:32,511 - INFO - Epoch [234/300], Batch [14/43], Training Loss: 0.00001227
2024-11-06 14:07:32,514 - INFO - Epoch [234/300], Batch [15/43], Training Loss: 0.00001201
2024-11-06 14:07:32,517 - INFO - Epoch [234/300], Batch [16/43], Training Loss: 0.00000613
2024-11-06 14:07:32,521 - INFO - Epoch [234/300], Batch [17/43], Training Loss: 0.00001752
2024-11-06 14:07:32,524 - INFO - Epoch [234/300], Batch [18/43], Training Loss: 0.00000793
2024-11-06 14:07:32,527 - INFO - Epoch [234/300], Batch [19/43], Training Loss: 0.00001788
2024-11-06 14:07:32,530 - INFO - Epoch [234/300], Batch [20/43], Training Loss: 0.00000543
2024-11-06 14:07:32,533 - INFO - Epoch [234/300], Batch [21/43], Training Loss: 0.00001056
2024-11-06 14:07:32,537 - INFO - Epoch [234/300], Batch [22/43], Training Loss: 0.00001595
2024-11-06 14:07:32,541 - INFO - Epoch [234/300], Batch [23/43], Training Loss: 0.00001964
2024-11-06 14:07:32,545 - INFO - Epoch [234/300], Batch [24/43], Training Loss: 0.00000513
2024-11-06 14:07:32,549 - INFO - Epoch [234/300], Batch [25/43], Training Loss: 0.00000970
2024-11-06 14:07:32,553 - INFO - Epoch [234/300], Batch [26/43], Training Loss: 0.00000724
2024-11-06 14:07:32,557 - INFO - Epoch [234/300], Batch [27/43], Training Loss: 0.00001094
2024-11-06 14:07:32,561 - INFO - Epoch [234/300], Batch [28/43], Training Loss: 0.00000928
2024-11-06 14:07:32,565 - INFO - Epoch [234/300], Batch [29/43], Training Loss: 0.00001024
2024-11-06 14:07:32,568 - INFO - Epoch [234/300], Batch [30/43], Training Loss: 0.00000937
2024-11-06 14:07:32,571 - INFO - Epoch [234/300], Batch [31/43], Training Loss: 0.00001191
2024-11-06 14:07:32,575 - INFO - Epoch [234/300], Batch [32/43], Training Loss: 0.00001092
2024-11-06 14:07:32,579 - INFO - Epoch [234/300], Batch [33/43], Training Loss: 0.00000836
2024-11-06 14:07:32,582 - INFO - Epoch [234/300], Batch [34/43], Training Loss: 0.00000896
2024-11-06 14:07:32,585 - INFO - Epoch [234/300], Batch [35/43], Training Loss: 0.00001727
2024-11-06 14:07:32,589 - INFO - Epoch [234/300], Batch [36/43], Training Loss: 0.00000837
2024-11-06 14:07:32,592 - INFO - Epoch [234/300], Batch [37/43], Training Loss: 0.00001909
2024-11-06 14:07:32,596 - INFO - Epoch [234/300], Batch [38/43], Training Loss: 0.00000596
2024-11-06 14:07:32,600 - INFO - Epoch [234/300], Batch [39/43], Training Loss: 0.00000920
2024-11-06 14:07:32,604 - INFO - Epoch [234/300], Batch [40/43], Training Loss: 0.00000683
2024-11-06 14:07:32,610 - INFO - Epoch [234/300], Batch [41/43], Training Loss: 0.00001119
2024-11-06 14:07:32,615 - INFO - Epoch [234/300], Batch [42/43], Training Loss: 0.00002394
2024-11-06 14:07:32,620 - INFO - Epoch [234/300], Batch [43/43], Training Loss: 0.00000737
2024-11-06 14:07:32,632 - INFO - Epoch [234/300], Average Training Loss: 0.00001041, Validation Loss: 0.00001591
2024-11-06 14:07:32,637 - INFO - Epoch [235/300], Batch [1/43], Training Loss: 0.00001505
2024-11-06 14:07:32,640 - INFO - Epoch [235/300], Batch [2/43], Training Loss: 0.00002448
2024-11-06 14:07:32,644 - INFO - Epoch [235/300], Batch [3/43], Training Loss: 0.00000884
2024-11-06 14:07:32,648 - INFO - Epoch [235/300], Batch [4/43], Training Loss: 0.00000163
2024-11-06 14:07:32,651 - INFO - Epoch [235/300], Batch [5/43], Training Loss: 0.00001189
2024-11-06 14:07:32,654 - INFO - Epoch [235/300], Batch [6/43], Training Loss: 0.00001174
2024-11-06 14:07:32,658 - INFO - Epoch [235/300], Batch [7/43], Training Loss: 0.00000665
2024-11-06 14:07:32,661 - INFO - Epoch [235/300], Batch [8/43], Training Loss: 0.00000293
2024-11-06 14:07:32,665 - INFO - Epoch [235/300], Batch [9/43], Training Loss: 0.00001289
2024-11-06 14:07:32,669 - INFO - Epoch [235/300], Batch [10/43], Training Loss: 0.00003261
2024-11-06 14:07:32,673 - INFO - Epoch [235/300], Batch [11/43], Training Loss: 0.00000331
2024-11-06 14:07:32,677 - INFO - Epoch [235/300], Batch [12/43], Training Loss: 0.00000465
2024-11-06 14:07:32,682 - INFO - Epoch [235/300], Batch [13/43], Training Loss: 0.00000637
2024-11-06 14:07:32,686 - INFO - Epoch [235/300], Batch [14/43], Training Loss: 0.00003258
2024-11-06 14:07:32,690 - INFO - Epoch [235/300], Batch [15/43], Training Loss: 0.00000791
2024-11-06 14:07:32,695 - INFO - Epoch [235/300], Batch [16/43], Training Loss: 0.00000351
2024-11-06 14:07:32,699 - INFO - Epoch [235/300], Batch [17/43], Training Loss: 0.00002682
2024-11-06 14:07:32,703 - INFO - Epoch [235/300], Batch [18/43], Training Loss: 0.00000713
2024-11-06 14:07:32,707 - INFO - Epoch [235/300], Batch [19/43], Training Loss: 0.00001151
2024-11-06 14:07:32,711 - INFO - Epoch [235/300], Batch [20/43], Training Loss: 0.00001046
2024-11-06 14:07:32,715 - INFO - Epoch [235/300], Batch [21/43], Training Loss: 0.00001499
2024-11-06 14:07:32,720 - INFO - Epoch [235/300], Batch [22/43], Training Loss: 0.00002225
2024-11-06 14:07:32,725 - INFO - Epoch [235/300], Batch [23/43], Training Loss: 0.00001241
2024-11-06 14:07:32,729 - INFO - Epoch [235/300], Batch [24/43], Training Loss: 0.00000505
2024-11-06 14:07:32,732 - INFO - Epoch [235/300], Batch [25/43], Training Loss: 0.00001313
2024-11-06 14:07:32,736 - INFO - Epoch [235/300], Batch [26/43], Training Loss: 0.00000633
2024-11-06 14:07:32,740 - INFO - Epoch [235/300], Batch [27/43], Training Loss: 0.00001023
2024-11-06 14:07:32,744 - INFO - Epoch [235/300], Batch [28/43], Training Loss: 0.00000322
2024-11-06 14:07:32,747 - INFO - Epoch [235/300], Batch [29/43], Training Loss: 0.00000468
2024-11-06 14:07:32,751 - INFO - Epoch [235/300], Batch [30/43], Training Loss: 0.00000743
2024-11-06 14:07:32,755 - INFO - Epoch [235/300], Batch [31/43], Training Loss: 0.00001094
2024-11-06 14:07:32,759 - INFO - Epoch [235/300], Batch [32/43], Training Loss: 0.00001245
2024-11-06 14:07:32,763 - INFO - Epoch [235/300], Batch [33/43], Training Loss: 0.00000836
2024-11-06 14:07:32,767 - INFO - Epoch [235/300], Batch [34/43], Training Loss: 0.00001015
2024-11-06 14:07:32,772 - INFO - Epoch [235/300], Batch [35/43], Training Loss: 0.00000800
2024-11-06 14:07:32,776 - INFO - Epoch [235/300], Batch [36/43], Training Loss: 0.00001163
2024-11-06 14:07:32,781 - INFO - Epoch [235/300], Batch [37/43], Training Loss: 0.00001839
2024-11-06 14:07:32,785 - INFO - Epoch [235/300], Batch [38/43], Training Loss: 0.00001704
2024-11-06 14:07:32,789 - INFO - Epoch [235/300], Batch [39/43], Training Loss: 0.00000768
2024-11-06 14:07:32,792 - INFO - Epoch [235/300], Batch [40/43], Training Loss: 0.00000531
2024-11-06 14:07:32,797 - INFO - Epoch [235/300], Batch [41/43], Training Loss: 0.00001982
2024-11-06 14:07:32,802 - INFO - Epoch [235/300], Batch [42/43], Training Loss: 0.00000912
2024-11-06 14:07:32,806 - INFO - Epoch [235/300], Batch [43/43], Training Loss: 0.00000519
2024-11-06 14:07:32,818 - INFO - Epoch [235/300], Average Training Loss: 0.00001132, Validation Loss: 0.00001362
2024-11-06 14:07:32,822 - INFO - Epoch [236/300], Batch [1/43], Training Loss: 0.00001117
2024-11-06 14:07:32,826 - INFO - Epoch [236/300], Batch [2/43], Training Loss: 0.00000408
2024-11-06 14:07:32,829 - INFO - Epoch [236/300], Batch [3/43], Training Loss: 0.00000915
2024-11-06 14:07:32,833 - INFO - Epoch [236/300], Batch [4/43], Training Loss: 0.00000591
2024-11-06 14:07:32,837 - INFO - Epoch [236/300], Batch [5/43], Training Loss: 0.00000415
2024-11-06 14:07:32,841 - INFO - Epoch [236/300], Batch [6/43], Training Loss: 0.00001789
2024-11-06 14:07:32,844 - INFO - Epoch [236/300], Batch [7/43], Training Loss: 0.00000837
2024-11-06 14:07:32,848 - INFO - Epoch [236/300], Batch [8/43], Training Loss: 0.00000925
2024-11-06 14:07:32,851 - INFO - Epoch [236/300], Batch [9/43], Training Loss: 0.00001088
2024-11-06 14:07:32,855 - INFO - Epoch [236/300], Batch [10/43], Training Loss: 0.00000251
2024-11-06 14:07:32,859 - INFO - Epoch [236/300], Batch [11/43], Training Loss: 0.00001031
2024-11-06 14:07:32,864 - INFO - Epoch [236/300], Batch [12/43], Training Loss: 0.00000398
2024-11-06 14:07:32,869 - INFO - Epoch [236/300], Batch [13/43], Training Loss: 0.00000927
2024-11-06 14:07:32,874 - INFO - Epoch [236/300], Batch [14/43], Training Loss: 0.00000980
2024-11-06 14:07:32,878 - INFO - Epoch [236/300], Batch [15/43], Training Loss: 0.00001853
2024-11-06 14:07:32,882 - INFO - Epoch [236/300], Batch [16/43], Training Loss: 0.00001246
2024-11-06 14:07:32,886 - INFO - Epoch [236/300], Batch [17/43], Training Loss: 0.00000753
2024-11-06 14:07:32,890 - INFO - Epoch [236/300], Batch [18/43], Training Loss: 0.00000875
2024-11-06 14:07:32,894 - INFO - Epoch [236/300], Batch [19/43], Training Loss: 0.00000361
2024-11-06 14:07:32,898 - INFO - Epoch [236/300], Batch [20/43], Training Loss: 0.00000936
2024-11-06 14:07:32,902 - INFO - Epoch [236/300], Batch [21/43], Training Loss: 0.00001176
2024-11-06 14:07:32,908 - INFO - Epoch [236/300], Batch [22/43], Training Loss: 0.00000641
2024-11-06 14:07:32,911 - INFO - Epoch [236/300], Batch [23/43], Training Loss: 0.00001692
2024-11-06 14:07:32,915 - INFO - Epoch [236/300], Batch [24/43], Training Loss: 0.00000718
2024-11-06 14:07:32,919 - INFO - Epoch [236/300], Batch [25/43], Training Loss: 0.00000529
2024-11-06 14:07:32,923 - INFO - Epoch [236/300], Batch [26/43], Training Loss: 0.00000605
2024-11-06 14:07:32,927 - INFO - Epoch [236/300], Batch [27/43], Training Loss: 0.00000582
2024-11-06 14:07:32,930 - INFO - Epoch [236/300], Batch [28/43], Training Loss: 0.00001491
2024-11-06 14:07:32,934 - INFO - Epoch [236/300], Batch [29/43], Training Loss: 0.00001046
2024-11-06 14:07:32,938 - INFO - Epoch [236/300], Batch [30/43], Training Loss: 0.00003711
2024-11-06 14:07:32,943 - INFO - Epoch [236/300], Batch [31/43], Training Loss: 0.00000122
2024-11-06 14:07:32,949 - INFO - Epoch [236/300], Batch [32/43], Training Loss: 0.00001966
2024-11-06 14:07:32,953 - INFO - Epoch [236/300], Batch [33/43], Training Loss: 0.00001167
2024-11-06 14:07:32,958 - INFO - Epoch [236/300], Batch [34/43], Training Loss: 0.00002644
2024-11-06 14:07:32,963 - INFO - Epoch [236/300], Batch [35/43], Training Loss: 0.00000599
2024-11-06 14:07:32,967 - INFO - Epoch [236/300], Batch [36/43], Training Loss: 0.00000316
2024-11-06 14:07:32,972 - INFO - Epoch [236/300], Batch [37/43], Training Loss: 0.00002141
2024-11-06 14:07:32,977 - INFO - Epoch [236/300], Batch [38/43], Training Loss: 0.00000224
2024-11-06 14:07:32,981 - INFO - Epoch [236/300], Batch [39/43], Training Loss: 0.00001025
2024-11-06 14:07:32,985 - INFO - Epoch [236/300], Batch [40/43], Training Loss: 0.00001089
2024-11-06 14:07:32,989 - INFO - Epoch [236/300], Batch [41/43], Training Loss: 0.00001538
2024-11-06 14:07:32,993 - INFO - Epoch [236/300], Batch [42/43], Training Loss: 0.00001254
2024-11-06 14:07:32,997 - INFO - Epoch [236/300], Batch [43/43], Training Loss: 0.00001369
2024-11-06 14:07:33,011 - INFO - Epoch [236/300], Average Training Loss: 0.00001054, Validation Loss: 0.00001262
2024-11-06 14:07:33,016 - INFO - Epoch [237/300], Batch [1/43], Training Loss: 0.00001152
2024-11-06 14:07:33,022 - INFO - Epoch [237/300], Batch [2/43], Training Loss: 0.00000562
2024-11-06 14:07:33,026 - INFO - Epoch [237/300], Batch [3/43], Training Loss: 0.00000383
2024-11-06 14:07:33,031 - INFO - Epoch [237/300], Batch [4/43], Training Loss: 0.00001707
2024-11-06 14:07:33,036 - INFO - Epoch [237/300], Batch [5/43], Training Loss: 0.00000539
2024-11-06 14:07:33,040 - INFO - Epoch [237/300], Batch [6/43], Training Loss: 0.00000689
2024-11-06 14:07:33,044 - INFO - Epoch [237/300], Batch [7/43], Training Loss: 0.00000584
2024-11-06 14:07:33,048 - INFO - Epoch [237/300], Batch [8/43], Training Loss: 0.00002903
2024-11-06 14:07:33,052 - INFO - Epoch [237/300], Batch [9/43], Training Loss: 0.00000505
2024-11-06 14:07:33,056 - INFO - Epoch [237/300], Batch [10/43], Training Loss: 0.00000483
2024-11-06 14:07:33,060 - INFO - Epoch [237/300], Batch [11/43], Training Loss: 0.00001742
2024-11-06 14:07:33,064 - INFO - Epoch [237/300], Batch [12/43], Training Loss: 0.00002015
2024-11-06 14:07:33,068 - INFO - Epoch [237/300], Batch [13/43], Training Loss: 0.00000903
2024-11-06 14:07:33,073 - INFO - Epoch [237/300], Batch [14/43], Training Loss: 0.00002380
2024-11-06 14:07:33,077 - INFO - Epoch [237/300], Batch [15/43], Training Loss: 0.00001020
2024-11-06 14:07:33,081 - INFO - Epoch [237/300], Batch [16/43], Training Loss: 0.00000615
2024-11-06 14:07:33,085 - INFO - Epoch [237/300], Batch [17/43], Training Loss: 0.00001483
2024-11-06 14:07:33,088 - INFO - Epoch [237/300], Batch [18/43], Training Loss: 0.00000758
2024-11-06 14:07:33,091 - INFO - Epoch [237/300], Batch [19/43], Training Loss: 0.00001429
2024-11-06 14:07:33,094 - INFO - Epoch [237/300], Batch [20/43], Training Loss: 0.00000158
2024-11-06 14:07:33,097 - INFO - Epoch [237/300], Batch [21/43], Training Loss: 0.00002083
2024-11-06 14:07:33,101 - INFO - Epoch [237/300], Batch [22/43], Training Loss: 0.00001142
2024-11-06 14:07:33,104 - INFO - Epoch [237/300], Batch [23/43], Training Loss: 0.00000804
2024-11-06 14:07:33,107 - INFO - Epoch [237/300], Batch [24/43], Training Loss: 0.00000512
2024-11-06 14:07:33,111 - INFO - Epoch [237/300], Batch [25/43], Training Loss: 0.00000453
2024-11-06 14:07:33,114 - INFO - Epoch [237/300], Batch [26/43], Training Loss: 0.00001897
2024-11-06 14:07:33,118 - INFO - Epoch [237/300], Batch [27/43], Training Loss: 0.00001009
2024-11-06 14:07:33,121 - INFO - Epoch [237/300], Batch [28/43], Training Loss: 0.00000959
2024-11-06 14:07:33,125 - INFO - Epoch [237/300], Batch [29/43], Training Loss: 0.00000903
2024-11-06 14:07:33,128 - INFO - Epoch [237/300], Batch [30/43], Training Loss: 0.00000892
2024-11-06 14:07:33,131 - INFO - Epoch [237/300], Batch [31/43], Training Loss: 0.00000300
2024-11-06 14:07:33,135 - INFO - Epoch [237/300], Batch [32/43], Training Loss: 0.00001000
2024-11-06 14:07:33,137 - INFO - Epoch [237/300], Batch [33/43], Training Loss: 0.00000849
2024-11-06 14:07:33,141 - INFO - Epoch [237/300], Batch [34/43], Training Loss: 0.00001499
2024-11-06 14:07:33,145 - INFO - Epoch [237/300], Batch [35/43], Training Loss: 0.00001771
2024-11-06 14:07:33,148 - INFO - Epoch [237/300], Batch [36/43], Training Loss: 0.00002655
2024-11-06 14:07:33,151 - INFO - Epoch [237/300], Batch [37/43], Training Loss: 0.00001463
2024-11-06 14:07:33,154 - INFO - Epoch [237/300], Batch [38/43], Training Loss: 0.00001419
2024-11-06 14:07:33,159 - INFO - Epoch [237/300], Batch [39/43], Training Loss: 0.00001485
2024-11-06 14:07:33,164 - INFO - Epoch [237/300], Batch [40/43], Training Loss: 0.00001741
2024-11-06 14:07:33,168 - INFO - Epoch [237/300], Batch [41/43], Training Loss: 0.00001085
2024-11-06 14:07:33,171 - INFO - Epoch [237/300], Batch [42/43], Training Loss: 0.00000852
2024-11-06 14:07:33,176 - INFO - Epoch [237/300], Batch [43/43], Training Loss: 0.00000345
2024-11-06 14:07:33,189 - INFO - Epoch [237/300], Average Training Loss: 0.00001143, Validation Loss: 0.00001691
2024-11-06 14:07:33,194 - INFO - Epoch [238/300], Batch [1/43], Training Loss: 0.00001185
2024-11-06 14:07:33,199 - INFO - Epoch [238/300], Batch [2/43], Training Loss: 0.00001693
2024-11-06 14:07:33,203 - INFO - Epoch [238/300], Batch [3/43], Training Loss: 0.00000949
2024-11-06 14:07:33,208 - INFO - Epoch [238/300], Batch [4/43], Training Loss: 0.00000362
2024-11-06 14:07:33,212 - INFO - Epoch [238/300], Batch [5/43], Training Loss: 0.00000933
2024-11-06 14:07:33,216 - INFO - Epoch [238/300], Batch [6/43], Training Loss: 0.00000527
2024-11-06 14:07:33,220 - INFO - Epoch [238/300], Batch [7/43], Training Loss: 0.00002479
2024-11-06 14:07:33,224 - INFO - Epoch [238/300], Batch [8/43], Training Loss: 0.00000754
2024-11-06 14:07:33,229 - INFO - Epoch [238/300], Batch [9/43], Training Loss: 0.00000822
2024-11-06 14:07:33,232 - INFO - Epoch [238/300], Batch [10/43], Training Loss: 0.00001423
2024-11-06 14:07:33,236 - INFO - Epoch [238/300], Batch [11/43], Training Loss: 0.00000836
2024-11-06 14:07:33,240 - INFO - Epoch [238/300], Batch [12/43], Training Loss: 0.00001612
2024-11-06 14:07:33,244 - INFO - Epoch [238/300], Batch [13/43], Training Loss: 0.00000868
2024-11-06 14:07:33,248 - INFO - Epoch [238/300], Batch [14/43], Training Loss: 0.00001730
2024-11-06 14:07:33,251 - INFO - Epoch [238/300], Batch [15/43], Training Loss: 0.00002141
2024-11-06 14:07:33,256 - INFO - Epoch [238/300], Batch [16/43], Training Loss: 0.00001083
2024-11-06 14:07:33,260 - INFO - Epoch [238/300], Batch [17/43], Training Loss: 0.00000671
2024-11-06 14:07:33,264 - INFO - Epoch [238/300], Batch [18/43], Training Loss: 0.00001765
2024-11-06 14:07:33,268 - INFO - Epoch [238/300], Batch [19/43], Training Loss: 0.00001141
2024-11-06 14:07:33,272 - INFO - Epoch [238/300], Batch [20/43], Training Loss: 0.00001485
2024-11-06 14:07:33,276 - INFO - Epoch [238/300], Batch [21/43], Training Loss: 0.00000846
2024-11-06 14:07:33,279 - INFO - Epoch [238/300], Batch [22/43], Training Loss: 0.00002544
2024-11-06 14:07:33,283 - INFO - Epoch [238/300], Batch [23/43], Training Loss: 0.00001877
2024-11-06 14:07:33,287 - INFO - Epoch [238/300], Batch [24/43], Training Loss: 0.00000623
2024-11-06 14:07:33,291 - INFO - Epoch [238/300], Batch [25/43], Training Loss: 0.00001155
2024-11-06 14:07:33,296 - INFO - Epoch [238/300], Batch [26/43], Training Loss: 0.00003356
2024-11-06 14:07:33,300 - INFO - Epoch [238/300], Batch [27/43], Training Loss: 0.00000864
2024-11-06 14:07:33,304 - INFO - Epoch [238/300], Batch [28/43], Training Loss: 0.00000694
2024-11-06 14:07:33,309 - INFO - Epoch [238/300], Batch [29/43], Training Loss: 0.00001465
2024-11-06 14:07:33,314 - INFO - Epoch [238/300], Batch [30/43], Training Loss: 0.00001374
2024-11-06 14:07:33,318 - INFO - Epoch [238/300], Batch [31/43], Training Loss: 0.00001325
2024-11-06 14:07:33,322 - INFO - Epoch [238/300], Batch [32/43], Training Loss: 0.00000470
2024-11-06 14:07:33,327 - INFO - Epoch [238/300], Batch [33/43], Training Loss: 0.00001482
2024-11-06 14:07:33,332 - INFO - Epoch [238/300], Batch [34/43], Training Loss: 0.00000631
2024-11-06 14:07:33,336 - INFO - Epoch [238/300], Batch [35/43], Training Loss: 0.00000745
2024-11-06 14:07:33,341 - INFO - Epoch [238/300], Batch [36/43], Training Loss: 0.00001277
2024-11-06 14:07:33,348 - INFO - Epoch [238/300], Batch [37/43], Training Loss: 0.00000333
2024-11-06 14:07:33,353 - INFO - Epoch [238/300], Batch [38/43], Training Loss: 0.00002322
2024-11-06 14:07:33,357 - INFO - Epoch [238/300], Batch [39/43], Training Loss: 0.00000758
2024-11-06 14:07:33,362 - INFO - Epoch [238/300], Batch [40/43], Training Loss: 0.00000263
2024-11-06 14:07:33,367 - INFO - Epoch [238/300], Batch [41/43], Training Loss: 0.00001160
2024-11-06 14:07:33,371 - INFO - Epoch [238/300], Batch [42/43], Training Loss: 0.00000450
2024-11-06 14:07:33,376 - INFO - Epoch [238/300], Batch [43/43], Training Loss: 0.00002359
2024-11-06 14:07:33,388 - INFO - Epoch [238/300], Average Training Loss: 0.00001229, Validation Loss: 0.00001238
2024-11-06 14:07:33,391 - INFO - Epoch [239/300], Batch [1/43], Training Loss: 0.00002015
2024-11-06 14:07:33,395 - INFO - Epoch [239/300], Batch [2/43], Training Loss: 0.00000394
2024-11-06 14:07:33,398 - INFO - Epoch [239/300], Batch [3/43], Training Loss: 0.00001067
2024-11-06 14:07:33,403 - INFO - Epoch [239/300], Batch [4/43], Training Loss: 0.00001682
2024-11-06 14:07:33,407 - INFO - Epoch [239/300], Batch [5/43], Training Loss: 0.00000844
2024-11-06 14:07:33,410 - INFO - Epoch [239/300], Batch [6/43], Training Loss: 0.00001136
2024-11-06 14:07:33,414 - INFO - Epoch [239/300], Batch [7/43], Training Loss: 0.00002536
2024-11-06 14:07:33,417 - INFO - Epoch [239/300], Batch [8/43], Training Loss: 0.00001055
2024-11-06 14:07:33,421 - INFO - Epoch [239/300], Batch [9/43], Training Loss: 0.00000387
2024-11-06 14:07:33,425 - INFO - Epoch [239/300], Batch [10/43], Training Loss: 0.00001617
2024-11-06 14:07:33,429 - INFO - Epoch [239/300], Batch [11/43], Training Loss: 0.00000835
2024-11-06 14:07:33,432 - INFO - Epoch [239/300], Batch [12/43], Training Loss: 0.00000726
2024-11-06 14:07:33,435 - INFO - Epoch [239/300], Batch [13/43], Training Loss: 0.00000485
2024-11-06 14:07:33,438 - INFO - Epoch [239/300], Batch [14/43], Training Loss: 0.00000592
2024-11-06 14:07:33,442 - INFO - Epoch [239/300], Batch [15/43], Training Loss: 0.00001681
2024-11-06 14:07:33,446 - INFO - Epoch [239/300], Batch [16/43], Training Loss: 0.00001858
2024-11-06 14:07:33,449 - INFO - Epoch [239/300], Batch [17/43], Training Loss: 0.00000526
2024-11-06 14:07:33,452 - INFO - Epoch [239/300], Batch [18/43], Training Loss: 0.00002778
2024-11-06 14:07:33,455 - INFO - Epoch [239/300], Batch [19/43], Training Loss: 0.00000972
2024-11-06 14:07:33,457 - INFO - Epoch [239/300], Batch [20/43], Training Loss: 0.00000592
2024-11-06 14:07:33,461 - INFO - Epoch [239/300], Batch [21/43], Training Loss: 0.00000657
2024-11-06 14:07:33,463 - INFO - Epoch [239/300], Batch [22/43], Training Loss: 0.00002033
2024-11-06 14:07:33,467 - INFO - Epoch [239/300], Batch [23/43], Training Loss: 0.00001350
2024-11-06 14:07:33,471 - INFO - Epoch [239/300], Batch [24/43], Training Loss: 0.00000808
2024-11-06 14:07:33,475 - INFO - Epoch [239/300], Batch [25/43], Training Loss: 0.00001021
2024-11-06 14:07:33,477 - INFO - Epoch [239/300], Batch [26/43], Training Loss: 0.00000623
2024-11-06 14:07:33,480 - INFO - Epoch [239/300], Batch [27/43], Training Loss: 0.00001098
2024-11-06 14:07:33,483 - INFO - Epoch [239/300], Batch [28/43], Training Loss: 0.00000396
2024-11-06 14:07:33,486 - INFO - Epoch [239/300], Batch [29/43], Training Loss: 0.00000658
2024-11-06 14:07:33,489 - INFO - Epoch [239/300], Batch [30/43], Training Loss: 0.00000788
2024-11-06 14:07:33,495 - INFO - Epoch [239/300], Batch [31/43], Training Loss: 0.00000547
2024-11-06 14:07:33,498 - INFO - Epoch [239/300], Batch [32/43], Training Loss: 0.00000499
2024-11-06 14:07:33,501 - INFO - Epoch [239/300], Batch [33/43], Training Loss: 0.00000473
2024-11-06 14:07:33,505 - INFO - Epoch [239/300], Batch [34/43], Training Loss: 0.00001555
2024-11-06 14:07:33,510 - INFO - Epoch [239/300], Batch [35/43], Training Loss: 0.00001644
2024-11-06 14:07:33,513 - INFO - Epoch [239/300], Batch [36/43], Training Loss: 0.00001429
2024-11-06 14:07:33,515 - INFO - Epoch [239/300], Batch [37/43], Training Loss: 0.00002197
2024-11-06 14:07:33,519 - INFO - Epoch [239/300], Batch [38/43], Training Loss: 0.00000842
2024-11-06 14:07:33,524 - INFO - Epoch [239/300], Batch [39/43], Training Loss: 0.00000980
2024-11-06 14:07:33,528 - INFO - Epoch [239/300], Batch [40/43], Training Loss: 0.00001456
2024-11-06 14:07:33,532 - INFO - Epoch [239/300], Batch [41/43], Training Loss: 0.00000661
2024-11-06 14:07:33,537 - INFO - Epoch [239/300], Batch [42/43], Training Loss: 0.00001817
2024-11-06 14:07:33,541 - INFO - Epoch [239/300], Batch [43/43], Training Loss: 0.00000900
2024-11-06 14:07:33,555 - INFO - Epoch [239/300], Average Training Loss: 0.00001121, Validation Loss: 0.00001322
2024-11-06 14:07:33,559 - INFO - Epoch [240/300], Batch [1/43], Training Loss: 0.00000613
2024-11-06 14:07:33,563 - INFO - Epoch [240/300], Batch [2/43], Training Loss: 0.00000694
2024-11-06 14:07:33,566 - INFO - Epoch [240/300], Batch [3/43], Training Loss: 0.00000803
2024-11-06 14:07:33,569 - INFO - Epoch [240/300], Batch [4/43], Training Loss: 0.00001076
2024-11-06 14:07:33,572 - INFO - Epoch [240/300], Batch [5/43], Training Loss: 0.00000863
2024-11-06 14:07:33,575 - INFO - Epoch [240/300], Batch [6/43], Training Loss: 0.00001891
2024-11-06 14:07:33,578 - INFO - Epoch [240/300], Batch [7/43], Training Loss: 0.00000613
2024-11-06 14:07:33,581 - INFO - Epoch [240/300], Batch [8/43], Training Loss: 0.00001208
2024-11-06 14:07:33,585 - INFO - Epoch [240/300], Batch [9/43], Training Loss: 0.00002741
2024-11-06 14:07:33,588 - INFO - Epoch [240/300], Batch [10/43], Training Loss: 0.00002414
2024-11-06 14:07:33,592 - INFO - Epoch [240/300], Batch [11/43], Training Loss: 0.00000521
2024-11-06 14:07:33,595 - INFO - Epoch [240/300], Batch [12/43], Training Loss: 0.00002034
2024-11-06 14:07:33,597 - INFO - Epoch [240/300], Batch [13/43], Training Loss: 0.00001188
2024-11-06 14:07:33,600 - INFO - Epoch [240/300], Batch [14/43], Training Loss: 0.00000525
2024-11-06 14:07:33,604 - INFO - Epoch [240/300], Batch [15/43], Training Loss: 0.00000479
2024-11-06 14:07:33,607 - INFO - Epoch [240/300], Batch [16/43], Training Loss: 0.00001125
2024-11-06 14:07:33,610 - INFO - Epoch [240/300], Batch [17/43], Training Loss: 0.00001451
2024-11-06 14:07:33,613 - INFO - Epoch [240/300], Batch [18/43], Training Loss: 0.00000610
2024-11-06 14:07:33,616 - INFO - Epoch [240/300], Batch [19/43], Training Loss: 0.00000801
2024-11-06 14:07:33,619 - INFO - Epoch [240/300], Batch [20/43], Training Loss: 0.00000613
2024-11-06 14:07:33,623 - INFO - Epoch [240/300], Batch [21/43], Training Loss: 0.00000990
2024-11-06 14:07:33,626 - INFO - Epoch [240/300], Batch [22/43], Training Loss: 0.00002397
2024-11-06 14:07:33,629 - INFO - Epoch [240/300], Batch [23/43], Training Loss: 0.00002607
2024-11-06 14:07:33,632 - INFO - Epoch [240/300], Batch [24/43], Training Loss: 0.00000724
2024-11-06 14:07:33,636 - INFO - Epoch [240/300], Batch [25/43], Training Loss: 0.00000841
2024-11-06 14:07:33,640 - INFO - Epoch [240/300], Batch [26/43], Training Loss: 0.00000874
2024-11-06 14:07:33,643 - INFO - Epoch [240/300], Batch [27/43], Training Loss: 0.00001639
2024-11-06 14:07:33,646 - INFO - Epoch [240/300], Batch [28/43], Training Loss: 0.00000911
2024-11-06 14:07:33,650 - INFO - Epoch [240/300], Batch [29/43], Training Loss: 0.00000357
2024-11-06 14:07:33,654 - INFO - Epoch [240/300], Batch [30/43], Training Loss: 0.00001669
2024-11-06 14:07:33,658 - INFO - Epoch [240/300], Batch [31/43], Training Loss: 0.00000468
2024-11-06 14:07:33,661 - INFO - Epoch [240/300], Batch [32/43], Training Loss: 0.00001264
2024-11-06 14:07:33,666 - INFO - Epoch [240/300], Batch [33/43], Training Loss: 0.00001665
2024-11-06 14:07:33,669 - INFO - Epoch [240/300], Batch [34/43], Training Loss: 0.00000655
2024-11-06 14:07:33,673 - INFO - Epoch [240/300], Batch [35/43], Training Loss: 0.00001098
2024-11-06 14:07:33,677 - INFO - Epoch [240/300], Batch [36/43], Training Loss: 0.00001132
2024-11-06 14:07:33,680 - INFO - Epoch [240/300], Batch [37/43], Training Loss: 0.00001008
2024-11-06 14:07:33,685 - INFO - Epoch [240/300], Batch [38/43], Training Loss: 0.00000743
2024-11-06 14:07:33,689 - INFO - Epoch [240/300], Batch [39/43], Training Loss: 0.00002544
2024-11-06 14:07:33,693 - INFO - Epoch [240/300], Batch [40/43], Training Loss: 0.00002110
2024-11-06 14:07:33,696 - INFO - Epoch [240/300], Batch [41/43], Training Loss: 0.00000380
2024-11-06 14:07:33,699 - INFO - Epoch [240/300], Batch [42/43], Training Loss: 0.00000526
2024-11-06 14:07:33,704 - INFO - Epoch [240/300], Batch [43/43], Training Loss: 0.00001944
2024-11-06 14:07:33,716 - INFO - Epoch [240/300], Average Training Loss: 0.00001182, Validation Loss: 0.00001311
2024-11-06 14:07:33,720 - INFO - Epoch [241/300], Batch [1/43], Training Loss: 0.00001273
2024-11-06 14:07:33,723 - INFO - Epoch [241/300], Batch [2/43], Training Loss: 0.00001050
2024-11-06 14:07:33,727 - INFO - Epoch [241/300], Batch [3/43], Training Loss: 0.00000802
2024-11-06 14:07:33,731 - INFO - Epoch [241/300], Batch [4/43], Training Loss: 0.00001106
2024-11-06 14:07:33,734 - INFO - Epoch [241/300], Batch [5/43], Training Loss: 0.00000636
2024-11-06 14:07:33,738 - INFO - Epoch [241/300], Batch [6/43], Training Loss: 0.00001392
2024-11-06 14:07:33,742 - INFO - Epoch [241/300], Batch [7/43], Training Loss: 0.00000907
2024-11-06 14:07:33,746 - INFO - Epoch [241/300], Batch [8/43], Training Loss: 0.00001577
2024-11-06 14:07:33,749 - INFO - Epoch [241/300], Batch [9/43], Training Loss: 0.00001713
2024-11-06 14:07:33,752 - INFO - Epoch [241/300], Batch [10/43], Training Loss: 0.00000722
2024-11-06 14:07:33,755 - INFO - Epoch [241/300], Batch [11/43], Training Loss: 0.00000944
2024-11-06 14:07:33,759 - INFO - Epoch [241/300], Batch [12/43], Training Loss: 0.00000956
2024-11-06 14:07:33,762 - INFO - Epoch [241/300], Batch [13/43], Training Loss: 0.00001266
2024-11-06 14:07:33,766 - INFO - Epoch [241/300], Batch [14/43], Training Loss: 0.00000963
2024-11-06 14:07:33,769 - INFO - Epoch [241/300], Batch [15/43], Training Loss: 0.00002314
2024-11-06 14:07:33,775 - INFO - Epoch [241/300], Batch [16/43], Training Loss: 0.00001730
2024-11-06 14:07:33,779 - INFO - Epoch [241/300], Batch [17/43], Training Loss: 0.00001189
2024-11-06 14:07:33,784 - INFO - Epoch [241/300], Batch [18/43], Training Loss: 0.00000379
2024-11-06 14:07:33,788 - INFO - Epoch [241/300], Batch [19/43], Training Loss: 0.00000673
2024-11-06 14:07:33,793 - INFO - Epoch [241/300], Batch [20/43], Training Loss: 0.00000730
2024-11-06 14:07:33,797 - INFO - Epoch [241/300], Batch [21/43], Training Loss: 0.00000513
2024-11-06 14:07:33,801 - INFO - Epoch [241/300], Batch [22/43], Training Loss: 0.00001752
2024-11-06 14:07:33,806 - INFO - Epoch [241/300], Batch [23/43], Training Loss: 0.00001293
2024-11-06 14:07:33,810 - INFO - Epoch [241/300], Batch [24/43], Training Loss: 0.00001735
2024-11-06 14:07:33,815 - INFO - Epoch [241/300], Batch [25/43], Training Loss: 0.00002599
2024-11-06 14:07:33,820 - INFO - Epoch [241/300], Batch [26/43], Training Loss: 0.00000610
2024-11-06 14:07:33,826 - INFO - Epoch [241/300], Batch [27/43], Training Loss: 0.00001024
2024-11-06 14:07:33,830 - INFO - Epoch [241/300], Batch [28/43], Training Loss: 0.00001170
2024-11-06 14:07:33,834 - INFO - Epoch [241/300], Batch [29/43], Training Loss: 0.00000873
2024-11-06 14:07:33,838 - INFO - Epoch [241/300], Batch [30/43], Training Loss: 0.00000488
2024-11-06 14:07:33,843 - INFO - Epoch [241/300], Batch [31/43], Training Loss: 0.00000409
2024-11-06 14:07:33,847 - INFO - Epoch [241/300], Batch [32/43], Training Loss: 0.00000799
2024-11-06 14:07:33,851 - INFO - Epoch [241/300], Batch [33/43], Training Loss: 0.00001567
2024-11-06 14:07:33,856 - INFO - Epoch [241/300], Batch [34/43], Training Loss: 0.00000304
2024-11-06 14:07:33,860 - INFO - Epoch [241/300], Batch [35/43], Training Loss: 0.00000294
2024-11-06 14:07:33,864 - INFO - Epoch [241/300], Batch [36/43], Training Loss: 0.00000782
2024-11-06 14:07:33,868 - INFO - Epoch [241/300], Batch [37/43], Training Loss: 0.00001048
2024-11-06 14:07:33,872 - INFO - Epoch [241/300], Batch [38/43], Training Loss: 0.00000572
2024-11-06 14:07:33,876 - INFO - Epoch [241/300], Batch [39/43], Training Loss: 0.00001154
2024-11-06 14:07:33,880 - INFO - Epoch [241/300], Batch [40/43], Training Loss: 0.00002388
2024-11-06 14:07:33,885 - INFO - Epoch [241/300], Batch [41/43], Training Loss: 0.00001656
2024-11-06 14:07:33,889 - INFO - Epoch [241/300], Batch [42/43], Training Loss: 0.00000497
2024-11-06 14:07:33,893 - INFO - Epoch [241/300], Batch [43/43], Training Loss: 0.00000816
2024-11-06 14:07:33,904 - INFO - Epoch [241/300], Average Training Loss: 0.00001085, Validation Loss: 0.00001360
2024-11-06 14:07:33,908 - INFO - Epoch [242/300], Batch [1/43], Training Loss: 0.00000312
2024-11-06 14:07:33,913 - INFO - Epoch [242/300], Batch [2/43], Training Loss: 0.00000485
2024-11-06 14:07:33,917 - INFO - Epoch [242/300], Batch [3/43], Training Loss: 0.00001378
2024-11-06 14:07:33,921 - INFO - Epoch [242/300], Batch [4/43], Training Loss: 0.00000864
2024-11-06 14:07:33,925 - INFO - Epoch [242/300], Batch [5/43], Training Loss: 0.00000605
2024-11-06 14:07:33,930 - INFO - Epoch [242/300], Batch [6/43], Training Loss: 0.00001276
2024-11-06 14:07:33,934 - INFO - Epoch [242/300], Batch [7/43], Training Loss: 0.00002430
2024-11-06 14:07:33,938 - INFO - Epoch [242/300], Batch [8/43], Training Loss: 0.00001011
2024-11-06 14:07:33,943 - INFO - Epoch [242/300], Batch [9/43], Training Loss: 0.00000636
2024-11-06 14:07:33,948 - INFO - Epoch [242/300], Batch [10/43], Training Loss: 0.00000267
2024-11-06 14:07:33,951 - INFO - Epoch [242/300], Batch [11/43], Training Loss: 0.00000554
2024-11-06 14:07:33,955 - INFO - Epoch [242/300], Batch [12/43], Training Loss: 0.00000931
2024-11-06 14:07:33,959 - INFO - Epoch [242/300], Batch [13/43], Training Loss: 0.00000521
2024-11-06 14:07:33,964 - INFO - Epoch [242/300], Batch [14/43], Training Loss: 0.00000635
2024-11-06 14:07:33,968 - INFO - Epoch [242/300], Batch [15/43], Training Loss: 0.00001009
2024-11-06 14:07:33,972 - INFO - Epoch [242/300], Batch [16/43], Training Loss: 0.00001140
2024-11-06 14:07:33,976 - INFO - Epoch [242/300], Batch [17/43], Training Loss: 0.00001459
2024-11-06 14:07:33,980 - INFO - Epoch [242/300], Batch [18/43], Training Loss: 0.00001227
2024-11-06 14:07:33,984 - INFO - Epoch [242/300], Batch [19/43], Training Loss: 0.00000432
2024-11-06 14:07:33,988 - INFO - Epoch [242/300], Batch [20/43], Training Loss: 0.00000916
2024-11-06 14:07:33,992 - INFO - Epoch [242/300], Batch [21/43], Training Loss: 0.00000702
2024-11-06 14:07:33,995 - INFO - Epoch [242/300], Batch [22/43], Training Loss: 0.00000856
2024-11-06 14:07:33,999 - INFO - Epoch [242/300], Batch [23/43], Training Loss: 0.00001472
2024-11-06 14:07:34,002 - INFO - Epoch [242/300], Batch [24/43], Training Loss: 0.00001568
2024-11-06 14:07:34,005 - INFO - Epoch [242/300], Batch [25/43], Training Loss: 0.00000980
2024-11-06 14:07:34,009 - INFO - Epoch [242/300], Batch [26/43], Training Loss: 0.00001029
2024-11-06 14:07:34,013 - INFO - Epoch [242/300], Batch [27/43], Training Loss: 0.00001059
2024-11-06 14:07:34,016 - INFO - Epoch [242/300], Batch [28/43], Training Loss: 0.00000452
2024-11-06 14:07:34,019 - INFO - Epoch [242/300], Batch [29/43], Training Loss: 0.00000459
2024-11-06 14:07:34,022 - INFO - Epoch [242/300], Batch [30/43], Training Loss: 0.00000966
2024-11-06 14:07:34,025 - INFO - Epoch [242/300], Batch [31/43], Training Loss: 0.00000477
2024-11-06 14:07:34,028 - INFO - Epoch [242/300], Batch [32/43], Training Loss: 0.00001064
2024-11-06 14:07:34,032 - INFO - Epoch [242/300], Batch [33/43], Training Loss: 0.00001313
2024-11-06 14:07:34,035 - INFO - Epoch [242/300], Batch [34/43], Training Loss: 0.00000465
2024-11-06 14:07:34,038 - INFO - Epoch [242/300], Batch [35/43], Training Loss: 0.00000559
2024-11-06 14:07:34,041 - INFO - Epoch [242/300], Batch [36/43], Training Loss: 0.00000357
2024-11-06 14:07:34,044 - INFO - Epoch [242/300], Batch [37/43], Training Loss: 0.00000383
2024-11-06 14:07:34,047 - INFO - Epoch [242/300], Batch [38/43], Training Loss: 0.00001428
2024-11-06 14:07:34,050 - INFO - Epoch [242/300], Batch [39/43], Training Loss: 0.00000777
2024-11-06 14:07:34,052 - INFO - Epoch [242/300], Batch [40/43], Training Loss: 0.00001731
2024-11-06 14:07:34,055 - INFO - Epoch [242/300], Batch [41/43], Training Loss: 0.00002341
2024-11-06 14:07:34,058 - INFO - Epoch [242/300], Batch [42/43], Training Loss: 0.00001294
2024-11-06 14:07:34,062 - INFO - Epoch [242/300], Batch [43/43], Training Loss: 0.00000802
2024-11-06 14:07:34,074 - INFO - Epoch [242/300], Average Training Loss: 0.00000945, Validation Loss: 0.00001369
2024-11-06 14:07:34,077 - INFO - Epoch [243/300], Batch [1/43], Training Loss: 0.00000914
2024-11-06 14:07:34,081 - INFO - Epoch [243/300], Batch [2/43], Training Loss: 0.00001233
2024-11-06 14:07:34,085 - INFO - Epoch [243/300], Batch [3/43], Training Loss: 0.00001461
2024-11-06 14:07:34,088 - INFO - Epoch [243/300], Batch [4/43], Training Loss: 0.00000478
2024-11-06 14:07:34,092 - INFO - Epoch [243/300], Batch [5/43], Training Loss: 0.00001440
2024-11-06 14:07:34,097 - INFO - Epoch [243/300], Batch [6/43], Training Loss: 0.00000817
2024-11-06 14:07:34,101 - INFO - Epoch [243/300], Batch [7/43], Training Loss: 0.00000710
2024-11-06 14:07:34,106 - INFO - Epoch [243/300], Batch [8/43], Training Loss: 0.00001062
2024-11-06 14:07:34,112 - INFO - Epoch [243/300], Batch [9/43], Training Loss: 0.00000936
2024-11-06 14:07:34,117 - INFO - Epoch [243/300], Batch [10/43], Training Loss: 0.00000861
2024-11-06 14:07:34,122 - INFO - Epoch [243/300], Batch [11/43], Training Loss: 0.00000305
2024-11-06 14:07:34,126 - INFO - Epoch [243/300], Batch [12/43], Training Loss: 0.00001306
2024-11-06 14:07:34,131 - INFO - Epoch [243/300], Batch [13/43], Training Loss: 0.00001075
2024-11-06 14:07:34,135 - INFO - Epoch [243/300], Batch [14/43], Training Loss: 0.00001229
2024-11-06 14:07:34,139 - INFO - Epoch [243/300], Batch [15/43], Training Loss: 0.00000607
2024-11-06 14:07:34,143 - INFO - Epoch [243/300], Batch [16/43], Training Loss: 0.00001114
2024-11-06 14:07:34,147 - INFO - Epoch [243/300], Batch [17/43], Training Loss: 0.00001138
2024-11-06 14:07:34,150 - INFO - Epoch [243/300], Batch [18/43], Training Loss: 0.00001184
2024-11-06 14:07:34,153 - INFO - Epoch [243/300], Batch [19/43], Training Loss: 0.00000842
2024-11-06 14:07:34,157 - INFO - Epoch [243/300], Batch [20/43], Training Loss: 0.00001907
2024-11-06 14:07:34,160 - INFO - Epoch [243/300], Batch [21/43], Training Loss: 0.00001197
2024-11-06 14:07:34,163 - INFO - Epoch [243/300], Batch [22/43], Training Loss: 0.00002398
2024-11-06 14:07:34,166 - INFO - Epoch [243/300], Batch [23/43], Training Loss: 0.00000872
2024-11-06 14:07:34,170 - INFO - Epoch [243/300], Batch [24/43], Training Loss: 0.00000494
2024-11-06 14:07:34,173 - INFO - Epoch [243/300], Batch [25/43], Training Loss: 0.00001386
2024-11-06 14:07:34,176 - INFO - Epoch [243/300], Batch [26/43], Training Loss: 0.00000254
2024-11-06 14:07:34,179 - INFO - Epoch [243/300], Batch [27/43], Training Loss: 0.00000730
2024-11-06 14:07:34,183 - INFO - Epoch [243/300], Batch [28/43], Training Loss: 0.00000740
2024-11-06 14:07:34,186 - INFO - Epoch [243/300], Batch [29/43], Training Loss: 0.00000880
2024-11-06 14:07:34,189 - INFO - Epoch [243/300], Batch [30/43], Training Loss: 0.00002248
2024-11-06 14:07:34,192 - INFO - Epoch [243/300], Batch [31/43], Training Loss: 0.00000621
2024-11-06 14:07:34,195 - INFO - Epoch [243/300], Batch [32/43], Training Loss: 0.00000932
2024-11-06 14:07:34,198 - INFO - Epoch [243/300], Batch [33/43], Training Loss: 0.00000426
2024-11-06 14:07:34,201 - INFO - Epoch [243/300], Batch [34/43], Training Loss: 0.00000657
2024-11-06 14:07:34,204 - INFO - Epoch [243/300], Batch [35/43], Training Loss: 0.00000379
2024-11-06 14:07:34,208 - INFO - Epoch [243/300], Batch [36/43], Training Loss: 0.00000611
2024-11-06 14:07:34,211 - INFO - Epoch [243/300], Batch [37/43], Training Loss: 0.00000647
2024-11-06 14:07:34,215 - INFO - Epoch [243/300], Batch [38/43], Training Loss: 0.00000264
2024-11-06 14:07:34,218 - INFO - Epoch [243/300], Batch [39/43], Training Loss: 0.00000365
2024-11-06 14:07:34,222 - INFO - Epoch [243/300], Batch [40/43], Training Loss: 0.00000472
2024-11-06 14:07:34,225 - INFO - Epoch [243/300], Batch [41/43], Training Loss: 0.00000658
2024-11-06 14:07:34,229 - INFO - Epoch [243/300], Batch [42/43], Training Loss: 0.00002930
2024-11-06 14:07:34,232 - INFO - Epoch [243/300], Batch [43/43], Training Loss: 0.00000594
2024-11-06 14:07:34,243 - INFO - Epoch [243/300], Average Training Loss: 0.00000962, Validation Loss: 0.00001358
2024-11-06 14:07:34,247 - INFO - Epoch [244/300], Batch [1/43], Training Loss: 0.00001008
2024-11-06 14:07:34,252 - INFO - Epoch [244/300], Batch [2/43], Training Loss: 0.00000684
2024-11-06 14:07:34,255 - INFO - Epoch [244/300], Batch [3/43], Training Loss: 0.00000239
2024-11-06 14:07:34,261 - INFO - Epoch [244/300], Batch [4/43], Training Loss: 0.00000342
2024-11-06 14:07:34,266 - INFO - Epoch [244/300], Batch [5/43], Training Loss: 0.00000819
2024-11-06 14:07:34,271 - INFO - Epoch [244/300], Batch [6/43], Training Loss: 0.00000637
2024-11-06 14:07:34,276 - INFO - Epoch [244/300], Batch [7/43], Training Loss: 0.00000818
2024-11-06 14:07:34,281 - INFO - Epoch [244/300], Batch [8/43], Training Loss: 0.00000684
2024-11-06 14:07:34,286 - INFO - Epoch [244/300], Batch [9/43], Training Loss: 0.00000888
2024-11-06 14:07:34,291 - INFO - Epoch [244/300], Batch [10/43], Training Loss: 0.00000620
2024-11-06 14:07:34,297 - INFO - Epoch [244/300], Batch [11/43], Training Loss: 0.00000948
2024-11-06 14:07:34,301 - INFO - Epoch [244/300], Batch [12/43], Training Loss: 0.00000435
2024-11-06 14:07:34,305 - INFO - Epoch [244/300], Batch [13/43], Training Loss: 0.00001885
2024-11-06 14:07:34,310 - INFO - Epoch [244/300], Batch [14/43], Training Loss: 0.00000599
2024-11-06 14:07:34,315 - INFO - Epoch [244/300], Batch [15/43], Training Loss: 0.00001086
2024-11-06 14:07:34,319 - INFO - Epoch [244/300], Batch [16/43], Training Loss: 0.00000753
2024-11-06 14:07:34,324 - INFO - Epoch [244/300], Batch [17/43], Training Loss: 0.00001160
2024-11-06 14:07:34,328 - INFO - Epoch [244/300], Batch [18/43], Training Loss: 0.00000590
2024-11-06 14:07:34,332 - INFO - Epoch [244/300], Batch [19/43], Training Loss: 0.00001426
2024-11-06 14:07:34,336 - INFO - Epoch [244/300], Batch [20/43], Training Loss: 0.00000823
2024-11-06 14:07:34,339 - INFO - Epoch [244/300], Batch [21/43], Training Loss: 0.00001218
2024-11-06 14:07:34,343 - INFO - Epoch [244/300], Batch [22/43], Training Loss: 0.00000544
2024-11-06 14:07:34,348 - INFO - Epoch [244/300], Batch [23/43], Training Loss: 0.00001847
2024-11-06 14:07:34,353 - INFO - Epoch [244/300], Batch [24/43], Training Loss: 0.00001589
2024-11-06 14:07:34,358 - INFO - Epoch [244/300], Batch [25/43], Training Loss: 0.00000519
2024-11-06 14:07:34,362 - INFO - Epoch [244/300], Batch [26/43], Training Loss: 0.00002061
2024-11-06 14:07:34,366 - INFO - Epoch [244/300], Batch [27/43], Training Loss: 0.00001361
2024-11-06 14:07:34,370 - INFO - Epoch [244/300], Batch [28/43], Training Loss: 0.00001514
2024-11-06 14:07:34,374 - INFO - Epoch [244/300], Batch [29/43], Training Loss: 0.00001174
2024-11-06 14:07:34,378 - INFO - Epoch [244/300], Batch [30/43], Training Loss: 0.00001149
2024-11-06 14:07:34,383 - INFO - Epoch [244/300], Batch [31/43], Training Loss: 0.00000908
2024-11-06 14:07:34,386 - INFO - Epoch [244/300], Batch [32/43], Training Loss: 0.00000516
2024-11-06 14:07:34,390 - INFO - Epoch [244/300], Batch [33/43], Training Loss: 0.00000448
2024-11-06 14:07:34,395 - INFO - Epoch [244/300], Batch [34/43], Training Loss: 0.00001460
2024-11-06 14:07:34,400 - INFO - Epoch [244/300], Batch [35/43], Training Loss: 0.00001170
2024-11-06 14:07:34,403 - INFO - Epoch [244/300], Batch [36/43], Training Loss: 0.00000531
2024-11-06 14:07:34,408 - INFO - Epoch [244/300], Batch [37/43], Training Loss: 0.00001692
2024-11-06 14:07:34,412 - INFO - Epoch [244/300], Batch [38/43], Training Loss: 0.00001809
2024-11-06 14:07:34,416 - INFO - Epoch [244/300], Batch [39/43], Training Loss: 0.00000748
2024-11-06 14:07:34,422 - INFO - Epoch [244/300], Batch [40/43], Training Loss: 0.00000252
2024-11-06 14:07:34,426 - INFO - Epoch [244/300], Batch [41/43], Training Loss: 0.00001454
2024-11-06 14:07:34,431 - INFO - Epoch [244/300], Batch [42/43], Training Loss: 0.00001665
2024-11-06 14:07:34,436 - INFO - Epoch [244/300], Batch [43/43], Training Loss: 0.00001461
2024-11-06 14:07:34,449 - INFO - Epoch [244/300], Average Training Loss: 0.00001012, Validation Loss: 0.00001356
2024-11-06 14:07:34,454 - INFO - Epoch [245/300], Batch [1/43], Training Loss: 0.00001516
2024-11-06 14:07:34,458 - INFO - Epoch [245/300], Batch [2/43], Training Loss: 0.00000672
2024-11-06 14:07:34,462 - INFO - Epoch [245/300], Batch [3/43], Training Loss: 0.00000650
2024-11-06 14:07:34,466 - INFO - Epoch [245/300], Batch [4/43], Training Loss: 0.00000920
2024-11-06 14:07:34,470 - INFO - Epoch [245/300], Batch [5/43], Training Loss: 0.00001087
2024-11-06 14:07:34,475 - INFO - Epoch [245/300], Batch [6/43], Training Loss: 0.00000967
2024-11-06 14:07:34,481 - INFO - Epoch [245/300], Batch [7/43], Training Loss: 0.00000632
2024-11-06 14:07:34,487 - INFO - Epoch [245/300], Batch [8/43], Training Loss: 0.00000759
2024-11-06 14:07:34,490 - INFO - Epoch [245/300], Batch [9/43], Training Loss: 0.00000486
2024-11-06 14:07:34,494 - INFO - Epoch [245/300], Batch [10/43], Training Loss: 0.00001630
2024-11-06 14:07:34,498 - INFO - Epoch [245/300], Batch [11/43], Training Loss: 0.00002867
2024-11-06 14:07:34,502 - INFO - Epoch [245/300], Batch [12/43], Training Loss: 0.00000618
2024-11-06 14:07:34,506 - INFO - Epoch [245/300], Batch [13/43], Training Loss: 0.00001172
2024-11-06 14:07:34,509 - INFO - Epoch [245/300], Batch [14/43], Training Loss: 0.00000560
2024-11-06 14:07:34,513 - INFO - Epoch [245/300], Batch [15/43], Training Loss: 0.00000570
2024-11-06 14:07:34,517 - INFO - Epoch [245/300], Batch [16/43], Training Loss: 0.00000626
2024-11-06 14:07:34,521 - INFO - Epoch [245/300], Batch [17/43], Training Loss: 0.00000455
2024-11-06 14:07:34,526 - INFO - Epoch [245/300], Batch [18/43], Training Loss: 0.00000433
2024-11-06 14:07:34,530 - INFO - Epoch [245/300], Batch [19/43], Training Loss: 0.00002924
2024-11-06 14:07:34,535 - INFO - Epoch [245/300], Batch [20/43], Training Loss: 0.00000440
2024-11-06 14:07:34,539 - INFO - Epoch [245/300], Batch [21/43], Training Loss: 0.00000994
2024-11-06 14:07:34,544 - INFO - Epoch [245/300], Batch [22/43], Training Loss: 0.00001323
2024-11-06 14:07:34,549 - INFO - Epoch [245/300], Batch [23/43], Training Loss: 0.00000357
2024-11-06 14:07:34,553 - INFO - Epoch [245/300], Batch [24/43], Training Loss: 0.00000672
2024-11-06 14:07:34,556 - INFO - Epoch [245/300], Batch [25/43], Training Loss: 0.00000203
2024-11-06 14:07:34,560 - INFO - Epoch [245/300], Batch [26/43], Training Loss: 0.00000455
2024-11-06 14:07:34,564 - INFO - Epoch [245/300], Batch [27/43], Training Loss: 0.00001304
2024-11-06 14:07:34,568 - INFO - Epoch [245/300], Batch [28/43], Training Loss: 0.00000627
2024-11-06 14:07:34,573 - INFO - Epoch [245/300], Batch [29/43], Training Loss: 0.00000791
2024-11-06 14:07:34,578 - INFO - Epoch [245/300], Batch [30/43], Training Loss: 0.00000810
2024-11-06 14:07:34,581 - INFO - Epoch [245/300], Batch [31/43], Training Loss: 0.00000516
2024-11-06 14:07:34,585 - INFO - Epoch [245/300], Batch [32/43], Training Loss: 0.00000923
2024-11-06 14:07:34,590 - INFO - Epoch [245/300], Batch [33/43], Training Loss: 0.00001235
2024-11-06 14:07:34,594 - INFO - Epoch [245/300], Batch [34/43], Training Loss: 0.00000708
2024-11-06 14:07:34,598 - INFO - Epoch [245/300], Batch [35/43], Training Loss: 0.00000759
2024-11-06 14:07:34,603 - INFO - Epoch [245/300], Batch [36/43], Training Loss: 0.00000496
2024-11-06 14:07:34,607 - INFO - Epoch [245/300], Batch [37/43], Training Loss: 0.00000865
2024-11-06 14:07:34,611 - INFO - Epoch [245/300], Batch [38/43], Training Loss: 0.00001109
2024-11-06 14:07:34,615 - INFO - Epoch [245/300], Batch [39/43], Training Loss: 0.00000285
2024-11-06 14:07:34,619 - INFO - Epoch [245/300], Batch [40/43], Training Loss: 0.00001589
2024-11-06 14:07:34,623 - INFO - Epoch [245/300], Batch [41/43], Training Loss: 0.00002335
2024-11-06 14:07:34,627 - INFO - Epoch [245/300], Batch [42/43], Training Loss: 0.00001523
2024-11-06 14:07:34,631 - INFO - Epoch [245/300], Batch [43/43], Training Loss: 0.00001318
2024-11-06 14:07:34,642 - INFO - Epoch [245/300], Average Training Loss: 0.00000958, Validation Loss: 0.00001228
2024-11-06 14:07:34,646 - INFO - Epoch [246/300], Batch [1/43], Training Loss: 0.00001670
2024-11-06 14:07:34,650 - INFO - Epoch [246/300], Batch [2/43], Training Loss: 0.00001705
2024-11-06 14:07:34,653 - INFO - Epoch [246/300], Batch [3/43], Training Loss: 0.00000988
2024-11-06 14:07:34,656 - INFO - Epoch [246/300], Batch [4/43], Training Loss: 0.00000833
2024-11-06 14:07:34,659 - INFO - Epoch [246/300], Batch [5/43], Training Loss: 0.00001215
2024-11-06 14:07:34,662 - INFO - Epoch [246/300], Batch [6/43], Training Loss: 0.00001327
2024-11-06 14:07:34,665 - INFO - Epoch [246/300], Batch [7/43], Training Loss: 0.00002661
2024-11-06 14:07:34,668 - INFO - Epoch [246/300], Batch [8/43], Training Loss: 0.00000722
2024-11-06 14:07:34,671 - INFO - Epoch [246/300], Batch [9/43], Training Loss: 0.00000341
2024-11-06 14:07:34,674 - INFO - Epoch [246/300], Batch [10/43], Training Loss: 0.00000672
2024-11-06 14:07:34,676 - INFO - Epoch [246/300], Batch [11/43], Training Loss: 0.00000737
2024-11-06 14:07:34,679 - INFO - Epoch [246/300], Batch [12/43], Training Loss: 0.00001699
2024-11-06 14:07:34,682 - INFO - Epoch [246/300], Batch [13/43], Training Loss: 0.00001131
2024-11-06 14:07:34,685 - INFO - Epoch [246/300], Batch [14/43], Training Loss: 0.00000494
2024-11-06 14:07:34,689 - INFO - Epoch [246/300], Batch [15/43], Training Loss: 0.00000519
2024-11-06 14:07:34,693 - INFO - Epoch [246/300], Batch [16/43], Training Loss: 0.00002306
2024-11-06 14:07:34,696 - INFO - Epoch [246/300], Batch [17/43], Training Loss: 0.00001005
2024-11-06 14:07:34,699 - INFO - Epoch [246/300], Batch [18/43], Training Loss: 0.00000933
2024-11-06 14:07:34,702 - INFO - Epoch [246/300], Batch [19/43], Training Loss: 0.00000599
2024-11-06 14:07:34,705 - INFO - Epoch [246/300], Batch [20/43], Training Loss: 0.00001694
2024-11-06 14:07:34,708 - INFO - Epoch [246/300], Batch [21/43], Training Loss: 0.00000719
2024-11-06 14:07:34,711 - INFO - Epoch [246/300], Batch [22/43], Training Loss: 0.00000689
2024-11-06 14:07:34,714 - INFO - Epoch [246/300], Batch [23/43], Training Loss: 0.00001735
2024-11-06 14:07:34,718 - INFO - Epoch [246/300], Batch [24/43], Training Loss: 0.00000468
2024-11-06 14:07:34,721 - INFO - Epoch [246/300], Batch [25/43], Training Loss: 0.00000790
2024-11-06 14:07:34,725 - INFO - Epoch [246/300], Batch [26/43], Training Loss: 0.00000586
2024-11-06 14:07:34,728 - INFO - Epoch [246/300], Batch [27/43], Training Loss: 0.00000745
2024-11-06 14:07:34,732 - INFO - Epoch [246/300], Batch [28/43], Training Loss: 0.00000456
2024-11-06 14:07:34,737 - INFO - Epoch [246/300], Batch [29/43], Training Loss: 0.00001060
2024-11-06 14:07:34,741 - INFO - Epoch [246/300], Batch [30/43], Training Loss: 0.00001284
2024-11-06 14:07:34,745 - INFO - Epoch [246/300], Batch [31/43], Training Loss: 0.00000864
2024-11-06 14:07:34,749 - INFO - Epoch [246/300], Batch [32/43], Training Loss: 0.00001018
2024-11-06 14:07:34,752 - INFO - Epoch [246/300], Batch [33/43], Training Loss: 0.00000512
2024-11-06 14:07:34,756 - INFO - Epoch [246/300], Batch [34/43], Training Loss: 0.00000843
2024-11-06 14:07:34,760 - INFO - Epoch [246/300], Batch [35/43], Training Loss: 0.00001380
2024-11-06 14:07:34,764 - INFO - Epoch [246/300], Batch [36/43], Training Loss: 0.00000966
2024-11-06 14:07:34,768 - INFO - Epoch [246/300], Batch [37/43], Training Loss: 0.00001707
2024-11-06 14:07:34,772 - INFO - Epoch [246/300], Batch [38/43], Training Loss: 0.00000862
2024-11-06 14:07:34,776 - INFO - Epoch [246/300], Batch [39/43], Training Loss: 0.00001993
2024-11-06 14:07:34,781 - INFO - Epoch [246/300], Batch [40/43], Training Loss: 0.00000628
2024-11-06 14:07:34,785 - INFO - Epoch [246/300], Batch [41/43], Training Loss: 0.00001540
2024-11-06 14:07:34,789 - INFO - Epoch [246/300], Batch [42/43], Training Loss: 0.00001807
2024-11-06 14:07:34,793 - INFO - Epoch [246/300], Batch [43/43], Training Loss: 0.00000748
2024-11-06 14:07:34,806 - INFO - Epoch [246/300], Average Training Loss: 0.00001085, Validation Loss: 0.00001267
2024-11-06 14:07:34,811 - INFO - Epoch [247/300], Batch [1/43], Training Loss: 0.00000205
2024-11-06 14:07:34,816 - INFO - Epoch [247/300], Batch [2/43], Training Loss: 0.00002094
2024-11-06 14:07:34,820 - INFO - Epoch [247/300], Batch [3/43], Training Loss: 0.00000977
2024-11-06 14:07:34,825 - INFO - Epoch [247/300], Batch [4/43], Training Loss: 0.00000598
2024-11-06 14:07:34,829 - INFO - Epoch [247/300], Batch [5/43], Training Loss: 0.00000752
2024-11-06 14:07:34,833 - INFO - Epoch [247/300], Batch [6/43], Training Loss: 0.00000403
2024-11-06 14:07:34,836 - INFO - Epoch [247/300], Batch [7/43], Training Loss: 0.00001439
2024-11-06 14:07:34,840 - INFO - Epoch [247/300], Batch [8/43], Training Loss: 0.00000352
2024-11-06 14:07:34,843 - INFO - Epoch [247/300], Batch [9/43], Training Loss: 0.00000687
2024-11-06 14:07:34,847 - INFO - Epoch [247/300], Batch [10/43], Training Loss: 0.00001190
2024-11-06 14:07:34,851 - INFO - Epoch [247/300], Batch [11/43], Training Loss: 0.00000673
2024-11-06 14:07:34,855 - INFO - Epoch [247/300], Batch [12/43], Training Loss: 0.00001846
2024-11-06 14:07:34,859 - INFO - Epoch [247/300], Batch [13/43], Training Loss: 0.00000373
2024-11-06 14:07:34,863 - INFO - Epoch [247/300], Batch [14/43], Training Loss: 0.00000683
2024-11-06 14:07:34,866 - INFO - Epoch [247/300], Batch [15/43], Training Loss: 0.00002771
2024-11-06 14:07:34,870 - INFO - Epoch [247/300], Batch [16/43], Training Loss: 0.00000444
2024-11-06 14:07:34,875 - INFO - Epoch [247/300], Batch [17/43], Training Loss: 0.00000263
2024-11-06 14:07:34,878 - INFO - Epoch [247/300], Batch [18/43], Training Loss: 0.00000965
2024-11-06 14:07:34,882 - INFO - Epoch [247/300], Batch [19/43], Training Loss: 0.00001096
2024-11-06 14:07:34,887 - INFO - Epoch [247/300], Batch [20/43], Training Loss: 0.00000585
2024-11-06 14:07:34,891 - INFO - Epoch [247/300], Batch [21/43], Training Loss: 0.00001217
2024-11-06 14:07:34,894 - INFO - Epoch [247/300], Batch [22/43], Training Loss: 0.00001188
2024-11-06 14:07:34,898 - INFO - Epoch [247/300], Batch [23/43], Training Loss: 0.00000519
2024-11-06 14:07:34,902 - INFO - Epoch [247/300], Batch [24/43], Training Loss: 0.00003050
2024-11-06 14:07:34,905 - INFO - Epoch [247/300], Batch [25/43], Training Loss: 0.00001825
2024-11-06 14:07:34,908 - INFO - Epoch [247/300], Batch [26/43], Training Loss: 0.00000893
2024-11-06 14:07:34,911 - INFO - Epoch [247/300], Batch [27/43], Training Loss: 0.00002053
2024-11-06 14:07:34,914 - INFO - Epoch [247/300], Batch [28/43], Training Loss: 0.00000728
2024-11-06 14:07:34,918 - INFO - Epoch [247/300], Batch [29/43], Training Loss: 0.00000974
2024-11-06 14:07:34,921 - INFO - Epoch [247/300], Batch [30/43], Training Loss: 0.00000176
2024-11-06 14:07:34,924 - INFO - Epoch [247/300], Batch [31/43], Training Loss: 0.00001153
2024-11-06 14:07:34,927 - INFO - Epoch [247/300], Batch [32/43], Training Loss: 0.00002034
2024-11-06 14:07:34,930 - INFO - Epoch [247/300], Batch [33/43], Training Loss: 0.00000946
2024-11-06 14:07:34,933 - INFO - Epoch [247/300], Batch [34/43], Training Loss: 0.00000554
2024-11-06 14:07:34,936 - INFO - Epoch [247/300], Batch [35/43], Training Loss: 0.00000782
2024-11-06 14:07:34,940 - INFO - Epoch [247/300], Batch [36/43], Training Loss: 0.00001524
2024-11-06 14:07:34,944 - INFO - Epoch [247/300], Batch [37/43], Training Loss: 0.00001211
2024-11-06 14:07:34,947 - INFO - Epoch [247/300], Batch [38/43], Training Loss: 0.00000828
2024-11-06 14:07:34,952 - INFO - Epoch [247/300], Batch [39/43], Training Loss: 0.00000921
2024-11-06 14:07:34,956 - INFO - Epoch [247/300], Batch [40/43], Training Loss: 0.00000493
2024-11-06 14:07:34,960 - INFO - Epoch [247/300], Batch [41/43], Training Loss: 0.00000515
2024-11-06 14:07:34,964 - INFO - Epoch [247/300], Batch [42/43], Training Loss: 0.00001269
2024-11-06 14:07:34,969 - INFO - Epoch [247/300], Batch [43/43], Training Loss: 0.00002861
2024-11-06 14:07:34,982 - INFO - Epoch [247/300], Average Training Loss: 0.00001072, Validation Loss: 0.00001375
2024-11-06 14:07:34,986 - INFO - Epoch [248/300], Batch [1/43], Training Loss: 0.00001093
2024-11-06 14:07:34,991 - INFO - Epoch [248/300], Batch [2/43], Training Loss: 0.00000630
2024-11-06 14:07:34,995 - INFO - Epoch [248/300], Batch [3/43], Training Loss: 0.00000532
2024-11-06 14:07:34,998 - INFO - Epoch [248/300], Batch [4/43], Training Loss: 0.00000553
2024-11-06 14:07:35,001 - INFO - Epoch [248/300], Batch [5/43], Training Loss: 0.00000491
2024-11-06 14:07:35,005 - INFO - Epoch [248/300], Batch [6/43], Training Loss: 0.00001425
2024-11-06 14:07:35,009 - INFO - Epoch [248/300], Batch [7/43], Training Loss: 0.00000241
2024-11-06 14:07:35,012 - INFO - Epoch [248/300], Batch [8/43], Training Loss: 0.00001574
2024-11-06 14:07:35,016 - INFO - Epoch [248/300], Batch [9/43], Training Loss: 0.00001829
2024-11-06 14:07:35,019 - INFO - Epoch [248/300], Batch [10/43], Training Loss: 0.00000586
2024-11-06 14:07:35,023 - INFO - Epoch [248/300], Batch [11/43], Training Loss: 0.00000233
2024-11-06 14:07:35,026 - INFO - Epoch [248/300], Batch [12/43], Training Loss: 0.00001638
2024-11-06 14:07:35,030 - INFO - Epoch [248/300], Batch [13/43], Training Loss: 0.00000394
2024-11-06 14:07:35,033 - INFO - Epoch [248/300], Batch [14/43], Training Loss: 0.00001589
2024-11-06 14:07:35,036 - INFO - Epoch [248/300], Batch [15/43], Training Loss: 0.00000823
2024-11-06 14:07:35,039 - INFO - Epoch [248/300], Batch [16/43], Training Loss: 0.00002837
2024-11-06 14:07:35,042 - INFO - Epoch [248/300], Batch [17/43], Training Loss: 0.00000733
2024-11-06 14:07:35,045 - INFO - Epoch [248/300], Batch [18/43], Training Loss: 0.00001216
2024-11-06 14:07:35,048 - INFO - Epoch [248/300], Batch [19/43], Training Loss: 0.00000291
2024-11-06 14:07:35,051 - INFO - Epoch [248/300], Batch [20/43], Training Loss: 0.00001050
2024-11-06 14:07:35,054 - INFO - Epoch [248/300], Batch [21/43], Training Loss: 0.00001043
2024-11-06 14:07:35,058 - INFO - Epoch [248/300], Batch [22/43], Training Loss: 0.00000886
2024-11-06 14:07:35,062 - INFO - Epoch [248/300], Batch [23/43], Training Loss: 0.00001057
2024-11-06 14:07:35,066 - INFO - Epoch [248/300], Batch [24/43], Training Loss: 0.00001009
2024-11-06 14:07:35,069 - INFO - Epoch [248/300], Batch [25/43], Training Loss: 0.00001643
2024-11-06 14:07:35,073 - INFO - Epoch [248/300], Batch [26/43], Training Loss: 0.00000644
2024-11-06 14:07:35,078 - INFO - Epoch [248/300], Batch [27/43], Training Loss: 0.00000870
2024-11-06 14:07:35,082 - INFO - Epoch [248/300], Batch [28/43], Training Loss: 0.00001364
2024-11-06 14:07:35,086 - INFO - Epoch [248/300], Batch [29/43], Training Loss: 0.00001538
2024-11-06 14:07:35,092 - INFO - Epoch [248/300], Batch [30/43], Training Loss: 0.00000394
2024-11-06 14:07:35,097 - INFO - Epoch [248/300], Batch [31/43], Training Loss: 0.00000540
2024-11-06 14:07:35,102 - INFO - Epoch [248/300], Batch [32/43], Training Loss: 0.00001993
2024-11-06 14:07:35,107 - INFO - Epoch [248/300], Batch [33/43], Training Loss: 0.00000883
2024-11-06 14:07:35,113 - INFO - Epoch [248/300], Batch [34/43], Training Loss: 0.00000352
2024-11-06 14:07:35,118 - INFO - Epoch [248/300], Batch [35/43], Training Loss: 0.00001741
2024-11-06 14:07:35,122 - INFO - Epoch [248/300], Batch [36/43], Training Loss: 0.00001088
2024-11-06 14:07:35,127 - INFO - Epoch [248/300], Batch [37/43], Training Loss: 0.00000730
2024-11-06 14:07:35,131 - INFO - Epoch [248/300], Batch [38/43], Training Loss: 0.00001014
2024-11-06 14:07:35,135 - INFO - Epoch [248/300], Batch [39/43], Training Loss: 0.00002127
2024-11-06 14:07:35,140 - INFO - Epoch [248/300], Batch [40/43], Training Loss: 0.00002574
2024-11-06 14:07:35,143 - INFO - Epoch [248/300], Batch [41/43], Training Loss: 0.00001141
2024-11-06 14:07:35,149 - INFO - Epoch [248/300], Batch [42/43], Training Loss: 0.00000657
2024-11-06 14:07:35,153 - INFO - Epoch [248/300], Batch [43/43], Training Loss: 0.00001402
2024-11-06 14:07:35,166 - INFO - Epoch [248/300], Average Training Loss: 0.00001080, Validation Loss: 0.00001487
2024-11-06 14:07:35,170 - INFO - Epoch [249/300], Batch [1/43], Training Loss: 0.00001744
2024-11-06 14:07:35,175 - INFO - Epoch [249/300], Batch [2/43], Training Loss: 0.00000918
2024-11-06 14:07:35,179 - INFO - Epoch [249/300], Batch [3/43], Training Loss: 0.00000800
2024-11-06 14:07:35,182 - INFO - Epoch [249/300], Batch [4/43], Training Loss: 0.00000256
2024-11-06 14:07:35,186 - INFO - Epoch [249/300], Batch [5/43], Training Loss: 0.00001225
2024-11-06 14:07:35,190 - INFO - Epoch [249/300], Batch [6/43], Training Loss: 0.00001919
2024-11-06 14:07:35,194 - INFO - Epoch [249/300], Batch [7/43], Training Loss: 0.00000217
2024-11-06 14:07:35,198 - INFO - Epoch [249/300], Batch [8/43], Training Loss: 0.00000524
2024-11-06 14:07:35,202 - INFO - Epoch [249/300], Batch [9/43], Training Loss: 0.00000676
2024-11-06 14:07:35,206 - INFO - Epoch [249/300], Batch [10/43], Training Loss: 0.00001323
2024-11-06 14:07:35,210 - INFO - Epoch [249/300], Batch [11/43], Training Loss: 0.00000363
2024-11-06 14:07:35,213 - INFO - Epoch [249/300], Batch [12/43], Training Loss: 0.00001939
2024-11-06 14:07:35,216 - INFO - Epoch [249/300], Batch [13/43], Training Loss: 0.00000246
2024-11-06 14:07:35,219 - INFO - Epoch [249/300], Batch [14/43], Training Loss: 0.00001634
2024-11-06 14:07:35,222 - INFO - Epoch [249/300], Batch [15/43], Training Loss: 0.00001444
2024-11-06 14:07:35,225 - INFO - Epoch [249/300], Batch [16/43], Training Loss: 0.00000705
2024-11-06 14:07:35,228 - INFO - Epoch [249/300], Batch [17/43], Training Loss: 0.00000905
2024-11-06 14:07:35,232 - INFO - Epoch [249/300], Batch [18/43], Training Loss: 0.00000756
2024-11-06 14:07:35,236 - INFO - Epoch [249/300], Batch [19/43], Training Loss: 0.00001497
2024-11-06 14:07:35,239 - INFO - Epoch [249/300], Batch [20/43], Training Loss: 0.00000712
2024-11-06 14:07:35,242 - INFO - Epoch [249/300], Batch [21/43], Training Loss: 0.00002613
2024-11-06 14:07:35,245 - INFO - Epoch [249/300], Batch [22/43], Training Loss: 0.00003058
2024-11-06 14:07:35,247 - INFO - Epoch [249/300], Batch [23/43], Training Loss: 0.00000403
2024-11-06 14:07:35,251 - INFO - Epoch [249/300], Batch [24/43], Training Loss: 0.00000924
2024-11-06 14:07:35,254 - INFO - Epoch [249/300], Batch [25/43], Training Loss: 0.00000680
2024-11-06 14:07:35,258 - INFO - Epoch [249/300], Batch [26/43], Training Loss: 0.00001004
2024-11-06 14:07:35,262 - INFO - Epoch [249/300], Batch [27/43], Training Loss: 0.00001065
2024-11-06 14:07:35,265 - INFO - Epoch [249/300], Batch [28/43], Training Loss: 0.00001433
2024-11-06 14:07:35,268 - INFO - Epoch [249/300], Batch [29/43], Training Loss: 0.00001338
2024-11-06 14:07:35,271 - INFO - Epoch [249/300], Batch [30/43], Training Loss: 0.00000217
2024-11-06 14:07:35,275 - INFO - Epoch [249/300], Batch [31/43], Training Loss: 0.00003103
2024-11-06 14:07:35,278 - INFO - Epoch [249/300], Batch [32/43], Training Loss: 0.00000523
2024-11-06 14:07:35,281 - INFO - Epoch [249/300], Batch [33/43], Training Loss: 0.00001072
2024-11-06 14:07:35,284 - INFO - Epoch [249/300], Batch [34/43], Training Loss: 0.00002690
2024-11-06 14:07:35,287 - INFO - Epoch [249/300], Batch [35/43], Training Loss: 0.00000653
2024-11-06 14:07:35,290 - INFO - Epoch [249/300], Batch [36/43], Training Loss: 0.00000783
2024-11-06 14:07:35,293 - INFO - Epoch [249/300], Batch [37/43], Training Loss: 0.00000401
2024-11-06 14:07:35,296 - INFO - Epoch [249/300], Batch [38/43], Training Loss: 0.00000556
2024-11-06 14:07:35,300 - INFO - Epoch [249/300], Batch [39/43], Training Loss: 0.00000874
2024-11-06 14:07:35,304 - INFO - Epoch [249/300], Batch [40/43], Training Loss: 0.00002096
2024-11-06 14:07:35,307 - INFO - Epoch [249/300], Batch [41/43], Training Loss: 0.00001106
2024-11-06 14:07:35,312 - INFO - Epoch [249/300], Batch [42/43], Training Loss: 0.00000291
2024-11-06 14:07:35,316 - INFO - Epoch [249/300], Batch [43/43], Training Loss: 0.00000428
2024-11-06 14:07:35,328 - INFO - Epoch [249/300], Average Training Loss: 0.00001096, Validation Loss: 0.00001257
2024-11-06 14:07:35,331 - INFO - Epoch [250/300], Batch [1/43], Training Loss: 0.00000477
2024-11-06 14:07:35,335 - INFO - Epoch [250/300], Batch [2/43], Training Loss: 0.00000780
2024-11-06 14:07:35,339 - INFO - Epoch [250/300], Batch [3/43], Training Loss: 0.00000521
2024-11-06 14:07:35,342 - INFO - Epoch [250/300], Batch [4/43], Training Loss: 0.00001800
2024-11-06 14:07:35,346 - INFO - Epoch [250/300], Batch [5/43], Training Loss: 0.00000561
2024-11-06 14:07:35,349 - INFO - Epoch [250/300], Batch [6/43], Training Loss: 0.00001336
2024-11-06 14:07:35,353 - INFO - Epoch [250/300], Batch [7/43], Training Loss: 0.00000947
2024-11-06 14:07:35,356 - INFO - Epoch [250/300], Batch [8/43], Training Loss: 0.00000483
2024-11-06 14:07:35,360 - INFO - Epoch [250/300], Batch [9/43], Training Loss: 0.00002201
2024-11-06 14:07:35,364 - INFO - Epoch [250/300], Batch [10/43], Training Loss: 0.00000921
2024-11-06 14:07:35,367 - INFO - Epoch [250/300], Batch [11/43], Training Loss: 0.00001893
2024-11-06 14:07:35,371 - INFO - Epoch [250/300], Batch [12/43], Training Loss: 0.00001001
2024-11-06 14:07:35,375 - INFO - Epoch [250/300], Batch [13/43], Training Loss: 0.00000954
2024-11-06 14:07:35,378 - INFO - Epoch [250/300], Batch [14/43], Training Loss: 0.00000254
2024-11-06 14:07:35,382 - INFO - Epoch [250/300], Batch [15/43], Training Loss: 0.00001011
2024-11-06 14:07:35,385 - INFO - Epoch [250/300], Batch [16/43], Training Loss: 0.00000872
2024-11-06 14:07:35,388 - INFO - Epoch [250/300], Batch [17/43], Training Loss: 0.00000496
2024-11-06 14:07:35,391 - INFO - Epoch [250/300], Batch [18/43], Training Loss: 0.00000584
2024-11-06 14:07:35,395 - INFO - Epoch [250/300], Batch [19/43], Training Loss: 0.00001709
2024-11-06 14:07:35,398 - INFO - Epoch [250/300], Batch [20/43], Training Loss: 0.00001262
2024-11-06 14:07:35,400 - INFO - Epoch [250/300], Batch [21/43], Training Loss: 0.00002425
2024-11-06 14:07:35,404 - INFO - Epoch [250/300], Batch [22/43], Training Loss: 0.00000782
2024-11-06 14:07:35,408 - INFO - Epoch [250/300], Batch [23/43], Training Loss: 0.00001235
2024-11-06 14:07:35,411 - INFO - Epoch [250/300], Batch [24/43], Training Loss: 0.00000319
2024-11-06 14:07:35,415 - INFO - Epoch [250/300], Batch [25/43], Training Loss: 0.00001938
2024-11-06 14:07:35,418 - INFO - Epoch [250/300], Batch [26/43], Training Loss: 0.00001653
2024-11-06 14:07:35,421 - INFO - Epoch [250/300], Batch [27/43], Training Loss: 0.00000881
2024-11-06 14:07:35,424 - INFO - Epoch [250/300], Batch [28/43], Training Loss: 0.00001463
2024-11-06 14:07:35,427 - INFO - Epoch [250/300], Batch [29/43], Training Loss: 0.00003064
2024-11-06 14:07:35,429 - INFO - Epoch [250/300], Batch [30/43], Training Loss: 0.00001068
2024-11-06 14:07:35,432 - INFO - Epoch [250/300], Batch [31/43], Training Loss: 0.00000663
2024-11-06 14:07:35,437 - INFO - Epoch [250/300], Batch [32/43], Training Loss: 0.00000665
2024-11-06 14:07:35,441 - INFO - Epoch [250/300], Batch [33/43], Training Loss: 0.00001369
2024-11-06 14:07:35,445 - INFO - Epoch [250/300], Batch [34/43], Training Loss: 0.00000537
2024-11-06 14:07:35,449 - INFO - Epoch [250/300], Batch [35/43], Training Loss: 0.00000646
2024-11-06 14:07:35,453 - INFO - Epoch [250/300], Batch [36/43], Training Loss: 0.00001089
2024-11-06 14:07:35,457 - INFO - Epoch [250/300], Batch [37/43], Training Loss: 0.00000380
2024-11-06 14:07:35,461 - INFO - Epoch [250/300], Batch [38/43], Training Loss: 0.00000753
2024-11-06 14:07:35,465 - INFO - Epoch [250/300], Batch [39/43], Training Loss: 0.00000847
2024-11-06 14:07:35,470 - INFO - Epoch [250/300], Batch [40/43], Training Loss: 0.00000446
2024-11-06 14:07:35,474 - INFO - Epoch [250/300], Batch [41/43], Training Loss: 0.00000990
2024-11-06 14:07:35,477 - INFO - Epoch [250/300], Batch [42/43], Training Loss: 0.00000939
2024-11-06 14:07:35,519 - INFO - Epoch [250/300], Batch [43/43], Training Loss: 0.00000653
2024-11-06 14:07:35,548 - INFO - Epoch [250/300], Average Training Loss: 0.00001043, Validation Loss: 0.00001195
2024-11-06 14:07:35,554 - INFO - Epoch [251/300], Batch [1/43], Training Loss: 0.00000851
2024-11-06 14:07:35,559 - INFO - Epoch [251/300], Batch [2/43], Training Loss: 0.00000703
2024-11-06 14:07:35,563 - INFO - Epoch [251/300], Batch [3/43], Training Loss: 0.00000882
2024-11-06 14:07:35,567 - INFO - Epoch [251/300], Batch [4/43], Training Loss: 0.00000857
2024-11-06 14:07:35,570 - INFO - Epoch [251/300], Batch [5/43], Training Loss: 0.00000953
2024-11-06 14:07:35,574 - INFO - Epoch [251/300], Batch [6/43], Training Loss: 0.00001322
2024-11-06 14:07:35,577 - INFO - Epoch [251/300], Batch [7/43], Training Loss: 0.00001102
2024-11-06 14:07:35,580 - INFO - Epoch [251/300], Batch [8/43], Training Loss: 0.00000747
2024-11-06 14:07:35,584 - INFO - Epoch [251/300], Batch [9/43], Training Loss: 0.00002383
2024-11-06 14:07:35,588 - INFO - Epoch [251/300], Batch [10/43], Training Loss: 0.00001591
2024-11-06 14:07:35,591 - INFO - Epoch [251/300], Batch [11/43], Training Loss: 0.00000941
2024-11-06 14:07:35,595 - INFO - Epoch [251/300], Batch [12/43], Training Loss: 0.00000673
2024-11-06 14:07:35,600 - INFO - Epoch [251/300], Batch [13/43], Training Loss: 0.00001572
2024-11-06 14:07:35,603 - INFO - Epoch [251/300], Batch [14/43], Training Loss: 0.00002862
2024-11-06 14:07:35,607 - INFO - Epoch [251/300], Batch [15/43], Training Loss: 0.00000724
2024-11-06 14:07:35,610 - INFO - Epoch [251/300], Batch [16/43], Training Loss: 0.00000478
2024-11-06 14:07:35,614 - INFO - Epoch [251/300], Batch [17/43], Training Loss: 0.00000736
2024-11-06 14:07:35,618 - INFO - Epoch [251/300], Batch [18/43], Training Loss: 0.00000644
2024-11-06 14:07:35,622 - INFO - Epoch [251/300], Batch [19/43], Training Loss: 0.00001282
2024-11-06 14:07:35,626 - INFO - Epoch [251/300], Batch [20/43], Training Loss: 0.00000510
2024-11-06 14:07:35,629 - INFO - Epoch [251/300], Batch [21/43], Training Loss: 0.00001822
2024-11-06 14:07:35,633 - INFO - Epoch [251/300], Batch [22/43], Training Loss: 0.00001126
2024-11-06 14:07:35,636 - INFO - Epoch [251/300], Batch [23/43], Training Loss: 0.00000915
2024-11-06 14:07:35,640 - INFO - Epoch [251/300], Batch [24/43], Training Loss: 0.00001461
2024-11-06 14:07:35,643 - INFO - Epoch [251/300], Batch [25/43], Training Loss: 0.00001060
2024-11-06 14:07:35,647 - INFO - Epoch [251/300], Batch [26/43], Training Loss: 0.00000984
2024-11-06 14:07:35,651 - INFO - Epoch [251/300], Batch [27/43], Training Loss: 0.00000930
2024-11-06 14:07:35,655 - INFO - Epoch [251/300], Batch [28/43], Training Loss: 0.00002221
2024-11-06 14:07:35,658 - INFO - Epoch [251/300], Batch [29/43], Training Loss: 0.00000211
2024-11-06 14:07:35,662 - INFO - Epoch [251/300], Batch [30/43], Training Loss: 0.00000987
2024-11-06 14:07:35,666 - INFO - Epoch [251/300], Batch [31/43], Training Loss: 0.00001375
2024-11-06 14:07:35,669 - INFO - Epoch [251/300], Batch [32/43], Training Loss: 0.00002317
2024-11-06 14:07:35,673 - INFO - Epoch [251/300], Batch [33/43], Training Loss: 0.00000928
2024-11-06 14:07:35,676 - INFO - Epoch [251/300], Batch [34/43], Training Loss: 0.00001247
2024-11-06 14:07:35,679 - INFO - Epoch [251/300], Batch [35/43], Training Loss: 0.00001800
2024-11-06 14:07:35,683 - INFO - Epoch [251/300], Batch [36/43], Training Loss: 0.00001168
2024-11-06 14:07:35,687 - INFO - Epoch [251/300], Batch [37/43], Training Loss: 0.00000307
2024-11-06 14:07:35,691 - INFO - Epoch [251/300], Batch [38/43], Training Loss: 0.00000628
2024-11-06 14:07:35,694 - INFO - Epoch [251/300], Batch [39/43], Training Loss: 0.00001088
2024-11-06 14:07:35,699 - INFO - Epoch [251/300], Batch [40/43], Training Loss: 0.00000784
2024-11-06 14:07:35,702 - INFO - Epoch [251/300], Batch [41/43], Training Loss: 0.00000314
2024-11-06 14:07:35,707 - INFO - Epoch [251/300], Batch [42/43], Training Loss: 0.00000600
2024-11-06 14:07:35,711 - INFO - Epoch [251/300], Batch [43/43], Training Loss: 0.00001901
2024-11-06 14:07:35,722 - INFO - Epoch [251/300], Average Training Loss: 0.00001116, Validation Loss: 0.00001366
2024-11-06 14:07:35,726 - INFO - Epoch [252/300], Batch [1/43], Training Loss: 0.00000458
2024-11-06 14:07:35,729 - INFO - Epoch [252/300], Batch [2/43], Training Loss: 0.00000815
2024-11-06 14:07:35,733 - INFO - Epoch [252/300], Batch [3/43], Training Loss: 0.00000540
2024-11-06 14:07:35,737 - INFO - Epoch [252/300], Batch [4/43], Training Loss: 0.00000327
2024-11-06 14:07:35,740 - INFO - Epoch [252/300], Batch [5/43], Training Loss: 0.00001554
2024-11-06 14:07:35,744 - INFO - Epoch [252/300], Batch [6/43], Training Loss: 0.00000668
2024-11-06 14:07:35,749 - INFO - Epoch [252/300], Batch [7/43], Training Loss: 0.00001282
2024-11-06 14:07:35,753 - INFO - Epoch [252/300], Batch [8/43], Training Loss: 0.00002513
2024-11-06 14:07:35,757 - INFO - Epoch [252/300], Batch [9/43], Training Loss: 0.00000362
2024-11-06 14:07:35,762 - INFO - Epoch [252/300], Batch [10/43], Training Loss: 0.00000861
2024-11-06 14:07:35,767 - INFO - Epoch [252/300], Batch [11/43], Training Loss: 0.00000275
2024-11-06 14:07:35,772 - INFO - Epoch [252/300], Batch [12/43], Training Loss: 0.00000458
2024-11-06 14:07:35,776 - INFO - Epoch [252/300], Batch [13/43], Training Loss: 0.00000768
2024-11-06 14:07:35,781 - INFO - Epoch [252/300], Batch [14/43], Training Loss: 0.00000733
2024-11-06 14:07:35,785 - INFO - Epoch [252/300], Batch [15/43], Training Loss: 0.00001376
2024-11-06 14:07:35,790 - INFO - Epoch [252/300], Batch [16/43], Training Loss: 0.00002712
2024-11-06 14:07:35,794 - INFO - Epoch [252/300], Batch [17/43], Training Loss: 0.00001705
2024-11-06 14:07:35,798 - INFO - Epoch [252/300], Batch [18/43], Training Loss: 0.00002002
2024-11-06 14:07:35,802 - INFO - Epoch [252/300], Batch [19/43], Training Loss: 0.00002884
2024-11-06 14:07:35,808 - INFO - Epoch [252/300], Batch [20/43], Training Loss: 0.00001282
2024-11-06 14:07:35,817 - INFO - Epoch [252/300], Batch [21/43], Training Loss: 0.00000660
2024-11-06 14:07:35,822 - INFO - Epoch [252/300], Batch [22/43], Training Loss: 0.00001049
2024-11-06 14:07:35,827 - INFO - Epoch [252/300], Batch [23/43], Training Loss: 0.00001741
2024-11-06 14:07:35,832 - INFO - Epoch [252/300], Batch [24/43], Training Loss: 0.00001235
2024-11-06 14:07:35,837 - INFO - Epoch [252/300], Batch [25/43], Training Loss: 0.00001389
2024-11-06 14:07:35,842 - INFO - Epoch [252/300], Batch [26/43], Training Loss: 0.00000974
2024-11-06 14:07:35,846 - INFO - Epoch [252/300], Batch [27/43], Training Loss: 0.00000396
2024-11-06 14:07:35,851 - INFO - Epoch [252/300], Batch [28/43], Training Loss: 0.00000486
2024-11-06 14:07:35,857 - INFO - Epoch [252/300], Batch [29/43], Training Loss: 0.00000733
2024-11-06 14:07:35,862 - INFO - Epoch [252/300], Batch [30/43], Training Loss: 0.00000553
2024-11-06 14:07:35,867 - INFO - Epoch [252/300], Batch [31/43], Training Loss: 0.00000742
2024-11-06 14:07:35,871 - INFO - Epoch [252/300], Batch [32/43], Training Loss: 0.00000261
2024-11-06 14:07:35,876 - INFO - Epoch [252/300], Batch [33/43], Training Loss: 0.00000311
2024-11-06 14:07:35,881 - INFO - Epoch [252/300], Batch [34/43], Training Loss: 0.00000558
2024-11-06 14:07:35,884 - INFO - Epoch [252/300], Batch [35/43], Training Loss: 0.00000363
2024-11-06 14:07:35,888 - INFO - Epoch [252/300], Batch [36/43], Training Loss: 0.00000263
2024-11-06 14:07:35,892 - INFO - Epoch [252/300], Batch [37/43], Training Loss: 0.00000340
2024-11-06 14:07:35,896 - INFO - Epoch [252/300], Batch [38/43], Training Loss: 0.00001342
2024-11-06 14:07:35,901 - INFO - Epoch [252/300], Batch [39/43], Training Loss: 0.00000578
2024-11-06 14:07:35,905 - INFO - Epoch [252/300], Batch [40/43], Training Loss: 0.00000640
2024-11-06 14:07:35,909 - INFO - Epoch [252/300], Batch [41/43], Training Loss: 0.00001335
2024-11-06 14:07:35,913 - INFO - Epoch [252/300], Batch [42/43], Training Loss: 0.00000268
2024-11-06 14:07:35,918 - INFO - Epoch [252/300], Batch [43/43], Training Loss: 0.00001810
2024-11-06 14:07:35,931 - INFO - Epoch [252/300], Average Training Loss: 0.00000968, Validation Loss: 0.00001784
2024-11-06 14:07:35,935 - INFO - Epoch [253/300], Batch [1/43], Training Loss: 0.00001719
2024-11-06 14:07:35,939 - INFO - Epoch [253/300], Batch [2/43], Training Loss: 0.00001587
2024-11-06 14:07:35,943 - INFO - Epoch [253/300], Batch [3/43], Training Loss: 0.00000521
2024-11-06 14:07:35,948 - INFO - Epoch [253/300], Batch [4/43], Training Loss: 0.00000753
2024-11-06 14:07:35,952 - INFO - Epoch [253/300], Batch [5/43], Training Loss: 0.00000625
2024-11-06 14:07:35,956 - INFO - Epoch [253/300], Batch [6/43], Training Loss: 0.00001094
2024-11-06 14:07:35,960 - INFO - Epoch [253/300], Batch [7/43], Training Loss: 0.00000451
2024-11-06 14:07:35,963 - INFO - Epoch [253/300], Batch [8/43], Training Loss: 0.00002028
2024-11-06 14:07:35,967 - INFO - Epoch [253/300], Batch [9/43], Training Loss: 0.00001802
2024-11-06 14:07:35,971 - INFO - Epoch [253/300], Batch [10/43], Training Loss: 0.00000417
2024-11-06 14:07:35,974 - INFO - Epoch [253/300], Batch [11/43], Training Loss: 0.00000264
2024-11-06 14:07:35,977 - INFO - Epoch [253/300], Batch [12/43], Training Loss: 0.00001344
2024-11-06 14:07:35,981 - INFO - Epoch [253/300], Batch [13/43], Training Loss: 0.00000858
2024-11-06 14:07:35,985 - INFO - Epoch [253/300], Batch [14/43], Training Loss: 0.00000972
2024-11-06 14:07:35,988 - INFO - Epoch [253/300], Batch [15/43], Training Loss: 0.00001009
2024-11-06 14:07:35,992 - INFO - Epoch [253/300], Batch [16/43], Training Loss: 0.00000547
2024-11-06 14:07:35,996 - INFO - Epoch [253/300], Batch [17/43], Training Loss: 0.00003541
2024-11-06 14:07:36,000 - INFO - Epoch [253/300], Batch [18/43], Training Loss: 0.00001231
2024-11-06 14:07:36,004 - INFO - Epoch [253/300], Batch [19/43], Training Loss: 0.00000581
2024-11-06 14:07:36,008 - INFO - Epoch [253/300], Batch [20/43], Training Loss: 0.00000915
2024-11-06 14:07:36,014 - INFO - Epoch [253/300], Batch [21/43], Training Loss: 0.00001905
2024-11-06 14:07:36,018 - INFO - Epoch [253/300], Batch [22/43], Training Loss: 0.00001448
2024-11-06 14:07:36,021 - INFO - Epoch [253/300], Batch [23/43], Training Loss: 0.00000375
2024-11-06 14:07:36,025 - INFO - Epoch [253/300], Batch [24/43], Training Loss: 0.00001298
2024-11-06 14:07:36,028 - INFO - Epoch [253/300], Batch [25/43], Training Loss: 0.00001045
2024-11-06 14:07:36,032 - INFO - Epoch [253/300], Batch [26/43], Training Loss: 0.00000425
2024-11-06 14:07:36,036 - INFO - Epoch [253/300], Batch [27/43], Training Loss: 0.00000521
2024-11-06 14:07:36,040 - INFO - Epoch [253/300], Batch [28/43], Training Loss: 0.00000480
2024-11-06 14:07:36,044 - INFO - Epoch [253/300], Batch [29/43], Training Loss: 0.00001082
2024-11-06 14:07:36,048 - INFO - Epoch [253/300], Batch [30/43], Training Loss: 0.00001451
2024-11-06 14:07:36,052 - INFO - Epoch [253/300], Batch [31/43], Training Loss: 0.00000696
2024-11-06 14:07:36,056 - INFO - Epoch [253/300], Batch [32/43], Training Loss: 0.00001288
2024-11-06 14:07:36,060 - INFO - Epoch [253/300], Batch [33/43], Training Loss: 0.00001467
2024-11-06 14:07:36,064 - INFO - Epoch [253/300], Batch [34/43], Training Loss: 0.00000369
2024-11-06 14:07:36,067 - INFO - Epoch [253/300], Batch [35/43], Training Loss: 0.00000491
2024-11-06 14:07:36,070 - INFO - Epoch [253/300], Batch [36/43], Training Loss: 0.00000766
2024-11-06 14:07:36,074 - INFO - Epoch [253/300], Batch [37/43], Training Loss: 0.00001248
2024-11-06 14:07:36,078 - INFO - Epoch [253/300], Batch [38/43], Training Loss: 0.00001229
2024-11-06 14:07:36,082 - INFO - Epoch [253/300], Batch [39/43], Training Loss: 0.00002534
2024-11-06 14:07:36,085 - INFO - Epoch [253/300], Batch [40/43], Training Loss: 0.00000793
2024-11-06 14:07:36,089 - INFO - Epoch [253/300], Batch [41/43], Training Loss: 0.00001448
2024-11-06 14:07:36,093 - INFO - Epoch [253/300], Batch [42/43], Training Loss: 0.00001886
2024-11-06 14:07:36,098 - INFO - Epoch [253/300], Batch [43/43], Training Loss: 0.00001273
2024-11-06 14:07:36,111 - INFO - Epoch [253/300], Average Training Loss: 0.00001111, Validation Loss: 0.00001424
2024-11-06 14:07:36,116 - INFO - Epoch [254/300], Batch [1/43], Training Loss: 0.00001895
2024-11-06 14:07:36,121 - INFO - Epoch [254/300], Batch [2/43], Training Loss: 0.00001459
2024-11-06 14:07:36,127 - INFO - Epoch [254/300], Batch [3/43], Training Loss: 0.00000695
2024-11-06 14:07:36,132 - INFO - Epoch [254/300], Batch [4/43], Training Loss: 0.00002688
2024-11-06 14:07:36,136 - INFO - Epoch [254/300], Batch [5/43], Training Loss: 0.00001221
2024-11-06 14:07:36,141 - INFO - Epoch [254/300], Batch [6/43], Training Loss: 0.00002808
2024-11-06 14:07:36,146 - INFO - Epoch [254/300], Batch [7/43], Training Loss: 0.00001177
2024-11-06 14:07:36,151 - INFO - Epoch [254/300], Batch [8/43], Training Loss: 0.00001120
2024-11-06 14:07:36,158 - INFO - Epoch [254/300], Batch [9/43], Training Loss: 0.00001063
2024-11-06 14:07:36,163 - INFO - Epoch [254/300], Batch [10/43], Training Loss: 0.00001393
2024-11-06 14:07:36,169 - INFO - Epoch [254/300], Batch [11/43], Training Loss: 0.00002076
2024-11-06 14:07:36,174 - INFO - Epoch [254/300], Batch [12/43], Training Loss: 0.00000855
2024-11-06 14:07:36,179 - INFO - Epoch [254/300], Batch [13/43], Training Loss: 0.00001446
2024-11-06 14:07:36,184 - INFO - Epoch [254/300], Batch [14/43], Training Loss: 0.00001230
2024-11-06 14:07:36,188 - INFO - Epoch [254/300], Batch [15/43], Training Loss: 0.00000650
2024-11-06 14:07:36,193 - INFO - Epoch [254/300], Batch [16/43], Training Loss: 0.00000574
2024-11-06 14:07:36,198 - INFO - Epoch [254/300], Batch [17/43], Training Loss: 0.00000462
2024-11-06 14:07:36,202 - INFO - Epoch [254/300], Batch [18/43], Training Loss: 0.00000782
2024-11-06 14:07:36,207 - INFO - Epoch [254/300], Batch [19/43], Training Loss: 0.00000522
2024-11-06 14:07:36,211 - INFO - Epoch [254/300], Batch [20/43], Training Loss: 0.00000514
2024-11-06 14:07:36,216 - INFO - Epoch [254/300], Batch [21/43], Training Loss: 0.00000941
2024-11-06 14:07:36,220 - INFO - Epoch [254/300], Batch [22/43], Training Loss: 0.00002345
2024-11-06 14:07:36,225 - INFO - Epoch [254/300], Batch [23/43], Training Loss: 0.00000349
2024-11-06 14:07:36,228 - INFO - Epoch [254/300], Batch [24/43], Training Loss: 0.00000883
2024-11-06 14:07:36,233 - INFO - Epoch [254/300], Batch [25/43], Training Loss: 0.00000965
2024-11-06 14:07:36,237 - INFO - Epoch [254/300], Batch [26/43], Training Loss: 0.00001684
2024-11-06 14:07:36,241 - INFO - Epoch [254/300], Batch [27/43], Training Loss: 0.00000418
2024-11-06 14:07:36,245 - INFO - Epoch [254/300], Batch [28/43], Training Loss: 0.00000927
2024-11-06 14:07:36,249 - INFO - Epoch [254/300], Batch [29/43], Training Loss: 0.00000628
2024-11-06 14:07:36,254 - INFO - Epoch [254/300], Batch [30/43], Training Loss: 0.00000324
2024-11-06 14:07:36,258 - INFO - Epoch [254/300], Batch [31/43], Training Loss: 0.00001686
2024-11-06 14:07:36,263 - INFO - Epoch [254/300], Batch [32/43], Training Loss: 0.00001085
2024-11-06 14:07:36,267 - INFO - Epoch [254/300], Batch [33/43], Training Loss: 0.00000663
2024-11-06 14:07:36,271 - INFO - Epoch [254/300], Batch [34/43], Training Loss: 0.00001144
2024-11-06 14:07:36,275 - INFO - Epoch [254/300], Batch [35/43], Training Loss: 0.00000503
2024-11-06 14:07:36,279 - INFO - Epoch [254/300], Batch [36/43], Training Loss: 0.00000648
2024-11-06 14:07:36,283 - INFO - Epoch [254/300], Batch [37/43], Training Loss: 0.00002105
2024-11-06 14:07:36,288 - INFO - Epoch [254/300], Batch [38/43], Training Loss: 0.00000681
2024-11-06 14:07:36,292 - INFO - Epoch [254/300], Batch [39/43], Training Loss: 0.00000428
2024-11-06 14:07:36,295 - INFO - Epoch [254/300], Batch [40/43], Training Loss: 0.00000662
2024-11-06 14:07:36,300 - INFO - Epoch [254/300], Batch [41/43], Training Loss: 0.00000826
2024-11-06 14:07:36,304 - INFO - Epoch [254/300], Batch [42/43], Training Loss: 0.00001721
2024-11-06 14:07:36,309 - INFO - Epoch [254/300], Batch [43/43], Training Loss: 0.00001596
2024-11-06 14:07:36,324 - INFO - Epoch [254/300], Average Training Loss: 0.00001113, Validation Loss: 0.00001159
2024-11-06 14:07:36,330 - INFO - Epoch [255/300], Batch [1/43], Training Loss: 0.00000431
2024-11-06 14:07:36,337 - INFO - Epoch [255/300], Batch [2/43], Training Loss: 0.00000498
2024-11-06 14:07:36,342 - INFO - Epoch [255/300], Batch [3/43], Training Loss: 0.00002286
2024-11-06 14:07:36,347 - INFO - Epoch [255/300], Batch [4/43], Training Loss: 0.00000839
2024-11-06 14:07:36,352 - INFO - Epoch [255/300], Batch [5/43], Training Loss: 0.00000689
2024-11-06 14:07:36,357 - INFO - Epoch [255/300], Batch [6/43], Training Loss: 0.00001551
2024-11-06 14:07:36,361 - INFO - Epoch [255/300], Batch [7/43], Training Loss: 0.00001183
2024-11-06 14:07:36,365 - INFO - Epoch [255/300], Batch [8/43], Training Loss: 0.00000877
2024-11-06 14:07:36,370 - INFO - Epoch [255/300], Batch [9/43], Training Loss: 0.00001094
2024-11-06 14:07:36,374 - INFO - Epoch [255/300], Batch [10/43], Training Loss: 0.00002355
2024-11-06 14:07:36,380 - INFO - Epoch [255/300], Batch [11/43], Training Loss: 0.00000633
2024-11-06 14:07:36,385 - INFO - Epoch [255/300], Batch [12/43], Training Loss: 0.00000645
2024-11-06 14:07:36,389 - INFO - Epoch [255/300], Batch [13/43], Training Loss: 0.00000874
2024-11-06 14:07:36,393 - INFO - Epoch [255/300], Batch [14/43], Training Loss: 0.00000652
2024-11-06 14:07:36,397 - INFO - Epoch [255/300], Batch [15/43], Training Loss: 0.00000928
2024-11-06 14:07:36,401 - INFO - Epoch [255/300], Batch [16/43], Training Loss: 0.00001051
2024-11-06 14:07:36,404 - INFO - Epoch [255/300], Batch [17/43], Training Loss: 0.00001130
2024-11-06 14:07:36,408 - INFO - Epoch [255/300], Batch [18/43], Training Loss: 0.00002105
2024-11-06 14:07:36,412 - INFO - Epoch [255/300], Batch [19/43], Training Loss: 0.00000924
2024-11-06 14:07:36,415 - INFO - Epoch [255/300], Batch [20/43], Training Loss: 0.00001712
2024-11-06 14:07:36,420 - INFO - Epoch [255/300], Batch [21/43], Training Loss: 0.00000582
2024-11-06 14:07:36,424 - INFO - Epoch [255/300], Batch [22/43], Training Loss: 0.00000657
2024-11-06 14:07:36,428 - INFO - Epoch [255/300], Batch [23/43], Training Loss: 0.00001162
2024-11-06 14:07:36,432 - INFO - Epoch [255/300], Batch [24/43], Training Loss: 0.00000844
2024-11-06 14:07:36,437 - INFO - Epoch [255/300], Batch [25/43], Training Loss: 0.00000417
2024-11-06 14:07:36,441 - INFO - Epoch [255/300], Batch [26/43], Training Loss: 0.00000970
2024-11-06 14:07:36,446 - INFO - Epoch [255/300], Batch [27/43], Training Loss: 0.00001324
2024-11-06 14:07:36,450 - INFO - Epoch [255/300], Batch [28/43], Training Loss: 0.00000660
2024-11-06 14:07:36,454 - INFO - Epoch [255/300], Batch [29/43], Training Loss: 0.00001138
2024-11-06 14:07:36,458 - INFO - Epoch [255/300], Batch [30/43], Training Loss: 0.00001374
2024-11-06 14:07:36,462 - INFO - Epoch [255/300], Batch [31/43], Training Loss: 0.00000797
2024-11-06 14:07:36,466 - INFO - Epoch [255/300], Batch [32/43], Training Loss: 0.00001846
2024-11-06 14:07:36,470 - INFO - Epoch [255/300], Batch [33/43], Training Loss: 0.00000107
2024-11-06 14:07:36,475 - INFO - Epoch [255/300], Batch [34/43], Training Loss: 0.00000670
2024-11-06 14:07:36,479 - INFO - Epoch [255/300], Batch [35/43], Training Loss: 0.00003803
2024-11-06 14:07:36,484 - INFO - Epoch [255/300], Batch [36/43], Training Loss: 0.00000981
2024-11-06 14:07:36,488 - INFO - Epoch [255/300], Batch [37/43], Training Loss: 0.00000647
2024-11-06 14:07:36,492 - INFO - Epoch [255/300], Batch [38/43], Training Loss: 0.00001537
2024-11-06 14:07:36,496 - INFO - Epoch [255/300], Batch [39/43], Training Loss: 0.00002363
2024-11-06 14:07:36,501 - INFO - Epoch [255/300], Batch [40/43], Training Loss: 0.00002072
2024-11-06 14:07:36,505 - INFO - Epoch [255/300], Batch [41/43], Training Loss: 0.00001571
2024-11-06 14:07:36,509 - INFO - Epoch [255/300], Batch [42/43], Training Loss: 0.00001331
2024-11-06 14:07:36,514 - INFO - Epoch [255/300], Batch [43/43], Training Loss: 0.00001692
2024-11-06 14:07:36,527 - INFO - Epoch [255/300], Average Training Loss: 0.00001186, Validation Loss: 0.00001455
2024-11-06 14:07:36,531 - INFO - Epoch [256/300], Batch [1/43], Training Loss: 0.00000521
2024-11-06 14:07:36,535 - INFO - Epoch [256/300], Batch [2/43], Training Loss: 0.00000617
2024-11-06 14:07:36,538 - INFO - Epoch [256/300], Batch [3/43], Training Loss: 0.00001196
2024-11-06 14:07:36,541 - INFO - Epoch [256/300], Batch [4/43], Training Loss: 0.00001514
2024-11-06 14:07:36,544 - INFO - Epoch [256/300], Batch [5/43], Training Loss: 0.00001335
2024-11-06 14:07:36,547 - INFO - Epoch [256/300], Batch [6/43], Training Loss: 0.00000304
2024-11-06 14:07:36,550 - INFO - Epoch [256/300], Batch [7/43], Training Loss: 0.00000643
2024-11-06 14:07:36,553 - INFO - Epoch [256/300], Batch [8/43], Training Loss: 0.00000990
2024-11-06 14:07:36,557 - INFO - Epoch [256/300], Batch [9/43], Training Loss: 0.00000610
2024-11-06 14:07:36,561 - INFO - Epoch [256/300], Batch [10/43], Training Loss: 0.00001435
2024-11-06 14:07:36,564 - INFO - Epoch [256/300], Batch [11/43], Training Loss: 0.00000539
2024-11-06 14:07:36,567 - INFO - Epoch [256/300], Batch [12/43], Training Loss: 0.00000799
2024-11-06 14:07:36,570 - INFO - Epoch [256/300], Batch [13/43], Training Loss: 0.00000392
2024-11-06 14:07:36,573 - INFO - Epoch [256/300], Batch [14/43], Training Loss: 0.00001802
2024-11-06 14:07:36,576 - INFO - Epoch [256/300], Batch [15/43], Training Loss: 0.00002171
2024-11-06 14:07:36,580 - INFO - Epoch [256/300], Batch [16/43], Training Loss: 0.00001332
2024-11-06 14:07:36,584 - INFO - Epoch [256/300], Batch [17/43], Training Loss: 0.00000690
2024-11-06 14:07:36,587 - INFO - Epoch [256/300], Batch [18/43], Training Loss: 0.00000936
2024-11-06 14:07:36,590 - INFO - Epoch [256/300], Batch [19/43], Training Loss: 0.00000595
2024-11-06 14:07:36,594 - INFO - Epoch [256/300], Batch [20/43], Training Loss: 0.00000722
2024-11-06 14:07:36,598 - INFO - Epoch [256/300], Batch [21/43], Training Loss: 0.00000558
2024-11-06 14:07:36,602 - INFO - Epoch [256/300], Batch [22/43], Training Loss: 0.00000551
2024-11-06 14:07:36,606 - INFO - Epoch [256/300], Batch [23/43], Training Loss: 0.00000982
2024-11-06 14:07:36,609 - INFO - Epoch [256/300], Batch [24/43], Training Loss: 0.00001211
2024-11-06 14:07:36,612 - INFO - Epoch [256/300], Batch [25/43], Training Loss: 0.00001614
2024-11-06 14:07:36,615 - INFO - Epoch [256/300], Batch [26/43], Training Loss: 0.00001388
2024-11-06 14:07:36,618 - INFO - Epoch [256/300], Batch [27/43], Training Loss: 0.00001642
2024-11-06 14:07:36,622 - INFO - Epoch [256/300], Batch [28/43], Training Loss: 0.00001887
2024-11-06 14:07:36,625 - INFO - Epoch [256/300], Batch [29/43], Training Loss: 0.00001553
2024-11-06 14:07:36,629 - INFO - Epoch [256/300], Batch [30/43], Training Loss: 0.00002196
2024-11-06 14:07:36,633 - INFO - Epoch [256/300], Batch [31/43], Training Loss: 0.00001405
2024-11-06 14:07:36,638 - INFO - Epoch [256/300], Batch [32/43], Training Loss: 0.00002797
2024-11-06 14:07:36,642 - INFO - Epoch [256/300], Batch [33/43], Training Loss: 0.00000984
2024-11-06 14:07:36,647 - INFO - Epoch [256/300], Batch [34/43], Training Loss: 0.00000925
2024-11-06 14:07:36,650 - INFO - Epoch [256/300], Batch [35/43], Training Loss: 0.00001015
2024-11-06 14:07:36,654 - INFO - Epoch [256/300], Batch [36/43], Training Loss: 0.00001139
2024-11-06 14:07:36,658 - INFO - Epoch [256/300], Batch [37/43], Training Loss: 0.00001374
2024-11-06 14:07:36,662 - INFO - Epoch [256/300], Batch [38/43], Training Loss: 0.00000937
2024-11-06 14:07:36,665 - INFO - Epoch [256/300], Batch [39/43], Training Loss: 0.00001346
2024-11-06 14:07:36,669 - INFO - Epoch [256/300], Batch [40/43], Training Loss: 0.00001158
2024-11-06 14:07:36,673 - INFO - Epoch [256/300], Batch [41/43], Training Loss: 0.00000613
2024-11-06 14:07:36,676 - INFO - Epoch [256/300], Batch [42/43], Training Loss: 0.00001982
2024-11-06 14:07:36,680 - INFO - Epoch [256/300], Batch [43/43], Training Loss: 0.00000616
2024-11-06 14:07:36,691 - INFO - Epoch [256/300], Average Training Loss: 0.00001140, Validation Loss: 0.00001408
2024-11-06 14:07:36,695 - INFO - Epoch [257/300], Batch [1/43], Training Loss: 0.00000709
2024-11-06 14:07:36,698 - INFO - Epoch [257/300], Batch [2/43], Training Loss: 0.00002949
2024-11-06 14:07:36,701 - INFO - Epoch [257/300], Batch [3/43], Training Loss: 0.00001209
2024-11-06 14:07:36,704 - INFO - Epoch [257/300], Batch [4/43], Training Loss: 0.00001002
2024-11-06 14:07:36,708 - INFO - Epoch [257/300], Batch [5/43], Training Loss: 0.00000408
2024-11-06 14:07:36,711 - INFO - Epoch [257/300], Batch [6/43], Training Loss: 0.00000490
2024-11-06 14:07:36,714 - INFO - Epoch [257/300], Batch [7/43], Training Loss: 0.00000793
2024-11-06 14:07:36,718 - INFO - Epoch [257/300], Batch [8/43], Training Loss: 0.00001055
2024-11-06 14:07:36,721 - INFO - Epoch [257/300], Batch [9/43], Training Loss: 0.00000926
2024-11-06 14:07:36,723 - INFO - Epoch [257/300], Batch [10/43], Training Loss: 0.00001002
2024-11-06 14:07:36,727 - INFO - Epoch [257/300], Batch [11/43], Training Loss: 0.00000862
2024-11-06 14:07:36,731 - INFO - Epoch [257/300], Batch [12/43], Training Loss: 0.00001192
2024-11-06 14:07:36,734 - INFO - Epoch [257/300], Batch [13/43], Training Loss: 0.00001788
2024-11-06 14:07:36,737 - INFO - Epoch [257/300], Batch [14/43], Training Loss: 0.00003369
2024-11-06 14:07:36,740 - INFO - Epoch [257/300], Batch [15/43], Training Loss: 0.00000765
2024-11-06 14:07:36,744 - INFO - Epoch [257/300], Batch [16/43], Training Loss: 0.00000764
2024-11-06 14:07:36,747 - INFO - Epoch [257/300], Batch [17/43], Training Loss: 0.00001841
2024-11-06 14:07:36,750 - INFO - Epoch [257/300], Batch [18/43], Training Loss: 0.00000657
2024-11-06 14:07:36,753 - INFO - Epoch [257/300], Batch [19/43], Training Loss: 0.00001262
2024-11-06 14:07:36,756 - INFO - Epoch [257/300], Batch [20/43], Training Loss: 0.00000978
2024-11-06 14:07:36,759 - INFO - Epoch [257/300], Batch [21/43], Training Loss: 0.00001454
2024-11-06 14:07:36,762 - INFO - Epoch [257/300], Batch [22/43], Training Loss: 0.00000398
2024-11-06 14:07:36,766 - INFO - Epoch [257/300], Batch [23/43], Training Loss: 0.00000847
2024-11-06 14:07:36,769 - INFO - Epoch [257/300], Batch [24/43], Training Loss: 0.00001407
2024-11-06 14:07:36,772 - INFO - Epoch [257/300], Batch [25/43], Training Loss: 0.00000808
2024-11-06 14:07:36,776 - INFO - Epoch [257/300], Batch [26/43], Training Loss: 0.00000740
2024-11-06 14:07:36,780 - INFO - Epoch [257/300], Batch [27/43], Training Loss: 0.00000642
2024-11-06 14:07:36,784 - INFO - Epoch [257/300], Batch [28/43], Training Loss: 0.00001739
2024-11-06 14:07:36,788 - INFO - Epoch [257/300], Batch [29/43], Training Loss: 0.00001259
2024-11-06 14:07:36,792 - INFO - Epoch [257/300], Batch [30/43], Training Loss: 0.00001860
2024-11-06 14:07:36,797 - INFO - Epoch [257/300], Batch [31/43], Training Loss: 0.00000368
2024-11-06 14:07:36,801 - INFO - Epoch [257/300], Batch [32/43], Training Loss: 0.00000531
2024-11-06 14:07:36,804 - INFO - Epoch [257/300], Batch [33/43], Training Loss: 0.00000254
2024-11-06 14:07:36,808 - INFO - Epoch [257/300], Batch [34/43], Training Loss: 0.00001039
2024-11-06 14:07:36,812 - INFO - Epoch [257/300], Batch [35/43], Training Loss: 0.00000522
2024-11-06 14:07:36,816 - INFO - Epoch [257/300], Batch [36/43], Training Loss: 0.00001043
2024-11-06 14:07:36,820 - INFO - Epoch [257/300], Batch [37/43], Training Loss: 0.00000772
2024-11-06 14:07:36,824 - INFO - Epoch [257/300], Batch [38/43], Training Loss: 0.00001312
2024-11-06 14:07:36,827 - INFO - Epoch [257/300], Batch [39/43], Training Loss: 0.00001792
2024-11-06 14:07:36,831 - INFO - Epoch [257/300], Batch [40/43], Training Loss: 0.00001792
2024-11-06 14:07:36,834 - INFO - Epoch [257/300], Batch [41/43], Training Loss: 0.00001205
2024-11-06 14:07:36,837 - INFO - Epoch [257/300], Batch [42/43], Training Loss: 0.00002059
2024-11-06 14:07:36,840 - INFO - Epoch [257/300], Batch [43/43], Training Loss: 0.00000713
2024-11-06 14:07:36,850 - INFO - Epoch [257/300], Average Training Loss: 0.00001130, Validation Loss: 0.00001763
2024-11-06 14:07:36,854 - INFO - Epoch [258/300], Batch [1/43], Training Loss: 0.00000612
2024-11-06 14:07:36,858 - INFO - Epoch [258/300], Batch [2/43], Training Loss: 0.00000930
2024-11-06 14:07:36,861 - INFO - Epoch [258/300], Batch [3/43], Training Loss: 0.00001988
2024-11-06 14:07:36,864 - INFO - Epoch [258/300], Batch [4/43], Training Loss: 0.00000450
2024-11-06 14:07:36,867 - INFO - Epoch [258/300], Batch [5/43], Training Loss: 0.00000400
2024-11-06 14:07:36,872 - INFO - Epoch [258/300], Batch [6/43], Training Loss: 0.00001785
2024-11-06 14:07:36,875 - INFO - Epoch [258/300], Batch [7/43], Training Loss: 0.00000335
2024-11-06 14:07:36,879 - INFO - Epoch [258/300], Batch [8/43], Training Loss: 0.00000866
2024-11-06 14:07:36,882 - INFO - Epoch [258/300], Batch [9/43], Training Loss: 0.00000964
2024-11-06 14:07:36,885 - INFO - Epoch [258/300], Batch [10/43], Training Loss: 0.00000253
2024-11-06 14:07:36,888 - INFO - Epoch [258/300], Batch [11/43], Training Loss: 0.00000922
2024-11-06 14:07:36,890 - INFO - Epoch [258/300], Batch [12/43], Training Loss: 0.00001231
2024-11-06 14:07:36,893 - INFO - Epoch [258/300], Batch [13/43], Training Loss: 0.00001091
2024-11-06 14:07:36,896 - INFO - Epoch [258/300], Batch [14/43], Training Loss: 0.00000645
2024-11-06 14:07:36,899 - INFO - Epoch [258/300], Batch [15/43], Training Loss: 0.00000260
2024-11-06 14:07:36,902 - INFO - Epoch [258/300], Batch [16/43], Training Loss: 0.00000457
2024-11-06 14:07:36,905 - INFO - Epoch [258/300], Batch [17/43], Training Loss: 0.00000698
2024-11-06 14:07:36,908 - INFO - Epoch [258/300], Batch [18/43], Training Loss: 0.00001598
2024-11-06 14:07:36,912 - INFO - Epoch [258/300], Batch [19/43], Training Loss: 0.00000414
2024-11-06 14:07:36,916 - INFO - Epoch [258/300], Batch [20/43], Training Loss: 0.00001077
2024-11-06 14:07:36,919 - INFO - Epoch [258/300], Batch [21/43], Training Loss: 0.00000408
2024-11-06 14:07:36,922 - INFO - Epoch [258/300], Batch [22/43], Training Loss: 0.00000861
2024-11-06 14:07:36,926 - INFO - Epoch [258/300], Batch [23/43], Training Loss: 0.00002337
2024-11-06 14:07:36,929 - INFO - Epoch [258/300], Batch [24/43], Training Loss: 0.00000554
2024-11-06 14:07:36,931 - INFO - Epoch [258/300], Batch [25/43], Training Loss: 0.00000902
2024-11-06 14:07:36,934 - INFO - Epoch [258/300], Batch [26/43], Training Loss: 0.00000880
2024-11-06 14:07:36,938 - INFO - Epoch [258/300], Batch [27/43], Training Loss: 0.00000423
2024-11-06 14:07:36,942 - INFO - Epoch [258/300], Batch [28/43], Training Loss: 0.00000253
2024-11-06 14:07:36,946 - INFO - Epoch [258/300], Batch [29/43], Training Loss: 0.00001074
2024-11-06 14:07:36,951 - INFO - Epoch [258/300], Batch [30/43], Training Loss: 0.00000785
2024-11-06 14:07:36,956 - INFO - Epoch [258/300], Batch [31/43], Training Loss: 0.00001649
2024-11-06 14:07:36,961 - INFO - Epoch [258/300], Batch [32/43], Training Loss: 0.00001768
2024-11-06 14:07:36,965 - INFO - Epoch [258/300], Batch [33/43], Training Loss: 0.00002040
2024-11-06 14:07:36,969 - INFO - Epoch [258/300], Batch [34/43], Training Loss: 0.00000749
2024-11-06 14:07:36,973 - INFO - Epoch [258/300], Batch [35/43], Training Loss: 0.00000939
2024-11-06 14:07:36,976 - INFO - Epoch [258/300], Batch [36/43], Training Loss: 0.00001115
2024-11-06 14:07:36,979 - INFO - Epoch [258/300], Batch [37/43], Training Loss: 0.00003363
2024-11-06 14:07:36,983 - INFO - Epoch [258/300], Batch [38/43], Training Loss: 0.00002302
2024-11-06 14:07:36,986 - INFO - Epoch [258/300], Batch [39/43], Training Loss: 0.00002758
2024-11-06 14:07:36,990 - INFO - Epoch [258/300], Batch [40/43], Training Loss: 0.00001267
2024-11-06 14:07:36,993 - INFO - Epoch [258/300], Batch [41/43], Training Loss: 0.00001997
2024-11-06 14:07:36,996 - INFO - Epoch [258/300], Batch [42/43], Training Loss: 0.00002491
2024-11-06 14:07:37,000 - INFO - Epoch [258/300], Batch [43/43], Training Loss: 0.00001448
2024-11-06 14:07:37,010 - INFO - Epoch [258/300], Average Training Loss: 0.00001147, Validation Loss: 0.00001685
2024-11-06 14:07:37,014 - INFO - Epoch [259/300], Batch [1/43], Training Loss: 0.00001390
2024-11-06 14:07:37,017 - INFO - Epoch [259/300], Batch [2/43], Training Loss: 0.00003222
2024-11-06 14:07:37,021 - INFO - Epoch [259/300], Batch [3/43], Training Loss: 0.00000732
2024-11-06 14:07:37,025 - INFO - Epoch [259/300], Batch [4/43], Training Loss: 0.00001143
2024-11-06 14:07:37,028 - INFO - Epoch [259/300], Batch [5/43], Training Loss: 0.00001359
2024-11-06 14:07:37,031 - INFO - Epoch [259/300], Batch [6/43], Training Loss: 0.00002255
2024-11-06 14:07:37,034 - INFO - Epoch [259/300], Batch [7/43], Training Loss: 0.00001596
2024-11-06 14:07:37,036 - INFO - Epoch [259/300], Batch [8/43], Training Loss: 0.00001747
2024-11-06 14:07:37,039 - INFO - Epoch [259/300], Batch [9/43], Training Loss: 0.00001215
2024-11-06 14:07:37,042 - INFO - Epoch [259/300], Batch [10/43], Training Loss: 0.00000623
2024-11-06 14:07:37,045 - INFO - Epoch [259/300], Batch [11/43], Training Loss: 0.00001079
2024-11-06 14:07:37,049 - INFO - Epoch [259/300], Batch [12/43], Training Loss: 0.00001871
2024-11-06 14:07:37,053 - INFO - Epoch [259/300], Batch [13/43], Training Loss: 0.00000837
2024-11-06 14:07:37,057 - INFO - Epoch [259/300], Batch [14/43], Training Loss: 0.00001440
2024-11-06 14:07:37,060 - INFO - Epoch [259/300], Batch [15/43], Training Loss: 0.00000543
2024-11-06 14:07:37,064 - INFO - Epoch [259/300], Batch [16/43], Training Loss: 0.00001027
2024-11-06 14:07:37,067 - INFO - Epoch [259/300], Batch [17/43], Training Loss: 0.00000874
2024-11-06 14:07:37,070 - INFO - Epoch [259/300], Batch [18/43], Training Loss: 0.00001321
2024-11-06 14:07:37,073 - INFO - Epoch [259/300], Batch [19/43], Training Loss: 0.00000486
2024-11-06 14:07:37,077 - INFO - Epoch [259/300], Batch [20/43], Training Loss: 0.00000903
2024-11-06 14:07:37,080 - INFO - Epoch [259/300], Batch [21/43], Training Loss: 0.00000468
2024-11-06 14:07:37,083 - INFO - Epoch [259/300], Batch [22/43], Training Loss: 0.00001626
2024-11-06 14:07:37,085 - INFO - Epoch [259/300], Batch [23/43], Training Loss: 0.00001454
2024-11-06 14:07:37,088 - INFO - Epoch [259/300], Batch [24/43], Training Loss: 0.00001672
2024-11-06 14:07:37,091 - INFO - Epoch [259/300], Batch [25/43], Training Loss: 0.00002753
2024-11-06 14:07:37,096 - INFO - Epoch [259/300], Batch [26/43], Training Loss: 0.00000964
2024-11-06 14:07:37,100 - INFO - Epoch [259/300], Batch [27/43], Training Loss: 0.00001617
2024-11-06 14:07:37,104 - INFO - Epoch [259/300], Batch [28/43], Training Loss: 0.00002159
2024-11-06 14:07:37,108 - INFO - Epoch [259/300], Batch [29/43], Training Loss: 0.00000652
2024-11-06 14:07:37,113 - INFO - Epoch [259/300], Batch [30/43], Training Loss: 0.00000518
2024-11-06 14:07:37,118 - INFO - Epoch [259/300], Batch [31/43], Training Loss: 0.00001314
2024-11-06 14:07:37,121 - INFO - Epoch [259/300], Batch [32/43], Training Loss: 0.00001985
2024-11-06 14:07:37,125 - INFO - Epoch [259/300], Batch [33/43], Training Loss: 0.00000842
2024-11-06 14:07:37,129 - INFO - Epoch [259/300], Batch [34/43], Training Loss: 0.00000732
2024-11-06 14:07:37,132 - INFO - Epoch [259/300], Batch [35/43], Training Loss: 0.00000575
2024-11-06 14:07:37,136 - INFO - Epoch [259/300], Batch [36/43], Training Loss: 0.00000822
2024-11-06 14:07:37,140 - INFO - Epoch [259/300], Batch [37/43], Training Loss: 0.00001216
2024-11-06 14:07:37,144 - INFO - Epoch [259/300], Batch [38/43], Training Loss: 0.00000966
2024-11-06 14:07:37,148 - INFO - Epoch [259/300], Batch [39/43], Training Loss: 0.00001024
2024-11-06 14:07:37,151 - INFO - Epoch [259/300], Batch [40/43], Training Loss: 0.00001155
2024-11-06 14:07:37,155 - INFO - Epoch [259/300], Batch [41/43], Training Loss: 0.00000755
2024-11-06 14:07:37,159 - INFO - Epoch [259/300], Batch [42/43], Training Loss: 0.00000794
2024-11-06 14:07:37,163 - INFO - Epoch [259/300], Batch [43/43], Training Loss: 0.00000913
2024-11-06 14:07:37,175 - INFO - Epoch [259/300], Average Training Loss: 0.00001224, Validation Loss: 0.00001696
2024-11-06 14:07:37,179 - INFO - Epoch [260/300], Batch [1/43], Training Loss: 0.00000542
2024-11-06 14:07:37,183 - INFO - Epoch [260/300], Batch [2/43], Training Loss: 0.00001040
2024-11-06 14:07:37,187 - INFO - Epoch [260/300], Batch [3/43], Training Loss: 0.00001705
2024-11-06 14:07:37,191 - INFO - Epoch [260/300], Batch [4/43], Training Loss: 0.00000920
2024-11-06 14:07:37,195 - INFO - Epoch [260/300], Batch [5/43], Training Loss: 0.00001575
2024-11-06 14:07:37,199 - INFO - Epoch [260/300], Batch [6/43], Training Loss: 0.00001298
2024-11-06 14:07:37,202 - INFO - Epoch [260/300], Batch [7/43], Training Loss: 0.00000953
2024-11-06 14:07:37,206 - INFO - Epoch [260/300], Batch [8/43], Training Loss: 0.00001217
2024-11-06 14:07:37,209 - INFO - Epoch [260/300], Batch [9/43], Training Loss: 0.00001651
2024-11-06 14:07:37,212 - INFO - Epoch [260/300], Batch [10/43], Training Loss: 0.00001284
2024-11-06 14:07:37,214 - INFO - Epoch [260/300], Batch [11/43], Training Loss: 0.00001195
2024-11-06 14:07:37,218 - INFO - Epoch [260/300], Batch [12/43], Training Loss: 0.00000663
2024-11-06 14:07:37,221 - INFO - Epoch [260/300], Batch [13/43], Training Loss: 0.00000707
2024-11-06 14:07:37,225 - INFO - Epoch [260/300], Batch [14/43], Training Loss: 0.00000601
2024-11-06 14:07:37,230 - INFO - Epoch [260/300], Batch [15/43], Training Loss: 0.00001204
2024-11-06 14:07:37,234 - INFO - Epoch [260/300], Batch [16/43], Training Loss: 0.00000409
2024-11-06 14:07:37,239 - INFO - Epoch [260/300], Batch [17/43], Training Loss: 0.00000942
2024-11-06 14:07:37,244 - INFO - Epoch [260/300], Batch [18/43], Training Loss: 0.00002045
2024-11-06 14:07:37,248 - INFO - Epoch [260/300], Batch [19/43], Training Loss: 0.00000681
2024-11-06 14:07:37,252 - INFO - Epoch [260/300], Batch [20/43], Training Loss: 0.00001151
2024-11-06 14:07:37,256 - INFO - Epoch [260/300], Batch [21/43], Training Loss: 0.00000955
2024-11-06 14:07:37,259 - INFO - Epoch [260/300], Batch [22/43], Training Loss: 0.00000794
2024-11-06 14:07:37,262 - INFO - Epoch [260/300], Batch [23/43], Training Loss: 0.00000579
2024-11-06 14:07:37,265 - INFO - Epoch [260/300], Batch [24/43], Training Loss: 0.00000819
2024-11-06 14:07:37,269 - INFO - Epoch [260/300], Batch [25/43], Training Loss: 0.00000697
2024-11-06 14:07:37,272 - INFO - Epoch [260/300], Batch [26/43], Training Loss: 0.00001057
2024-11-06 14:07:37,276 - INFO - Epoch [260/300], Batch [27/43], Training Loss: 0.00000548
2024-11-06 14:07:37,279 - INFO - Epoch [260/300], Batch [28/43], Training Loss: 0.00000217
2024-11-06 14:07:37,283 - INFO - Epoch [260/300], Batch [29/43], Training Loss: 0.00002174
2024-11-06 14:07:37,287 - INFO - Epoch [260/300], Batch [30/43], Training Loss: 0.00001080
2024-11-06 14:07:37,289 - INFO - Epoch [260/300], Batch [31/43], Training Loss: 0.00001014
2024-11-06 14:07:37,292 - INFO - Epoch [260/300], Batch [32/43], Training Loss: 0.00001107
2024-11-06 14:07:37,295 - INFO - Epoch [260/300], Batch [33/43], Training Loss: 0.00000804
2024-11-06 14:07:37,299 - INFO - Epoch [260/300], Batch [34/43], Training Loss: 0.00001255
2024-11-06 14:07:37,303 - INFO - Epoch [260/300], Batch [35/43], Training Loss: 0.00004136
2024-11-06 14:07:37,306 - INFO - Epoch [260/300], Batch [36/43], Training Loss: 0.00001632
2024-11-06 14:07:37,311 - INFO - Epoch [260/300], Batch [37/43], Training Loss: 0.00001329
2024-11-06 14:07:37,316 - INFO - Epoch [260/300], Batch [38/43], Training Loss: 0.00002393
2024-11-06 14:07:37,319 - INFO - Epoch [260/300], Batch [39/43], Training Loss: 0.00002340
2024-11-06 14:07:37,322 - INFO - Epoch [260/300], Batch [40/43], Training Loss: 0.00000945
2024-11-06 14:07:37,326 - INFO - Epoch [260/300], Batch [41/43], Training Loss: 0.00000617
2024-11-06 14:07:37,330 - INFO - Epoch [260/300], Batch [42/43], Training Loss: 0.00002086
2024-11-06 14:07:37,334 - INFO - Epoch [260/300], Batch [43/43], Training Loss: 0.00001775
2024-11-06 14:07:37,346 - INFO - Epoch [260/300], Average Training Loss: 0.00001212, Validation Loss: 0.00001286
2024-11-06 14:07:37,350 - INFO - Epoch [261/300], Batch [1/43], Training Loss: 0.00002402
2024-11-06 14:07:37,354 - INFO - Epoch [261/300], Batch [2/43], Training Loss: 0.00001678
2024-11-06 14:07:37,357 - INFO - Epoch [261/300], Batch [3/43], Training Loss: 0.00003577
2024-11-06 14:07:37,360 - INFO - Epoch [261/300], Batch [4/43], Training Loss: 0.00002622
2024-11-06 14:07:37,363 - INFO - Epoch [261/300], Batch [5/43], Training Loss: 0.00000837
2024-11-06 14:07:37,366 - INFO - Epoch [261/300], Batch [6/43], Training Loss: 0.00001466
2024-11-06 14:07:37,369 - INFO - Epoch [261/300], Batch [7/43], Training Loss: 0.00002324
2024-11-06 14:07:37,373 - INFO - Epoch [261/300], Batch [8/43], Training Loss: 0.00001605
2024-11-06 14:07:37,377 - INFO - Epoch [261/300], Batch [9/43], Training Loss: 0.00001660
2024-11-06 14:07:37,381 - INFO - Epoch [261/300], Batch [10/43], Training Loss: 0.00000789
2024-11-06 14:07:37,385 - INFO - Epoch [261/300], Batch [11/43], Training Loss: 0.00002512
2024-11-06 14:07:37,388 - INFO - Epoch [261/300], Batch [12/43], Training Loss: 0.00003013
2024-11-06 14:07:37,392 - INFO - Epoch [261/300], Batch [13/43], Training Loss: 0.00001753
2024-11-06 14:07:37,395 - INFO - Epoch [261/300], Batch [14/43], Training Loss: 0.00001715
2024-11-06 14:07:37,399 - INFO - Epoch [261/300], Batch [15/43], Training Loss: 0.00002710
2024-11-06 14:07:37,403 - INFO - Epoch [261/300], Batch [16/43], Training Loss: 0.00001796
2024-11-06 14:07:37,406 - INFO - Epoch [261/300], Batch [17/43], Training Loss: 0.00001719
2024-11-06 14:07:37,410 - INFO - Epoch [261/300], Batch [18/43], Training Loss: 0.00000599
2024-11-06 14:07:37,413 - INFO - Epoch [261/300], Batch [19/43], Training Loss: 0.00003561
2024-11-06 14:07:37,416 - INFO - Epoch [261/300], Batch [20/43], Training Loss: 0.00001214
2024-11-06 14:07:37,419 - INFO - Epoch [261/300], Batch [21/43], Training Loss: 0.00000507
2024-11-06 14:07:37,423 - INFO - Epoch [261/300], Batch [22/43], Training Loss: 0.00001004
2024-11-06 14:07:37,427 - INFO - Epoch [261/300], Batch [23/43], Training Loss: 0.00001916
2024-11-06 14:07:37,431 - INFO - Epoch [261/300], Batch [24/43], Training Loss: 0.00001739
2024-11-06 14:07:37,434 - INFO - Epoch [261/300], Batch [25/43], Training Loss: 0.00000806
2024-11-06 14:07:37,438 - INFO - Epoch [261/300], Batch [26/43], Training Loss: 0.00000857
2024-11-06 14:07:37,441 - INFO - Epoch [261/300], Batch [27/43], Training Loss: 0.00000672
2024-11-06 14:07:37,445 - INFO - Epoch [261/300], Batch [28/43], Training Loss: 0.00001175
2024-11-06 14:07:37,449 - INFO - Epoch [261/300], Batch [29/43], Training Loss: 0.00000443
2024-11-06 14:07:37,453 - INFO - Epoch [261/300], Batch [30/43], Training Loss: 0.00001075
2024-11-06 14:07:37,458 - INFO - Epoch [261/300], Batch [31/43], Training Loss: 0.00000345
2024-11-06 14:07:37,461 - INFO - Epoch [261/300], Batch [32/43], Training Loss: 0.00001689
2024-11-06 14:07:37,466 - INFO - Epoch [261/300], Batch [33/43], Training Loss: 0.00001087
2024-11-06 14:07:37,470 - INFO - Epoch [261/300], Batch [34/43], Training Loss: 0.00001448
2024-11-06 14:07:37,475 - INFO - Epoch [261/300], Batch [35/43], Training Loss: 0.00000779
2024-11-06 14:07:37,479 - INFO - Epoch [261/300], Batch [36/43], Training Loss: 0.00002056
2024-11-06 14:07:37,482 - INFO - Epoch [261/300], Batch [37/43], Training Loss: 0.00001888
2024-11-06 14:07:37,486 - INFO - Epoch [261/300], Batch [38/43], Training Loss: 0.00000526
2024-11-06 14:07:37,490 - INFO - Epoch [261/300], Batch [39/43], Training Loss: 0.00000681
2024-11-06 14:07:37,493 - INFO - Epoch [261/300], Batch [40/43], Training Loss: 0.00000795
2024-11-06 14:07:37,496 - INFO - Epoch [261/300], Batch [41/43], Training Loss: 0.00000402
2024-11-06 14:07:37,499 - INFO - Epoch [261/300], Batch [42/43], Training Loss: 0.00000742
2024-11-06 14:07:37,502 - INFO - Epoch [261/300], Batch [43/43], Training Loss: 0.00000966
2024-11-06 14:07:37,516 - INFO - Epoch [261/300], Average Training Loss: 0.00001469, Validation Loss: 0.00001463
2024-11-06 14:07:37,520 - INFO - Epoch [262/300], Batch [1/43], Training Loss: 0.00000738
2024-11-06 14:07:37,523 - INFO - Epoch [262/300], Batch [2/43], Training Loss: 0.00001057
2024-11-06 14:07:37,527 - INFO - Epoch [262/300], Batch [3/43], Training Loss: 0.00000828
2024-11-06 14:07:37,532 - INFO - Epoch [262/300], Batch [4/43], Training Loss: 0.00000994
2024-11-06 14:07:37,535 - INFO - Epoch [262/300], Batch [5/43], Training Loss: 0.00000581
2024-11-06 14:07:37,540 - INFO - Epoch [262/300], Batch [6/43], Training Loss: 0.00001067
2024-11-06 14:07:37,544 - INFO - Epoch [262/300], Batch [7/43], Training Loss: 0.00000597
2024-11-06 14:07:37,548 - INFO - Epoch [262/300], Batch [8/43], Training Loss: 0.00000519
2024-11-06 14:07:37,552 - INFO - Epoch [262/300], Batch [9/43], Training Loss: 0.00001132
2024-11-06 14:07:37,558 - INFO - Epoch [262/300], Batch [10/43], Training Loss: 0.00001188
2024-11-06 14:07:37,562 - INFO - Epoch [262/300], Batch [11/43], Training Loss: 0.00001458
2024-11-06 14:07:37,568 - INFO - Epoch [262/300], Batch [12/43], Training Loss: 0.00000513
2024-11-06 14:07:37,572 - INFO - Epoch [262/300], Batch [13/43], Training Loss: 0.00002629
2024-11-06 14:07:37,577 - INFO - Epoch [262/300], Batch [14/43], Training Loss: 0.00002204
2024-11-06 14:07:37,584 - INFO - Epoch [262/300], Batch [15/43], Training Loss: 0.00001436
2024-11-06 14:07:37,589 - INFO - Epoch [262/300], Batch [16/43], Training Loss: 0.00000697
2024-11-06 14:07:37,594 - INFO - Epoch [262/300], Batch [17/43], Training Loss: 0.00000627
2024-11-06 14:07:37,598 - INFO - Epoch [262/300], Batch [18/43], Training Loss: 0.00001022
2024-11-06 14:07:37,602 - INFO - Epoch [262/300], Batch [19/43], Training Loss: 0.00001204
2024-11-06 14:07:37,606 - INFO - Epoch [262/300], Batch [20/43], Training Loss: 0.00001438
2024-11-06 14:07:37,611 - INFO - Epoch [262/300], Batch [21/43], Training Loss: 0.00000905
2024-11-06 14:07:37,614 - INFO - Epoch [262/300], Batch [22/43], Training Loss: 0.00000421
2024-11-06 14:07:37,620 - INFO - Epoch [262/300], Batch [23/43], Training Loss: 0.00002323
2024-11-06 14:07:37,627 - INFO - Epoch [262/300], Batch [24/43], Training Loss: 0.00001245
2024-11-06 14:07:37,632 - INFO - Epoch [262/300], Batch [25/43], Training Loss: 0.00001457
2024-11-06 14:07:37,637 - INFO - Epoch [262/300], Batch [26/43], Training Loss: 0.00001264
2024-11-06 14:07:37,643 - INFO - Epoch [262/300], Batch [27/43], Training Loss: 0.00001091
2024-11-06 14:07:37,648 - INFO - Epoch [262/300], Batch [28/43], Training Loss: 0.00000304
2024-11-06 14:07:37,652 - INFO - Epoch [262/300], Batch [29/43], Training Loss: 0.00001581
2024-11-06 14:07:37,658 - INFO - Epoch [262/300], Batch [30/43], Training Loss: 0.00001198
2024-11-06 14:07:37,662 - INFO - Epoch [262/300], Batch [31/43], Training Loss: 0.00003127
2024-11-06 14:07:37,667 - INFO - Epoch [262/300], Batch [32/43], Training Loss: 0.00000632
2024-11-06 14:07:37,671 - INFO - Epoch [262/300], Batch [33/43], Training Loss: 0.00000551
2024-11-06 14:07:37,676 - INFO - Epoch [262/300], Batch [34/43], Training Loss: 0.00002068
2024-11-06 14:07:37,680 - INFO - Epoch [262/300], Batch [35/43], Training Loss: 0.00000539
2024-11-06 14:07:37,684 - INFO - Epoch [262/300], Batch [36/43], Training Loss: 0.00001416
2024-11-06 14:07:37,688 - INFO - Epoch [262/300], Batch [37/43], Training Loss: 0.00001366
2024-11-06 14:07:37,692 - INFO - Epoch [262/300], Batch [38/43], Training Loss: 0.00001709
2024-11-06 14:07:37,695 - INFO - Epoch [262/300], Batch [39/43], Training Loss: 0.00000855
2024-11-06 14:07:37,698 - INFO - Epoch [262/300], Batch [40/43], Training Loss: 0.00001234
2024-11-06 14:07:37,701 - INFO - Epoch [262/300], Batch [41/43], Training Loss: 0.00000959
2024-11-06 14:07:37,704 - INFO - Epoch [262/300], Batch [42/43], Training Loss: 0.00000676
2024-11-06 14:07:37,708 - INFO - Epoch [262/300], Batch [43/43], Training Loss: 0.00002389
2024-11-06 14:07:37,718 - INFO - Epoch [262/300], Average Training Loss: 0.00001192, Validation Loss: 0.00001301
2024-11-06 14:07:37,721 - INFO - Epoch [263/300], Batch [1/43], Training Loss: 0.00000476
2024-11-06 14:07:37,725 - INFO - Epoch [263/300], Batch [2/43], Training Loss: 0.00000870
2024-11-06 14:07:37,728 - INFO - Epoch [263/300], Batch [3/43], Training Loss: 0.00001494
2024-11-06 14:07:37,732 - INFO - Epoch [263/300], Batch [4/43], Training Loss: 0.00002422
2024-11-06 14:07:37,736 - INFO - Epoch [263/300], Batch [5/43], Training Loss: 0.00000416
2024-11-06 14:07:37,740 - INFO - Epoch [263/300], Batch [6/43], Training Loss: 0.00000657
2024-11-06 14:07:37,744 - INFO - Epoch [263/300], Batch [7/43], Training Loss: 0.00000779
2024-11-06 14:07:37,747 - INFO - Epoch [263/300], Batch [8/43], Training Loss: 0.00001118
2024-11-06 14:07:37,751 - INFO - Epoch [263/300], Batch [9/43], Training Loss: 0.00001681
2024-11-06 14:07:37,754 - INFO - Epoch [263/300], Batch [10/43], Training Loss: 0.00000336
2024-11-06 14:07:37,757 - INFO - Epoch [263/300], Batch [11/43], Training Loss: 0.00000828
2024-11-06 14:07:37,760 - INFO - Epoch [263/300], Batch [12/43], Training Loss: 0.00000380
2024-11-06 14:07:37,763 - INFO - Epoch [263/300], Batch [13/43], Training Loss: 0.00000682
2024-11-06 14:07:37,767 - INFO - Epoch [263/300], Batch [14/43], Training Loss: 0.00000496
2024-11-06 14:07:37,771 - INFO - Epoch [263/300], Batch [15/43], Training Loss: 0.00000300
2024-11-06 14:07:37,775 - INFO - Epoch [263/300], Batch [16/43], Training Loss: 0.00001488
2024-11-06 14:07:37,780 - INFO - Epoch [263/300], Batch [17/43], Training Loss: 0.00000681
2024-11-06 14:07:37,785 - INFO - Epoch [263/300], Batch [18/43], Training Loss: 0.00000816
2024-11-06 14:07:37,789 - INFO - Epoch [263/300], Batch [19/43], Training Loss: 0.00002761
2024-11-06 14:07:37,794 - INFO - Epoch [263/300], Batch [20/43], Training Loss: 0.00002797
2024-11-06 14:07:37,798 - INFO - Epoch [263/300], Batch [21/43], Training Loss: 0.00001344
2024-11-06 14:07:37,801 - INFO - Epoch [263/300], Batch [22/43], Training Loss: 0.00001735
2024-11-06 14:07:37,805 - INFO - Epoch [263/300], Batch [23/43], Training Loss: 0.00000720
2024-11-06 14:07:37,809 - INFO - Epoch [263/300], Batch [24/43], Training Loss: 0.00000569
2024-11-06 14:07:37,813 - INFO - Epoch [263/300], Batch [25/43], Training Loss: 0.00000930
2024-11-06 14:07:37,817 - INFO - Epoch [263/300], Batch [26/43], Training Loss: 0.00001569
2024-11-06 14:07:37,821 - INFO - Epoch [263/300], Batch [27/43], Training Loss: 0.00000927
2024-11-06 14:07:37,825 - INFO - Epoch [263/300], Batch [28/43], Training Loss: 0.00001717
2024-11-06 14:07:37,828 - INFO - Epoch [263/300], Batch [29/43], Training Loss: 0.00003076
2024-11-06 14:07:37,832 - INFO - Epoch [263/300], Batch [30/43], Training Loss: 0.00000643
2024-11-06 14:07:37,835 - INFO - Epoch [263/300], Batch [31/43], Training Loss: 0.00001134
2024-11-06 14:07:37,838 - INFO - Epoch [263/300], Batch [32/43], Training Loss: 0.00001407
2024-11-06 14:07:37,842 - INFO - Epoch [263/300], Batch [33/43], Training Loss: 0.00001747
2024-11-06 14:07:37,845 - INFO - Epoch [263/300], Batch [34/43], Training Loss: 0.00000860
2024-11-06 14:07:37,849 - INFO - Epoch [263/300], Batch [35/43], Training Loss: 0.00000409
2024-11-06 14:07:37,852 - INFO - Epoch [263/300], Batch [36/43], Training Loss: 0.00000585
2024-11-06 14:07:37,855 - INFO - Epoch [263/300], Batch [37/43], Training Loss: 0.00000865
2024-11-06 14:07:37,858 - INFO - Epoch [263/300], Batch [38/43], Training Loss: 0.00000977
2024-11-06 14:07:37,861 - INFO - Epoch [263/300], Batch [39/43], Training Loss: 0.00000544
2024-11-06 14:07:37,864 - INFO - Epoch [263/300], Batch [40/43], Training Loss: 0.00001867
2024-11-06 14:07:37,866 - INFO - Epoch [263/300], Batch [41/43], Training Loss: 0.00000748
2024-11-06 14:07:37,869 - INFO - Epoch [263/300], Batch [42/43], Training Loss: 0.00001062
2024-11-06 14:07:37,872 - INFO - Epoch [263/300], Batch [43/43], Training Loss: 0.00000812
2024-11-06 14:07:37,883 - INFO - Epoch [263/300], Average Training Loss: 0.00001110, Validation Loss: 0.00001197
2024-11-06 14:07:37,886 - INFO - Epoch [264/300], Batch [1/43], Training Loss: 0.00000815
2024-11-06 14:07:37,890 - INFO - Epoch [264/300], Batch [2/43], Training Loss: 0.00000539
2024-11-06 14:07:37,894 - INFO - Epoch [264/300], Batch [3/43], Training Loss: 0.00002006
2024-11-06 14:07:37,897 - INFO - Epoch [264/300], Batch [4/43], Training Loss: 0.00001762
2024-11-06 14:07:37,900 - INFO - Epoch [264/300], Batch [5/43], Training Loss: 0.00000630
2024-11-06 14:07:37,904 - INFO - Epoch [264/300], Batch [6/43], Training Loss: 0.00000864
2024-11-06 14:07:37,907 - INFO - Epoch [264/300], Batch [7/43], Training Loss: 0.00002748
2024-11-06 14:07:37,911 - INFO - Epoch [264/300], Batch [8/43], Training Loss: 0.00000669
2024-11-06 14:07:37,914 - INFO - Epoch [264/300], Batch [9/43], Training Loss: 0.00001591
2024-11-06 14:07:37,917 - INFO - Epoch [264/300], Batch [10/43], Training Loss: 0.00000662
2024-11-06 14:07:37,922 - INFO - Epoch [264/300], Batch [11/43], Training Loss: 0.00000421
2024-11-06 14:07:37,926 - INFO - Epoch [264/300], Batch [12/43], Training Loss: 0.00001646
2024-11-06 14:07:37,929 - INFO - Epoch [264/300], Batch [13/43], Training Loss: 0.00000684
2024-11-06 14:07:37,934 - INFO - Epoch [264/300], Batch [14/43], Training Loss: 0.00000844
2024-11-06 14:07:37,938 - INFO - Epoch [264/300], Batch [15/43], Training Loss: 0.00001546
2024-11-06 14:07:37,942 - INFO - Epoch [264/300], Batch [16/43], Training Loss: 0.00001118
2024-11-06 14:07:37,947 - INFO - Epoch [264/300], Batch [17/43], Training Loss: 0.00000978
2024-11-06 14:07:37,951 - INFO - Epoch [264/300], Batch [18/43], Training Loss: 0.00000951
2024-11-06 14:07:37,955 - INFO - Epoch [264/300], Batch [19/43], Training Loss: 0.00000902
2024-11-06 14:07:37,959 - INFO - Epoch [264/300], Batch [20/43], Training Loss: 0.00001301
2024-11-06 14:07:37,962 - INFO - Epoch [264/300], Batch [21/43], Training Loss: 0.00000529
2024-11-06 14:07:37,967 - INFO - Epoch [264/300], Batch [22/43], Training Loss: 0.00000544
2024-11-06 14:07:37,972 - INFO - Epoch [264/300], Batch [23/43], Training Loss: 0.00000478
2024-11-06 14:07:37,977 - INFO - Epoch [264/300], Batch [24/43], Training Loss: 0.00001842
2024-11-06 14:07:37,982 - INFO - Epoch [264/300], Batch [25/43], Training Loss: 0.00000625
2024-11-06 14:07:37,986 - INFO - Epoch [264/300], Batch [26/43], Training Loss: 0.00000820
2024-11-06 14:07:37,991 - INFO - Epoch [264/300], Batch [27/43], Training Loss: 0.00003065
2024-11-06 14:07:37,995 - INFO - Epoch [264/300], Batch [28/43], Training Loss: 0.00000506
2024-11-06 14:07:37,999 - INFO - Epoch [264/300], Batch [29/43], Training Loss: 0.00000576
2024-11-06 14:07:38,004 - INFO - Epoch [264/300], Batch [30/43], Training Loss: 0.00000343
2024-11-06 14:07:38,007 - INFO - Epoch [264/300], Batch [31/43], Training Loss: 0.00000261
2024-11-06 14:07:38,010 - INFO - Epoch [264/300], Batch [32/43], Training Loss: 0.00000810
2024-11-06 14:07:38,013 - INFO - Epoch [264/300], Batch [33/43], Training Loss: 0.00001936
2024-11-06 14:07:38,017 - INFO - Epoch [264/300], Batch [34/43], Training Loss: 0.00001292
2024-11-06 14:07:38,020 - INFO - Epoch [264/300], Batch [35/43], Training Loss: 0.00001229
2024-11-06 14:07:38,024 - INFO - Epoch [264/300], Batch [36/43], Training Loss: 0.00000712
2024-11-06 14:07:38,029 - INFO - Epoch [264/300], Batch [37/43], Training Loss: 0.00000337
2024-11-06 14:07:38,034 - INFO - Epoch [264/300], Batch [38/43], Training Loss: 0.00000880
2024-11-06 14:07:38,039 - INFO - Epoch [264/300], Batch [39/43], Training Loss: 0.00000415
2024-11-06 14:07:38,042 - INFO - Epoch [264/300], Batch [40/43], Training Loss: 0.00001817
2024-11-06 14:07:38,046 - INFO - Epoch [264/300], Batch [41/43], Training Loss: 0.00000639
2024-11-06 14:07:38,050 - INFO - Epoch [264/300], Batch [42/43], Training Loss: 0.00000797
2024-11-06 14:07:38,054 - INFO - Epoch [264/300], Batch [43/43], Training Loss: 0.00000910
2024-11-06 14:07:38,066 - INFO - Epoch [264/300], Average Training Loss: 0.00001024, Validation Loss: 0.00001408
2024-11-06 14:07:38,070 - INFO - Epoch [265/300], Batch [1/43], Training Loss: 0.00001129
2024-11-06 14:07:38,074 - INFO - Epoch [265/300], Batch [2/43], Training Loss: 0.00000875
2024-11-06 14:07:38,078 - INFO - Epoch [265/300], Batch [3/43], Training Loss: 0.00002316
2024-11-06 14:07:38,084 - INFO - Epoch [265/300], Batch [4/43], Training Loss: 0.00001237
2024-11-06 14:07:38,087 - INFO - Epoch [265/300], Batch [5/43], Training Loss: 0.00001088
2024-11-06 14:07:38,091 - INFO - Epoch [265/300], Batch [6/43], Training Loss: 0.00001585
2024-11-06 14:07:38,095 - INFO - Epoch [265/300], Batch [7/43], Training Loss: 0.00001582
2024-11-06 14:07:38,099 - INFO - Epoch [265/300], Batch [8/43], Training Loss: 0.00001136
2024-11-06 14:07:38,103 - INFO - Epoch [265/300], Batch [9/43], Training Loss: 0.00002711
2024-11-06 14:07:38,107 - INFO - Epoch [265/300], Batch [10/43], Training Loss: 0.00000370
2024-11-06 14:07:38,111 - INFO - Epoch [265/300], Batch [11/43], Training Loss: 0.00000614
2024-11-06 14:07:38,115 - INFO - Epoch [265/300], Batch [12/43], Training Loss: 0.00004168
2024-11-06 14:07:38,119 - INFO - Epoch [265/300], Batch [13/43], Training Loss: 0.00000630
2024-11-06 14:07:38,123 - INFO - Epoch [265/300], Batch [14/43], Training Loss: 0.00000924
2024-11-06 14:07:38,126 - INFO - Epoch [265/300], Batch [15/43], Training Loss: 0.00001235
2024-11-06 14:07:38,129 - INFO - Epoch [265/300], Batch [16/43], Training Loss: 0.00001377
2024-11-06 14:07:38,133 - INFO - Epoch [265/300], Batch [17/43], Training Loss: 0.00000725
2024-11-06 14:07:38,136 - INFO - Epoch [265/300], Batch [18/43], Training Loss: 0.00000610
2024-11-06 14:07:38,140 - INFO - Epoch [265/300], Batch [19/43], Training Loss: 0.00000314
2024-11-06 14:07:38,143 - INFO - Epoch [265/300], Batch [20/43], Training Loss: 0.00000574
2024-11-06 14:07:38,146 - INFO - Epoch [265/300], Batch [21/43], Training Loss: 0.00001537
2024-11-06 14:07:38,150 - INFO - Epoch [265/300], Batch [22/43], Training Loss: 0.00002080
2024-11-06 14:07:38,153 - INFO - Epoch [265/300], Batch [23/43], Training Loss: 0.00000462
2024-11-06 14:07:38,156 - INFO - Epoch [265/300], Batch [24/43], Training Loss: 0.00002234
2024-11-06 14:07:38,159 - INFO - Epoch [265/300], Batch [25/43], Training Loss: 0.00000744
2024-11-06 14:07:38,163 - INFO - Epoch [265/300], Batch [26/43], Training Loss: 0.00000701
2024-11-06 14:07:38,166 - INFO - Epoch [265/300], Batch [27/43], Training Loss: 0.00000988
2024-11-06 14:07:38,169 - INFO - Epoch [265/300], Batch [28/43], Training Loss: 0.00001594
2024-11-06 14:07:38,173 - INFO - Epoch [265/300], Batch [29/43], Training Loss: 0.00001254
2024-11-06 14:07:38,176 - INFO - Epoch [265/300], Batch [30/43], Training Loss: 0.00002773
2024-11-06 14:07:38,180 - INFO - Epoch [265/300], Batch [31/43], Training Loss: 0.00000766
2024-11-06 14:07:38,183 - INFO - Epoch [265/300], Batch [32/43], Training Loss: 0.00002629
2024-11-06 14:07:38,186 - INFO - Epoch [265/300], Batch [33/43], Training Loss: 0.00001241
2024-11-06 14:07:38,190 - INFO - Epoch [265/300], Batch [34/43], Training Loss: 0.00000461
2024-11-06 14:07:38,194 - INFO - Epoch [265/300], Batch [35/43], Training Loss: 0.00001018
2024-11-06 14:07:38,197 - INFO - Epoch [265/300], Batch [36/43], Training Loss: 0.00001413
2024-11-06 14:07:38,201 - INFO - Epoch [265/300], Batch [37/43], Training Loss: 0.00002389
2024-11-06 14:07:38,205 - INFO - Epoch [265/300], Batch [38/43], Training Loss: 0.00000472
2024-11-06 14:07:38,209 - INFO - Epoch [265/300], Batch [39/43], Training Loss: 0.00001958
2024-11-06 14:07:38,213 - INFO - Epoch [265/300], Batch [40/43], Training Loss: 0.00001150
2024-11-06 14:07:38,217 - INFO - Epoch [265/300], Batch [41/43], Training Loss: 0.00001185
2024-11-06 14:07:38,221 - INFO - Epoch [265/300], Batch [42/43], Training Loss: 0.00000782
2024-11-06 14:07:38,226 - INFO - Epoch [265/300], Batch [43/43], Training Loss: 0.00001487
2024-11-06 14:07:38,238 - INFO - Epoch [265/300], Average Training Loss: 0.00001314, Validation Loss: 0.00002017
2024-11-06 14:07:38,284 - INFO - Epoch [266/300], Batch [1/43], Training Loss: 0.00001838
2024-11-06 14:07:38,289 - INFO - Epoch [266/300], Batch [2/43], Training Loss: 0.00001771
2024-11-06 14:07:38,295 - INFO - Epoch [266/300], Batch [3/43], Training Loss: 0.00000817
2024-11-06 14:07:38,302 - INFO - Epoch [266/300], Batch [4/43], Training Loss: 0.00000773
2024-11-06 14:07:38,305 - INFO - Epoch [266/300], Batch [5/43], Training Loss: 0.00000845
2024-11-06 14:07:38,309 - INFO - Epoch [266/300], Batch [6/43], Training Loss: 0.00001449
2024-11-06 14:07:38,319 - INFO - Epoch [266/300], Batch [7/43], Training Loss: 0.00000564
2024-11-06 14:07:38,324 - INFO - Epoch [266/300], Batch [8/43], Training Loss: 0.00000642
2024-11-06 14:07:38,328 - INFO - Epoch [266/300], Batch [9/43], Training Loss: 0.00001331
2024-11-06 14:07:38,332 - INFO - Epoch [266/300], Batch [10/43], Training Loss: 0.00000259
2024-11-06 14:07:38,335 - INFO - Epoch [266/300], Batch [11/43], Training Loss: 0.00002003
2024-11-06 14:07:38,338 - INFO - Epoch [266/300], Batch [12/43], Training Loss: 0.00000380
2024-11-06 14:07:38,341 - INFO - Epoch [266/300], Batch [13/43], Training Loss: 0.00001283
2024-11-06 14:07:38,345 - INFO - Epoch [266/300], Batch [14/43], Training Loss: 0.00000592
2024-11-06 14:07:38,349 - INFO - Epoch [266/300], Batch [15/43], Training Loss: 0.00001695
2024-11-06 14:07:38,352 - INFO - Epoch [266/300], Batch [16/43], Training Loss: 0.00000708
2024-11-06 14:07:38,356 - INFO - Epoch [266/300], Batch [17/43], Training Loss: 0.00001259
2024-11-06 14:07:38,360 - INFO - Epoch [266/300], Batch [18/43], Training Loss: 0.00000905
2024-11-06 14:07:38,364 - INFO - Epoch [266/300], Batch [19/43], Training Loss: 0.00000340
2024-11-06 14:07:38,367 - INFO - Epoch [266/300], Batch [20/43], Training Loss: 0.00000468
2024-11-06 14:07:38,371 - INFO - Epoch [266/300], Batch [21/43], Training Loss: 0.00001603
2024-11-06 14:07:38,375 - INFO - Epoch [266/300], Batch [22/43], Training Loss: 0.00000880
2024-11-06 14:07:38,378 - INFO - Epoch [266/300], Batch [23/43], Training Loss: 0.00001277
2024-11-06 14:07:38,382 - INFO - Epoch [266/300], Batch [24/43], Training Loss: 0.00000813
2024-11-06 14:07:38,386 - INFO - Epoch [266/300], Batch [25/43], Training Loss: 0.00002092
2024-11-06 14:07:38,390 - INFO - Epoch [266/300], Batch [26/43], Training Loss: 0.00000227
2024-11-06 14:07:38,394 - INFO - Epoch [266/300], Batch [27/43], Training Loss: 0.00001395
2024-11-06 14:07:38,398 - INFO - Epoch [266/300], Batch [28/43], Training Loss: 0.00001135
2024-11-06 14:07:38,401 - INFO - Epoch [266/300], Batch [29/43], Training Loss: 0.00001963
2024-11-06 14:07:38,405 - INFO - Epoch [266/300], Batch [30/43], Training Loss: 0.00001444
2024-11-06 14:07:38,408 - INFO - Epoch [266/300], Batch [31/43], Training Loss: 0.00001304
2024-11-06 14:07:38,412 - INFO - Epoch [266/300], Batch [32/43], Training Loss: 0.00001034
2024-11-06 14:07:38,415 - INFO - Epoch [266/300], Batch [33/43], Training Loss: 0.00000833
2024-11-06 14:07:38,418 - INFO - Epoch [266/300], Batch [34/43], Training Loss: 0.00001498
2024-11-06 14:07:38,421 - INFO - Epoch [266/300], Batch [35/43], Training Loss: 0.00000801
2024-11-06 14:07:38,425 - INFO - Epoch [266/300], Batch [36/43], Training Loss: 0.00002102
2024-11-06 14:07:38,428 - INFO - Epoch [266/300], Batch [37/43], Training Loss: 0.00000925
2024-11-06 14:07:38,431 - INFO - Epoch [266/300], Batch [38/43], Training Loss: 0.00000327
2024-11-06 14:07:38,434 - INFO - Epoch [266/300], Batch [39/43], Training Loss: 0.00000888
2024-11-06 14:07:38,438 - INFO - Epoch [266/300], Batch [40/43], Training Loss: 0.00001844
2024-11-06 14:07:38,442 - INFO - Epoch [266/300], Batch [41/43], Training Loss: 0.00000481
2024-11-06 14:07:38,446 - INFO - Epoch [266/300], Batch [42/43], Training Loss: 0.00000307
2024-11-06 14:07:38,450 - INFO - Epoch [266/300], Batch [43/43], Training Loss: 0.00000657
2024-11-06 14:07:38,462 - INFO - Epoch [266/300], Average Training Loss: 0.00001064, Validation Loss: 0.00001217
2024-11-06 14:07:38,466 - INFO - Epoch [267/300], Batch [1/43], Training Loss: 0.00001182
2024-11-06 14:07:38,469 - INFO - Epoch [267/300], Batch [2/43], Training Loss: 0.00000799
2024-11-06 14:07:38,472 - INFO - Epoch [267/300], Batch [3/43], Training Loss: 0.00000668
2024-11-06 14:07:38,476 - INFO - Epoch [267/300], Batch [4/43], Training Loss: 0.00001092
2024-11-06 14:07:38,480 - INFO - Epoch [267/300], Batch [5/43], Training Loss: 0.00000590
2024-11-06 14:07:38,484 - INFO - Epoch [267/300], Batch [6/43], Training Loss: 0.00002043
2024-11-06 14:07:38,487 - INFO - Epoch [267/300], Batch [7/43], Training Loss: 0.00003070
2024-11-06 14:07:38,490 - INFO - Epoch [267/300], Batch [8/43], Training Loss: 0.00001126
2024-11-06 14:07:38,493 - INFO - Epoch [267/300], Batch [9/43], Training Loss: 0.00000457
2024-11-06 14:07:38,496 - INFO - Epoch [267/300], Batch [10/43], Training Loss: 0.00000799
2024-11-06 14:07:38,500 - INFO - Epoch [267/300], Batch [11/43], Training Loss: 0.00000854
2024-11-06 14:07:38,503 - INFO - Epoch [267/300], Batch [12/43], Training Loss: 0.00001138
2024-11-06 14:07:38,506 - INFO - Epoch [267/300], Batch [13/43], Training Loss: 0.00000273
2024-11-06 14:07:38,509 - INFO - Epoch [267/300], Batch [14/43], Training Loss: 0.00000705
2024-11-06 14:07:38,512 - INFO - Epoch [267/300], Batch [15/43], Training Loss: 0.00000591
2024-11-06 14:07:38,516 - INFO - Epoch [267/300], Batch [16/43], Training Loss: 0.00000511
2024-11-06 14:07:38,520 - INFO - Epoch [267/300], Batch [17/43], Training Loss: 0.00000675
2024-11-06 14:07:38,523 - INFO - Epoch [267/300], Batch [18/43], Training Loss: 0.00003238
2024-11-06 14:07:38,527 - INFO - Epoch [267/300], Batch [19/43], Training Loss: 0.00000457
2024-11-06 14:07:38,530 - INFO - Epoch [267/300], Batch [20/43], Training Loss: 0.00000666
2024-11-06 14:07:38,535 - INFO - Epoch [267/300], Batch [21/43], Training Loss: 0.00001320
2024-11-06 14:07:38,539 - INFO - Epoch [267/300], Batch [22/43], Training Loss: 0.00000782
2024-11-06 14:07:38,542 - INFO - Epoch [267/300], Batch [23/43], Training Loss: 0.00001705
2024-11-06 14:07:38,546 - INFO - Epoch [267/300], Batch [24/43], Training Loss: 0.00000628
2024-11-06 14:07:38,550 - INFO - Epoch [267/300], Batch [25/43], Training Loss: 0.00000570
2024-11-06 14:07:38,554 - INFO - Epoch [267/300], Batch [26/43], Training Loss: 0.00000700
2024-11-06 14:07:38,558 - INFO - Epoch [267/300], Batch [27/43], Training Loss: 0.00001645
2024-11-06 14:07:38,563 - INFO - Epoch [267/300], Batch [28/43], Training Loss: 0.00001083
2024-11-06 14:07:38,569 - INFO - Epoch [267/300], Batch [29/43], Training Loss: 0.00000173
2024-11-06 14:07:38,574 - INFO - Epoch [267/300], Batch [30/43], Training Loss: 0.00000852
2024-11-06 14:07:38,581 - INFO - Epoch [267/300], Batch [31/43], Training Loss: 0.00001363
2024-11-06 14:07:38,587 - INFO - Epoch [267/300], Batch [32/43], Training Loss: 0.00001437
2024-11-06 14:07:38,592 - INFO - Epoch [267/300], Batch [33/43], Training Loss: 0.00000988
2024-11-06 14:07:38,597 - INFO - Epoch [267/300], Batch [34/43], Training Loss: 0.00000501
2024-11-06 14:07:38,604 - INFO - Epoch [267/300], Batch [35/43], Training Loss: 0.00002005
2024-11-06 14:07:38,609 - INFO - Epoch [267/300], Batch [36/43], Training Loss: 0.00000983
2024-11-06 14:07:38,615 - INFO - Epoch [267/300], Batch [37/43], Training Loss: 0.00000968
2024-11-06 14:07:38,621 - INFO - Epoch [267/300], Batch [38/43], Training Loss: 0.00001584
2024-11-06 14:07:38,627 - INFO - Epoch [267/300], Batch [39/43], Training Loss: 0.00001313
2024-11-06 14:07:38,634 - INFO - Epoch [267/300], Batch [40/43], Training Loss: 0.00001711
2024-11-06 14:07:38,640 - INFO - Epoch [267/300], Batch [41/43], Training Loss: 0.00000981
2024-11-06 14:07:38,647 - INFO - Epoch [267/300], Batch [42/43], Training Loss: 0.00000805
2024-11-06 14:07:38,653 - INFO - Epoch [267/300], Batch [43/43], Training Loss: 0.00000755
2024-11-06 14:07:38,668 - INFO - Epoch [267/300], Average Training Loss: 0.00001065, Validation Loss: 0.00001687
2024-11-06 14:07:38,673 - INFO - Epoch [268/300], Batch [1/43], Training Loss: 0.00000774
2024-11-06 14:07:38,678 - INFO - Epoch [268/300], Batch [2/43], Training Loss: 0.00000908
2024-11-06 14:07:38,682 - INFO - Epoch [268/300], Batch [3/43], Training Loss: 0.00000376
2024-11-06 14:07:38,686 - INFO - Epoch [268/300], Batch [4/43], Training Loss: 0.00000558
2024-11-06 14:07:38,691 - INFO - Epoch [268/300], Batch [5/43], Training Loss: 0.00001176
2024-11-06 14:07:38,694 - INFO - Epoch [268/300], Batch [6/43], Training Loss: 0.00000876
2024-11-06 14:07:38,699 - INFO - Epoch [268/300], Batch [7/43], Training Loss: 0.00000220
2024-11-06 14:07:38,704 - INFO - Epoch [268/300], Batch [8/43], Training Loss: 0.00000347
2024-11-06 14:07:38,709 - INFO - Epoch [268/300], Batch [9/43], Training Loss: 0.00000255
2024-11-06 14:07:38,713 - INFO - Epoch [268/300], Batch [10/43], Training Loss: 0.00000827
2024-11-06 14:07:38,717 - INFO - Epoch [268/300], Batch [11/43], Training Loss: 0.00001439
2024-11-06 14:07:38,722 - INFO - Epoch [268/300], Batch [12/43], Training Loss: 0.00000997
2024-11-06 14:07:38,726 - INFO - Epoch [268/300], Batch [13/43], Training Loss: 0.00000766
2024-11-06 14:07:38,730 - INFO - Epoch [268/300], Batch [14/43], Training Loss: 0.00002284
2024-11-06 14:07:38,734 - INFO - Epoch [268/300], Batch [15/43], Training Loss: 0.00000430
2024-11-06 14:07:38,739 - INFO - Epoch [268/300], Batch [16/43], Training Loss: 0.00001030
2024-11-06 14:07:38,743 - INFO - Epoch [268/300], Batch [17/43], Training Loss: 0.00000305
2024-11-06 14:07:38,748 - INFO - Epoch [268/300], Batch [18/43], Training Loss: 0.00000719
2024-11-06 14:07:38,753 - INFO - Epoch [268/300], Batch [19/43], Training Loss: 0.00001245
2024-11-06 14:07:38,757 - INFO - Epoch [268/300], Batch [20/43], Training Loss: 0.00002094
2024-11-06 14:07:38,761 - INFO - Epoch [268/300], Batch [21/43], Training Loss: 0.00002084
2024-11-06 14:07:38,766 - INFO - Epoch [268/300], Batch [22/43], Training Loss: 0.00001251
2024-11-06 14:07:38,770 - INFO - Epoch [268/300], Batch [23/43], Training Loss: 0.00000918
2024-11-06 14:07:38,773 - INFO - Epoch [268/300], Batch [24/43], Training Loss: 0.00001092
2024-11-06 14:07:38,777 - INFO - Epoch [268/300], Batch [25/43], Training Loss: 0.00001142
2024-11-06 14:07:38,780 - INFO - Epoch [268/300], Batch [26/43], Training Loss: 0.00001696
2024-11-06 14:07:38,784 - INFO - Epoch [268/300], Batch [27/43], Training Loss: 0.00001325
2024-11-06 14:07:38,789 - INFO - Epoch [268/300], Batch [28/43], Training Loss: 0.00001081
2024-11-06 14:07:38,794 - INFO - Epoch [268/300], Batch [29/43], Training Loss: 0.00000353
2024-11-06 14:07:38,799 - INFO - Epoch [268/300], Batch [30/43], Training Loss: 0.00000789
2024-11-06 14:07:38,803 - INFO - Epoch [268/300], Batch [31/43], Training Loss: 0.00000565
2024-11-06 14:07:38,807 - INFO - Epoch [268/300], Batch [32/43], Training Loss: 0.00001303
2024-11-06 14:07:38,811 - INFO - Epoch [268/300], Batch [33/43], Training Loss: 0.00001199
2024-11-06 14:07:38,815 - INFO - Epoch [268/300], Batch [34/43], Training Loss: 0.00000972
2024-11-06 14:07:38,818 - INFO - Epoch [268/300], Batch [35/43], Training Loss: 0.00000697
2024-11-06 14:07:38,822 - INFO - Epoch [268/300], Batch [36/43], Training Loss: 0.00001806
2024-11-06 14:07:38,825 - INFO - Epoch [268/300], Batch [37/43], Training Loss: 0.00000525
2024-11-06 14:07:38,828 - INFO - Epoch [268/300], Batch [38/43], Training Loss: 0.00000921
2024-11-06 14:07:38,831 - INFO - Epoch [268/300], Batch [39/43], Training Loss: 0.00001442
2024-11-06 14:07:38,834 - INFO - Epoch [268/300], Batch [40/43], Training Loss: 0.00000892
2024-11-06 14:07:38,837 - INFO - Epoch [268/300], Batch [41/43], Training Loss: 0.00002061
2024-11-06 14:07:38,840 - INFO - Epoch [268/300], Batch [42/43], Training Loss: 0.00000805
2024-11-06 14:07:38,844 - INFO - Epoch [268/300], Batch [43/43], Training Loss: 0.00002035
2024-11-06 14:07:38,854 - INFO - Epoch [268/300], Average Training Loss: 0.00001037, Validation Loss: 0.00001290
2024-11-06 14:07:38,858 - INFO - Epoch [269/300], Batch [1/43], Training Loss: 0.00000638
2024-11-06 14:07:38,862 - INFO - Epoch [269/300], Batch [2/43], Training Loss: 0.00000557
2024-11-06 14:07:38,866 - INFO - Epoch [269/300], Batch [3/43], Training Loss: 0.00002261
2024-11-06 14:07:38,869 - INFO - Epoch [269/300], Batch [4/43], Training Loss: 0.00000456
2024-11-06 14:07:38,873 - INFO - Epoch [269/300], Batch [5/43], Training Loss: 0.00001638
2024-11-06 14:07:38,876 - INFO - Epoch [269/300], Batch [6/43], Training Loss: 0.00000781
2024-11-06 14:07:38,880 - INFO - Epoch [269/300], Batch [7/43], Training Loss: 0.00000693
2024-11-06 14:07:38,883 - INFO - Epoch [269/300], Batch [8/43], Training Loss: 0.00000604
2024-11-06 14:07:38,888 - INFO - Epoch [269/300], Batch [9/43], Training Loss: 0.00000746
2024-11-06 14:07:38,892 - INFO - Epoch [269/300], Batch [10/43], Training Loss: 0.00000248
2024-11-06 14:07:38,895 - INFO - Epoch [269/300], Batch [11/43], Training Loss: 0.00000596
2024-11-06 14:07:38,898 - INFO - Epoch [269/300], Batch [12/43], Training Loss: 0.00000587
2024-11-06 14:07:38,902 - INFO - Epoch [269/300], Batch [13/43], Training Loss: 0.00000105
2024-11-06 14:07:38,905 - INFO - Epoch [269/300], Batch [14/43], Training Loss: 0.00001002
2024-11-06 14:07:38,908 - INFO - Epoch [269/300], Batch [15/43], Training Loss: 0.00000581
2024-11-06 14:07:38,912 - INFO - Epoch [269/300], Batch [16/43], Training Loss: 0.00001831
2024-11-06 14:07:38,916 - INFO - Epoch [269/300], Batch [17/43], Training Loss: 0.00000649
2024-11-06 14:07:38,922 - INFO - Epoch [269/300], Batch [18/43], Training Loss: 0.00001480
2024-11-06 14:07:38,927 - INFO - Epoch [269/300], Batch [19/43], Training Loss: 0.00001849
2024-11-06 14:07:38,931 - INFO - Epoch [269/300], Batch [20/43], Training Loss: 0.00001466
2024-11-06 14:07:38,935 - INFO - Epoch [269/300], Batch [21/43], Training Loss: 0.00000996
2024-11-06 14:07:38,941 - INFO - Epoch [269/300], Batch [22/43], Training Loss: 0.00001041
2024-11-06 14:07:38,945 - INFO - Epoch [269/300], Batch [23/43], Training Loss: 0.00001698
2024-11-06 14:07:38,949 - INFO - Epoch [269/300], Batch [24/43], Training Loss: 0.00001126
2024-11-06 14:07:38,954 - INFO - Epoch [269/300], Batch [25/43], Training Loss: 0.00000621
2024-11-06 14:07:38,958 - INFO - Epoch [269/300], Batch [26/43], Training Loss: 0.00000735
2024-11-06 14:07:38,963 - INFO - Epoch [269/300], Batch [27/43], Training Loss: 0.00001220
2024-11-06 14:07:38,968 - INFO - Epoch [269/300], Batch [28/43], Training Loss: 0.00001253
2024-11-06 14:07:38,972 - INFO - Epoch [269/300], Batch [29/43], Training Loss: 0.00000397
2024-11-06 14:07:38,976 - INFO - Epoch [269/300], Batch [30/43], Training Loss: 0.00001221
2024-11-06 14:07:38,981 - INFO - Epoch [269/300], Batch [31/43], Training Loss: 0.00000624
2024-11-06 14:07:38,985 - INFO - Epoch [269/300], Batch [32/43], Training Loss: 0.00002977
2024-11-06 14:07:38,989 - INFO - Epoch [269/300], Batch [33/43], Training Loss: 0.00001162
2024-11-06 14:07:38,993 - INFO - Epoch [269/300], Batch [34/43], Training Loss: 0.00000856
2024-11-06 14:07:38,996 - INFO - Epoch [269/300], Batch [35/43], Training Loss: 0.00001766
2024-11-06 14:07:39,000 - INFO - Epoch [269/300], Batch [36/43], Training Loss: 0.00001395
2024-11-06 14:07:39,004 - INFO - Epoch [269/300], Batch [37/43], Training Loss: 0.00003370
2024-11-06 14:07:39,008 - INFO - Epoch [269/300], Batch [38/43], Training Loss: 0.00000405
2024-11-06 14:07:39,012 - INFO - Epoch [269/300], Batch [39/43], Training Loss: 0.00001619
2024-11-06 14:07:39,016 - INFO - Epoch [269/300], Batch [40/43], Training Loss: 0.00000884
2024-11-06 14:07:39,019 - INFO - Epoch [269/300], Batch [41/43], Training Loss: 0.00000287
2024-11-06 14:07:39,022 - INFO - Epoch [269/300], Batch [42/43], Training Loss: 0.00000334
2024-11-06 14:07:39,025 - INFO - Epoch [269/300], Batch [43/43], Training Loss: 0.00001114
2024-11-06 14:07:39,036 - INFO - Epoch [269/300], Average Training Loss: 0.00001067, Validation Loss: 0.00001771
2024-11-06 14:07:39,040 - INFO - Epoch [270/300], Batch [1/43], Training Loss: 0.00001194
2024-11-06 14:07:39,043 - INFO - Epoch [270/300], Batch [2/43], Training Loss: 0.00000764
2024-11-06 14:07:39,046 - INFO - Epoch [270/300], Batch [3/43], Training Loss: 0.00000417
2024-11-06 14:07:39,049 - INFO - Epoch [270/300], Batch [4/43], Training Loss: 0.00000511
2024-11-06 14:07:39,052 - INFO - Epoch [270/300], Batch [5/43], Training Loss: 0.00000855
2024-11-06 14:07:39,056 - INFO - Epoch [270/300], Batch [6/43], Training Loss: 0.00000997
2024-11-06 14:07:39,059 - INFO - Epoch [270/300], Batch [7/43], Training Loss: 0.00000312
2024-11-06 14:07:39,062 - INFO - Epoch [270/300], Batch [8/43], Training Loss: 0.00000444
2024-11-06 14:07:39,065 - INFO - Epoch [270/300], Batch [9/43], Training Loss: 0.00001063
2024-11-06 14:07:39,068 - INFO - Epoch [270/300], Batch [10/43], Training Loss: 0.00000526
2024-11-06 14:07:39,071 - INFO - Epoch [270/300], Batch [11/43], Training Loss: 0.00003097
2024-11-06 14:07:39,075 - INFO - Epoch [270/300], Batch [12/43], Training Loss: 0.00001341
2024-11-06 14:07:39,078 - INFO - Epoch [270/300], Batch [13/43], Training Loss: 0.00003229
2024-11-06 14:07:39,081 - INFO - Epoch [270/300], Batch [14/43], Training Loss: 0.00000884
2024-11-06 14:07:39,085 - INFO - Epoch [270/300], Batch [15/43], Training Loss: 0.00000273
2024-11-06 14:07:39,090 - INFO - Epoch [270/300], Batch [16/43], Training Loss: 0.00002052
2024-11-06 14:07:39,095 - INFO - Epoch [270/300], Batch [17/43], Training Loss: 0.00001020
2024-11-06 14:07:39,101 - INFO - Epoch [270/300], Batch [18/43], Training Loss: 0.00000518
2024-11-06 14:07:39,105 - INFO - Epoch [270/300], Batch [19/43], Training Loss: 0.00000604
2024-11-06 14:07:39,109 - INFO - Epoch [270/300], Batch [20/43], Training Loss: 0.00001603
2024-11-06 14:07:39,113 - INFO - Epoch [270/300], Batch [21/43], Training Loss: 0.00001782
2024-11-06 14:07:39,117 - INFO - Epoch [270/300], Batch [22/43], Training Loss: 0.00000574
2024-11-06 14:07:39,121 - INFO - Epoch [270/300], Batch [23/43], Training Loss: 0.00001077
2024-11-06 14:07:39,125 - INFO - Epoch [270/300], Batch [24/43], Training Loss: 0.00000950
2024-11-06 14:07:39,129 - INFO - Epoch [270/300], Batch [25/43], Training Loss: 0.00000612
2024-11-06 14:07:39,132 - INFO - Epoch [270/300], Batch [26/43], Training Loss: 0.00001409
2024-11-06 14:07:39,136 - INFO - Epoch [270/300], Batch [27/43], Training Loss: 0.00001045
2024-11-06 14:07:39,140 - INFO - Epoch [270/300], Batch [28/43], Training Loss: 0.00001533
2024-11-06 14:07:39,144 - INFO - Epoch [270/300], Batch [29/43], Training Loss: 0.00000644
2024-11-06 14:07:39,147 - INFO - Epoch [270/300], Batch [30/43], Training Loss: 0.00001760
2024-11-06 14:07:39,151 - INFO - Epoch [270/300], Batch [31/43], Training Loss: 0.00000552
2024-11-06 14:07:39,155 - INFO - Epoch [270/300], Batch [32/43], Training Loss: 0.00001506
2024-11-06 14:07:39,158 - INFO - Epoch [270/300], Batch [33/43], Training Loss: 0.00001431
2024-11-06 14:07:39,161 - INFO - Epoch [270/300], Batch [34/43], Training Loss: 0.00001441
2024-11-06 14:07:39,164 - INFO - Epoch [270/300], Batch [35/43], Training Loss: 0.00001005
2024-11-06 14:07:39,167 - INFO - Epoch [270/300], Batch [36/43], Training Loss: 0.00001190
2024-11-06 14:07:39,170 - INFO - Epoch [270/300], Batch [37/43], Training Loss: 0.00000455
2024-11-06 14:07:39,173 - INFO - Epoch [270/300], Batch [38/43], Training Loss: 0.00000532
2024-11-06 14:07:39,176 - INFO - Epoch [270/300], Batch [39/43], Training Loss: 0.00001195
2024-11-06 14:07:39,179 - INFO - Epoch [270/300], Batch [40/43], Training Loss: 0.00002267
2024-11-06 14:07:39,182 - INFO - Epoch [270/300], Batch [41/43], Training Loss: 0.00002198
2024-11-06 14:07:39,186 - INFO - Epoch [270/300], Batch [42/43], Training Loss: 0.00000607
2024-11-06 14:07:39,190 - INFO - Epoch [270/300], Batch [43/43], Training Loss: 0.00001132
2024-11-06 14:07:39,202 - INFO - Epoch [270/300], Average Training Loss: 0.00001130, Validation Loss: 0.00001223
2024-11-06 14:07:39,206 - INFO - Epoch [271/300], Batch [1/43], Training Loss: 0.00000543
2024-11-06 14:07:39,210 - INFO - Epoch [271/300], Batch [2/43], Training Loss: 0.00000749
2024-11-06 14:07:39,215 - INFO - Epoch [271/300], Batch [3/43], Training Loss: 0.00001096
2024-11-06 14:07:39,218 - INFO - Epoch [271/300], Batch [4/43], Training Loss: 0.00001661
2024-11-06 14:07:39,222 - INFO - Epoch [271/300], Batch [5/43], Training Loss: 0.00001480
2024-11-06 14:07:39,226 - INFO - Epoch [271/300], Batch [6/43], Training Loss: 0.00000602
2024-11-06 14:07:39,230 - INFO - Epoch [271/300], Batch [7/43], Training Loss: 0.00000230
2024-11-06 14:07:39,235 - INFO - Epoch [271/300], Batch [8/43], Training Loss: 0.00001125
2024-11-06 14:07:39,239 - INFO - Epoch [271/300], Batch [9/43], Training Loss: 0.00000891
2024-11-06 14:07:39,244 - INFO - Epoch [271/300], Batch [10/43], Training Loss: 0.00000465
2024-11-06 14:07:39,248 - INFO - Epoch [271/300], Batch [11/43], Training Loss: 0.00001197
2024-11-06 14:07:39,252 - INFO - Epoch [271/300], Batch [12/43], Training Loss: 0.00000930
2024-11-06 14:07:39,255 - INFO - Epoch [271/300], Batch [13/43], Training Loss: 0.00000505
2024-11-06 14:07:39,260 - INFO - Epoch [271/300], Batch [14/43], Training Loss: 0.00000337
2024-11-06 14:07:39,263 - INFO - Epoch [271/300], Batch [15/43], Training Loss: 0.00000396
2024-11-06 14:07:39,267 - INFO - Epoch [271/300], Batch [16/43], Training Loss: 0.00001047
2024-11-06 14:07:39,272 - INFO - Epoch [271/300], Batch [17/43], Training Loss: 0.00000485
2024-11-06 14:07:39,276 - INFO - Epoch [271/300], Batch [18/43], Training Loss: 0.00000567
2024-11-06 14:07:39,281 - INFO - Epoch [271/300], Batch [19/43], Training Loss: 0.00000386
2024-11-06 14:07:39,287 - INFO - Epoch [271/300], Batch [20/43], Training Loss: 0.00001299
2024-11-06 14:07:39,293 - INFO - Epoch [271/300], Batch [21/43], Training Loss: 0.00000524
2024-11-06 14:07:39,298 - INFO - Epoch [271/300], Batch [22/43], Training Loss: 0.00000642
2024-11-06 14:07:39,302 - INFO - Epoch [271/300], Batch [23/43], Training Loss: 0.00000237
2024-11-06 14:07:39,306 - INFO - Epoch [271/300], Batch [24/43], Training Loss: 0.00000618
2024-11-06 14:07:39,311 - INFO - Epoch [271/300], Batch [25/43], Training Loss: 0.00001199
2024-11-06 14:07:39,314 - INFO - Epoch [271/300], Batch [26/43], Training Loss: 0.00002009
2024-11-06 14:07:39,318 - INFO - Epoch [271/300], Batch [27/43], Training Loss: 0.00000525
2024-11-06 14:07:39,322 - INFO - Epoch [271/300], Batch [28/43], Training Loss: 0.00003766
2024-11-06 14:07:39,327 - INFO - Epoch [271/300], Batch [29/43], Training Loss: 0.00000244
2024-11-06 14:07:39,331 - INFO - Epoch [271/300], Batch [30/43], Training Loss: 0.00001642
2024-11-06 14:07:39,336 - INFO - Epoch [271/300], Batch [31/43], Training Loss: 0.00000540
2024-11-06 14:07:39,341 - INFO - Epoch [271/300], Batch [32/43], Training Loss: 0.00000809
2024-11-06 14:07:39,344 - INFO - Epoch [271/300], Batch [33/43], Training Loss: 0.00001674
2024-11-06 14:07:39,349 - INFO - Epoch [271/300], Batch [34/43], Training Loss: 0.00001727
2024-11-06 14:07:39,354 - INFO - Epoch [271/300], Batch [35/43], Training Loss: 0.00000838
2024-11-06 14:07:39,358 - INFO - Epoch [271/300], Batch [36/43], Training Loss: 0.00001044
2024-11-06 14:07:39,362 - INFO - Epoch [271/300], Batch [37/43], Training Loss: 0.00000875
2024-11-06 14:07:39,366 - INFO - Epoch [271/300], Batch [38/43], Training Loss: 0.00001366
2024-11-06 14:07:39,371 - INFO - Epoch [271/300], Batch [39/43], Training Loss: 0.00001412
2024-11-06 14:07:39,374 - INFO - Epoch [271/300], Batch [40/43], Training Loss: 0.00000970
2024-11-06 14:07:39,378 - INFO - Epoch [271/300], Batch [41/43], Training Loss: 0.00001490
2024-11-06 14:07:39,381 - INFO - Epoch [271/300], Batch [42/43], Training Loss: 0.00001025
2024-11-06 14:07:39,385 - INFO - Epoch [271/300], Batch [43/43], Training Loss: 0.00000720
2024-11-06 14:07:39,397 - INFO - Epoch [271/300], Average Training Loss: 0.00000974, Validation Loss: 0.00001201
2024-11-06 14:07:39,401 - INFO - Epoch [272/300], Batch [1/43], Training Loss: 0.00001779
2024-11-06 14:07:39,405 - INFO - Epoch [272/300], Batch [2/43], Training Loss: 0.00001399
2024-11-06 14:07:39,408 - INFO - Epoch [272/300], Batch [3/43], Training Loss: 0.00000631
2024-11-06 14:07:39,412 - INFO - Epoch [272/300], Batch [4/43], Training Loss: 0.00000690
2024-11-06 14:07:39,416 - INFO - Epoch [272/300], Batch [5/43], Training Loss: 0.00001130
2024-11-06 14:07:39,421 - INFO - Epoch [272/300], Batch [6/43], Training Loss: 0.00000634
2024-11-06 14:07:39,424 - INFO - Epoch [272/300], Batch [7/43], Training Loss: 0.00000723
2024-11-06 14:07:39,428 - INFO - Epoch [272/300], Batch [8/43], Training Loss: 0.00000793
2024-11-06 14:07:39,431 - INFO - Epoch [272/300], Batch [9/43], Training Loss: 0.00000635
2024-11-06 14:07:39,435 - INFO - Epoch [272/300], Batch [10/43], Training Loss: 0.00001134
2024-11-06 14:07:39,438 - INFO - Epoch [272/300], Batch [11/43], Training Loss: 0.00001276
2024-11-06 14:07:39,441 - INFO - Epoch [272/300], Batch [12/43], Training Loss: 0.00000440
2024-11-06 14:07:39,444 - INFO - Epoch [272/300], Batch [13/43], Training Loss: 0.00002040
2024-11-06 14:07:39,448 - INFO - Epoch [272/300], Batch [14/43], Training Loss: 0.00000883
2024-11-06 14:07:39,451 - INFO - Epoch [272/300], Batch [15/43], Training Loss: 0.00001877
2024-11-06 14:07:39,454 - INFO - Epoch [272/300], Batch [16/43], Training Loss: 0.00000586
2024-11-06 14:07:39,457 - INFO - Epoch [272/300], Batch [17/43], Training Loss: 0.00000779
2024-11-06 14:07:39,459 - INFO - Epoch [272/300], Batch [18/43], Training Loss: 0.00000424
2024-11-06 14:07:39,462 - INFO - Epoch [272/300], Batch [19/43], Training Loss: 0.00000478
2024-11-06 14:07:39,466 - INFO - Epoch [272/300], Batch [20/43], Training Loss: 0.00001906
2024-11-06 14:07:39,469 - INFO - Epoch [272/300], Batch [21/43], Training Loss: 0.00000915
2024-11-06 14:07:39,472 - INFO - Epoch [272/300], Batch [22/43], Training Loss: 0.00000380
2024-11-06 14:07:39,475 - INFO - Epoch [272/300], Batch [23/43], Training Loss: 0.00000530
2024-11-06 14:07:39,477 - INFO - Epoch [272/300], Batch [24/43], Training Loss: 0.00000666
2024-11-06 14:07:39,480 - INFO - Epoch [272/300], Batch [25/43], Training Loss: 0.00001825
2024-11-06 14:07:39,483 - INFO - Epoch [272/300], Batch [26/43], Training Loss: 0.00001337
2024-11-06 14:07:39,486 - INFO - Epoch [272/300], Batch [27/43], Training Loss: 0.00000490
2024-11-06 14:07:39,489 - INFO - Epoch [272/300], Batch [28/43], Training Loss: 0.00000743
2024-11-06 14:07:39,492 - INFO - Epoch [272/300], Batch [29/43], Training Loss: 0.00000476
2024-11-06 14:07:39,496 - INFO - Epoch [272/300], Batch [30/43], Training Loss: 0.00000244
2024-11-06 14:07:39,500 - INFO - Epoch [272/300], Batch [31/43], Training Loss: 0.00000412
2024-11-06 14:07:39,503 - INFO - Epoch [272/300], Batch [32/43], Training Loss: 0.00000864
2024-11-06 14:07:39,506 - INFO - Epoch [272/300], Batch [33/43], Training Loss: 0.00001710
2024-11-06 14:07:39,509 - INFO - Epoch [272/300], Batch [34/43], Training Loss: 0.00000424
2024-11-06 14:07:39,512 - INFO - Epoch [272/300], Batch [35/43], Training Loss: 0.00000260
2024-11-06 14:07:39,516 - INFO - Epoch [272/300], Batch [36/43], Training Loss: 0.00000334
2024-11-06 14:07:39,519 - INFO - Epoch [272/300], Batch [37/43], Training Loss: 0.00001245
2024-11-06 14:07:39,523 - INFO - Epoch [272/300], Batch [38/43], Training Loss: 0.00001933
2024-11-06 14:07:39,526 - INFO - Epoch [272/300], Batch [39/43], Training Loss: 0.00000516
2024-11-06 14:07:39,530 - INFO - Epoch [272/300], Batch [40/43], Training Loss: 0.00000491
2024-11-06 14:07:39,534 - INFO - Epoch [272/300], Batch [41/43], Training Loss: 0.00001126
2024-11-06 14:07:39,538 - INFO - Epoch [272/300], Batch [42/43], Training Loss: 0.00001939
2024-11-06 14:07:39,543 - INFO - Epoch [272/300], Batch [43/43], Training Loss: 0.00002134
2024-11-06 14:07:39,556 - INFO - Epoch [272/300], Average Training Loss: 0.00000959, Validation Loss: 0.00001177
2024-11-06 14:07:39,560 - INFO - Epoch [273/300], Batch [1/43], Training Loss: 0.00000613
2024-11-06 14:07:39,564 - INFO - Epoch [273/300], Batch [2/43], Training Loss: 0.00000456
2024-11-06 14:07:39,567 - INFO - Epoch [273/300], Batch [3/43], Training Loss: 0.00000847
2024-11-06 14:07:39,571 - INFO - Epoch [273/300], Batch [4/43], Training Loss: 0.00001304
2024-11-06 14:07:39,574 - INFO - Epoch [273/300], Batch [5/43], Training Loss: 0.00000563
2024-11-06 14:07:39,579 - INFO - Epoch [273/300], Batch [6/43], Training Loss: 0.00000582
2024-11-06 14:07:39,583 - INFO - Epoch [273/300], Batch [7/43], Training Loss: 0.00001085
2024-11-06 14:07:39,586 - INFO - Epoch [273/300], Batch [8/43], Training Loss: 0.00001388
2024-11-06 14:07:39,590 - INFO - Epoch [273/300], Batch [9/43], Training Loss: 0.00001250
2024-11-06 14:07:39,593 - INFO - Epoch [273/300], Batch [10/43], Training Loss: 0.00000245
2024-11-06 14:07:39,596 - INFO - Epoch [273/300], Batch [11/43], Training Loss: 0.00001905
2024-11-06 14:07:39,600 - INFO - Epoch [273/300], Batch [12/43], Training Loss: 0.00000739
2024-11-06 14:07:39,603 - INFO - Epoch [273/300], Batch [13/43], Training Loss: 0.00000965
2024-11-06 14:07:39,607 - INFO - Epoch [273/300], Batch [14/43], Training Loss: 0.00001374
2024-11-06 14:07:39,611 - INFO - Epoch [273/300], Batch [15/43], Training Loss: 0.00000635
2024-11-06 14:07:39,614 - INFO - Epoch [273/300], Batch [16/43], Training Loss: 0.00000310
2024-11-06 14:07:39,618 - INFO - Epoch [273/300], Batch [17/43], Training Loss: 0.00000466
2024-11-06 14:07:39,622 - INFO - Epoch [273/300], Batch [18/43], Training Loss: 0.00000677
2024-11-06 14:07:39,625 - INFO - Epoch [273/300], Batch [19/43], Training Loss: 0.00001866
2024-11-06 14:07:39,628 - INFO - Epoch [273/300], Batch [20/43], Training Loss: 0.00001329
2024-11-06 14:07:39,632 - INFO - Epoch [273/300], Batch [21/43], Training Loss: 0.00000432
2024-11-06 14:07:39,635 - INFO - Epoch [273/300], Batch [22/43], Training Loss: 0.00001007
2024-11-06 14:07:39,638 - INFO - Epoch [273/300], Batch [23/43], Training Loss: 0.00000473
2024-11-06 14:07:39,641 - INFO - Epoch [273/300], Batch [24/43], Training Loss: 0.00000507
2024-11-06 14:07:39,643 - INFO - Epoch [273/300], Batch [25/43], Training Loss: 0.00000906
2024-11-06 14:07:39,646 - INFO - Epoch [273/300], Batch [26/43], Training Loss: 0.00002235
2024-11-06 14:07:39,649 - INFO - Epoch [273/300], Batch [27/43], Training Loss: 0.00000521
2024-11-06 14:07:39,654 - INFO - Epoch [273/300], Batch [28/43], Training Loss: 0.00000530
2024-11-06 14:07:39,657 - INFO - Epoch [273/300], Batch [29/43], Training Loss: 0.00000841
2024-11-06 14:07:39,661 - INFO - Epoch [273/300], Batch [30/43], Training Loss: 0.00000749
2024-11-06 14:07:39,664 - INFO - Epoch [273/300], Batch [31/43], Training Loss: 0.00001643
2024-11-06 14:07:39,667 - INFO - Epoch [273/300], Batch [32/43], Training Loss: 0.00002146
2024-11-06 14:07:39,671 - INFO - Epoch [273/300], Batch [33/43], Training Loss: 0.00000774
2024-11-06 14:07:39,674 - INFO - Epoch [273/300], Batch [34/43], Training Loss: 0.00001190
2024-11-06 14:07:39,677 - INFO - Epoch [273/300], Batch [35/43], Training Loss: 0.00001194
2024-11-06 14:07:39,680 - INFO - Epoch [273/300], Batch [36/43], Training Loss: 0.00000239
2024-11-06 14:07:39,685 - INFO - Epoch [273/300], Batch [37/43], Training Loss: 0.00002016
2024-11-06 14:07:39,689 - INFO - Epoch [273/300], Batch [38/43], Training Loss: 0.00001250
2024-11-06 14:07:39,693 - INFO - Epoch [273/300], Batch [39/43], Training Loss: 0.00002045
2024-11-06 14:07:39,698 - INFO - Epoch [273/300], Batch [40/43], Training Loss: 0.00000549
2024-11-06 14:07:39,702 - INFO - Epoch [273/300], Batch [41/43], Training Loss: 0.00000853
2024-11-06 14:07:39,706 - INFO - Epoch [273/300], Batch [42/43], Training Loss: 0.00001850
2024-11-06 14:07:39,710 - INFO - Epoch [273/300], Batch [43/43], Training Loss: 0.00001260
2024-11-06 14:07:39,721 - INFO - Epoch [273/300], Average Training Loss: 0.00001019, Validation Loss: 0.00001183
2024-11-06 14:07:39,725 - INFO - Epoch [274/300], Batch [1/43], Training Loss: 0.00000340
2024-11-06 14:07:39,728 - INFO - Epoch [274/300], Batch [2/43], Training Loss: 0.00002210
2024-11-06 14:07:39,731 - INFO - Epoch [274/300], Batch [3/43], Training Loss: 0.00001280
2024-11-06 14:07:39,735 - INFO - Epoch [274/300], Batch [4/43], Training Loss: 0.00001923
2024-11-06 14:07:39,738 - INFO - Epoch [274/300], Batch [5/43], Training Loss: 0.00001419
2024-11-06 14:07:39,741 - INFO - Epoch [274/300], Batch [6/43], Training Loss: 0.00000505
2024-11-06 14:07:39,744 - INFO - Epoch [274/300], Batch [7/43], Training Loss: 0.00000541
2024-11-06 14:07:39,748 - INFO - Epoch [274/300], Batch [8/43], Training Loss: 0.00001696
2024-11-06 14:07:39,752 - INFO - Epoch [274/300], Batch [9/43], Training Loss: 0.00001733
2024-11-06 14:07:39,755 - INFO - Epoch [274/300], Batch [10/43], Training Loss: 0.00000578
2024-11-06 14:07:39,759 - INFO - Epoch [274/300], Batch [11/43], Training Loss: 0.00001468
2024-11-06 14:07:39,762 - INFO - Epoch [274/300], Batch [12/43], Training Loss: 0.00000878
2024-11-06 14:07:39,765 - INFO - Epoch [274/300], Batch [13/43], Training Loss: 0.00000920
2024-11-06 14:07:39,769 - INFO - Epoch [274/300], Batch [14/43], Training Loss: 0.00000333
2024-11-06 14:07:39,772 - INFO - Epoch [274/300], Batch [15/43], Training Loss: 0.00000926
2024-11-06 14:07:39,776 - INFO - Epoch [274/300], Batch [16/43], Training Loss: 0.00000939
2024-11-06 14:07:39,779 - INFO - Epoch [274/300], Batch [17/43], Training Loss: 0.00000461
2024-11-06 14:07:39,782 - INFO - Epoch [274/300], Batch [18/43], Training Loss: 0.00001447
2024-11-06 14:07:39,786 - INFO - Epoch [274/300], Batch [19/43], Training Loss: 0.00001531
2024-11-06 14:07:39,789 - INFO - Epoch [274/300], Batch [20/43], Training Loss: 0.00000728
2024-11-06 14:07:39,792 - INFO - Epoch [274/300], Batch [21/43], Training Loss: 0.00001040
2024-11-06 14:07:39,795 - INFO - Epoch [274/300], Batch [22/43], Training Loss: 0.00000640
2024-11-06 14:07:39,799 - INFO - Epoch [274/300], Batch [23/43], Training Loss: 0.00000464
2024-11-06 14:07:39,802 - INFO - Epoch [274/300], Batch [24/43], Training Loss: 0.00001183
2024-11-06 14:07:39,806 - INFO - Epoch [274/300], Batch [25/43], Training Loss: 0.00001515
2024-11-06 14:07:39,810 - INFO - Epoch [274/300], Batch [26/43], Training Loss: 0.00000646
2024-11-06 14:07:39,814 - INFO - Epoch [274/300], Batch [27/43], Training Loss: 0.00001133
2024-11-06 14:07:39,818 - INFO - Epoch [274/300], Batch [28/43], Training Loss: 0.00001693
2024-11-06 14:07:39,821 - INFO - Epoch [274/300], Batch [29/43], Training Loss: 0.00000828
2024-11-06 14:07:39,824 - INFO - Epoch [274/300], Batch [30/43], Training Loss: 0.00001121
2024-11-06 14:07:39,827 - INFO - Epoch [274/300], Batch [31/43], Training Loss: 0.00000677
2024-11-06 14:07:39,832 - INFO - Epoch [274/300], Batch [32/43], Training Loss: 0.00001480
2024-11-06 14:07:39,835 - INFO - Epoch [274/300], Batch [33/43], Training Loss: 0.00001456
2024-11-06 14:07:39,839 - INFO - Epoch [274/300], Batch [34/43], Training Loss: 0.00000815
2024-11-06 14:07:39,843 - INFO - Epoch [274/300], Batch [35/43], Training Loss: 0.00000888
2024-11-06 14:07:39,848 - INFO - Epoch [274/300], Batch [36/43], Training Loss: 0.00001420
2024-11-06 14:07:39,852 - INFO - Epoch [274/300], Batch [37/43], Training Loss: 0.00000731
2024-11-06 14:07:39,858 - INFO - Epoch [274/300], Batch [38/43], Training Loss: 0.00000478
2024-11-06 14:07:39,862 - INFO - Epoch [274/300], Batch [39/43], Training Loss: 0.00001742
2024-11-06 14:07:39,866 - INFO - Epoch [274/300], Batch [40/43], Training Loss: 0.00001003
2024-11-06 14:07:39,870 - INFO - Epoch [274/300], Batch [41/43], Training Loss: 0.00000480
2024-11-06 14:07:39,874 - INFO - Epoch [274/300], Batch [42/43], Training Loss: 0.00001305
2024-11-06 14:07:39,878 - INFO - Epoch [274/300], Batch [43/43], Training Loss: 0.00000855
2024-11-06 14:07:39,892 - INFO - Epoch [274/300], Average Training Loss: 0.00001057, Validation Loss: 0.00001695
2024-11-06 14:07:39,896 - INFO - Epoch [275/300], Batch [1/43], Training Loss: 0.00002642
2024-11-06 14:07:39,901 - INFO - Epoch [275/300], Batch [2/43], Training Loss: 0.00001084
2024-11-06 14:07:39,905 - INFO - Epoch [275/300], Batch [3/43], Training Loss: 0.00000884
2024-11-06 14:07:39,909 - INFO - Epoch [275/300], Batch [4/43], Training Loss: 0.00001999
2024-11-06 14:07:39,913 - INFO - Epoch [275/300], Batch [5/43], Training Loss: 0.00001059
2024-11-06 14:07:39,917 - INFO - Epoch [275/300], Batch [6/43], Training Loss: 0.00001775
2024-11-06 14:07:39,922 - INFO - Epoch [275/300], Batch [7/43], Training Loss: 0.00001171
2024-11-06 14:07:39,926 - INFO - Epoch [275/300], Batch [8/43], Training Loss: 0.00001176
2024-11-06 14:07:39,931 - INFO - Epoch [275/300], Batch [9/43], Training Loss: 0.00001605
2024-11-06 14:07:39,935 - INFO - Epoch [275/300], Batch [10/43], Training Loss: 0.00001303
2024-11-06 14:07:39,939 - INFO - Epoch [275/300], Batch [11/43], Training Loss: 0.00000725
2024-11-06 14:07:39,944 - INFO - Epoch [275/300], Batch [12/43], Training Loss: 0.00002177
2024-11-06 14:07:39,949 - INFO - Epoch [275/300], Batch [13/43], Training Loss: 0.00001579
2024-11-06 14:07:39,954 - INFO - Epoch [275/300], Batch [14/43], Training Loss: 0.00001847
2024-11-06 14:07:39,958 - INFO - Epoch [275/300], Batch [15/43], Training Loss: 0.00001248
2024-11-06 14:07:39,962 - INFO - Epoch [275/300], Batch [16/43], Training Loss: 0.00000795
2024-11-06 14:07:39,967 - INFO - Epoch [275/300], Batch [17/43], Training Loss: 0.00000554
2024-11-06 14:07:39,971 - INFO - Epoch [275/300], Batch [18/43], Training Loss: 0.00000545
2024-11-06 14:07:39,975 - INFO - Epoch [275/300], Batch [19/43], Training Loss: 0.00001056
2024-11-06 14:07:39,979 - INFO - Epoch [275/300], Batch [20/43], Training Loss: 0.00001386
2024-11-06 14:07:39,984 - INFO - Epoch [275/300], Batch [21/43], Training Loss: 0.00000841
2024-11-06 14:07:39,989 - INFO - Epoch [275/300], Batch [22/43], Training Loss: 0.00000882
2024-11-06 14:07:39,995 - INFO - Epoch [275/300], Batch [23/43], Training Loss: 0.00000508
2024-11-06 14:07:40,001 - INFO - Epoch [275/300], Batch [24/43], Training Loss: 0.00000585
2024-11-06 14:07:40,006 - INFO - Epoch [275/300], Batch [25/43], Training Loss: 0.00001114
2024-11-06 14:07:40,010 - INFO - Epoch [275/300], Batch [26/43], Training Loss: 0.00000994
2024-11-06 14:07:40,015 - INFO - Epoch [275/300], Batch [27/43], Training Loss: 0.00001480
2024-11-06 14:07:40,020 - INFO - Epoch [275/300], Batch [28/43], Training Loss: 0.00001871
2024-11-06 14:07:40,025 - INFO - Epoch [275/300], Batch [29/43], Training Loss: 0.00000339
2024-11-06 14:07:40,030 - INFO - Epoch [275/300], Batch [30/43], Training Loss: 0.00000860
2024-11-06 14:07:40,035 - INFO - Epoch [275/300], Batch [31/43], Training Loss: 0.00001650
2024-11-06 14:07:40,040 - INFO - Epoch [275/300], Batch [32/43], Training Loss: 0.00001142
2024-11-06 14:07:40,044 - INFO - Epoch [275/300], Batch [33/43], Training Loss: 0.00000545
2024-11-06 14:07:40,048 - INFO - Epoch [275/300], Batch [34/43], Training Loss: 0.00000618
2024-11-06 14:07:40,053 - INFO - Epoch [275/300], Batch [35/43], Training Loss: 0.00003556
2024-11-06 14:07:40,057 - INFO - Epoch [275/300], Batch [36/43], Training Loss: 0.00002087
2024-11-06 14:07:40,062 - INFO - Epoch [275/300], Batch [37/43], Training Loss: 0.00000401
2024-11-06 14:07:40,066 - INFO - Epoch [275/300], Batch [38/43], Training Loss: 0.00001425
2024-11-06 14:07:40,070 - INFO - Epoch [275/300], Batch [39/43], Training Loss: 0.00000707
2024-11-06 14:07:40,075 - INFO - Epoch [275/300], Batch [40/43], Training Loss: 0.00001786
2024-11-06 14:07:40,078 - INFO - Epoch [275/300], Batch [41/43], Training Loss: 0.00000880
2024-11-06 14:07:40,082 - INFO - Epoch [275/300], Batch [42/43], Training Loss: 0.00000374
2024-11-06 14:07:40,087 - INFO - Epoch [275/300], Batch [43/43], Training Loss: 0.00000312
2024-11-06 14:07:40,100 - INFO - Epoch [275/300], Average Training Loss: 0.00001199, Validation Loss: 0.00001393
2024-11-06 14:07:40,105 - INFO - Epoch [276/300], Batch [1/43], Training Loss: 0.00000358
2024-11-06 14:07:40,109 - INFO - Epoch [276/300], Batch [2/43], Training Loss: 0.00000677
2024-11-06 14:07:40,113 - INFO - Epoch [276/300], Batch [3/43], Training Loss: 0.00000555
2024-11-06 14:07:40,117 - INFO - Epoch [276/300], Batch [4/43], Training Loss: 0.00000412
2024-11-06 14:07:40,121 - INFO - Epoch [276/300], Batch [5/43], Training Loss: 0.00001402
2024-11-06 14:07:40,125 - INFO - Epoch [276/300], Batch [6/43], Training Loss: 0.00002277
2024-11-06 14:07:40,128 - INFO - Epoch [276/300], Batch [7/43], Training Loss: 0.00000719
2024-11-06 14:07:40,133 - INFO - Epoch [276/300], Batch [8/43], Training Loss: 0.00000418
2024-11-06 14:07:40,136 - INFO - Epoch [276/300], Batch [9/43], Training Loss: 0.00001358
2024-11-06 14:07:40,142 - INFO - Epoch [276/300], Batch [10/43], Training Loss: 0.00000901
2024-11-06 14:07:40,147 - INFO - Epoch [276/300], Batch [11/43], Training Loss: 0.00000477
2024-11-06 14:07:40,151 - INFO - Epoch [276/300], Batch [12/43], Training Loss: 0.00000905
2024-11-06 14:07:40,156 - INFO - Epoch [276/300], Batch [13/43], Training Loss: 0.00001232
2024-11-06 14:07:40,160 - INFO - Epoch [276/300], Batch [14/43], Training Loss: 0.00000758
2024-11-06 14:07:40,166 - INFO - Epoch [276/300], Batch [15/43], Training Loss: 0.00001019
2024-11-06 14:07:40,170 - INFO - Epoch [276/300], Batch [16/43], Training Loss: 0.00000370
2024-11-06 14:07:40,175 - INFO - Epoch [276/300], Batch [17/43], Training Loss: 0.00000886
2024-11-06 14:07:40,179 - INFO - Epoch [276/300], Batch [18/43], Training Loss: 0.00000980
2024-11-06 14:07:40,183 - INFO - Epoch [276/300], Batch [19/43], Training Loss: 0.00001661
2024-11-06 14:07:40,187 - INFO - Epoch [276/300], Batch [20/43], Training Loss: 0.00000465
2024-11-06 14:07:40,190 - INFO - Epoch [276/300], Batch [21/43], Training Loss: 0.00000796
2024-11-06 14:07:40,195 - INFO - Epoch [276/300], Batch [22/43], Training Loss: 0.00000876
2024-11-06 14:07:40,199 - INFO - Epoch [276/300], Batch [23/43], Training Loss: 0.00000535
2024-11-06 14:07:40,203 - INFO - Epoch [276/300], Batch [24/43], Training Loss: 0.00000761
2024-11-06 14:07:40,208 - INFO - Epoch [276/300], Batch [25/43], Training Loss: 0.00000293
2024-11-06 14:07:40,213 - INFO - Epoch [276/300], Batch [26/43], Training Loss: 0.00000513
2024-11-06 14:07:40,217 - INFO - Epoch [276/300], Batch [27/43], Training Loss: 0.00000565
2024-11-06 14:07:40,221 - INFO - Epoch [276/300], Batch [28/43], Training Loss: 0.00000814
2024-11-06 14:07:40,225 - INFO - Epoch [276/300], Batch [29/43], Training Loss: 0.00000485
2024-11-06 14:07:40,228 - INFO - Epoch [276/300], Batch [30/43], Training Loss: 0.00001684
2024-11-06 14:07:40,232 - INFO - Epoch [276/300], Batch [31/43], Training Loss: 0.00000653
2024-11-06 14:07:40,236 - INFO - Epoch [276/300], Batch [32/43], Training Loss: 0.00002268
2024-11-06 14:07:40,240 - INFO - Epoch [276/300], Batch [33/43], Training Loss: 0.00001111
2024-11-06 14:07:40,245 - INFO - Epoch [276/300], Batch [34/43], Training Loss: 0.00000348
2024-11-06 14:07:40,249 - INFO - Epoch [276/300], Batch [35/43], Training Loss: 0.00001235
2024-11-06 14:07:40,253 - INFO - Epoch [276/300], Batch [36/43], Training Loss: 0.00001093
2024-11-06 14:07:40,257 - INFO - Epoch [276/300], Batch [37/43], Training Loss: 0.00000533
2024-11-06 14:07:40,262 - INFO - Epoch [276/300], Batch [38/43], Training Loss: 0.00001350
2024-11-06 14:07:40,266 - INFO - Epoch [276/300], Batch [39/43], Training Loss: 0.00001780
2024-11-06 14:07:40,270 - INFO - Epoch [276/300], Batch [40/43], Training Loss: 0.00001608
2024-11-06 14:07:40,273 - INFO - Epoch [276/300], Batch [41/43], Training Loss: 0.00000482
2024-11-06 14:07:40,277 - INFO - Epoch [276/300], Batch [42/43], Training Loss: 0.00001865
2024-11-06 14:07:40,281 - INFO - Epoch [276/300], Batch [43/43], Training Loss: 0.00001236
2024-11-06 14:07:40,293 - INFO - Epoch [276/300], Average Training Loss: 0.00000947, Validation Loss: 0.00001363
2024-11-06 14:07:40,299 - INFO - Epoch [277/300], Batch [1/43], Training Loss: 0.00001005
2024-11-06 14:07:40,308 - INFO - Epoch [277/300], Batch [2/43], Training Loss: 0.00000957
2024-11-06 14:07:40,314 - INFO - Epoch [277/300], Batch [3/43], Training Loss: 0.00000830
2024-11-06 14:07:40,319 - INFO - Epoch [277/300], Batch [4/43], Training Loss: 0.00001141
2024-11-06 14:07:40,325 - INFO - Epoch [277/300], Batch [5/43], Training Loss: 0.00001430
2024-11-06 14:07:40,330 - INFO - Epoch [277/300], Batch [6/43], Training Loss: 0.00000989
2024-11-06 14:07:40,335 - INFO - Epoch [277/300], Batch [7/43], Training Loss: 0.00000790
2024-11-06 14:07:40,339 - INFO - Epoch [277/300], Batch [8/43], Training Loss: 0.00001508
2024-11-06 14:07:40,343 - INFO - Epoch [277/300], Batch [9/43], Training Loss: 0.00000981
2024-11-06 14:07:40,347 - INFO - Epoch [277/300], Batch [10/43], Training Loss: 0.00000094
2024-11-06 14:07:40,352 - INFO - Epoch [277/300], Batch [11/43], Training Loss: 0.00001532
2024-11-06 14:07:40,357 - INFO - Epoch [277/300], Batch [12/43], Training Loss: 0.00000555
2024-11-06 14:07:40,362 - INFO - Epoch [277/300], Batch [13/43], Training Loss: 0.00001303
2024-11-06 14:07:40,368 - INFO - Epoch [277/300], Batch [14/43], Training Loss: 0.00000569
2024-11-06 14:07:40,372 - INFO - Epoch [277/300], Batch [15/43], Training Loss: 0.00000303
2024-11-06 14:07:40,377 - INFO - Epoch [277/300], Batch [16/43], Training Loss: 0.00001221
2024-11-06 14:07:40,381 - INFO - Epoch [277/300], Batch [17/43], Training Loss: 0.00002350
2024-11-06 14:07:40,386 - INFO - Epoch [277/300], Batch [18/43], Training Loss: 0.00000184
2024-11-06 14:07:40,391 - INFO - Epoch [277/300], Batch [19/43], Training Loss: 0.00000426
2024-11-06 14:07:40,396 - INFO - Epoch [277/300], Batch [20/43], Training Loss: 0.00000871
2024-11-06 14:07:40,401 - INFO - Epoch [277/300], Batch [21/43], Training Loss: 0.00001458
2024-11-06 14:07:40,405 - INFO - Epoch [277/300], Batch [22/43], Training Loss: 0.00001052
2024-11-06 14:07:40,409 - INFO - Epoch [277/300], Batch [23/43], Training Loss: 0.00003358
2024-11-06 14:07:40,414 - INFO - Epoch [277/300], Batch [24/43], Training Loss: 0.00002347
2024-11-06 14:07:40,418 - INFO - Epoch [277/300], Batch [25/43], Training Loss: 0.00000616
2024-11-06 14:07:40,421 - INFO - Epoch [277/300], Batch [26/43], Training Loss: 0.00001134
2024-11-06 14:07:40,426 - INFO - Epoch [277/300], Batch [27/43], Training Loss: 0.00001327
2024-11-06 14:07:40,429 - INFO - Epoch [277/300], Batch [28/43], Training Loss: 0.00002363
2024-11-06 14:07:40,433 - INFO - Epoch [277/300], Batch [29/43], Training Loss: 0.00000624
2024-11-06 14:07:40,437 - INFO - Epoch [277/300], Batch [30/43], Training Loss: 0.00001351
2024-11-06 14:07:40,441 - INFO - Epoch [277/300], Batch [31/43], Training Loss: 0.00001179
2024-11-06 14:07:40,445 - INFO - Epoch [277/300], Batch [32/43], Training Loss: 0.00001851
2024-11-06 14:07:40,449 - INFO - Epoch [277/300], Batch [33/43], Training Loss: 0.00000510
2024-11-06 14:07:40,452 - INFO - Epoch [277/300], Batch [34/43], Training Loss: 0.00000768
2024-11-06 14:07:40,456 - INFO - Epoch [277/300], Batch [35/43], Training Loss: 0.00002658
2024-11-06 14:07:40,461 - INFO - Epoch [277/300], Batch [36/43], Training Loss: 0.00001097
2024-11-06 14:07:40,466 - INFO - Epoch [277/300], Batch [37/43], Training Loss: 0.00001281
2024-11-06 14:07:40,471 - INFO - Epoch [277/300], Batch [38/43], Training Loss: 0.00002864
2024-11-06 14:07:40,477 - INFO - Epoch [277/300], Batch [39/43], Training Loss: 0.00000674
2024-11-06 14:07:40,482 - INFO - Epoch [277/300], Batch [40/43], Training Loss: 0.00000656
2024-11-06 14:07:40,487 - INFO - Epoch [277/300], Batch [41/43], Training Loss: 0.00000501
2024-11-06 14:07:40,492 - INFO - Epoch [277/300], Batch [42/43], Training Loss: 0.00000311
2024-11-06 14:07:40,497 - INFO - Epoch [277/300], Batch [43/43], Training Loss: 0.00001122
2024-11-06 14:07:40,511 - INFO - Epoch [277/300], Average Training Loss: 0.00001166, Validation Loss: 0.00001352
2024-11-06 14:07:40,515 - INFO - Epoch [278/300], Batch [1/43], Training Loss: 0.00000581
2024-11-06 14:07:40,519 - INFO - Epoch [278/300], Batch [2/43], Training Loss: 0.00000692
2024-11-06 14:07:40,524 - INFO - Epoch [278/300], Batch [3/43], Training Loss: 0.00000478
2024-11-06 14:07:40,528 - INFO - Epoch [278/300], Batch [4/43], Training Loss: 0.00001097
2024-11-06 14:07:40,533 - INFO - Epoch [278/300], Batch [5/43], Training Loss: 0.00000337
2024-11-06 14:07:40,537 - INFO - Epoch [278/300], Batch [6/43], Training Loss: 0.00000432
2024-11-06 14:07:40,541 - INFO - Epoch [278/300], Batch [7/43], Training Loss: 0.00000297
2024-11-06 14:07:40,545 - INFO - Epoch [278/300], Batch [8/43], Training Loss: 0.00001686
2024-11-06 14:07:40,549 - INFO - Epoch [278/300], Batch [9/43], Training Loss: 0.00000617
2024-11-06 14:07:40,554 - INFO - Epoch [278/300], Batch [10/43], Training Loss: 0.00002533
2024-11-06 14:07:40,559 - INFO - Epoch [278/300], Batch [11/43], Training Loss: 0.00000638
2024-11-06 14:07:40,564 - INFO - Epoch [278/300], Batch [12/43], Training Loss: 0.00001348
2024-11-06 14:07:40,568 - INFO - Epoch [278/300], Batch [13/43], Training Loss: 0.00000899
2024-11-06 14:07:40,572 - INFO - Epoch [278/300], Batch [14/43], Training Loss: 0.00001119
2024-11-06 14:07:40,576 - INFO - Epoch [278/300], Batch [15/43], Training Loss: 0.00002379
2024-11-06 14:07:40,579 - INFO - Epoch [278/300], Batch [16/43], Training Loss: 0.00001336
2024-11-06 14:07:40,583 - INFO - Epoch [278/300], Batch [17/43], Training Loss: 0.00000422
2024-11-06 14:07:40,587 - INFO - Epoch [278/300], Batch [18/43], Training Loss: 0.00000932
2024-11-06 14:07:40,592 - INFO - Epoch [278/300], Batch [19/43], Training Loss: 0.00000674
2024-11-06 14:07:40,596 - INFO - Epoch [278/300], Batch [20/43], Training Loss: 0.00001137
2024-11-06 14:07:40,600 - INFO - Epoch [278/300], Batch [21/43], Training Loss: 0.00000324
2024-11-06 14:07:40,607 - INFO - Epoch [278/300], Batch [22/43], Training Loss: 0.00001563
2024-11-06 14:07:40,611 - INFO - Epoch [278/300], Batch [23/43], Training Loss: 0.00000971
2024-11-06 14:07:40,615 - INFO - Epoch [278/300], Batch [24/43], Training Loss: 0.00000783
2024-11-06 14:07:40,619 - INFO - Epoch [278/300], Batch [25/43], Training Loss: 0.00000712
2024-11-06 14:07:40,623 - INFO - Epoch [278/300], Batch [26/43], Training Loss: 0.00001705
2024-11-06 14:07:40,627 - INFO - Epoch [278/300], Batch [27/43], Training Loss: 0.00000964
2024-11-06 14:07:40,631 - INFO - Epoch [278/300], Batch [28/43], Training Loss: 0.00000918
2024-11-06 14:07:40,635 - INFO - Epoch [278/300], Batch [29/43], Training Loss: 0.00001219
2024-11-06 14:07:40,639 - INFO - Epoch [278/300], Batch [30/43], Training Loss: 0.00002322
2024-11-06 14:07:40,643 - INFO - Epoch [278/300], Batch [31/43], Training Loss: 0.00000988
2024-11-06 14:07:40,649 - INFO - Epoch [278/300], Batch [32/43], Training Loss: 0.00000912
2024-11-06 14:07:40,653 - INFO - Epoch [278/300], Batch [33/43], Training Loss: 0.00001173
2024-11-06 14:07:40,658 - INFO - Epoch [278/300], Batch [34/43], Training Loss: 0.00001020
2024-11-06 14:07:40,661 - INFO - Epoch [278/300], Batch [35/43], Training Loss: 0.00000882
2024-11-06 14:07:40,666 - INFO - Epoch [278/300], Batch [36/43], Training Loss: 0.00002053
2024-11-06 14:07:40,670 - INFO - Epoch [278/300], Batch [37/43], Training Loss: 0.00001728
2024-11-06 14:07:40,674 - INFO - Epoch [278/300], Batch [38/43], Training Loss: 0.00000653
2024-11-06 14:07:40,679 - INFO - Epoch [278/300], Batch [39/43], Training Loss: 0.00001581
2024-11-06 14:07:40,684 - INFO - Epoch [278/300], Batch [40/43], Training Loss: 0.00004345
2024-11-06 14:07:40,688 - INFO - Epoch [278/300], Batch [41/43], Training Loss: 0.00001794
2024-11-06 14:07:40,691 - INFO - Epoch [278/300], Batch [42/43], Training Loss: 0.00000811
2024-11-06 14:07:40,696 - INFO - Epoch [278/300], Batch [43/43], Training Loss: 0.00000812
2024-11-06 14:07:40,708 - INFO - Epoch [278/300], Average Training Loss: 0.00001160, Validation Loss: 0.00002319
2024-11-06 14:07:40,712 - INFO - Epoch [279/300], Batch [1/43], Training Loss: 0.00002165
2024-11-06 14:07:40,716 - INFO - Epoch [279/300], Batch [2/43], Training Loss: 0.00001300
2024-11-06 14:07:40,719 - INFO - Epoch [279/300], Batch [3/43], Training Loss: 0.00000942
2024-11-06 14:07:40,723 - INFO - Epoch [279/300], Batch [4/43], Training Loss: 0.00000969
2024-11-06 14:07:40,727 - INFO - Epoch [279/300], Batch [5/43], Training Loss: 0.00001933
2024-11-06 14:07:40,732 - INFO - Epoch [279/300], Batch [6/43], Training Loss: 0.00000670
2024-11-06 14:07:40,735 - INFO - Epoch [279/300], Batch [7/43], Training Loss: 0.00000827
2024-11-06 14:07:40,738 - INFO - Epoch [279/300], Batch [8/43], Training Loss: 0.00001083
2024-11-06 14:07:40,742 - INFO - Epoch [279/300], Batch [9/43], Training Loss: 0.00000472
2024-11-06 14:07:40,745 - INFO - Epoch [279/300], Batch [10/43], Training Loss: 0.00001153
2024-11-06 14:07:40,748 - INFO - Epoch [279/300], Batch [11/43], Training Loss: 0.00000913
2024-11-06 14:07:40,752 - INFO - Epoch [279/300], Batch [12/43], Training Loss: 0.00002549
2024-11-06 14:07:40,755 - INFO - Epoch [279/300], Batch [13/43], Training Loss: 0.00000844
2024-11-06 14:07:40,758 - INFO - Epoch [279/300], Batch [14/43], Training Loss: 0.00000820
2024-11-06 14:07:40,761 - INFO - Epoch [279/300], Batch [15/43], Training Loss: 0.00001004
2024-11-06 14:07:40,764 - INFO - Epoch [279/300], Batch [16/43], Training Loss: 0.00000238
2024-11-06 14:07:40,766 - INFO - Epoch [279/300], Batch [17/43], Training Loss: 0.00000637
2024-11-06 14:07:40,769 - INFO - Epoch [279/300], Batch [18/43], Training Loss: 0.00001768
2024-11-06 14:07:40,772 - INFO - Epoch [279/300], Batch [19/43], Training Loss: 0.00001316
2024-11-06 14:07:40,775 - INFO - Epoch [279/300], Batch [20/43], Training Loss: 0.00001164
2024-11-06 14:07:40,779 - INFO - Epoch [279/300], Batch [21/43], Training Loss: 0.00001400
2024-11-06 14:07:40,782 - INFO - Epoch [279/300], Batch [22/43], Training Loss: 0.00001594
2024-11-06 14:07:40,785 - INFO - Epoch [279/300], Batch [23/43], Training Loss: 0.00000581
2024-11-06 14:07:40,789 - INFO - Epoch [279/300], Batch [24/43], Training Loss: 0.00000829
2024-11-06 14:07:40,792 - INFO - Epoch [279/300], Batch [25/43], Training Loss: 0.00001470
2024-11-06 14:07:40,796 - INFO - Epoch [279/300], Batch [26/43], Training Loss: 0.00001043
2024-11-06 14:07:40,800 - INFO - Epoch [279/300], Batch [27/43], Training Loss: 0.00000734
2024-11-06 14:07:40,803 - INFO - Epoch [279/300], Batch [28/43], Training Loss: 0.00001192
2024-11-06 14:07:40,807 - INFO - Epoch [279/300], Batch [29/43], Training Loss: 0.00001112
2024-11-06 14:07:40,811 - INFO - Epoch [279/300], Batch [30/43], Training Loss: 0.00001389
2024-11-06 14:07:40,815 - INFO - Epoch [279/300], Batch [31/43], Training Loss: 0.00000550
2024-11-06 14:07:40,818 - INFO - Epoch [279/300], Batch [32/43], Training Loss: 0.00001004
2024-11-06 14:07:40,822 - INFO - Epoch [279/300], Batch [33/43], Training Loss: 0.00001652
2024-11-06 14:07:40,826 - INFO - Epoch [279/300], Batch [34/43], Training Loss: 0.00000968
2024-11-06 14:07:40,831 - INFO - Epoch [279/300], Batch [35/43], Training Loss: 0.00000691
2024-11-06 14:07:40,838 - INFO - Epoch [279/300], Batch [36/43], Training Loss: 0.00000414
2024-11-06 14:07:40,844 - INFO - Epoch [279/300], Batch [37/43], Training Loss: 0.00000615
2024-11-06 14:07:40,849 - INFO - Epoch [279/300], Batch [38/43], Training Loss: 0.00001382
2024-11-06 14:07:40,853 - INFO - Epoch [279/300], Batch [39/43], Training Loss: 0.00000523
2024-11-06 14:07:40,857 - INFO - Epoch [279/300], Batch [40/43], Training Loss: 0.00002219
2024-11-06 14:07:40,861 - INFO - Epoch [279/300], Batch [41/43], Training Loss: 0.00001685
2024-11-06 14:07:40,865 - INFO - Epoch [279/300], Batch [42/43], Training Loss: 0.00002229
2024-11-06 14:07:40,869 - INFO - Epoch [279/300], Batch [43/43], Training Loss: 0.00001246
2024-11-06 14:07:40,881 - INFO - Epoch [279/300], Average Training Loss: 0.00001146, Validation Loss: 0.00001564
2024-11-06 14:07:40,885 - INFO - Epoch [280/300], Batch [1/43], Training Loss: 0.00001019
2024-11-06 14:07:40,889 - INFO - Epoch [280/300], Batch [2/43], Training Loss: 0.00001069
2024-11-06 14:07:40,892 - INFO - Epoch [280/300], Batch [3/43], Training Loss: 0.00001548
2024-11-06 14:07:40,896 - INFO - Epoch [280/300], Batch [4/43], Training Loss: 0.00000884
2024-11-06 14:07:40,900 - INFO - Epoch [280/300], Batch [5/43], Training Loss: 0.00000476
2024-11-06 14:07:40,904 - INFO - Epoch [280/300], Batch [6/43], Training Loss: 0.00000394
2024-11-06 14:07:40,907 - INFO - Epoch [280/300], Batch [7/43], Training Loss: 0.00000880
2024-11-06 14:07:40,911 - INFO - Epoch [280/300], Batch [8/43], Training Loss: 0.00001188
2024-11-06 14:07:40,914 - INFO - Epoch [280/300], Batch [9/43], Training Loss: 0.00002280
2024-11-06 14:07:40,917 - INFO - Epoch [280/300], Batch [10/43], Training Loss: 0.00000806
2024-11-06 14:07:40,921 - INFO - Epoch [280/300], Batch [11/43], Training Loss: 0.00001019
2024-11-06 14:07:40,925 - INFO - Epoch [280/300], Batch [12/43], Training Loss: 0.00000392
2024-11-06 14:07:40,928 - INFO - Epoch [280/300], Batch [13/43], Training Loss: 0.00001063
2024-11-06 14:07:40,932 - INFO - Epoch [280/300], Batch [14/43], Training Loss: 0.00002327
2024-11-06 14:07:40,936 - INFO - Epoch [280/300], Batch [15/43], Training Loss: 0.00000956
2024-11-06 14:07:40,939 - INFO - Epoch [280/300], Batch [16/43], Training Loss: 0.00000404
2024-11-06 14:07:40,943 - INFO - Epoch [280/300], Batch [17/43], Training Loss: 0.00001589
2024-11-06 14:07:40,947 - INFO - Epoch [280/300], Batch [18/43], Training Loss: 0.00000466
2024-11-06 14:07:40,951 - INFO - Epoch [280/300], Batch [19/43], Training Loss: 0.00000394
2024-11-06 14:07:40,955 - INFO - Epoch [280/300], Batch [20/43], Training Loss: 0.00000265
2024-11-06 14:07:40,959 - INFO - Epoch [280/300], Batch [21/43], Training Loss: 0.00000598
2024-11-06 14:07:40,964 - INFO - Epoch [280/300], Batch [22/43], Training Loss: 0.00000714
2024-11-06 14:07:40,968 - INFO - Epoch [280/300], Batch [23/43], Training Loss: 0.00000507
2024-11-06 14:07:40,972 - INFO - Epoch [280/300], Batch [24/43], Training Loss: 0.00001689
2024-11-06 14:07:40,975 - INFO - Epoch [280/300], Batch [25/43], Training Loss: 0.00001268
2024-11-06 14:07:40,980 - INFO - Epoch [280/300], Batch [26/43], Training Loss: 0.00000514
2024-11-06 14:07:40,984 - INFO - Epoch [280/300], Batch [27/43], Training Loss: 0.00000875
2024-11-06 14:07:40,988 - INFO - Epoch [280/300], Batch [28/43], Training Loss: 0.00000764
2024-11-06 14:07:40,992 - INFO - Epoch [280/300], Batch [29/43], Training Loss: 0.00000878
2024-11-06 14:07:40,996 - INFO - Epoch [280/300], Batch [30/43], Training Loss: 0.00000365
2024-11-06 14:07:41,000 - INFO - Epoch [280/300], Batch [31/43], Training Loss: 0.00001028
2024-11-06 14:07:41,004 - INFO - Epoch [280/300], Batch [32/43], Training Loss: 0.00000862
2024-11-06 14:07:41,008 - INFO - Epoch [280/300], Batch [33/43], Training Loss: 0.00000693
2024-11-06 14:07:41,012 - INFO - Epoch [280/300], Batch [34/43], Training Loss: 0.00000779
2024-11-06 14:07:41,015 - INFO - Epoch [280/300], Batch [35/43], Training Loss: 0.00001273
2024-11-06 14:07:41,019 - INFO - Epoch [280/300], Batch [36/43], Training Loss: 0.00003291
2024-11-06 14:07:41,023 - INFO - Epoch [280/300], Batch [37/43], Training Loss: 0.00000333
2024-11-06 14:07:41,027 - INFO - Epoch [280/300], Batch [38/43], Training Loss: 0.00000960
2024-11-06 14:07:41,030 - INFO - Epoch [280/300], Batch [39/43], Training Loss: 0.00001520
2024-11-06 14:07:41,033 - INFO - Epoch [280/300], Batch [40/43], Training Loss: 0.00000468
2024-11-06 14:07:41,037 - INFO - Epoch [280/300], Batch [41/43], Training Loss: 0.00000540
2024-11-06 14:07:41,041 - INFO - Epoch [280/300], Batch [42/43], Training Loss: 0.00000438
2024-11-06 14:07:41,045 - INFO - Epoch [280/300], Batch [43/43], Training Loss: 0.00002666
2024-11-06 14:07:41,057 - INFO - Epoch [280/300], Average Training Loss: 0.00000987, Validation Loss: 0.00001203
2024-11-06 14:07:41,062 - INFO - Epoch [281/300], Batch [1/43], Training Loss: 0.00000785
2024-11-06 14:07:41,065 - INFO - Epoch [281/300], Batch [2/43], Training Loss: 0.00002302
2024-11-06 14:07:41,069 - INFO - Epoch [281/300], Batch [3/43], Training Loss: 0.00000553
2024-11-06 14:07:41,074 - INFO - Epoch [281/300], Batch [4/43], Training Loss: 0.00000909
2024-11-06 14:07:41,078 - INFO - Epoch [281/300], Batch [5/43], Training Loss: 0.00001279
2024-11-06 14:07:41,082 - INFO - Epoch [281/300], Batch [6/43], Training Loss: 0.00000771
2024-11-06 14:07:41,086 - INFO - Epoch [281/300], Batch [7/43], Training Loss: 0.00001192
2024-11-06 14:07:41,089 - INFO - Epoch [281/300], Batch [8/43], Training Loss: 0.00000627
2024-11-06 14:07:41,093 - INFO - Epoch [281/300], Batch [9/43], Training Loss: 0.00001047
2024-11-06 14:07:41,097 - INFO - Epoch [281/300], Batch [10/43], Training Loss: 0.00001495
2024-11-06 14:07:41,101 - INFO - Epoch [281/300], Batch [11/43], Training Loss: 0.00000334
2024-11-06 14:07:41,104 - INFO - Epoch [281/300], Batch [12/43], Training Loss: 0.00001507
2024-11-06 14:07:41,109 - INFO - Epoch [281/300], Batch [13/43], Training Loss: 0.00000526
2024-11-06 14:07:41,113 - INFO - Epoch [281/300], Batch [14/43], Training Loss: 0.00000350
2024-11-06 14:07:41,117 - INFO - Epoch [281/300], Batch [15/43], Training Loss: 0.00000726
2024-11-06 14:07:41,121 - INFO - Epoch [281/300], Batch [16/43], Training Loss: 0.00000688
2024-11-06 14:07:41,124 - INFO - Epoch [281/300], Batch [17/43], Training Loss: 0.00000892
2024-11-06 14:07:41,127 - INFO - Epoch [281/300], Batch [18/43], Training Loss: 0.00000499
2024-11-06 14:07:41,132 - INFO - Epoch [281/300], Batch [19/43], Training Loss: 0.00001047
2024-11-06 14:07:41,136 - INFO - Epoch [281/300], Batch [20/43], Training Loss: 0.00001939
2024-11-06 14:07:41,140 - INFO - Epoch [281/300], Batch [21/43], Training Loss: 0.00000583
2024-11-06 14:07:41,143 - INFO - Epoch [281/300], Batch [22/43], Training Loss: 0.00002575
2024-11-06 14:07:41,147 - INFO - Epoch [281/300], Batch [23/43], Training Loss: 0.00001695
2024-11-06 14:07:41,151 - INFO - Epoch [281/300], Batch [24/43], Training Loss: 0.00001099
2024-11-06 14:07:41,155 - INFO - Epoch [281/300], Batch [25/43], Training Loss: 0.00001158
2024-11-06 14:07:41,158 - INFO - Epoch [281/300], Batch [26/43], Training Loss: 0.00000774
2024-11-06 14:07:41,162 - INFO - Epoch [281/300], Batch [27/43], Training Loss: 0.00000834
2024-11-06 14:07:41,166 - INFO - Epoch [281/300], Batch [28/43], Training Loss: 0.00000759
2024-11-06 14:07:41,169 - INFO - Epoch [281/300], Batch [29/43], Training Loss: 0.00000721
2024-11-06 14:07:41,172 - INFO - Epoch [281/300], Batch [30/43], Training Loss: 0.00001405
2024-11-06 14:07:41,175 - INFO - Epoch [281/300], Batch [31/43], Training Loss: 0.00000956
2024-11-06 14:07:41,180 - INFO - Epoch [281/300], Batch [32/43], Training Loss: 0.00000380
2024-11-06 14:07:41,185 - INFO - Epoch [281/300], Batch [33/43], Training Loss: 0.00000704
2024-11-06 14:07:41,190 - INFO - Epoch [281/300], Batch [34/43], Training Loss: 0.00001017
2024-11-06 14:07:41,194 - INFO - Epoch [281/300], Batch [35/43], Training Loss: 0.00000649
2024-11-06 14:07:41,198 - INFO - Epoch [281/300], Batch [36/43], Training Loss: 0.00001002
2024-11-06 14:07:41,202 - INFO - Epoch [281/300], Batch [37/43], Training Loss: 0.00001841
2024-11-06 14:07:41,207 - INFO - Epoch [281/300], Batch [38/43], Training Loss: 0.00001023
2024-11-06 14:07:41,211 - INFO - Epoch [281/300], Batch [39/43], Training Loss: 0.00000349
2024-11-06 14:07:41,215 - INFO - Epoch [281/300], Batch [40/43], Training Loss: 0.00001092
2024-11-06 14:07:41,219 - INFO - Epoch [281/300], Batch [41/43], Training Loss: 0.00001202
2024-11-06 14:07:41,222 - INFO - Epoch [281/300], Batch [42/43], Training Loss: 0.00000544
2024-11-06 14:07:41,226 - INFO - Epoch [281/300], Batch [43/43], Training Loss: 0.00000930
2024-11-06 14:07:41,238 - INFO - Epoch [281/300], Average Training Loss: 0.00000994, Validation Loss: 0.00001210
2024-11-06 14:07:41,243 - INFO - Epoch [282/300], Batch [1/43], Training Loss: 0.00000397
2024-11-06 14:07:41,246 - INFO - Epoch [282/300], Batch [2/43], Training Loss: 0.00000503
2024-11-06 14:07:41,250 - INFO - Epoch [282/300], Batch [3/43], Training Loss: 0.00003007
2024-11-06 14:07:41,254 - INFO - Epoch [282/300], Batch [4/43], Training Loss: 0.00000636
2024-11-06 14:07:41,258 - INFO - Epoch [282/300], Batch [5/43], Training Loss: 0.00000234
2024-11-06 14:07:41,262 - INFO - Epoch [282/300], Batch [6/43], Training Loss: 0.00000933
2024-11-06 14:07:41,266 - INFO - Epoch [282/300], Batch [7/43], Training Loss: 0.00001447
2024-11-06 14:07:41,300 - INFO - Epoch [282/300], Batch [8/43], Training Loss: 0.00000520
2024-11-06 14:07:41,337 - INFO - Epoch [282/300], Batch [9/43], Training Loss: 0.00001112
2024-11-06 14:07:41,341 - INFO - Epoch [282/300], Batch [10/43], Training Loss: 0.00000598
2024-11-06 14:07:41,347 - INFO - Epoch [282/300], Batch [11/43], Training Loss: 0.00001879
2024-11-06 14:07:41,351 - INFO - Epoch [282/300], Batch [12/43], Training Loss: 0.00000722
2024-11-06 14:07:41,356 - INFO - Epoch [282/300], Batch [13/43], Training Loss: 0.00001405
2024-11-06 14:07:41,360 - INFO - Epoch [282/300], Batch [14/43], Training Loss: 0.00001096
2024-11-06 14:07:41,363 - INFO - Epoch [282/300], Batch [15/43], Training Loss: 0.00003121
2024-11-06 14:07:41,368 - INFO - Epoch [282/300], Batch [16/43], Training Loss: 0.00001177
2024-11-06 14:07:41,372 - INFO - Epoch [282/300], Batch [17/43], Training Loss: 0.00000302
2024-11-06 14:07:41,377 - INFO - Epoch [282/300], Batch [18/43], Training Loss: 0.00000906
2024-11-06 14:07:41,382 - INFO - Epoch [282/300], Batch [19/43], Training Loss: 0.00000487
2024-11-06 14:07:41,387 - INFO - Epoch [282/300], Batch [20/43], Training Loss: 0.00001553
2024-11-06 14:07:41,391 - INFO - Epoch [282/300], Batch [21/43], Training Loss: 0.00000631
2024-11-06 14:07:41,395 - INFO - Epoch [282/300], Batch [22/43], Training Loss: 0.00000733
2024-11-06 14:07:41,399 - INFO - Epoch [282/300], Batch [23/43], Training Loss: 0.00000580
2024-11-06 14:07:41,404 - INFO - Epoch [282/300], Batch [24/43], Training Loss: 0.00000678
2024-11-06 14:07:41,408 - INFO - Epoch [282/300], Batch [25/43], Training Loss: 0.00000301
2024-11-06 14:07:41,414 - INFO - Epoch [282/300], Batch [26/43], Training Loss: 0.00001483
2024-11-06 14:07:41,420 - INFO - Epoch [282/300], Batch [27/43], Training Loss: 0.00001188
2024-11-06 14:07:41,425 - INFO - Epoch [282/300], Batch [28/43], Training Loss: 0.00000575
2024-11-06 14:07:41,429 - INFO - Epoch [282/300], Batch [29/43], Training Loss: 0.00000314
2024-11-06 14:07:41,433 - INFO - Epoch [282/300], Batch [30/43], Training Loss: 0.00000525
2024-11-06 14:07:41,437 - INFO - Epoch [282/300], Batch [31/43], Training Loss: 0.00000280
2024-11-06 14:07:41,440 - INFO - Epoch [282/300], Batch [32/43], Training Loss: 0.00000649
2024-11-06 14:07:41,444 - INFO - Epoch [282/300], Batch [33/43], Training Loss: 0.00001330
2024-11-06 14:07:41,449 - INFO - Epoch [282/300], Batch [34/43], Training Loss: 0.00000406
2024-11-06 14:07:41,453 - INFO - Epoch [282/300], Batch [35/43], Training Loss: 0.00003390
2024-11-06 14:07:41,458 - INFO - Epoch [282/300], Batch [36/43], Training Loss: 0.00001871
2024-11-06 14:07:41,462 - INFO - Epoch [282/300], Batch [37/43], Training Loss: 0.00001306
2024-11-06 14:07:41,466 - INFO - Epoch [282/300], Batch [38/43], Training Loss: 0.00000308
2024-11-06 14:07:41,470 - INFO - Epoch [282/300], Batch [39/43], Training Loss: 0.00000534
2024-11-06 14:07:41,474 - INFO - Epoch [282/300], Batch [40/43], Training Loss: 0.00001249
2024-11-06 14:07:41,477 - INFO - Epoch [282/300], Batch [41/43], Training Loss: 0.00001515
2024-11-06 14:07:41,481 - INFO - Epoch [282/300], Batch [42/43], Training Loss: 0.00001194
2024-11-06 14:07:41,485 - INFO - Epoch [282/300], Batch [43/43], Training Loss: 0.00001158
2024-11-06 14:07:41,496 - INFO - Epoch [282/300], Average Training Loss: 0.00001029, Validation Loss: 0.00001362
2024-11-06 14:07:41,500 - INFO - Epoch [283/300], Batch [1/43], Training Loss: 0.00000882
2024-11-06 14:07:41,504 - INFO - Epoch [283/300], Batch [2/43], Training Loss: 0.00001528
2024-11-06 14:07:41,508 - INFO - Epoch [283/300], Batch [3/43], Training Loss: 0.00000625
2024-11-06 14:07:41,511 - INFO - Epoch [283/300], Batch [4/43], Training Loss: 0.00001734
2024-11-06 14:07:41,515 - INFO - Epoch [283/300], Batch [5/43], Training Loss: 0.00001177
2024-11-06 14:07:41,519 - INFO - Epoch [283/300], Batch [6/43], Training Loss: 0.00000682
2024-11-06 14:07:41,522 - INFO - Epoch [283/300], Batch [7/43], Training Loss: 0.00000744
2024-11-06 14:07:41,526 - INFO - Epoch [283/300], Batch [8/43], Training Loss: 0.00002266
2024-11-06 14:07:41,531 - INFO - Epoch [283/300], Batch [9/43], Training Loss: 0.00001993
2024-11-06 14:07:41,536 - INFO - Epoch [283/300], Batch [10/43], Training Loss: 0.00000910
2024-11-06 14:07:41,541 - INFO - Epoch [283/300], Batch [11/43], Training Loss: 0.00000734
2024-11-06 14:07:41,546 - INFO - Epoch [283/300], Batch [12/43], Training Loss: 0.00001184
2024-11-06 14:07:41,550 - INFO - Epoch [283/300], Batch [13/43], Training Loss: 0.00001554
2024-11-06 14:07:41,554 - INFO - Epoch [283/300], Batch [14/43], Training Loss: 0.00000423
2024-11-06 14:07:41,558 - INFO - Epoch [283/300], Batch [15/43], Training Loss: 0.00002353
2024-11-06 14:07:41,563 - INFO - Epoch [283/300], Batch [16/43], Training Loss: 0.00000374
2024-11-06 14:07:41,567 - INFO - Epoch [283/300], Batch [17/43], Training Loss: 0.00000957
2024-11-06 14:07:41,571 - INFO - Epoch [283/300], Batch [18/43], Training Loss: 0.00001987
2024-11-06 14:07:41,574 - INFO - Epoch [283/300], Batch [19/43], Training Loss: 0.00001328
2024-11-06 14:07:41,577 - INFO - Epoch [283/300], Batch [20/43], Training Loss: 0.00001280
2024-11-06 14:07:41,580 - INFO - Epoch [283/300], Batch [21/43], Training Loss: 0.00000887
2024-11-06 14:07:41,584 - INFO - Epoch [283/300], Batch [22/43], Training Loss: 0.00000762
2024-11-06 14:07:41,588 - INFO - Epoch [283/300], Batch [23/43], Training Loss: 0.00000263
2024-11-06 14:07:41,591 - INFO - Epoch [283/300], Batch [24/43], Training Loss: 0.00000893
2024-11-06 14:07:41,595 - INFO - Epoch [283/300], Batch [25/43], Training Loss: 0.00000528
2024-11-06 14:07:41,599 - INFO - Epoch [283/300], Batch [26/43], Training Loss: 0.00000619
2024-11-06 14:07:41,603 - INFO - Epoch [283/300], Batch [27/43], Training Loss: 0.00000470
2024-11-06 14:07:41,606 - INFO - Epoch [283/300], Batch [28/43], Training Loss: 0.00000788
2024-11-06 14:07:41,610 - INFO - Epoch [283/300], Batch [29/43], Training Loss: 0.00000326
2024-11-06 14:07:41,613 - INFO - Epoch [283/300], Batch [30/43], Training Loss: 0.00001335
2024-11-06 14:07:41,617 - INFO - Epoch [283/300], Batch [31/43], Training Loss: 0.00000846
2024-11-06 14:07:41,621 - INFO - Epoch [283/300], Batch [32/43], Training Loss: 0.00001717
2024-11-06 14:07:41,624 - INFO - Epoch [283/300], Batch [33/43], Training Loss: 0.00000920
2024-11-06 14:07:41,627 - INFO - Epoch [283/300], Batch [34/43], Training Loss: 0.00000346
2024-11-06 14:07:41,631 - INFO - Epoch [283/300], Batch [35/43], Training Loss: 0.00001574
2024-11-06 14:07:41,635 - INFO - Epoch [283/300], Batch [36/43], Training Loss: 0.00000218
2024-11-06 14:07:41,639 - INFO - Epoch [283/300], Batch [37/43], Training Loss: 0.00000437
2024-11-06 14:07:41,643 - INFO - Epoch [283/300], Batch [38/43], Training Loss: 0.00000646
2024-11-06 14:07:41,646 - INFO - Epoch [283/300], Batch [39/43], Training Loss: 0.00000825
2024-11-06 14:07:41,649 - INFO - Epoch [283/300], Batch [40/43], Training Loss: 0.00001154
2024-11-06 14:07:41,652 - INFO - Epoch [283/300], Batch [41/43], Training Loss: 0.00000934
2024-11-06 14:07:41,656 - INFO - Epoch [283/300], Batch [42/43], Training Loss: 0.00001090
2024-11-06 14:07:41,659 - INFO - Epoch [283/300], Batch [43/43], Training Loss: 0.00000912
2024-11-06 14:07:41,670 - INFO - Epoch [283/300], Average Training Loss: 0.00001005, Validation Loss: 0.00001433
2024-11-06 14:07:41,674 - INFO - Epoch [284/300], Batch [1/43], Training Loss: 0.00001576
2024-11-06 14:07:41,677 - INFO - Epoch [284/300], Batch [2/43], Training Loss: 0.00001413
2024-11-06 14:07:41,683 - INFO - Epoch [284/300], Batch [3/43], Training Loss: 0.00000453
2024-11-06 14:07:41,687 - INFO - Epoch [284/300], Batch [4/43], Training Loss: 0.00001448
2024-11-06 14:07:41,690 - INFO - Epoch [284/300], Batch [5/43], Training Loss: 0.00002584
2024-11-06 14:07:41,695 - INFO - Epoch [284/300], Batch [6/43], Training Loss: 0.00001814
2024-11-06 14:07:41,700 - INFO - Epoch [284/300], Batch [7/43], Training Loss: 0.00000706
2024-11-06 14:07:41,703 - INFO - Epoch [284/300], Batch [8/43], Training Loss: 0.00000319
2024-11-06 14:07:41,707 - INFO - Epoch [284/300], Batch [9/43], Training Loss: 0.00000354
2024-11-06 14:07:41,711 - INFO - Epoch [284/300], Batch [10/43], Training Loss: 0.00001019
2024-11-06 14:07:41,714 - INFO - Epoch [284/300], Batch [11/43], Training Loss: 0.00000835
2024-11-06 14:07:41,718 - INFO - Epoch [284/300], Batch [12/43], Training Loss: 0.00001320
2024-11-06 14:07:41,721 - INFO - Epoch [284/300], Batch [13/43], Training Loss: 0.00000835
2024-11-06 14:07:41,725 - INFO - Epoch [284/300], Batch [14/43], Training Loss: 0.00000349
2024-11-06 14:07:41,729 - INFO - Epoch [284/300], Batch [15/43], Training Loss: 0.00001046
2024-11-06 14:07:41,733 - INFO - Epoch [284/300], Batch [16/43], Training Loss: 0.00001943
2024-11-06 14:07:41,737 - INFO - Epoch [284/300], Batch [17/43], Training Loss: 0.00000519
2024-11-06 14:07:41,741 - INFO - Epoch [284/300], Batch [18/43], Training Loss: 0.00001442
2024-11-06 14:07:41,746 - INFO - Epoch [284/300], Batch [19/43], Training Loss: 0.00000706
2024-11-06 14:07:41,750 - INFO - Epoch [284/300], Batch [20/43], Training Loss: 0.00002759
2024-11-06 14:07:41,754 - INFO - Epoch [284/300], Batch [21/43], Training Loss: 0.00000452
2024-11-06 14:07:41,758 - INFO - Epoch [284/300], Batch [22/43], Training Loss: 0.00000714
2024-11-06 14:07:41,761 - INFO - Epoch [284/300], Batch [23/43], Training Loss: 0.00002048
2024-11-06 14:07:41,763 - INFO - Epoch [284/300], Batch [24/43], Training Loss: 0.00000439
2024-11-06 14:07:41,766 - INFO - Epoch [284/300], Batch [25/43], Training Loss: 0.00001341
2024-11-06 14:07:41,770 - INFO - Epoch [284/300], Batch [26/43], Training Loss: 0.00000599
2024-11-06 14:07:41,773 - INFO - Epoch [284/300], Batch [27/43], Training Loss: 0.00001221
2024-11-06 14:07:41,776 - INFO - Epoch [284/300], Batch [28/43], Training Loss: 0.00000652
2024-11-06 14:07:41,780 - INFO - Epoch [284/300], Batch [29/43], Training Loss: 0.00000505
2024-11-06 14:07:41,784 - INFO - Epoch [284/300], Batch [30/43], Training Loss: 0.00000451
2024-11-06 14:07:41,787 - INFO - Epoch [284/300], Batch [31/43], Training Loss: 0.00001276
2024-11-06 14:07:41,790 - INFO - Epoch [284/300], Batch [32/43], Training Loss: 0.00000301
2024-11-06 14:07:41,793 - INFO - Epoch [284/300], Batch [33/43], Training Loss: 0.00001043
2024-11-06 14:07:41,795 - INFO - Epoch [284/300], Batch [34/43], Training Loss: 0.00000583
2024-11-06 14:07:41,798 - INFO - Epoch [284/300], Batch [35/43], Training Loss: 0.00000685
2024-11-06 14:07:41,801 - INFO - Epoch [284/300], Batch [36/43], Training Loss: 0.00001014
2024-11-06 14:07:41,804 - INFO - Epoch [284/300], Batch [37/43], Training Loss: 0.00001044
2024-11-06 14:07:41,807 - INFO - Epoch [284/300], Batch [38/43], Training Loss: 0.00001349
2024-11-06 14:07:41,810 - INFO - Epoch [284/300], Batch [39/43], Training Loss: 0.00000713
2024-11-06 14:07:41,814 - INFO - Epoch [284/300], Batch [40/43], Training Loss: 0.00000532
2024-11-06 14:07:41,818 - INFO - Epoch [284/300], Batch [41/43], Training Loss: 0.00000634
2024-11-06 14:07:41,823 - INFO - Epoch [284/300], Batch [42/43], Training Loss: 0.00000787
2024-11-06 14:07:41,827 - INFO - Epoch [284/300], Batch [43/43], Training Loss: 0.00000536
2024-11-06 14:07:41,839 - INFO - Epoch [284/300], Average Training Loss: 0.00000985, Validation Loss: 0.00001205
2024-11-06 14:07:41,844 - INFO - Epoch [285/300], Batch [1/43], Training Loss: 0.00001267
2024-11-06 14:07:41,849 - INFO - Epoch [285/300], Batch [2/43], Training Loss: 0.00000325
2024-11-06 14:07:41,853 - INFO - Epoch [285/300], Batch [3/43], Training Loss: 0.00002068
2024-11-06 14:07:41,857 - INFO - Epoch [285/300], Batch [4/43], Training Loss: 0.00000282
2024-11-06 14:07:41,861 - INFO - Epoch [285/300], Batch [5/43], Training Loss: 0.00001056
2024-11-06 14:07:41,864 - INFO - Epoch [285/300], Batch [6/43], Training Loss: 0.00001372
2024-11-06 14:07:41,869 - INFO - Epoch [285/300], Batch [7/43], Training Loss: 0.00002013
2024-11-06 14:07:41,872 - INFO - Epoch [285/300], Batch [8/43], Training Loss: 0.00002099
2024-11-06 14:07:41,876 - INFO - Epoch [285/300], Batch [9/43], Training Loss: 0.00000708
2024-11-06 14:07:41,880 - INFO - Epoch [285/300], Batch [10/43], Training Loss: 0.00000357
2024-11-06 14:07:41,884 - INFO - Epoch [285/300], Batch [11/43], Training Loss: 0.00000630
2024-11-06 14:07:41,888 - INFO - Epoch [285/300], Batch [12/43], Training Loss: 0.00000517
2024-11-06 14:07:41,892 - INFO - Epoch [285/300], Batch [13/43], Training Loss: 0.00001857
2024-11-06 14:07:41,895 - INFO - Epoch [285/300], Batch [14/43], Training Loss: 0.00001309
2024-11-06 14:07:41,898 - INFO - Epoch [285/300], Batch [15/43], Training Loss: 0.00000659
2024-11-06 14:07:41,901 - INFO - Epoch [285/300], Batch [16/43], Training Loss: 0.00002218
2024-11-06 14:07:41,904 - INFO - Epoch [285/300], Batch [17/43], Training Loss: 0.00001315
2024-11-06 14:07:41,908 - INFO - Epoch [285/300], Batch [18/43], Training Loss: 0.00000627
2024-11-06 14:07:41,911 - INFO - Epoch [285/300], Batch [19/43], Training Loss: 0.00001329
2024-11-06 14:07:41,915 - INFO - Epoch [285/300], Batch [20/43], Training Loss: 0.00001087
2024-11-06 14:07:41,919 - INFO - Epoch [285/300], Batch [21/43], Training Loss: 0.00001289
2024-11-06 14:07:41,923 - INFO - Epoch [285/300], Batch [22/43], Training Loss: 0.00000965
2024-11-06 14:07:41,926 - INFO - Epoch [285/300], Batch [23/43], Training Loss: 0.00000634
2024-11-06 14:07:41,929 - INFO - Epoch [285/300], Batch [24/43], Training Loss: 0.00000862
2024-11-06 14:07:41,933 - INFO - Epoch [285/300], Batch [25/43], Training Loss: 0.00000746
2024-11-06 14:07:41,936 - INFO - Epoch [285/300], Batch [26/43], Training Loss: 0.00000164
2024-11-06 14:07:41,940 - INFO - Epoch [285/300], Batch [27/43], Training Loss: 0.00000791
2024-11-06 14:07:41,943 - INFO - Epoch [285/300], Batch [28/43], Training Loss: 0.00000609
2024-11-06 14:07:41,946 - INFO - Epoch [285/300], Batch [29/43], Training Loss: 0.00001079
2024-11-06 14:07:41,950 - INFO - Epoch [285/300], Batch [30/43], Training Loss: 0.00000478
2024-11-06 14:07:41,953 - INFO - Epoch [285/300], Batch [31/43], Training Loss: 0.00001274
2024-11-06 14:07:41,956 - INFO - Epoch [285/300], Batch [32/43], Training Loss: 0.00000292
2024-11-06 14:07:41,960 - INFO - Epoch [285/300], Batch [33/43], Training Loss: 0.00000739
2024-11-06 14:07:41,963 - INFO - Epoch [285/300], Batch [34/43], Training Loss: 0.00001020
2024-11-06 14:07:41,966 - INFO - Epoch [285/300], Batch [35/43], Training Loss: 0.00000573
2024-11-06 14:07:41,970 - INFO - Epoch [285/300], Batch [36/43], Training Loss: 0.00000901
2024-11-06 14:07:41,972 - INFO - Epoch [285/300], Batch [37/43], Training Loss: 0.00000590
2024-11-06 14:07:41,976 - INFO - Epoch [285/300], Batch [38/43], Training Loss: 0.00000602
2024-11-06 14:07:41,978 - INFO - Epoch [285/300], Batch [39/43], Training Loss: 0.00000446
2024-11-06 14:07:41,982 - INFO - Epoch [285/300], Batch [40/43], Training Loss: 0.00002679
2024-11-06 14:07:41,985 - INFO - Epoch [285/300], Batch [41/43], Training Loss: 0.00000975
2024-11-06 14:07:41,989 - INFO - Epoch [285/300], Batch [42/43], Training Loss: 0.00001989
2024-11-06 14:07:41,993 - INFO - Epoch [285/300], Batch [43/43], Training Loss: 0.00000915
2024-11-06 14:07:42,005 - INFO - Epoch [285/300], Average Training Loss: 0.00001016, Validation Loss: 0.00001223
2024-11-06 14:07:42,009 - INFO - Epoch [286/300], Batch [1/43], Training Loss: 0.00000749
2024-11-06 14:07:42,012 - INFO - Epoch [286/300], Batch [2/43], Training Loss: 0.00001468
2024-11-06 14:07:42,016 - INFO - Epoch [286/300], Batch [3/43], Training Loss: 0.00001217
2024-11-06 14:07:42,020 - INFO - Epoch [286/300], Batch [4/43], Training Loss: 0.00002042
2024-11-06 14:07:42,024 - INFO - Epoch [286/300], Batch [5/43], Training Loss: 0.00001212
2024-11-06 14:07:42,028 - INFO - Epoch [286/300], Batch [6/43], Training Loss: 0.00000849
2024-11-06 14:07:42,032 - INFO - Epoch [286/300], Batch [7/43], Training Loss: 0.00000967
2024-11-06 14:07:42,037 - INFO - Epoch [286/300], Batch [8/43], Training Loss: 0.00001609
2024-11-06 14:07:42,041 - INFO - Epoch [286/300], Batch [9/43], Training Loss: 0.00000588
2024-11-06 14:07:42,046 - INFO - Epoch [286/300], Batch [10/43], Training Loss: 0.00000697
2024-11-06 14:07:42,049 - INFO - Epoch [286/300], Batch [11/43], Training Loss: 0.00000538
2024-11-06 14:07:42,057 - INFO - Epoch [286/300], Batch [12/43], Training Loss: 0.00000673
2024-11-06 14:07:42,062 - INFO - Epoch [286/300], Batch [13/43], Training Loss: 0.00000747
2024-11-06 14:07:42,067 - INFO - Epoch [286/300], Batch [14/43], Training Loss: 0.00002139
2024-11-06 14:07:42,071 - INFO - Epoch [286/300], Batch [15/43], Training Loss: 0.00001684
2024-11-06 14:07:42,075 - INFO - Epoch [286/300], Batch [16/43], Training Loss: 0.00000471
2024-11-06 14:07:42,079 - INFO - Epoch [286/300], Batch [17/43], Training Loss: 0.00000868
2024-11-06 14:07:42,083 - INFO - Epoch [286/300], Batch [18/43], Training Loss: 0.00000957
2024-11-06 14:07:42,087 - INFO - Epoch [286/300], Batch [19/43], Training Loss: 0.00001127
2024-11-06 14:07:42,091 - INFO - Epoch [286/300], Batch [20/43], Training Loss: 0.00000535
2024-11-06 14:07:42,095 - INFO - Epoch [286/300], Batch [21/43], Training Loss: 0.00001495
2024-11-06 14:07:42,099 - INFO - Epoch [286/300], Batch [22/43], Training Loss: 0.00000507
2024-11-06 14:07:42,103 - INFO - Epoch [286/300], Batch [23/43], Training Loss: 0.00000691
2024-11-06 14:07:42,107 - INFO - Epoch [286/300], Batch [24/43], Training Loss: 0.00000993
2024-11-06 14:07:42,111 - INFO - Epoch [286/300], Batch [25/43], Training Loss: 0.00000628
2024-11-06 14:07:42,116 - INFO - Epoch [286/300], Batch [26/43], Training Loss: 0.00001342
2024-11-06 14:07:42,119 - INFO - Epoch [286/300], Batch [27/43], Training Loss: 0.00001943
2024-11-06 14:07:42,123 - INFO - Epoch [286/300], Batch [28/43], Training Loss: 0.00000480
2024-11-06 14:07:42,127 - INFO - Epoch [286/300], Batch [29/43], Training Loss: 0.00000776
2024-11-06 14:07:42,131 - INFO - Epoch [286/300], Batch [30/43], Training Loss: 0.00001113
2024-11-06 14:07:42,134 - INFO - Epoch [286/300], Batch [31/43], Training Loss: 0.00001318
2024-11-06 14:07:42,137 - INFO - Epoch [286/300], Batch [32/43], Training Loss: 0.00000920
2024-11-06 14:07:42,140 - INFO - Epoch [286/300], Batch [33/43], Training Loss: 0.00000452
2024-11-06 14:07:42,143 - INFO - Epoch [286/300], Batch [34/43], Training Loss: 0.00001682
2024-11-06 14:07:42,148 - INFO - Epoch [286/300], Batch [35/43], Training Loss: 0.00002282
2024-11-06 14:07:42,152 - INFO - Epoch [286/300], Batch [36/43], Training Loss: 0.00000296
2024-11-06 14:07:42,156 - INFO - Epoch [286/300], Batch [37/43], Training Loss: 0.00001311
2024-11-06 14:07:42,159 - INFO - Epoch [286/300], Batch [38/43], Training Loss: 0.00001011
2024-11-06 14:07:42,161 - INFO - Epoch [286/300], Batch [39/43], Training Loss: 0.00000851
2024-11-06 14:07:42,165 - INFO - Epoch [286/300], Batch [40/43], Training Loss: 0.00000852
2024-11-06 14:07:42,167 - INFO - Epoch [286/300], Batch [41/43], Training Loss: 0.00000520
2024-11-06 14:07:42,171 - INFO - Epoch [286/300], Batch [42/43], Training Loss: 0.00002127
2024-11-06 14:07:42,174 - INFO - Epoch [286/300], Batch [43/43], Training Loss: 0.00001653
2024-11-06 14:07:42,186 - INFO - Epoch [286/300], Average Training Loss: 0.00001079, Validation Loss: 0.00001250
2024-11-06 14:07:42,189 - INFO - Epoch [287/300], Batch [1/43], Training Loss: 0.00000971
2024-11-06 14:07:42,193 - INFO - Epoch [287/300], Batch [2/43], Training Loss: 0.00002086
2024-11-06 14:07:42,197 - INFO - Epoch [287/300], Batch [3/43], Training Loss: 0.00001204
2024-11-06 14:07:42,201 - INFO - Epoch [287/300], Batch [4/43], Training Loss: 0.00000591
2024-11-06 14:07:42,205 - INFO - Epoch [287/300], Batch [5/43], Training Loss: 0.00000236
2024-11-06 14:07:42,209 - INFO - Epoch [287/300], Batch [6/43], Training Loss: 0.00002734
2024-11-06 14:07:42,213 - INFO - Epoch [287/300], Batch [7/43], Training Loss: 0.00000696
2024-11-06 14:07:42,217 - INFO - Epoch [287/300], Batch [8/43], Training Loss: 0.00001012
2024-11-06 14:07:42,221 - INFO - Epoch [287/300], Batch [9/43], Training Loss: 0.00000888
2024-11-06 14:07:42,225 - INFO - Epoch [287/300], Batch [10/43], Training Loss: 0.00001268
2024-11-06 14:07:42,230 - INFO - Epoch [287/300], Batch [11/43], Training Loss: 0.00001749
2024-11-06 14:07:42,233 - INFO - Epoch [287/300], Batch [12/43], Training Loss: 0.00001557
2024-11-06 14:07:42,237 - INFO - Epoch [287/300], Batch [13/43], Training Loss: 0.00001583
2024-11-06 14:07:42,242 - INFO - Epoch [287/300], Batch [14/43], Training Loss: 0.00001084
2024-11-06 14:07:42,246 - INFO - Epoch [287/300], Batch [15/43], Training Loss: 0.00000580
2024-11-06 14:07:42,250 - INFO - Epoch [287/300], Batch [16/43], Training Loss: 0.00002003
2024-11-06 14:07:42,254 - INFO - Epoch [287/300], Batch [17/43], Training Loss: 0.00000748
2024-11-06 14:07:42,257 - INFO - Epoch [287/300], Batch [18/43], Training Loss: 0.00001392
2024-11-06 14:07:42,261 - INFO - Epoch [287/300], Batch [19/43], Training Loss: 0.00000775
2024-11-06 14:07:42,264 - INFO - Epoch [287/300], Batch [20/43], Training Loss: 0.00000444
2024-11-06 14:07:42,267 - INFO - Epoch [287/300], Batch [21/43], Training Loss: 0.00000368
2024-11-06 14:07:42,270 - INFO - Epoch [287/300], Batch [22/43], Training Loss: 0.00000618
2024-11-06 14:07:42,274 - INFO - Epoch [287/300], Batch [23/43], Training Loss: 0.00000486
2024-11-06 14:07:42,277 - INFO - Epoch [287/300], Batch [24/43], Training Loss: 0.00000437
2024-11-06 14:07:42,280 - INFO - Epoch [287/300], Batch [25/43], Training Loss: 0.00000431
2024-11-06 14:07:42,284 - INFO - Epoch [287/300], Batch [26/43], Training Loss: 0.00000949
2024-11-06 14:07:42,287 - INFO - Epoch [287/300], Batch [27/43], Training Loss: 0.00000451
2024-11-06 14:07:42,291 - INFO - Epoch [287/300], Batch [28/43], Training Loss: 0.00003038
2024-11-06 14:07:42,295 - INFO - Epoch [287/300], Batch [29/43], Training Loss: 0.00001517
2024-11-06 14:07:42,298 - INFO - Epoch [287/300], Batch [30/43], Training Loss: 0.00000895
2024-11-06 14:07:42,303 - INFO - Epoch [287/300], Batch [31/43], Training Loss: 0.00000820
2024-11-06 14:07:42,306 - INFO - Epoch [287/300], Batch [32/43], Training Loss: 0.00000417
2024-11-06 14:07:42,310 - INFO - Epoch [287/300], Batch [33/43], Training Loss: 0.00001112
2024-11-06 14:07:42,313 - INFO - Epoch [287/300], Batch [34/43], Training Loss: 0.00001138
2024-11-06 14:07:42,317 - INFO - Epoch [287/300], Batch [35/43], Training Loss: 0.00000245
2024-11-06 14:07:42,321 - INFO - Epoch [287/300], Batch [36/43], Training Loss: 0.00001295
2024-11-06 14:07:42,325 - INFO - Epoch [287/300], Batch [37/43], Training Loss: 0.00000712
2024-11-06 14:07:42,329 - INFO - Epoch [287/300], Batch [38/43], Training Loss: 0.00000176
2024-11-06 14:07:42,333 - INFO - Epoch [287/300], Batch [39/43], Training Loss: 0.00000469
2024-11-06 14:07:42,337 - INFO - Epoch [287/300], Batch [40/43], Training Loss: 0.00001881
2024-11-06 14:07:42,340 - INFO - Epoch [287/300], Batch [41/43], Training Loss: 0.00001212
2024-11-06 14:07:42,344 - INFO - Epoch [287/300], Batch [42/43], Training Loss: 0.00000779
2024-11-06 14:07:42,348 - INFO - Epoch [287/300], Batch [43/43], Training Loss: 0.00000957
2024-11-06 14:07:42,360 - INFO - Epoch [287/300], Average Training Loss: 0.00001023, Validation Loss: 0.00001202
2024-11-06 14:07:42,365 - INFO - Epoch [288/300], Batch [1/43], Training Loss: 0.00000794
2024-11-06 14:07:42,368 - INFO - Epoch [288/300], Batch [2/43], Training Loss: 0.00000993
2024-11-06 14:07:42,371 - INFO - Epoch [288/300], Batch [3/43], Training Loss: 0.00000704
2024-11-06 14:07:42,374 - INFO - Epoch [288/300], Batch [4/43], Training Loss: 0.00001417
2024-11-06 14:07:42,378 - INFO - Epoch [288/300], Batch [5/43], Training Loss: 0.00001942
2024-11-06 14:07:42,382 - INFO - Epoch [288/300], Batch [6/43], Training Loss: 0.00000342
2024-11-06 14:07:42,386 - INFO - Epoch [288/300], Batch [7/43], Training Loss: 0.00001382
2024-11-06 14:07:42,389 - INFO - Epoch [288/300], Batch [8/43], Training Loss: 0.00001146
2024-11-06 14:07:42,393 - INFO - Epoch [288/300], Batch [9/43], Training Loss: 0.00000395
2024-11-06 14:07:42,397 - INFO - Epoch [288/300], Batch [10/43], Training Loss: 0.00001011
2024-11-06 14:07:42,401 - INFO - Epoch [288/300], Batch [11/43], Training Loss: 0.00000552
2024-11-06 14:07:42,406 - INFO - Epoch [288/300], Batch [12/43], Training Loss: 0.00002748
2024-11-06 14:07:42,409 - INFO - Epoch [288/300], Batch [13/43], Training Loss: 0.00001131
2024-11-06 14:07:42,413 - INFO - Epoch [288/300], Batch [14/43], Training Loss: 0.00001137
2024-11-06 14:07:42,417 - INFO - Epoch [288/300], Batch [15/43], Training Loss: 0.00000376
2024-11-06 14:07:42,423 - INFO - Epoch [288/300], Batch [16/43], Training Loss: 0.00001749
2024-11-06 14:07:42,428 - INFO - Epoch [288/300], Batch [17/43], Training Loss: 0.00000416
2024-11-06 14:07:42,433 - INFO - Epoch [288/300], Batch [18/43], Training Loss: 0.00001076
2024-11-06 14:07:42,437 - INFO - Epoch [288/300], Batch [19/43], Training Loss: 0.00001740
2024-11-06 14:07:42,441 - INFO - Epoch [288/300], Batch [20/43], Training Loss: 0.00000335
2024-11-06 14:07:42,446 - INFO - Epoch [288/300], Batch [21/43], Training Loss: 0.00000736
2024-11-06 14:07:42,451 - INFO - Epoch [288/300], Batch [22/43], Training Loss: 0.00001010
2024-11-06 14:07:42,454 - INFO - Epoch [288/300], Batch [23/43], Training Loss: 0.00001025
2024-11-06 14:07:42,459 - INFO - Epoch [288/300], Batch [24/43], Training Loss: 0.00001041
2024-11-06 14:07:42,464 - INFO - Epoch [288/300], Batch [25/43], Training Loss: 0.00001020
2024-11-06 14:07:42,468 - INFO - Epoch [288/300], Batch [26/43], Training Loss: 0.00000897
2024-11-06 14:07:42,473 - INFO - Epoch [288/300], Batch [27/43], Training Loss: 0.00000910
2024-11-06 14:07:42,477 - INFO - Epoch [288/300], Batch [28/43], Training Loss: 0.00000969
2024-11-06 14:07:42,481 - INFO - Epoch [288/300], Batch [29/43], Training Loss: 0.00000591
2024-11-06 14:07:42,485 - INFO - Epoch [288/300], Batch [30/43], Training Loss: 0.00001252
2024-11-06 14:07:42,488 - INFO - Epoch [288/300], Batch [31/43], Training Loss: 0.00000479
2024-11-06 14:07:42,492 - INFO - Epoch [288/300], Batch [32/43], Training Loss: 0.00001493
2024-11-06 14:07:42,496 - INFO - Epoch [288/300], Batch [33/43], Training Loss: 0.00000808
2024-11-06 14:07:42,500 - INFO - Epoch [288/300], Batch [34/43], Training Loss: 0.00001104
2024-11-06 14:07:42,504 - INFO - Epoch [288/300], Batch [35/43], Training Loss: 0.00000900
2024-11-06 14:07:42,508 - INFO - Epoch [288/300], Batch [36/43], Training Loss: 0.00000714
2024-11-06 14:07:42,511 - INFO - Epoch [288/300], Batch [37/43], Training Loss: 0.00000979
2024-11-06 14:07:42,516 - INFO - Epoch [288/300], Batch [38/43], Training Loss: 0.00000948
2024-11-06 14:07:42,520 - INFO - Epoch [288/300], Batch [39/43], Training Loss: 0.00000842
2024-11-06 14:07:42,523 - INFO - Epoch [288/300], Batch [40/43], Training Loss: 0.00001572
2024-11-06 14:07:42,527 - INFO - Epoch [288/300], Batch [41/43], Training Loss: 0.00000350
2024-11-06 14:07:42,531 - INFO - Epoch [288/300], Batch [42/43], Training Loss: 0.00000780
2024-11-06 14:07:42,536 - INFO - Epoch [288/300], Batch [43/43], Training Loss: 0.00000146
2024-11-06 14:07:42,547 - INFO - Epoch [288/300], Average Training Loss: 0.00000976, Validation Loss: 0.00001436
2024-11-06 14:07:42,551 - INFO - Epoch [289/300], Batch [1/43], Training Loss: 0.00000770
2024-11-06 14:07:42,554 - INFO - Epoch [289/300], Batch [2/43], Training Loss: 0.00000362
2024-11-06 14:07:42,559 - INFO - Epoch [289/300], Batch [3/43], Training Loss: 0.00000625
2024-11-06 14:07:42,562 - INFO - Epoch [289/300], Batch [4/43], Training Loss: 0.00002123
2024-11-06 14:07:42,567 - INFO - Epoch [289/300], Batch [5/43], Training Loss: 0.00000810
2024-11-06 14:07:42,571 - INFO - Epoch [289/300], Batch [6/43], Training Loss: 0.00000638
2024-11-06 14:07:42,575 - INFO - Epoch [289/300], Batch [7/43], Training Loss: 0.00003064
2024-11-06 14:07:42,580 - INFO - Epoch [289/300], Batch [8/43], Training Loss: 0.00001595
2024-11-06 14:07:42,584 - INFO - Epoch [289/300], Batch [9/43], Training Loss: 0.00000523
2024-11-06 14:07:42,588 - INFO - Epoch [289/300], Batch [10/43], Training Loss: 0.00000188
2024-11-06 14:07:42,592 - INFO - Epoch [289/300], Batch [11/43], Training Loss: 0.00000877
2024-11-06 14:07:42,596 - INFO - Epoch [289/300], Batch [12/43], Training Loss: 0.00000486
2024-11-06 14:07:42,599 - INFO - Epoch [289/300], Batch [13/43], Training Loss: 0.00000669
2024-11-06 14:07:42,603 - INFO - Epoch [289/300], Batch [14/43], Training Loss: 0.00000552
2024-11-06 14:07:42,606 - INFO - Epoch [289/300], Batch [15/43], Training Loss: 0.00001105
2024-11-06 14:07:42,609 - INFO - Epoch [289/300], Batch [16/43], Training Loss: 0.00000479
2024-11-06 14:07:42,614 - INFO - Epoch [289/300], Batch [17/43], Training Loss: 0.00000801
2024-11-06 14:07:42,618 - INFO - Epoch [289/300], Batch [18/43], Training Loss: 0.00000470
2024-11-06 14:07:42,622 - INFO - Epoch [289/300], Batch [19/43], Training Loss: 0.00000827
2024-11-06 14:07:42,626 - INFO - Epoch [289/300], Batch [20/43], Training Loss: 0.00001376
2024-11-06 14:07:42,629 - INFO - Epoch [289/300], Batch [21/43], Training Loss: 0.00001810
2024-11-06 14:07:42,633 - INFO - Epoch [289/300], Batch [22/43], Training Loss: 0.00000565
2024-11-06 14:07:42,636 - INFO - Epoch [289/300], Batch [23/43], Training Loss: 0.00001414
2024-11-06 14:07:42,639 - INFO - Epoch [289/300], Batch [24/43], Training Loss: 0.00001285
2024-11-06 14:07:42,643 - INFO - Epoch [289/300], Batch [25/43], Training Loss: 0.00000371
2024-11-06 14:07:42,646 - INFO - Epoch [289/300], Batch [26/43], Training Loss: 0.00000530
2024-11-06 14:07:42,649 - INFO - Epoch [289/300], Batch [27/43], Training Loss: 0.00000923
2024-11-06 14:07:42,653 - INFO - Epoch [289/300], Batch [28/43], Training Loss: 0.00001134
2024-11-06 14:07:42,656 - INFO - Epoch [289/300], Batch [29/43], Training Loss: 0.00000859
2024-11-06 14:07:42,660 - INFO - Epoch [289/300], Batch [30/43], Training Loss: 0.00001904
2024-11-06 14:07:42,663 - INFO - Epoch [289/300], Batch [31/43], Training Loss: 0.00002053
2024-11-06 14:07:42,666 - INFO - Epoch [289/300], Batch [32/43], Training Loss: 0.00002569
2024-11-06 14:07:42,669 - INFO - Epoch [289/300], Batch [33/43], Training Loss: 0.00001176
2024-11-06 14:07:42,672 - INFO - Epoch [289/300], Batch [34/43], Training Loss: 0.00000545
2024-11-06 14:07:42,675 - INFO - Epoch [289/300], Batch [35/43], Training Loss: 0.00001239
2024-11-06 14:07:42,678 - INFO - Epoch [289/300], Batch [36/43], Training Loss: 0.00001230
2024-11-06 14:07:42,682 - INFO - Epoch [289/300], Batch [37/43], Training Loss: 0.00000812
2024-11-06 14:07:42,685 - INFO - Epoch [289/300], Batch [38/43], Training Loss: 0.00001329
2024-11-06 14:07:42,688 - INFO - Epoch [289/300], Batch [39/43], Training Loss: 0.00000870
2024-11-06 14:07:42,690 - INFO - Epoch [289/300], Batch [40/43], Training Loss: 0.00000490
2024-11-06 14:07:42,693 - INFO - Epoch [289/300], Batch [41/43], Training Loss: 0.00001201
2024-11-06 14:07:42,696 - INFO - Epoch [289/300], Batch [42/43], Training Loss: 0.00000937
2024-11-06 14:07:42,699 - INFO - Epoch [289/300], Batch [43/43], Training Loss: 0.00001493
2024-11-06 14:07:42,711 - INFO - Epoch [289/300], Average Training Loss: 0.00001048, Validation Loss: 0.00001655
2024-11-06 14:07:42,714 - INFO - Epoch [290/300], Batch [1/43], Training Loss: 0.00001954
2024-11-06 14:07:42,717 - INFO - Epoch [290/300], Batch [2/43], Training Loss: 0.00003274
2024-11-06 14:07:42,721 - INFO - Epoch [290/300], Batch [3/43], Training Loss: 0.00000451
2024-11-06 14:07:42,725 - INFO - Epoch [290/300], Batch [4/43], Training Loss: 0.00000718
2024-11-06 14:07:42,729 - INFO - Epoch [290/300], Batch [5/43], Training Loss: 0.00001242
2024-11-06 14:07:42,733 - INFO - Epoch [290/300], Batch [6/43], Training Loss: 0.00002076
2024-11-06 14:07:42,737 - INFO - Epoch [290/300], Batch [7/43], Training Loss: 0.00000233
2024-11-06 14:07:42,741 - INFO - Epoch [290/300], Batch [8/43], Training Loss: 0.00002232
2024-11-06 14:07:42,745 - INFO - Epoch [290/300], Batch [9/43], Training Loss: 0.00003072
2024-11-06 14:07:42,748 - INFO - Epoch [290/300], Batch [10/43], Training Loss: 0.00001681
2024-11-06 14:07:42,752 - INFO - Epoch [290/300], Batch [11/43], Training Loss: 0.00000466
2024-11-06 14:07:42,756 - INFO - Epoch [290/300], Batch [12/43], Training Loss: 0.00000550
2024-11-06 14:07:42,760 - INFO - Epoch [290/300], Batch [13/43], Training Loss: 0.00004012
2024-11-06 14:07:42,764 - INFO - Epoch [290/300], Batch [14/43], Training Loss: 0.00000883
2024-11-06 14:07:42,767 - INFO - Epoch [290/300], Batch [15/43], Training Loss: 0.00000434
2024-11-06 14:07:42,770 - INFO - Epoch [290/300], Batch [16/43], Training Loss: 0.00001139
2024-11-06 14:07:42,773 - INFO - Epoch [290/300], Batch [17/43], Training Loss: 0.00001562
2024-11-06 14:07:42,776 - INFO - Epoch [290/300], Batch [18/43], Training Loss: 0.00001452
2024-11-06 14:07:42,779 - INFO - Epoch [290/300], Batch [19/43], Training Loss: 0.00001099
2024-11-06 14:07:42,782 - INFO - Epoch [290/300], Batch [20/43], Training Loss: 0.00001718
2024-11-06 14:07:42,785 - INFO - Epoch [290/300], Batch [21/43], Training Loss: 0.00000539
2024-11-06 14:07:42,788 - INFO - Epoch [290/300], Batch [22/43], Training Loss: 0.00000808
2024-11-06 14:07:42,791 - INFO - Epoch [290/300], Batch [23/43], Training Loss: 0.00001427
2024-11-06 14:07:42,794 - INFO - Epoch [290/300], Batch [24/43], Training Loss: 0.00001366
2024-11-06 14:07:42,797 - INFO - Epoch [290/300], Batch [25/43], Training Loss: 0.00001268
2024-11-06 14:07:42,800 - INFO - Epoch [290/300], Batch [26/43], Training Loss: 0.00001827
2024-11-06 14:07:42,804 - INFO - Epoch [290/300], Batch [27/43], Training Loss: 0.00001050
2024-11-06 14:07:42,807 - INFO - Epoch [290/300], Batch [28/43], Training Loss: 0.00001088
2024-11-06 14:07:42,810 - INFO - Epoch [290/300], Batch [29/43], Training Loss: 0.00000345
2024-11-06 14:07:42,814 - INFO - Epoch [290/300], Batch [30/43], Training Loss: 0.00000855
2024-11-06 14:07:42,818 - INFO - Epoch [290/300], Batch [31/43], Training Loss: 0.00001196
2024-11-06 14:07:42,821 - INFO - Epoch [290/300], Batch [32/43], Training Loss: 0.00000531
2024-11-06 14:07:42,825 - INFO - Epoch [290/300], Batch [33/43], Training Loss: 0.00000535
2024-11-06 14:07:42,829 - INFO - Epoch [290/300], Batch [34/43], Training Loss: 0.00000685
2024-11-06 14:07:42,832 - INFO - Epoch [290/300], Batch [35/43], Training Loss: 0.00001117
2024-11-06 14:07:42,836 - INFO - Epoch [290/300], Batch [36/43], Training Loss: 0.00000582
2024-11-06 14:07:42,840 - INFO - Epoch [290/300], Batch [37/43], Training Loss: 0.00001169
2024-11-06 14:07:42,844 - INFO - Epoch [290/300], Batch [38/43], Training Loss: 0.00001519
2024-11-06 14:07:42,847 - INFO - Epoch [290/300], Batch [39/43], Training Loss: 0.00000371
2024-11-06 14:07:42,850 - INFO - Epoch [290/300], Batch [40/43], Training Loss: 0.00000758
2024-11-06 14:07:42,854 - INFO - Epoch [290/300], Batch [41/43], Training Loss: 0.00000425
2024-11-06 14:07:42,857 - INFO - Epoch [290/300], Batch [42/43], Training Loss: 0.00000697
2024-11-06 14:07:42,861 - INFO - Epoch [290/300], Batch [43/43], Training Loss: 0.00000844
2024-11-06 14:07:42,874 - INFO - Epoch [290/300], Average Training Loss: 0.00001192, Validation Loss: 0.00001190
2024-11-06 14:07:42,878 - INFO - Epoch [291/300], Batch [1/43], Training Loss: 0.00000801
2024-11-06 14:07:42,881 - INFO - Epoch [291/300], Batch [2/43], Training Loss: 0.00001509
2024-11-06 14:07:42,884 - INFO - Epoch [291/300], Batch [3/43], Training Loss: 0.00000776
2024-11-06 14:07:42,888 - INFO - Epoch [291/300], Batch [4/43], Training Loss: 0.00002463
2024-11-06 14:07:42,891 - INFO - Epoch [291/300], Batch [5/43], Training Loss: 0.00001016
2024-11-06 14:07:42,894 - INFO - Epoch [291/300], Batch [6/43], Training Loss: 0.00001691
2024-11-06 14:07:42,898 - INFO - Epoch [291/300], Batch [7/43], Training Loss: 0.00000334
2024-11-06 14:07:42,901 - INFO - Epoch [291/300], Batch [8/43], Training Loss: 0.00000465
2024-11-06 14:07:42,904 - INFO - Epoch [291/300], Batch [9/43], Training Loss: 0.00001038
2024-11-06 14:07:42,908 - INFO - Epoch [291/300], Batch [10/43], Training Loss: 0.00000832
2024-11-06 14:07:42,912 - INFO - Epoch [291/300], Batch [11/43], Training Loss: 0.00001845
2024-11-06 14:07:42,917 - INFO - Epoch [291/300], Batch [12/43], Training Loss: 0.00001286
2024-11-06 14:07:42,921 - INFO - Epoch [291/300], Batch [13/43], Training Loss: 0.00000844
2024-11-06 14:07:42,925 - INFO - Epoch [291/300], Batch [14/43], Training Loss: 0.00000671
2024-11-06 14:07:42,929 - INFO - Epoch [291/300], Batch [15/43], Training Loss: 0.00002096
2024-11-06 14:07:42,933 - INFO - Epoch [291/300], Batch [16/43], Training Loss: 0.00002087
2024-11-06 14:07:42,937 - INFO - Epoch [291/300], Batch [17/43], Training Loss: 0.00000451
2024-11-06 14:07:42,943 - INFO - Epoch [291/300], Batch [18/43], Training Loss: 0.00000962
2024-11-06 14:07:42,948 - INFO - Epoch [291/300], Batch [19/43], Training Loss: 0.00001908
2024-11-06 14:07:42,952 - INFO - Epoch [291/300], Batch [20/43], Training Loss: 0.00002564
2024-11-06 14:07:42,956 - INFO - Epoch [291/300], Batch [21/43], Training Loss: 0.00000203
2024-11-06 14:07:42,960 - INFO - Epoch [291/300], Batch [22/43], Training Loss: 0.00001295
2024-11-06 14:07:42,964 - INFO - Epoch [291/300], Batch [23/43], Training Loss: 0.00002301
2024-11-06 14:07:42,969 - INFO - Epoch [291/300], Batch [24/43], Training Loss: 0.00002072
2024-11-06 14:07:42,973 - INFO - Epoch [291/300], Batch [25/43], Training Loss: 0.00000498
2024-11-06 14:07:42,976 - INFO - Epoch [291/300], Batch [26/43], Training Loss: 0.00001790
2024-11-06 14:07:42,980 - INFO - Epoch [291/300], Batch [27/43], Training Loss: 0.00002562
2024-11-06 14:07:42,984 - INFO - Epoch [291/300], Batch [28/43], Training Loss: 0.00001852
2024-11-06 14:07:42,988 - INFO - Epoch [291/300], Batch [29/43], Training Loss: 0.00001531
2024-11-06 14:07:42,992 - INFO - Epoch [291/300], Batch [30/43], Training Loss: 0.00000298
2024-11-06 14:07:42,996 - INFO - Epoch [291/300], Batch [31/43], Training Loss: 0.00001171
2024-11-06 14:07:43,001 - INFO - Epoch [291/300], Batch [32/43], Training Loss: 0.00000993
2024-11-06 14:07:43,005 - INFO - Epoch [291/300], Batch [33/43], Training Loss: 0.00001955
2024-11-06 14:07:43,009 - INFO - Epoch [291/300], Batch [34/43], Training Loss: 0.00000471
2024-11-06 14:07:43,013 - INFO - Epoch [291/300], Batch [35/43], Training Loss: 0.00000583
2024-11-06 14:07:43,017 - INFO - Epoch [291/300], Batch [36/43], Training Loss: 0.00001384
2024-11-06 14:07:43,021 - INFO - Epoch [291/300], Batch [37/43], Training Loss: 0.00000289
2024-11-06 14:07:43,024 - INFO - Epoch [291/300], Batch [38/43], Training Loss: 0.00000945
2024-11-06 14:07:43,028 - INFO - Epoch [291/300], Batch [39/43], Training Loss: 0.00001536
2024-11-06 14:07:43,033 - INFO - Epoch [291/300], Batch [40/43], Training Loss: 0.00000699
2024-11-06 14:07:43,037 - INFO - Epoch [291/300], Batch [41/43], Training Loss: 0.00001096
2024-11-06 14:07:43,042 - INFO - Epoch [291/300], Batch [42/43], Training Loss: 0.00000655
2024-11-06 14:07:43,046 - INFO - Epoch [291/300], Batch [43/43], Training Loss: 0.00001114
2024-11-06 14:07:43,057 - INFO - Epoch [291/300], Average Training Loss: 0.00001231, Validation Loss: 0.00001259
2024-11-06 14:07:43,061 - INFO - Epoch [292/300], Batch [1/43], Training Loss: 0.00001196
2024-11-06 14:07:43,065 - INFO - Epoch [292/300], Batch [2/43], Training Loss: 0.00001271
2024-11-06 14:07:43,070 - INFO - Epoch [292/300], Batch [3/43], Training Loss: 0.00001850
2024-11-06 14:07:43,073 - INFO - Epoch [292/300], Batch [4/43], Training Loss: 0.00001638
2024-11-06 14:07:43,077 - INFO - Epoch [292/300], Batch [5/43], Training Loss: 0.00001156
2024-11-06 14:07:43,081 - INFO - Epoch [292/300], Batch [6/43], Training Loss: 0.00000446
2024-11-06 14:07:43,084 - INFO - Epoch [292/300], Batch [7/43], Training Loss: 0.00000735
2024-11-06 14:07:43,088 - INFO - Epoch [292/300], Batch [8/43], Training Loss: 0.00001911
2024-11-06 14:07:43,094 - INFO - Epoch [292/300], Batch [9/43], Training Loss: 0.00000360
2024-11-06 14:07:43,102 - INFO - Epoch [292/300], Batch [10/43], Training Loss: 0.00000424
2024-11-06 14:07:43,107 - INFO - Epoch [292/300], Batch [11/43], Training Loss: 0.00002089
2024-11-06 14:07:43,112 - INFO - Epoch [292/300], Batch [12/43], Training Loss: 0.00000222
2024-11-06 14:07:43,116 - INFO - Epoch [292/300], Batch [13/43], Training Loss: 0.00000659
2024-11-06 14:07:43,120 - INFO - Epoch [292/300], Batch [14/43], Training Loss: 0.00000272
2024-11-06 14:07:43,124 - INFO - Epoch [292/300], Batch [15/43], Training Loss: 0.00000639
2024-11-06 14:07:43,128 - INFO - Epoch [292/300], Batch [16/43], Training Loss: 0.00001062
2024-11-06 14:07:43,133 - INFO - Epoch [292/300], Batch [17/43], Training Loss: 0.00000572
2024-11-06 14:07:43,137 - INFO - Epoch [292/300], Batch [18/43], Training Loss: 0.00000674
2024-11-06 14:07:43,141 - INFO - Epoch [292/300], Batch [19/43], Training Loss: 0.00001161
2024-11-06 14:07:43,145 - INFO - Epoch [292/300], Batch [20/43], Training Loss: 0.00000526
2024-11-06 14:07:43,151 - INFO - Epoch [292/300], Batch [21/43], Training Loss: 0.00000676
2024-11-06 14:07:43,155 - INFO - Epoch [292/300], Batch [22/43], Training Loss: 0.00000786
2024-11-06 14:07:43,159 - INFO - Epoch [292/300], Batch [23/43], Training Loss: 0.00000589
2024-11-06 14:07:43,163 - INFO - Epoch [292/300], Batch [24/43], Training Loss: 0.00001813
2024-11-06 14:07:43,168 - INFO - Epoch [292/300], Batch [25/43], Training Loss: 0.00000607
2024-11-06 14:07:43,172 - INFO - Epoch [292/300], Batch [26/43], Training Loss: 0.00001875
2024-11-06 14:07:43,176 - INFO - Epoch [292/300], Batch [27/43], Training Loss: 0.00001650
2024-11-06 14:07:43,180 - INFO - Epoch [292/300], Batch [28/43], Training Loss: 0.00000301
2024-11-06 14:07:43,185 - INFO - Epoch [292/300], Batch [29/43], Training Loss: 0.00000559
2024-11-06 14:07:43,189 - INFO - Epoch [292/300], Batch [30/43], Training Loss: 0.00000933
2024-11-06 14:07:43,194 - INFO - Epoch [292/300], Batch [31/43], Training Loss: 0.00000679
2024-11-06 14:07:43,198 - INFO - Epoch [292/300], Batch [32/43], Training Loss: 0.00000613
2024-11-06 14:07:43,201 - INFO - Epoch [292/300], Batch [33/43], Training Loss: 0.00001862
2024-11-06 14:07:43,206 - INFO - Epoch [292/300], Batch [34/43], Training Loss: 0.00001561
2024-11-06 14:07:43,209 - INFO - Epoch [292/300], Batch [35/43], Training Loss: 0.00001671
2024-11-06 14:07:43,212 - INFO - Epoch [292/300], Batch [36/43], Training Loss: 0.00000360
2024-11-06 14:07:43,216 - INFO - Epoch [292/300], Batch [37/43], Training Loss: 0.00000478
2024-11-06 14:07:43,220 - INFO - Epoch [292/300], Batch [38/43], Training Loss: 0.00000899
2024-11-06 14:07:43,225 - INFO - Epoch [292/300], Batch [39/43], Training Loss: 0.00001240
2024-11-06 14:07:43,229 - INFO - Epoch [292/300], Batch [40/43], Training Loss: 0.00000472
2024-11-06 14:07:43,232 - INFO - Epoch [292/300], Batch [41/43], Training Loss: 0.00001548
2024-11-06 14:07:43,236 - INFO - Epoch [292/300], Batch [42/43], Training Loss: 0.00001548
2024-11-06 14:07:43,241 - INFO - Epoch [292/300], Batch [43/43], Training Loss: 0.00001427
2024-11-06 14:07:43,254 - INFO - Epoch [292/300], Average Training Loss: 0.00001000, Validation Loss: 0.00001249
2024-11-06 14:07:43,257 - INFO - Epoch [293/300], Batch [1/43], Training Loss: 0.00000705
2024-11-06 14:07:43,262 - INFO - Epoch [293/300], Batch [2/43], Training Loss: 0.00000644
2024-11-06 14:07:43,266 - INFO - Epoch [293/300], Batch [3/43], Training Loss: 0.00000434
2024-11-06 14:07:43,269 - INFO - Epoch [293/300], Batch [4/43], Training Loss: 0.00001145
2024-11-06 14:07:43,272 - INFO - Epoch [293/300], Batch [5/43], Training Loss: 0.00001337
2024-11-06 14:07:43,276 - INFO - Epoch [293/300], Batch [6/43], Training Loss: 0.00000345
2024-11-06 14:07:43,280 - INFO - Epoch [293/300], Batch [7/43], Training Loss: 0.00001365
2024-11-06 14:07:43,284 - INFO - Epoch [293/300], Batch [8/43], Training Loss: 0.00001890
2024-11-06 14:07:43,288 - INFO - Epoch [293/300], Batch [9/43], Training Loss: 0.00000819
2024-11-06 14:07:43,291 - INFO - Epoch [293/300], Batch [10/43], Training Loss: 0.00000783
2024-11-06 14:07:43,295 - INFO - Epoch [293/300], Batch [11/43], Training Loss: 0.00000382
2024-11-06 14:07:43,299 - INFO - Epoch [293/300], Batch [12/43], Training Loss: 0.00000940
2024-11-06 14:07:43,302 - INFO - Epoch [293/300], Batch [13/43], Training Loss: 0.00001487
2024-11-06 14:07:43,306 - INFO - Epoch [293/300], Batch [14/43], Training Loss: 0.00001319
2024-11-06 14:07:43,310 - INFO - Epoch [293/300], Batch [15/43], Training Loss: 0.00000680
2024-11-06 14:07:43,313 - INFO - Epoch [293/300], Batch [16/43], Training Loss: 0.00001360
2024-11-06 14:07:43,316 - INFO - Epoch [293/300], Batch [17/43], Training Loss: 0.00001093
2024-11-06 14:07:43,319 - INFO - Epoch [293/300], Batch [18/43], Training Loss: 0.00000331
2024-11-06 14:07:43,321 - INFO - Epoch [293/300], Batch [19/43], Training Loss: 0.00000532
2024-11-06 14:07:43,324 - INFO - Epoch [293/300], Batch [20/43], Training Loss: 0.00000786
2024-11-06 14:07:43,327 - INFO - Epoch [293/300], Batch [21/43], Training Loss: 0.00002571
2024-11-06 14:07:43,331 - INFO - Epoch [293/300], Batch [22/43], Training Loss: 0.00000208
2024-11-06 14:07:43,334 - INFO - Epoch [293/300], Batch [23/43], Training Loss: 0.00000877
2024-11-06 14:07:43,338 - INFO - Epoch [293/300], Batch [24/43], Training Loss: 0.00001072
2024-11-06 14:07:43,342 - INFO - Epoch [293/300], Batch [25/43], Training Loss: 0.00001530
2024-11-06 14:07:43,346 - INFO - Epoch [293/300], Batch [26/43], Training Loss: 0.00001745
2024-11-06 14:07:43,349 - INFO - Epoch [293/300], Batch [27/43], Training Loss: 0.00000309
2024-11-06 14:07:43,353 - INFO - Epoch [293/300], Batch [28/43], Training Loss: 0.00001159
2024-11-06 14:07:43,356 - INFO - Epoch [293/300], Batch [29/43], Training Loss: 0.00000353
2024-11-06 14:07:43,360 - INFO - Epoch [293/300], Batch [30/43], Training Loss: 0.00000996
2024-11-06 14:07:43,363 - INFO - Epoch [293/300], Batch [31/43], Training Loss: 0.00000583
2024-11-06 14:07:43,366 - INFO - Epoch [293/300], Batch [32/43], Training Loss: 0.00001481
2024-11-06 14:07:43,369 - INFO - Epoch [293/300], Batch [33/43], Training Loss: 0.00000690
2024-11-06 14:07:43,371 - INFO - Epoch [293/300], Batch [34/43], Training Loss: 0.00001198
2024-11-06 14:07:43,374 - INFO - Epoch [293/300], Batch [35/43], Training Loss: 0.00000517
2024-11-06 14:07:43,379 - INFO - Epoch [293/300], Batch [36/43], Training Loss: 0.00000574
2024-11-06 14:07:43,383 - INFO - Epoch [293/300], Batch [37/43], Training Loss: 0.00001001
2024-11-06 14:07:43,387 - INFO - Epoch [293/300], Batch [38/43], Training Loss: 0.00001591
2024-11-06 14:07:43,391 - INFO - Epoch [293/300], Batch [39/43], Training Loss: 0.00001236
2024-11-06 14:07:43,395 - INFO - Epoch [293/300], Batch [40/43], Training Loss: 0.00000277
2024-11-06 14:07:43,399 - INFO - Epoch [293/300], Batch [41/43], Training Loss: 0.00000232
2024-11-06 14:07:43,403 - INFO - Epoch [293/300], Batch [42/43], Training Loss: 0.00000841
2024-11-06 14:07:43,407 - INFO - Epoch [293/300], Batch [43/43], Training Loss: 0.00001315
2024-11-06 14:07:43,419 - INFO - Epoch [293/300], Average Training Loss: 0.00000947, Validation Loss: 0.00001290
2024-11-06 14:07:43,422 - INFO - Epoch [294/300], Batch [1/43], Training Loss: 0.00001226
2024-11-06 14:07:43,426 - INFO - Epoch [294/300], Batch [2/43], Training Loss: 0.00000497
2024-11-06 14:07:43,430 - INFO - Epoch [294/300], Batch [3/43], Training Loss: 0.00000997
2024-11-06 14:07:43,433 - INFO - Epoch [294/300], Batch [4/43], Training Loss: 0.00002136
2024-11-06 14:07:43,437 - INFO - Epoch [294/300], Batch [5/43], Training Loss: 0.00001180
2024-11-06 14:07:43,441 - INFO - Epoch [294/300], Batch [6/43], Training Loss: 0.00000749
2024-11-06 14:07:43,445 - INFO - Epoch [294/300], Batch [7/43], Training Loss: 0.00001759
2024-11-06 14:07:43,449 - INFO - Epoch [294/300], Batch [8/43], Training Loss: 0.00000736
2024-11-06 14:07:43,453 - INFO - Epoch [294/300], Batch [9/43], Training Loss: 0.00000802
2024-11-06 14:07:43,456 - INFO - Epoch [294/300], Batch [10/43], Training Loss: 0.00002460
2024-11-06 14:07:43,459 - INFO - Epoch [294/300], Batch [11/43], Training Loss: 0.00001748
2024-11-06 14:07:43,463 - INFO - Epoch [294/300], Batch [12/43], Training Loss: 0.00001349
2024-11-06 14:07:43,467 - INFO - Epoch [294/300], Batch [13/43], Training Loss: 0.00002038
2024-11-06 14:07:43,471 - INFO - Epoch [294/300], Batch [14/43], Training Loss: 0.00000819
2024-11-06 14:07:43,476 - INFO - Epoch [294/300], Batch [15/43], Training Loss: 0.00001169
2024-11-06 14:07:43,480 - INFO - Epoch [294/300], Batch [16/43], Training Loss: 0.00000498
2024-11-06 14:07:43,484 - INFO - Epoch [294/300], Batch [17/43], Training Loss: 0.00000405
2024-11-06 14:07:43,488 - INFO - Epoch [294/300], Batch [18/43], Training Loss: 0.00001171
2024-11-06 14:07:43,492 - INFO - Epoch [294/300], Batch [19/43], Training Loss: 0.00000504
2024-11-06 14:07:43,495 - INFO - Epoch [294/300], Batch [20/43], Training Loss: 0.00001313
2024-11-06 14:07:43,500 - INFO - Epoch [294/300], Batch [21/43], Training Loss: 0.00000967
2024-11-06 14:07:43,504 - INFO - Epoch [294/300], Batch [22/43], Training Loss: 0.00000326
2024-11-06 14:07:43,508 - INFO - Epoch [294/300], Batch [23/43], Training Loss: 0.00002694
2024-11-06 14:07:43,512 - INFO - Epoch [294/300], Batch [24/43], Training Loss: 0.00000473
2024-11-06 14:07:43,516 - INFO - Epoch [294/300], Batch [25/43], Training Loss: 0.00001230
2024-11-06 14:07:43,520 - INFO - Epoch [294/300], Batch [26/43], Training Loss: 0.00001534
2024-11-06 14:07:43,524 - INFO - Epoch [294/300], Batch [27/43], Training Loss: 0.00002152
2024-11-06 14:07:43,528 - INFO - Epoch [294/300], Batch [28/43], Training Loss: 0.00000835
2024-11-06 14:07:43,532 - INFO - Epoch [294/300], Batch [29/43], Training Loss: 0.00000407
2024-11-06 14:07:43,538 - INFO - Epoch [294/300], Batch [30/43], Training Loss: 0.00002649
2024-11-06 14:07:43,543 - INFO - Epoch [294/300], Batch [31/43], Training Loss: 0.00001046
2024-11-06 14:07:43,547 - INFO - Epoch [294/300], Batch [32/43], Training Loss: 0.00001054
2024-11-06 14:07:43,551 - INFO - Epoch [294/300], Batch [33/43], Training Loss: 0.00000380
2024-11-06 14:07:43,556 - INFO - Epoch [294/300], Batch [34/43], Training Loss: 0.00000660
2024-11-06 14:07:43,561 - INFO - Epoch [294/300], Batch [35/43], Training Loss: 0.00003096
2024-11-06 14:07:43,565 - INFO - Epoch [294/300], Batch [36/43], Training Loss: 0.00000891
2024-11-06 14:07:43,570 - INFO - Epoch [294/300], Batch [37/43], Training Loss: 0.00000708
2024-11-06 14:07:43,574 - INFO - Epoch [294/300], Batch [38/43], Training Loss: 0.00001380
2024-11-06 14:07:43,578 - INFO - Epoch [294/300], Batch [39/43], Training Loss: 0.00001789
2024-11-06 14:07:43,582 - INFO - Epoch [294/300], Batch [40/43], Training Loss: 0.00000194
2024-11-06 14:07:43,586 - INFO - Epoch [294/300], Batch [41/43], Training Loss: 0.00001174
2024-11-06 14:07:43,591 - INFO - Epoch [294/300], Batch [42/43], Training Loss: 0.00001446
2024-11-06 14:07:43,596 - INFO - Epoch [294/300], Batch [43/43], Training Loss: 0.00001082
2024-11-06 14:07:43,609 - INFO - Epoch [294/300], Average Training Loss: 0.00001203, Validation Loss: 0.00001292
2024-11-06 14:07:43,613 - INFO - Epoch [295/300], Batch [1/43], Training Loss: 0.00001093
2024-11-06 14:07:43,617 - INFO - Epoch [295/300], Batch [2/43], Training Loss: 0.00001682
2024-11-06 14:07:43,622 - INFO - Epoch [295/300], Batch [3/43], Training Loss: 0.00000876
2024-11-06 14:07:43,627 - INFO - Epoch [295/300], Batch [4/43], Training Loss: 0.00001723
2024-11-06 14:07:43,632 - INFO - Epoch [295/300], Batch [5/43], Training Loss: 0.00000637
2024-11-06 14:07:43,638 - INFO - Epoch [295/300], Batch [6/43], Training Loss: 0.00000336
2024-11-06 14:07:43,642 - INFO - Epoch [295/300], Batch [7/43], Training Loss: 0.00000876
2024-11-06 14:07:43,646 - INFO - Epoch [295/300], Batch [8/43], Training Loss: 0.00000889
2024-11-06 14:07:43,650 - INFO - Epoch [295/300], Batch [9/43], Training Loss: 0.00001931
2024-11-06 14:07:43,654 - INFO - Epoch [295/300], Batch [10/43], Training Loss: 0.00001016
2024-11-06 14:07:43,657 - INFO - Epoch [295/300], Batch [11/43], Training Loss: 0.00001147
2024-11-06 14:07:43,662 - INFO - Epoch [295/300], Batch [12/43], Training Loss: 0.00000891
2024-11-06 14:07:43,666 - INFO - Epoch [295/300], Batch [13/43], Training Loss: 0.00000352
2024-11-06 14:07:43,670 - INFO - Epoch [295/300], Batch [14/43], Training Loss: 0.00000514
2024-11-06 14:07:43,676 - INFO - Epoch [295/300], Batch [15/43], Training Loss: 0.00000421
2024-11-06 14:07:43,680 - INFO - Epoch [295/300], Batch [16/43], Training Loss: 0.00001293
2024-11-06 14:07:43,684 - INFO - Epoch [295/300], Batch [17/43], Training Loss: 0.00000659
2024-11-06 14:07:43,688 - INFO - Epoch [295/300], Batch [18/43], Training Loss: 0.00000764
2024-11-06 14:07:43,692 - INFO - Epoch [295/300], Batch [19/43], Training Loss: 0.00000115
2024-11-06 14:07:43,695 - INFO - Epoch [295/300], Batch [20/43], Training Loss: 0.00000777
2024-11-06 14:07:43,700 - INFO - Epoch [295/300], Batch [21/43], Training Loss: 0.00000860
2024-11-06 14:07:43,704 - INFO - Epoch [295/300], Batch [22/43], Training Loss: 0.00002004
2024-11-06 14:07:43,707 - INFO - Epoch [295/300], Batch [23/43], Training Loss: 0.00000630
2024-11-06 14:07:43,711 - INFO - Epoch [295/300], Batch [24/43], Training Loss: 0.00000863
2024-11-06 14:07:43,715 - INFO - Epoch [295/300], Batch [25/43], Training Loss: 0.00000390
2024-11-06 14:07:43,719 - INFO - Epoch [295/300], Batch [26/43], Training Loss: 0.00001979
2024-11-06 14:07:43,724 - INFO - Epoch [295/300], Batch [27/43], Training Loss: 0.00001214
2024-11-06 14:07:43,728 - INFO - Epoch [295/300], Batch [28/43], Training Loss: 0.00002556
2024-11-06 14:07:43,734 - INFO - Epoch [295/300], Batch [29/43], Training Loss: 0.00001404
2024-11-06 14:07:43,740 - INFO - Epoch [295/300], Batch [30/43], Training Loss: 0.00000858
2024-11-06 14:07:43,746 - INFO - Epoch [295/300], Batch [31/43], Training Loss: 0.00001765
2024-11-06 14:07:43,750 - INFO - Epoch [295/300], Batch [32/43], Training Loss: 0.00001744
2024-11-06 14:07:43,756 - INFO - Epoch [295/300], Batch [33/43], Training Loss: 0.00001170
2024-11-06 14:07:43,761 - INFO - Epoch [295/300], Batch [34/43], Training Loss: 0.00000562
2024-11-06 14:07:43,766 - INFO - Epoch [295/300], Batch [35/43], Training Loss: 0.00001453
2024-11-06 14:07:43,772 - INFO - Epoch [295/300], Batch [36/43], Training Loss: 0.00000312
2024-11-06 14:07:43,776 - INFO - Epoch [295/300], Batch [37/43], Training Loss: 0.00000821
2024-11-06 14:07:43,781 - INFO - Epoch [295/300], Batch [38/43], Training Loss: 0.00000761
2024-11-06 14:07:43,785 - INFO - Epoch [295/300], Batch [39/43], Training Loss: 0.00000859
2024-11-06 14:07:43,789 - INFO - Epoch [295/300], Batch [40/43], Training Loss: 0.00000565
2024-11-06 14:07:43,793 - INFO - Epoch [295/300], Batch [41/43], Training Loss: 0.00001148
2024-11-06 14:07:43,797 - INFO - Epoch [295/300], Batch [42/43], Training Loss: 0.00001506
2024-11-06 14:07:43,802 - INFO - Epoch [295/300], Batch [43/43], Training Loss: 0.00000645
2024-11-06 14:07:43,816 - INFO - Epoch [295/300], Average Training Loss: 0.00001025, Validation Loss: 0.00001167
2024-11-06 14:07:43,821 - INFO - Epoch [296/300], Batch [1/43], Training Loss: 0.00000753
2024-11-06 14:07:43,825 - INFO - Epoch [296/300], Batch [2/43], Training Loss: 0.00000600
2024-11-06 14:07:43,829 - INFO - Epoch [296/300], Batch [3/43], Training Loss: 0.00000634
2024-11-06 14:07:43,834 - INFO - Epoch [296/300], Batch [4/43], Training Loss: 0.00000660
2024-11-06 14:07:43,838 - INFO - Epoch [296/300], Batch [5/43], Training Loss: 0.00000344
2024-11-06 14:07:43,842 - INFO - Epoch [296/300], Batch [6/43], Training Loss: 0.00000665
2024-11-06 14:07:43,846 - INFO - Epoch [296/300], Batch [7/43], Training Loss: 0.00000570
2024-11-06 14:07:43,850 - INFO - Epoch [296/300], Batch [8/43], Training Loss: 0.00000972
2024-11-06 14:07:43,854 - INFO - Epoch [296/300], Batch [9/43], Training Loss: 0.00000490
2024-11-06 14:07:43,859 - INFO - Epoch [296/300], Batch [10/43], Training Loss: 0.00000663
2024-11-06 14:07:43,864 - INFO - Epoch [296/300], Batch [11/43], Training Loss: 0.00000976
2024-11-06 14:07:43,868 - INFO - Epoch [296/300], Batch [12/43], Training Loss: 0.00001124
2024-11-06 14:07:43,872 - INFO - Epoch [296/300], Batch [13/43], Training Loss: 0.00001203
2024-11-06 14:07:43,876 - INFO - Epoch [296/300], Batch [14/43], Training Loss: 0.00001024
2024-11-06 14:07:43,880 - INFO - Epoch [296/300], Batch [15/43], Training Loss: 0.00000593
2024-11-06 14:07:43,885 - INFO - Epoch [296/300], Batch [16/43], Training Loss: 0.00002391
2024-11-06 14:07:43,890 - INFO - Epoch [296/300], Batch [17/43], Training Loss: 0.00000216
2024-11-06 14:07:43,895 - INFO - Epoch [296/300], Batch [18/43], Training Loss: 0.00001117
2024-11-06 14:07:43,899 - INFO - Epoch [296/300], Batch [19/43], Training Loss: 0.00000616
2024-11-06 14:07:43,903 - INFO - Epoch [296/300], Batch [20/43], Training Loss: 0.00001040
2024-11-06 14:07:43,908 - INFO - Epoch [296/300], Batch [21/43], Training Loss: 0.00000709
2024-11-06 14:07:43,913 - INFO - Epoch [296/300], Batch [22/43], Training Loss: 0.00000639
2024-11-06 14:07:43,919 - INFO - Epoch [296/300], Batch [23/43], Training Loss: 0.00000756
2024-11-06 14:07:43,924 - INFO - Epoch [296/300], Batch [24/43], Training Loss: 0.00001374
2024-11-06 14:07:43,929 - INFO - Epoch [296/300], Batch [25/43], Training Loss: 0.00001621
2024-11-06 14:07:43,934 - INFO - Epoch [296/300], Batch [26/43], Training Loss: 0.00001437
2024-11-06 14:07:43,938 - INFO - Epoch [296/300], Batch [27/43], Training Loss: 0.00001462
2024-11-06 14:07:43,943 - INFO - Epoch [296/300], Batch [28/43], Training Loss: 0.00001384
2024-11-06 14:07:43,948 - INFO - Epoch [296/300], Batch [29/43], Training Loss: 0.00001369
2024-11-06 14:07:43,952 - INFO - Epoch [296/300], Batch [30/43], Training Loss: 0.00000972
2024-11-06 14:07:43,956 - INFO - Epoch [296/300], Batch [31/43], Training Loss: 0.00001653
2024-11-06 14:07:43,959 - INFO - Epoch [296/300], Batch [32/43], Training Loss: 0.00000550
2024-11-06 14:07:43,963 - INFO - Epoch [296/300], Batch [33/43], Training Loss: 0.00000399
2024-11-06 14:07:43,967 - INFO - Epoch [296/300], Batch [34/43], Training Loss: 0.00000981
2024-11-06 14:07:44,019 - INFO - Epoch [296/300], Batch [35/43], Training Loss: 0.00001153
2024-11-06 14:07:44,036 - INFO - Epoch [296/300], Batch [36/43], Training Loss: 0.00001487
2024-11-06 14:07:44,040 - INFO - Epoch [296/300], Batch [37/43], Training Loss: 0.00002746
2024-11-06 14:07:44,046 - INFO - Epoch [296/300], Batch [38/43], Training Loss: 0.00001111
2024-11-06 14:07:44,065 - INFO - Epoch [296/300], Batch [39/43], Training Loss: 0.00002435
2024-11-06 14:07:44,072 - INFO - Epoch [296/300], Batch [40/43], Training Loss: 0.00001183
2024-11-06 14:07:44,077 - INFO - Epoch [296/300], Batch [41/43], Training Loss: 0.00000263
2024-11-06 14:07:44,081 - INFO - Epoch [296/300], Batch [42/43], Training Loss: 0.00000507
2024-11-06 14:07:44,086 - INFO - Epoch [296/300], Batch [43/43], Training Loss: 0.00000494
2024-11-06 14:07:44,101 - INFO - Epoch [296/300], Average Training Loss: 0.00001008, Validation Loss: 0.00001685
2024-11-06 14:07:44,106 - INFO - Epoch [297/300], Batch [1/43], Training Loss: 0.00001076
2024-11-06 14:07:44,112 - INFO - Epoch [297/300], Batch [2/43], Training Loss: 0.00000603
2024-11-06 14:07:44,117 - INFO - Epoch [297/300], Batch [3/43], Training Loss: 0.00000534
2024-11-06 14:07:44,122 - INFO - Epoch [297/300], Batch [4/43], Training Loss: 0.00001337
2024-11-06 14:07:44,127 - INFO - Epoch [297/300], Batch [5/43], Training Loss: 0.00003403
2024-11-06 14:07:44,132 - INFO - Epoch [297/300], Batch [6/43], Training Loss: 0.00000666
2024-11-06 14:07:44,138 - INFO - Epoch [297/300], Batch [7/43], Training Loss: 0.00000693
2024-11-06 14:07:44,142 - INFO - Epoch [297/300], Batch [8/43], Training Loss: 0.00001371
2024-11-06 14:07:44,147 - INFO - Epoch [297/300], Batch [9/43], Training Loss: 0.00000756
2024-11-06 14:07:44,152 - INFO - Epoch [297/300], Batch [10/43], Training Loss: 0.00000917
2024-11-06 14:07:44,157 - INFO - Epoch [297/300], Batch [11/43], Training Loss: 0.00001674
2024-11-06 14:07:44,161 - INFO - Epoch [297/300], Batch [12/43], Training Loss: 0.00001369
2024-11-06 14:07:44,165 - INFO - Epoch [297/300], Batch [13/43], Training Loss: 0.00000383
2024-11-06 14:07:44,171 - INFO - Epoch [297/300], Batch [14/43], Training Loss: 0.00002454
2024-11-06 14:07:44,175 - INFO - Epoch [297/300], Batch [15/43], Training Loss: 0.00000564
2024-11-06 14:07:44,179 - INFO - Epoch [297/300], Batch [16/43], Training Loss: 0.00001490
2024-11-06 14:07:44,184 - INFO - Epoch [297/300], Batch [17/43], Training Loss: 0.00001018
2024-11-06 14:07:44,188 - INFO - Epoch [297/300], Batch [18/43], Training Loss: 0.00001527
2024-11-06 14:07:44,193 - INFO - Epoch [297/300], Batch [19/43], Training Loss: 0.00000560
2024-11-06 14:07:44,197 - INFO - Epoch [297/300], Batch [20/43], Training Loss: 0.00000891
2024-11-06 14:07:44,201 - INFO - Epoch [297/300], Batch [21/43], Training Loss: 0.00000816
2024-11-06 14:07:44,205 - INFO - Epoch [297/300], Batch [22/43], Training Loss: 0.00003063
2024-11-06 14:07:44,208 - INFO - Epoch [297/300], Batch [23/43], Training Loss: 0.00001266
2024-11-06 14:07:44,212 - INFO - Epoch [297/300], Batch [24/43], Training Loss: 0.00000989
2024-11-06 14:07:44,215 - INFO - Epoch [297/300], Batch [25/43], Training Loss: 0.00000395
2024-11-06 14:07:44,220 - INFO - Epoch [297/300], Batch [26/43], Training Loss: 0.00000745
2024-11-06 14:07:44,224 - INFO - Epoch [297/300], Batch [27/43], Training Loss: 0.00001200
2024-11-06 14:07:44,229 - INFO - Epoch [297/300], Batch [28/43], Training Loss: 0.00000727
2024-11-06 14:07:44,236 - INFO - Epoch [297/300], Batch [29/43], Training Loss: 0.00000836
2024-11-06 14:07:44,240 - INFO - Epoch [297/300], Batch [30/43], Training Loss: 0.00000791
2024-11-06 14:07:44,245 - INFO - Epoch [297/300], Batch [31/43], Training Loss: 0.00000452
2024-11-06 14:07:44,250 - INFO - Epoch [297/300], Batch [32/43], Training Loss: 0.00002098
2024-11-06 14:07:44,255 - INFO - Epoch [297/300], Batch [33/43], Training Loss: 0.00000335
2024-11-06 14:07:44,260 - INFO - Epoch [297/300], Batch [34/43], Training Loss: 0.00002505
2024-11-06 14:07:44,264 - INFO - Epoch [297/300], Batch [35/43], Training Loss: 0.00001016
2024-11-06 14:07:44,270 - INFO - Epoch [297/300], Batch [36/43], Training Loss: 0.00000909
2024-11-06 14:07:44,275 - INFO - Epoch [297/300], Batch [37/43], Training Loss: 0.00000187
2024-11-06 14:07:44,280 - INFO - Epoch [297/300], Batch [38/43], Training Loss: 0.00001211
2024-11-06 14:07:44,286 - INFO - Epoch [297/300], Batch [39/43], Training Loss: 0.00001021
2024-11-06 14:07:44,292 - INFO - Epoch [297/300], Batch [40/43], Training Loss: 0.00002480
2024-11-06 14:07:44,298 - INFO - Epoch [297/300], Batch [41/43], Training Loss: 0.00000655
2024-11-06 14:07:44,302 - INFO - Epoch [297/300], Batch [42/43], Training Loss: 0.00001158
2024-11-06 14:07:44,307 - INFO - Epoch [297/300], Batch [43/43], Training Loss: 0.00000989
2024-11-06 14:07:44,320 - INFO - Epoch [297/300], Average Training Loss: 0.00001143, Validation Loss: 0.00001801
2024-11-06 14:07:44,324 - INFO - Epoch [298/300], Batch [1/43], Training Loss: 0.00002634
2024-11-06 14:07:44,329 - INFO - Epoch [298/300], Batch [2/43], Training Loss: 0.00000409
2024-11-06 14:07:44,334 - INFO - Epoch [298/300], Batch [3/43], Training Loss: 0.00000443
2024-11-06 14:07:44,339 - INFO - Epoch [298/300], Batch [4/43], Training Loss: 0.00002246
2024-11-06 14:07:44,342 - INFO - Epoch [298/300], Batch [5/43], Training Loss: 0.00001250
2024-11-06 14:07:44,346 - INFO - Epoch [298/300], Batch [6/43], Training Loss: 0.00001296
2024-11-06 14:07:44,351 - INFO - Epoch [298/300], Batch [7/43], Training Loss: 0.00000849
2024-11-06 14:07:44,355 - INFO - Epoch [298/300], Batch [8/43], Training Loss: 0.00002157
2024-11-06 14:07:44,359 - INFO - Epoch [298/300], Batch [9/43], Training Loss: 0.00002767
2024-11-06 14:07:44,362 - INFO - Epoch [298/300], Batch [10/43], Training Loss: 0.00000778
2024-11-06 14:07:44,367 - INFO - Epoch [298/300], Batch [11/43], Training Loss: 0.00000714
2024-11-06 14:07:44,371 - INFO - Epoch [298/300], Batch [12/43], Training Loss: 0.00001684
2024-11-06 14:07:44,374 - INFO - Epoch [298/300], Batch [13/43], Training Loss: 0.00002225
2024-11-06 14:07:44,379 - INFO - Epoch [298/300], Batch [14/43], Training Loss: 0.00000595
2024-11-06 14:07:44,383 - INFO - Epoch [298/300], Batch [15/43], Training Loss: 0.00000417
2024-11-06 14:07:44,387 - INFO - Epoch [298/300], Batch [16/43], Training Loss: 0.00002886
2024-11-06 14:07:44,391 - INFO - Epoch [298/300], Batch [17/43], Training Loss: 0.00002376
2024-11-06 14:07:44,395 - INFO - Epoch [298/300], Batch [18/43], Training Loss: 0.00002438
2024-11-06 14:07:44,399 - INFO - Epoch [298/300], Batch [19/43], Training Loss: 0.00001061
2024-11-06 14:07:44,403 - INFO - Epoch [298/300], Batch [20/43], Training Loss: 0.00002662
2024-11-06 14:07:44,408 - INFO - Epoch [298/300], Batch [21/43], Training Loss: 0.00000809
2024-11-06 14:07:44,412 - INFO - Epoch [298/300], Batch [22/43], Training Loss: 0.00000356
2024-11-06 14:07:44,416 - INFO - Epoch [298/300], Batch [23/43], Training Loss: 0.00001258
2024-11-06 14:07:44,420 - INFO - Epoch [298/300], Batch [24/43], Training Loss: 0.00000809
2024-11-06 14:07:44,425 - INFO - Epoch [298/300], Batch [25/43], Training Loss: 0.00001649
2024-11-06 14:07:44,429 - INFO - Epoch [298/300], Batch [26/43], Training Loss: 0.00001043
2024-11-06 14:07:44,433 - INFO - Epoch [298/300], Batch [27/43], Training Loss: 0.00001968
2024-11-06 14:07:44,438 - INFO - Epoch [298/300], Batch [28/43], Training Loss: 0.00000937
2024-11-06 14:07:44,443 - INFO - Epoch [298/300], Batch [29/43], Training Loss: 0.00001672
2024-11-06 14:07:44,448 - INFO - Epoch [298/300], Batch [30/43], Training Loss: 0.00001513
2024-11-06 14:07:44,452 - INFO - Epoch [298/300], Batch [31/43], Training Loss: 0.00000660
2024-11-06 14:07:44,456 - INFO - Epoch [298/300], Batch [32/43], Training Loss: 0.00000564
2024-11-06 14:07:44,460 - INFO - Epoch [298/300], Batch [33/43], Training Loss: 0.00000433
2024-11-06 14:07:44,464 - INFO - Epoch [298/300], Batch [34/43], Training Loss: 0.00000540
2024-11-06 14:07:44,468 - INFO - Epoch [298/300], Batch [35/43], Training Loss: 0.00000921
2024-11-06 14:07:44,473 - INFO - Epoch [298/300], Batch [36/43], Training Loss: 0.00000538
2024-11-06 14:07:44,477 - INFO - Epoch [298/300], Batch [37/43], Training Loss: 0.00000561
2024-11-06 14:07:44,481 - INFO - Epoch [298/300], Batch [38/43], Training Loss: 0.00001232
2024-11-06 14:07:44,485 - INFO - Epoch [298/300], Batch [39/43], Training Loss: 0.00000427
2024-11-06 14:07:44,488 - INFO - Epoch [298/300], Batch [40/43], Training Loss: 0.00000480
2024-11-06 14:07:44,492 - INFO - Epoch [298/300], Batch [41/43], Training Loss: 0.00000726
2024-11-06 14:07:44,495 - INFO - Epoch [298/300], Batch [42/43], Training Loss: 0.00001275
2024-11-06 14:07:44,499 - INFO - Epoch [298/300], Batch [43/43], Training Loss: 0.00001540
2024-11-06 14:07:44,511 - INFO - Epoch [298/300], Average Training Loss: 0.00001251, Validation Loss: 0.00001196
2024-11-06 14:07:44,516 - INFO - Epoch [299/300], Batch [1/43], Training Loss: 0.00000746
2024-11-06 14:07:44,520 - INFO - Epoch [299/300], Batch [2/43], Training Loss: 0.00000540
2024-11-06 14:07:44,523 - INFO - Epoch [299/300], Batch [3/43], Training Loss: 0.00000957
2024-11-06 14:07:44,526 - INFO - Epoch [299/300], Batch [4/43], Training Loss: 0.00001107
2024-11-06 14:07:44,530 - INFO - Epoch [299/300], Batch [5/43], Training Loss: 0.00000567
2024-11-06 14:07:44,533 - INFO - Epoch [299/300], Batch [6/43], Training Loss: 0.00000691
2024-11-06 14:07:44,536 - INFO - Epoch [299/300], Batch [7/43], Training Loss: 0.00001260
2024-11-06 14:07:44,540 - INFO - Epoch [299/300], Batch [8/43], Training Loss: 0.00001913
2024-11-06 14:07:44,543 - INFO - Epoch [299/300], Batch [9/43], Training Loss: 0.00000321
2024-11-06 14:07:44,547 - INFO - Epoch [299/300], Batch [10/43], Training Loss: 0.00001944
2024-11-06 14:07:44,550 - INFO - Epoch [299/300], Batch [11/43], Training Loss: 0.00000766
2024-11-06 14:07:44,554 - INFO - Epoch [299/300], Batch [12/43], Training Loss: 0.00000490
2024-11-06 14:07:44,558 - INFO - Epoch [299/300], Batch [13/43], Training Loss: 0.00000759
2024-11-06 14:07:44,561 - INFO - Epoch [299/300], Batch [14/43], Training Loss: 0.00001216
2024-11-06 14:07:44,565 - INFO - Epoch [299/300], Batch [15/43], Training Loss: 0.00000478
2024-11-06 14:07:44,569 - INFO - Epoch [299/300], Batch [16/43], Training Loss: 0.00000685
2024-11-06 14:07:44,573 - INFO - Epoch [299/300], Batch [17/43], Training Loss: 0.00001483
2024-11-06 14:07:44,577 - INFO - Epoch [299/300], Batch [18/43], Training Loss: 0.00000649
2024-11-06 14:07:44,581 - INFO - Epoch [299/300], Batch [19/43], Training Loss: 0.00004020
2024-11-06 14:07:44,585 - INFO - Epoch [299/300], Batch [20/43], Training Loss: 0.00000572
2024-11-06 14:07:44,589 - INFO - Epoch [299/300], Batch [21/43], Training Loss: 0.00002979
2024-11-06 14:07:44,593 - INFO - Epoch [299/300], Batch [22/43], Training Loss: 0.00001018
2024-11-06 14:07:44,598 - INFO - Epoch [299/300], Batch [23/43], Training Loss: 0.00000893
2024-11-06 14:07:44,602 - INFO - Epoch [299/300], Batch [24/43], Training Loss: 0.00000931
2024-11-06 14:07:44,606 - INFO - Epoch [299/300], Batch [25/43], Training Loss: 0.00001188
2024-11-06 14:07:44,611 - INFO - Epoch [299/300], Batch [26/43], Training Loss: 0.00000510
2024-11-06 14:07:44,615 - INFO - Epoch [299/300], Batch [27/43], Training Loss: 0.00001076
2024-11-06 14:07:44,618 - INFO - Epoch [299/300], Batch [28/43], Training Loss: 0.00001626
2024-11-06 14:07:44,622 - INFO - Epoch [299/300], Batch [29/43], Training Loss: 0.00001292
2024-11-06 14:07:44,626 - INFO - Epoch [299/300], Batch [30/43], Training Loss: 0.00001345
2024-11-06 14:07:44,629 - INFO - Epoch [299/300], Batch [31/43], Training Loss: 0.00001076
2024-11-06 14:07:44,633 - INFO - Epoch [299/300], Batch [32/43], Training Loss: 0.00001038
2024-11-06 14:07:44,637 - INFO - Epoch [299/300], Batch [33/43], Training Loss: 0.00000985
2024-11-06 14:07:44,641 - INFO - Epoch [299/300], Batch [34/43], Training Loss: 0.00000590
2024-11-06 14:07:44,644 - INFO - Epoch [299/300], Batch [35/43], Training Loss: 0.00001512
2024-11-06 14:07:44,647 - INFO - Epoch [299/300], Batch [36/43], Training Loss: 0.00001790
2024-11-06 14:07:44,650 - INFO - Epoch [299/300], Batch [37/43], Training Loss: 0.00001858
2024-11-06 14:07:44,653 - INFO - Epoch [299/300], Batch [38/43], Training Loss: 0.00001134
2024-11-06 14:07:44,656 - INFO - Epoch [299/300], Batch [39/43], Training Loss: 0.00001681
2024-11-06 14:07:44,660 - INFO - Epoch [299/300], Batch [40/43], Training Loss: 0.00000494
2024-11-06 14:07:44,664 - INFO - Epoch [299/300], Batch [41/43], Training Loss: 0.00000788
2024-11-06 14:07:44,668 - INFO - Epoch [299/300], Batch [42/43], Training Loss: 0.00000777
2024-11-06 14:07:44,672 - INFO - Epoch [299/300], Batch [43/43], Training Loss: 0.00000370
2024-11-06 14:07:44,682 - INFO - Epoch [299/300], Average Training Loss: 0.00001119, Validation Loss: 0.00001342
2024-11-06 14:07:44,686 - INFO - Epoch [300/300], Batch [1/43], Training Loss: 0.00000767
2024-11-06 14:07:44,689 - INFO - Epoch [300/300], Batch [2/43], Training Loss: 0.00002128
2024-11-06 14:07:44,692 - INFO - Epoch [300/300], Batch [3/43], Training Loss: 0.00000815
2024-11-06 14:07:44,695 - INFO - Epoch [300/300], Batch [4/43], Training Loss: 0.00000395
2024-11-06 14:07:44,698 - INFO - Epoch [300/300], Batch [5/43], Training Loss: 0.00000382
2024-11-06 14:07:44,702 - INFO - Epoch [300/300], Batch [6/43], Training Loss: 0.00000972
2024-11-06 14:07:44,705 - INFO - Epoch [300/300], Batch [7/43], Training Loss: 0.00001036
2024-11-06 14:07:44,708 - INFO - Epoch [300/300], Batch [8/43], Training Loss: 0.00001446
2024-11-06 14:07:44,711 - INFO - Epoch [300/300], Batch [9/43], Training Loss: 0.00000802
2024-11-06 14:07:44,714 - INFO - Epoch [300/300], Batch [10/43], Training Loss: 0.00000511
2024-11-06 14:07:44,717 - INFO - Epoch [300/300], Batch [11/43], Training Loss: 0.00000412
2024-11-06 14:07:44,721 - INFO - Epoch [300/300], Batch [12/43], Training Loss: 0.00001018
2024-11-06 14:07:44,725 - INFO - Epoch [300/300], Batch [13/43], Training Loss: 0.00000899
2024-11-06 14:07:44,729 - INFO - Epoch [300/300], Batch [14/43], Training Loss: 0.00000556
2024-11-06 14:07:44,733 - INFO - Epoch [300/300], Batch [15/43], Training Loss: 0.00000894
2024-11-06 14:07:44,738 - INFO - Epoch [300/300], Batch [16/43], Training Loss: 0.00000602
2024-11-06 14:07:44,743 - INFO - Epoch [300/300], Batch [17/43], Training Loss: 0.00001519
2024-11-06 14:07:44,748 - INFO - Epoch [300/300], Batch [18/43], Training Loss: 0.00002384
2024-11-06 14:07:44,753 - INFO - Epoch [300/300], Batch [19/43], Training Loss: 0.00001101
2024-11-06 14:07:44,757 - INFO - Epoch [300/300], Batch [20/43], Training Loss: 0.00000594
2024-11-06 14:07:44,761 - INFO - Epoch [300/300], Batch [21/43], Training Loss: 0.00000634
2024-11-06 14:07:44,765 - INFO - Epoch [300/300], Batch [22/43], Training Loss: 0.00000376
2024-11-06 14:07:44,769 - INFO - Epoch [300/300], Batch [23/43], Training Loss: 0.00001152
2024-11-06 14:07:44,772 - INFO - Epoch [300/300], Batch [24/43], Training Loss: 0.00001426
2024-11-06 14:07:44,776 - INFO - Epoch [300/300], Batch [25/43], Training Loss: 0.00001419
2024-11-06 14:07:44,779 - INFO - Epoch [300/300], Batch [26/43], Training Loss: 0.00000713
2024-11-06 14:07:44,783 - INFO - Epoch [300/300], Batch [27/43], Training Loss: 0.00000314
2024-11-06 14:07:44,787 - INFO - Epoch [300/300], Batch [28/43], Training Loss: 0.00000491
2024-11-06 14:07:44,790 - INFO - Epoch [300/300], Batch [29/43], Training Loss: 0.00000936
2024-11-06 14:07:44,793 - INFO - Epoch [300/300], Batch [30/43], Training Loss: 0.00000487
2024-11-06 14:07:44,796 - INFO - Epoch [300/300], Batch [31/43], Training Loss: 0.00001616
2024-11-06 14:07:44,800 - INFO - Epoch [300/300], Batch [32/43], Training Loss: 0.00001747
2024-11-06 14:07:44,804 - INFO - Epoch [300/300], Batch [33/43], Training Loss: 0.00001087
2024-11-06 14:07:44,807 - INFO - Epoch [300/300], Batch [34/43], Training Loss: 0.00000914
2024-11-06 14:07:44,810 - INFO - Epoch [300/300], Batch [35/43], Training Loss: 0.00001552
2024-11-06 14:07:44,812 - INFO - Epoch [300/300], Batch [36/43], Training Loss: 0.00002232
2024-11-06 14:07:44,817 - INFO - Epoch [300/300], Batch [37/43], Training Loss: 0.00000642
2024-11-06 14:07:44,820 - INFO - Epoch [300/300], Batch [38/43], Training Loss: 0.00000587
2024-11-06 14:07:44,823 - INFO - Epoch [300/300], Batch [39/43], Training Loss: 0.00000606
2024-11-06 14:07:44,826 - INFO - Epoch [300/300], Batch [40/43], Training Loss: 0.00000427
2024-11-06 14:07:44,829 - INFO - Epoch [300/300], Batch [41/43], Training Loss: 0.00001684
2024-11-06 14:07:44,832 - INFO - Epoch [300/300], Batch [42/43], Training Loss: 0.00000307
2024-11-06 14:07:44,836 - INFO - Epoch [300/300], Batch [43/43], Training Loss: 0.00000813
2024-11-06 14:07:44,847 - INFO - Epoch [300/300], Average Training Loss: 0.00000963, Validation Loss: 0.00001204
