{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# 示例文本数据\n",
    "documents = [\n",
    "    \"这是一篇关于LDA主题抽取的示例文本。\",\n",
    "    \"主题建模是一种用于理解文本内容的方法。\",\n",
    "    \"LDA是一种常见的主题建模技术。\",\n",
    "    # 添加更多文本...\n",
    "]\n",
    "\n",
    "# 预处理文本数据\n",
    "def preprocess_text(text):\n",
    "    # 分词\n",
    "    tokens = word_tokenize(text)\n",
    "    # 移除停用词和标点符号\n",
    "    stop_words = set(stopwords.words(\"chinese\") + list(string.punctuation))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# 构建文档-词频矩阵\n",
    "texts = [preprocess_text(doc) for doc in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# 训练LDA模型\n",
    "lda_model = LdaModel(corpus, num_topics=3, id2word=dictionary, passes=10)\n",
    "\n",
    "# 打印主题\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\requests\\compat.py\", line 11, in <module>\n",
      "    import chardet\n",
      "ModuleNotFoundError: No module named 'chardet'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\LJH\\AppData\\Local\\Temp\\ipykernel_19928\\1043834928.py\", line 3, in <module>\n",
      "    from gensim import corpora, models\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\gensim\\__init__.py\", line 5, in <module>\n",
      "    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\gensim\\parsing\\__init__.py\", line 4, in <module>\n",
      "    from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\gensim\\parsing\\preprocessing.py\", line 40, in <module>\n",
      "    from gensim import utils\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\gensim\\utils.py\", line 44, in <module>\n",
      "    from smart_open import smart_open\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\smart_open\\__init__.py\", line 27, in <module>\n",
      "    from .smart_open_lib import open, smart_open, register_compressor\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\smart_open\\smart_open_lib.py\", line 42, in <module>\n",
      "    import smart_open.webhdfs as smart_open_webhdfs\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\smart_open\\webhdfs.py\", line 18, in <module>\n",
      "    import requests\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\requests\\__init__.py\", line 45, in <module>\n",
      "    from .exceptions import RequestsDependencyWarning\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\requests\\exceptions.py\", line 9, in <module>\n",
      "    from .compat import JSONDecodeError as CompatJSONDecodeError\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\requests\\compat.py\", line 13, in <module>\n",
      "    import charset_normalizer as chardet\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\charset_normalizer\\__init__.py\", line 23, in <module>\n",
      "    from charset_normalizer.api import from_fp, from_path, from_bytes, normalize\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\charset_normalizer\\api.py\", line 10, in <module>\n",
      "    from charset_normalizer.md import mess_ratio\n",
      "AttributeError: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import os\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Topics:\n",
      "Topic 0: 0.000*\"麽\" + 0.000*\"其实\" + 0.000*\"创业\" + 0.000*\"估值\"\n",
      "Topic 1: 0.000*\"其实\" + 0.000*\"麽\" + 0.000*\"估值\" + 0.000*\"比较\"\n",
      "Topic 2: 0.000*\"其实\" + 0.000*\"麽\" + 0.000*\"比较\" + 0.000*\"创业\"\n",
      "Topic 3: 0.000*\"麽\" + 0.000*\"其实\" + 0.000*\"创业\" + 0.000*\"比较\"\n",
      "Topic 4: 0.026*\"其实\" + 0.024*\"麽\" + 0.014*\"创业\" + 0.012*\"比较\"\n",
      "\n",
      "\n",
      "Document 2 Topics:\n",
      "Topic 0: 0.000*\"会\" + 0.000*\"大家\" + 0.000*\"可能\" + 0.000*\"其实\"\n",
      "Topic 1: 0.000*\"会\" + 0.000*\"大家\" + 0.000*\"去\" + 0.000*\"可能\"\n",
      "Topic 2: 0.000*\"大家\" + 0.000*\"会\" + 0.000*\"其实\" + 0.000*\"去\"\n",
      "Topic 3: 0.000*\"大家\" + 0.000*\"其实\" + 0.000*\"可能\" + 0.000*\"板块\"\n",
      "Topic 4: 0.021*\"大家\" + 0.021*\"会\" + 0.015*\"可能\" + 0.014*\"其实\"\n",
      "\n",
      "\n",
      "Document 3 Topics:\n",
      "Topic 0: 0.015*\"创新\" + 0.014*\"其实\" + 0.013*\"说\" + 0.010*\"大家\"\n",
      "Topic 1: 0.000*\"说\" + 0.000*\"创新\" + 0.000*\"其实\" + 0.000*\"药\"\n",
      "Topic 2: 0.000*\"其实\" + 0.000*\"创新\" + 0.000*\"说\" + 0.000*\"大家\"\n",
      "Topic 3: 0.000*\"创新\" + 0.000*\"说\" + 0.000*\"医药\" + 0.000*\"会\"\n",
      "Topic 4: 0.000*\"其实\" + 0.000*\"创新\" + 0.000*\"大家\" + 0.000*\"药\"\n",
      "\n",
      "\n",
      "Document 4 Topics:\n",
      "Topic 0: 0.017*\"大家\" + 0.011*\"其实\" + 0.010*\"市场\" + 0.010*\"说\"\n",
      "Topic 1: 0.000*\"大家\" + 0.000*\"其实\" + 0.000*\"说\" + 0.000*\"比较\"\n",
      "Topic 2: 0.000*\"大家\" + 0.000*\"说\" + 0.000*\"其实\" + 0.000*\"比较\"\n",
      "Topic 3: 0.000*\"大家\" + 0.000*\"说\" + 0.000*\"其实\" + 0.000*\"市场\"\n",
      "Topic 4: 0.000*\"大家\" + 0.000*\"其实\" + 0.000*\"市场\" + 0.000*\"这种\"\n",
      "\n",
      "\n",
      "Document 5 Topics:\n",
      "Topic 0: 0.000*\"其实\" + 0.000*\"可能\" + 0.000*\"电池\" + 0.000*\"去\"\n",
      "Topic 1: 0.000*\"其实\" + 0.000*\"可能\" + 0.000*\"去\" + 0.000*\"说\"\n",
      "Topic 2: 0.000*\"其实\" + 0.000*\"去\" + 0.000*\"可能\" + 0.000*\"说\"\n",
      "Topic 3: 0.021*\"其实\" + 0.013*\"比较\" + 0.013*\"电池\" + 0.012*\"去\"\n",
      "Topic 4: 0.000*\"其实\" + 0.000*\"比较\" + 0.000*\"大家\" + 0.000*\"去\"\n",
      "\n",
      "\n",
      "Document 6 Topics:\n",
      "Topic 0: 0.000*\"大家\" + 0.000*\"去\" + 0.000*\"基金\" + 0.000*\"会\"\n",
      "Topic 1: 0.000*\"大家\" + 0.000*\"去\" + 0.000*\"基金\" + 0.000*\"会\"\n",
      "Topic 2: 0.000*\"大家\" + 0.000*\"去\" + 0.000*\"会\" + 0.000*\"说\"\n",
      "Topic 3: 0.024*\"大家\" + 0.017*\"去\" + 0.012*\"说\" + 0.011*\"基金\"\n",
      "Topic 4: 0.000*\"大家\" + 0.000*\"去\" + 0.000*\"可能\" + 0.000*\"说\"\n",
      "\n",
      "\n",
      "Document 7 Topics:\n",
      "Topic 0: 0.000*\"说\" + 0.000*\"不\" + 0.000*\"都\" + 0.000*\"现在\"\n",
      "Topic 1: 0.000*\"不\" + 0.000*\"说\" + 0.000*\"大家\" + 0.000*\"都\"\n",
      "Topic 2: 0.020*\"说\" + 0.015*\"不\" + 0.011*\"现在\" + 0.010*\"上\"\n",
      "Topic 3: 0.000*\"说\" + 0.000*\"不\" + 0.000*\"上\" + 0.000*\"现在\"\n",
      "Topic 4: 0.000*\"说\" + 0.000*\"现在\" + 0.000*\"不\" + 0.000*\"上\"\n",
      "\n",
      "\n",
      "Document 8 Topics:\n",
      "Topic 0: 0.000*\"其实\" + 0.000*\"大家\" + 0.000*\"说\" + 0.000*\"会\"\n",
      "Topic 1: 0.000*\"其实\" + 0.000*\"大家\" + 0.000*\"可能\" + 0.000*\"说\"\n",
      "Topic 2: 0.000*\"其实\" + 0.000*\"可能\" + 0.000*\"大家\" + 0.000*\"说\"\n",
      "Topic 3: 0.000*\"其实\" + 0.000*\"大家\" + 0.000*\"可能\" + 0.000*\"会\"\n",
      "Topic 4: 0.020*\"其实\" + 0.017*\"大家\" + 0.014*\"可能\" + 0.013*\"说\"\n",
      "\n",
      "\n",
      "Document 9 Topics:\n",
      "Topic 0: 0.000*\"去\" + 0.000*\"市场\" + 0.000*\"会\" + 0.000*\"其实\"\n",
      "Topic 1: 0.016*\"去\" + 0.012*\"会\" + 0.012*\"市场\" + 0.007*\"大家\"\n",
      "Topic 2: 0.000*\"去\" + 0.000*\"市场\" + 0.000*\"会\" + 0.000*\"行情\"\n",
      "Topic 3: 0.000*\"去\" + 0.000*\"会\" + 0.000*\"市场\" + 0.000*\"行情\"\n",
      "Topic 4: 0.000*\"去\" + 0.000*\"会\" + 0.000*\"市场\" + 0.000*\"大家\"\n",
      "\n",
      "\n",
      "Document 10 Topics:\n",
      "Topic 0: 0.000*\"红利\" + 0.000*\"比较\" + 0.000*\"其实\" + 0.000*\"指数\"\n",
      "Topic 1: 0.000*\"比较\" + 0.000*\"红利\" + 0.000*\"其实\" + 0.000*\"高\"\n",
      "Topic 2: 0.000*\"红利\" + 0.000*\"比较\" + 0.000*\"其实\" + 0.000*\"指数\"\n",
      "Topic 3: 0.027*\"红利\" + 0.025*\"比较\" + 0.021*\"其实\" + 0.013*\"指数\"\n",
      "Topic 4: 0.000*\"红利\" + 0.000*\"比较\" + 0.000*\"其实\" + 0.000*\"指数\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import os\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "# 读取文本文件\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# 读取停用词\n",
    "def load_stopwords(stopwords_path):\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "        return set([line.strip() for line in file])\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"C:/Users/LJH/Desktop/直播数据/文本/\"\n",
    "file_names = [f\"test{i}.txt\" for i in range(1, 11)]\n",
    "\n",
    "# 读取停用词\n",
    "stopwords = load_stopwords(\"C:\\\\Users\\\\LJH\\\\Desktop\\\\M3u8Download-master\\\\stopword.txt\") \n",
    "\n",
    "# 读取文档并进行分词和去停用词\n",
    "documents = []\n",
    "for file_name in file_names:\n",
    "    content = read_text_file(os.path.join(file_path, file_name))\n",
    "    words = [word for word in jieba.cut(content) if ((word not in stopwords )and (word!=\"\\n\"))]\n",
    "    documents.append(words)\n",
    "\n",
    "# 创建词典\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# 转换为词袋模型\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel([doc_bow], num_topics=3, id2word=dictionary, passes=15)\n",
    "# 对每个文档进行LDA建模并输出结果\n",
    "for i in enumerate(corpus):\n",
    "    print(f\"Document {i+1} Topics:\")\n",
    "    for idx, topic in lda_model.print_topics(num_words=4):\n",
    "        print(f\"Topic {idx}: {topic}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Topics:\n",
      "Topic 0: 0.019*\"会\" + 0.014*\"说\" + 0.009*\"都\" + 0.008*\"板块\"\n",
      "Topic 1: 0.010*\"红利\" + 0.009*\"说\" + 0.009*\"会\" + 0.009*\"麽\"\n",
      "Topic 2: 0.020*\"说\" + 0.016*\"不\" + 0.011*\"现在\" + 0.010*\"上\"\n",
      "Topic 3: 0.013*\"说\" + 0.011*\"会\" + 0.009*\"基金\" + 0.009*\"这种\"\n",
      "Topic 4: 0.012*\"会\" + 0.012*\"市场\" + 0.008*\"行情\" + 0.006*\"一下\"\n",
      "\n",
      "\n",
      "Document 2 Topics:\n",
      "Topic 0: 0.019*\"会\" + 0.014*\"说\" + 0.009*\"都\" + 0.008*\"板块\"\n",
      "Topic 1: 0.010*\"红利\" + 0.009*\"说\" + 0.009*\"会\" + 0.009*\"麽\"\n",
      "Topic 2: 0.020*\"说\" + 0.016*\"不\" + 0.011*\"现在\" + 0.010*\"上\"\n",
      "Topic 3: 0.013*\"说\" + 0.011*\"会\" + 0.009*\"基金\" + 0.009*\"这种\"\n",
      "Topic 4: 0.012*\"会\" + 0.012*\"市场\" + 0.008*\"行情\" + 0.006*\"一下\"\n",
      "\n",
      "\n",
      "Document 3 Topics:\n",
      "Topic 0: 0.019*\"会\" + 0.014*\"说\" + 0.009*\"都\" + 0.008*\"板块\"\n",
      "Topic 1: 0.010*\"红利\" + 0.009*\"说\" + 0.009*\"会\" + 0.009*\"麽\"\n",
      "Topic 2: 0.020*\"说\" + 0.016*\"不\" + 0.011*\"现在\" + 0.010*\"上\"\n",
      "Topic 3: 0.013*\"说\" + 0.011*\"会\" + 0.009*\"基金\" + 0.009*\"这种\"\n",
      "Topic 4: 0.012*\"会\" + 0.012*\"市场\" + 0.008*\"行情\" + 0.006*\"一下\"\n",
      "\n",
      "\n",
      "Document 4 Topics:\n",
      "Topic 0: 0.019*\"会\" + 0.014*\"说\" + 0.009*\"都\" + 0.008*\"板块\"\n",
      "Topic 1: 0.010*\"红利\" + 0.009*\"说\" + 0.009*\"会\" + 0.009*\"麽\"\n",
      "Topic 2: 0.020*\"说\" + 0.016*\"不\" + 0.011*\"现在\" + 0.010*\"上\"\n",
      "Topic 3: 0.013*\"说\" + 0.011*\"会\" + 0.009*\"基金\" + 0.009*\"这种\"\n",
      "Topic 4: 0.012*\"会\" + 0.012*\"市场\" + 0.008*\"行情\" + 0.006*\"一下\"\n",
      "\n",
      "\n",
      "Document 5 Topics:\n",
      "Topic 0: 0.019*\"会\" + 0.014*\"说\" + 0.009*\"都\" + 0.008*\"板块\"\n",
      "Topic 1: 0.010*\"红利\" + 0.009*\"说\" + 0.009*\"会\" + 0.009*\"麽\"\n",
      "Topic 2: 0.020*\"说\" + 0.016*\"不\" + 0.011*\"现在\" + 0.010*\"上\"\n",
      "Topic 3: 0.013*\"说\" + 0.011*\"会\" + 0.009*\"基金\" + 0.009*\"这种\"\n",
      "Topic 4: 0.012*\"会\" + 0.012*\"市场\" + 0.008*\"行情\" + 0.006*\"一下\"\n",
      "\n",
      "\n",
      "Document 6 Topics:\n",
      "Topic 0: 0.019*\"会\" + 0.014*\"说\" + 0.009*\"都\" + 0.008*\"板块\"\n",
      "Topic 1: 0.010*\"红利\" + 0.009*\"说\" + 0.009*\"会\" + 0.009*\"麽\"\n",
      "Topic 2: 0.020*\"说\" + 0.016*\"不\" + 0.011*\"现在\" + 0.010*\"上\"\n",
      "Topic 3: 0.013*\"说\" + 0.011*\"会\" + 0.009*\"基金\" + 0.009*\"这种\"\n",
      "Topic 4: 0.012*\"会\" + 0.012*\"市场\" + 0.008*\"行情\" + 0.006*\"一下\"\n",
      "\n",
      "\n",
      "Document 7 Topics:\n",
      "Topic 0: 0.019*\"会\" + 0.014*\"说\" + 0.009*\"都\" + 0.008*\"板块\"\n",
      "Topic 1: 0.010*\"红利\" + 0.009*\"说\" + 0.009*\"会\" + 0.009*\"麽\"\n",
      "Topic 2: 0.020*\"说\" + 0.016*\"不\" + 0.011*\"现在\" + 0.010*\"上\"\n",
      "Topic 3: 0.013*\"说\" + 0.011*\"会\" + 0.009*\"基金\" + 0.009*\"这种\"\n",
      "Topic 4: 0.012*\"会\" + 0.012*\"市场\" + 0.008*\"行情\" + 0.006*\"一下\"\n",
      "\n",
      "\n",
      "Document 8 Topics:\n",
      "Topic 0: 0.019*\"会\" + 0.014*\"说\" + 0.009*\"都\" + 0.008*\"板块\"\n",
      "Topic 1: 0.010*\"红利\" + 0.009*\"说\" + 0.009*\"会\" + 0.009*\"麽\"\n",
      "Topic 2: 0.020*\"说\" + 0.016*\"不\" + 0.011*\"现在\" + 0.010*\"上\"\n",
      "Topic 3: 0.013*\"说\" + 0.011*\"会\" + 0.009*\"基金\" + 0.009*\"这种\"\n",
      "Topic 4: 0.012*\"会\" + 0.012*\"市场\" + 0.008*\"行情\" + 0.006*\"一下\"\n",
      "\n",
      "\n",
      "Document 9 Topics:\n",
      "Topic 0: 0.019*\"会\" + 0.014*\"说\" + 0.009*\"都\" + 0.008*\"板块\"\n",
      "Topic 1: 0.010*\"红利\" + 0.009*\"说\" + 0.009*\"会\" + 0.009*\"麽\"\n",
      "Topic 2: 0.020*\"说\" + 0.016*\"不\" + 0.011*\"现在\" + 0.010*\"上\"\n",
      "Topic 3: 0.013*\"说\" + 0.011*\"会\" + 0.009*\"基金\" + 0.009*\"这种\"\n",
      "Topic 4: 0.012*\"会\" + 0.012*\"市场\" + 0.008*\"行情\" + 0.006*\"一下\"\n",
      "\n",
      "\n",
      "Document 10 Topics:\n",
      "Topic 0: 0.019*\"会\" + 0.014*\"说\" + 0.009*\"都\" + 0.008*\"板块\"\n",
      "Topic 1: 0.010*\"红利\" + 0.009*\"说\" + 0.009*\"会\" + 0.009*\"麽\"\n",
      "Topic 2: 0.020*\"说\" + 0.016*\"不\" + 0.011*\"现在\" + 0.010*\"上\"\n",
      "Topic 3: 0.013*\"说\" + 0.011*\"会\" + 0.009*\"基金\" + 0.009*\"这种\"\n",
      "Topic 4: 0.012*\"会\" + 0.012*\"市场\" + 0.008*\"行情\" + 0.006*\"一下\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "# 读取文本文件\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# 读取停用词\n",
    "def load_stopwords(stopwords_path):\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "        return set([line.strip() for line in file])\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"C:/Users/LJH/Desktop/直播数据/文本/\"\n",
    "file_names = [f\"test{i}.txt\" for i in range(1, 11)]\n",
    "\n",
    "# 读取停用词\n",
    "stopwords = load_stopwords(\"C:\\\\Users\\\\LJH\\\\Desktop\\\\M3u8Download-master\\\\stopword.txt\") \n",
    "\n",
    "# 读取文档并进行分词和去停用词\n",
    "documents = []\n",
    "for file_name in file_names:\n",
    "    content = read_text_file(os.path.join(file_path, file_name))\n",
    "    words = [word for word in jieba.cut(content) if ((word not in stopwords )and (word!=\"\\n\"))]\n",
    "    documents.append(words)\n",
    "\n",
    "# 创建词典\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# 转换为词袋模型\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# 训练LDA模型\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=20)\n",
    "\n",
    "# # 对每个文档进行主题分析\n",
    "for i in range(0,len(corpus)):\n",
    "    print(f\"Document {i+1} Topics:\")\n",
    "    for idx, topic in lda_model.print_topics(num_words=4):\n",
    "        print(f\"Topic {idx}: {topic}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Top 7 words: [('麽', 0.8879290176713939), ('创业', 0.20307039137488983), ('板子', 0.12247296795467504), ('创业板', 0.1174039383273157), ('什', 0.10206080662889586), ('版', 0.09831476536626861), ('P', 0.07144256464022711)]\n",
      "Document 2 Top 7 words: [('汽车', 0.288050446833334), ('优选', 0.24101586003848333), ('科技', 0.22802372466448245), ('智能', 0.22503941158854215), ('AI', 0.18903310573437543), ('接下去', 0.15493876716759641), ('财通', 0.15493876716759641)]\n",
      "Document 3 Top 7 words: [('医药行业', 0.39043162054661756), ('医疗器械', 0.3036690382029248), ('药', 0.27621042216205993), ('药品', 0.216906455859232), ('支付', 0.15878208567603222), ('创新', 0.15455648167231814), ('老龄化', 0.1518345191014624)]\n",
      "Document 4 Top 7 words: [('量化', 0.4236053936853866), ('多因子', 0.25416323621123194), ('回购', 0.21713080683142605), ('刘博', 0.2118026968426933), ('商业银行', 0.16944215747415464), ('逆', 0.16778289618792014), ('线', 0.14804373193051779)]\n",
      "Document 5 Top 7 words: [('电池', 0.4353568049823008), ('粒子', 0.21720397215472007), ('鲤', 0.21720397215472007), ('纳', 0.2051370848127912), ('产业链', 0.19687778739315906), ('车', 0.17767019837919232), ('汽车', 0.17035701064524814)]\n",
      "Document 6 Top 7 words: [('订', 0.3628762841579615), ('管理型', 0.3160535378149988), ('高高', 0.25752510488629526), ('型基金', 0.21422291501089408), ('武林', 0.1691406135431733), ('汽车', 0.1591370225795213), ('波动性', 0.15545655226823374)]\n",
      "Document 7 Top 7 words: [('老马', 0.36124002636911223), ('马', 0.24082668424607478), ('三千', 0.18062001318455612), ('探底', 0.15051667765379673), ('保卫', 0.13546500988841706), ('不知不觉', 0.12041334212303739), ('买进', 0.12041334212303739)]\n",
      "Document 8 Top 7 words: [('人员', 0.38117006664438885), ('焦煤', 0.36110848418942104), ('煤炭', 0.2953823329558738), ('煤', 0.25240399875965275), ('钢铁', 0.14685685087348263), ('减产', 0.14043107718477485), ('库存', 0.12036949472980703)]\n",
      "Document 9 Top 7 words: [('跨年', 0.3909631222655231), ('分散', 0.3420927319823327), ('节点', 0.24435195141595192), ('岁末年初', 0.14661117084957118), ('裁准', 0.14661117084957118), ('因素', 0.11668490119859186), ('宽松', 0.10247681072443354)]\n",
      "Document 10 Top 7 words: [('红利', 0.9256579523767767), ('股息', 0.1692062923699484), ('比较稳定', 0.09953311315879318), ('分红', 0.08847437384793659), (' ', 0.08089862208105797), ('行期', 0.05971986789527592), ('中正', 0.05545149107899109)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "# 创建TF-IDF模型\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# 使用TF-IDF模型转换语料库\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# 对每个文档按TF-IDF权重排序并提取前7个词\n",
    "top_words_per_document = []\n",
    "for doc in corpus_tfidf:\n",
    "    sorted_words = sorted(doc, key=lambda x: x[1], reverse=True)\n",
    "    top_words = sorted_words[:7]\n",
    "    top_words_per_document.append([(dictionary[word_id], weight) for word_id, weight in top_words])\n",
    "\n",
    "# 打印每个文档的前7个词\n",
    "for i, doc in enumerate(top_words_per_document):\n",
    "    print(f\"Document {i+1} Top 7 words: {doc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "vocab.txt: 100%|██████████| 110k/110k [00:00<00:00, 366kB/s]\n",
      "c:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\huggingface_hub-0.19.4-py3.8.egg\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LJH\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 28.9kB/s]\n",
      "config.json: 100%|██████████| 624/624 [00:00<00:00, 311kB/s]\n",
      "model.safetensors:  70%|███████   | 290M/412M [13:18<05:35, 363kB/s]  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Consistency check failed: file should be of size 411553788 but has size 289912538 (model.safetensors).\nWe are sorry for the inconvenience. Please retry download and pass `force_download=True, resume_download=False` as argument.\nIf the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LJH\\Desktop\\M3u8Download-master\\topic.ipynb 单元格 9\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LJH/Desktop/M3u8Download-master/topic.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbert-base-chinese\u001b[39m\u001b[39m'\u001b[39m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LJH/Desktop/M3u8Download-master/topic.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LJH/Desktop/M3u8Download-master/topic.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m BertModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LJH/Desktop/M3u8Download-master/topic.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# 函数：将文本转换为BERT嵌入向量\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LJH/Desktop/M3u8Download-master/topic.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_bert_embedding\u001b[39m(text):\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\transformers\\modeling_utils.py:2778\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2763\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2764\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2765\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m   2766\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcache_dir\u001b[39m\u001b[39m\"\u001b[39m: cache_dir,\n\u001b[0;32m   2767\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mforce_download\u001b[39m\u001b[39m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2776\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m: commit_hash,\n\u001b[0;32m   2777\u001b[0m     }\n\u001b[1;32m-> 2778\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   2780\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2781\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2782\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   2783\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\transformers\\utils\\hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    430\u001b[0m         path_or_repo_id,\n\u001b[0;32m    431\u001b[0m         filename,\n\u001b[0;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\huggingface_hub-0.19.4-py3.8.egg\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\huggingface_hub-0.19.4-py3.8.egg\\huggingface_hub\\file_download.py:1461\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1458\u001b[0m         \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1459\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[1;32m-> 1461\u001b[0m     http_get(\n\u001b[0;32m   1462\u001b[0m         url_to_download,\n\u001b[0;32m   1463\u001b[0m         temp_file,\n\u001b[0;32m   1464\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1465\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1466\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1467\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[0;32m   1468\u001b[0m     )\n\u001b[0;32m   1470\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1471\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\theme\\lib\\site-packages\\huggingface_hub-0.19.4-py3.8.egg\\huggingface_hub\\file_download.py:569\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, _nb_retries)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[39mreturn\u001b[39;00m http_get(\n\u001b[0;32m    559\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[0;32m    560\u001b[0m         temp_file\u001b[39m=\u001b[39mtemp_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    565\u001b[0m         _nb_retries\u001b[39m=\u001b[39m_nb_retries \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m expected_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m expected_size \u001b[39m!=\u001b[39m temp_file\u001b[39m.\u001b[39mtell():\n\u001b[1;32m--> 569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m         consistency_error_message\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    571\u001b[0m             actual_size\u001b[39m=\u001b[39mtemp_file\u001b[39m.\u001b[39mtell(),\n\u001b[0;32m    572\u001b[0m         )\n\u001b[0;32m    573\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Consistency check failed: file should be of size 411553788 but has size 289912538 (model.safetensors).\nWe are sorry for the inconvenience. Please retry download and pass `force_download=True, resume_download=False` as argument.\nIf the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# 加载BERT模型和分词器\n",
    "model_name = 'bert-base-chinese' \n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 函数：将文本转换为BERT嵌入向量\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "# 将所有文档转换为嵌入向量\n",
    "embeddings = []\n",
    "for doc in documents:\n",
    "    embeddings.append(get_bert_embedding(doc))\n",
    "\n",
    "# 转换为NumPy数组以便进行聚类\n",
    "embeddings = torch.stack(embeddings).numpy()\n",
    "\n",
    "# 使用K-means进行聚类，假设我们选择5个主题\n",
    "num_topics = 5\n",
    "kmeans = KMeans(n_clusters=num_topics)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "# 将每个文档分配给一个主题\n",
    "document_topics = [{'document': doc, 'topic': label} for doc, label in zip(documents, kmeans.labels_)]\n",
    "\n",
    "# 打印结果\n",
    "for i, item in enumerate(document_topics):\n",
    "    print(f\"Document {i+1}: Topic {item['topic']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2564"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
