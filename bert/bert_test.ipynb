{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取预训练模型据进行句子编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1947: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertModel, BertConfig\n",
    "import torch\n",
    "\n",
    "# 下载并加载预训练的中文BERT模型和分词器\n",
    "def load_pretrained_bert_model():\n",
    "    # tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "    # model = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "\n",
    "    # 指定模型权重文件路径和配置文件路径\n",
    "    model_weights_path = r\"E:\\obsidian\\Master\\fund_stream_project\\codes\\bert\\model\\pytorch_model.bin\"\n",
    "    model_config_path = r\"E:\\obsidian\\Master\\fund_stream_project\\codes\\bert\\model\\config.json\"\n",
    "        # 指定词汇表文件路径\n",
    "    vocab_file_path = r\"E:\\obsidian\\Master\\fund_stream_project\\codes\\bert\\model\\vocab.txt\"\n",
    "\n",
    "    # 使用词汇表文件路径加载分词器\n",
    "    tokenizer = BertTokenizer.from_pretrained(vocab_file_path)\n",
    "\n",
    "    # 加载BERT配置\n",
    "    config = BertConfig.from_pretrained(model_config_path)\n",
    "\n",
    "    # 使用本地路径加载BERT模型\n",
    "    model = BertModel.from_pretrained(model_weights_path, config=config)\n",
    "    return tokenizer, model\n",
    "\n",
    "# 加载预训练的BERT模型\n",
    "tokenizer, model = load_pretrained_bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对句子进行编码\n",
    "def encode_sentence(sentence, tokenizer, model):\n",
    "    # 使用分词器将句子分词并添加特殊标记\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # 将输入传递给BERT模型并获取表示向量\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 获取句子的表示向量（CLS token的输出）\n",
    "    sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    \n",
    "    return sentence_embedding[0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1947: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 对句子进行编码\n",
    "def encode_sentence(sentence, tokenizer, model):\n",
    "    # 使用分词器将句子分词并添加特殊标记\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # 将输入传递给BERT模型并获取表示向量\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 获取句子的表示向量（CLS token的输出）\n",
    "    sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    \n",
    "    return sentence_embedding[0].tolist()\n",
    "\n",
    "# 示例句子\n",
    "example_sentence = \"我爱自然语言处理，这是一个很有趣的领域，我希望能够深入学习并取得进步。\"\n",
    "\n",
    "# 加载预训练的BERT模型\n",
    "tokenizer, model = load_pretrained_bert_model()\n",
    "\n",
    "# 对示例句子进行编码\n",
    "encoded_sentence = encode_sentence(example_sentence, tokenizer, model)\n",
    "\n",
    "len(encoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46283578872680664,\n",
       " -0.17855283617973328,\n",
       " 0.5876517295837402,\n",
       " 0.2005506455898285,\n",
       " 0.10617582499980927,\n",
       " 0.4033048748970032,\n",
       " 0.03780314326286316,\n",
       " 0.2060239017009735,\n",
       " -0.10630224645137787,\n",
       " -0.09003029763698578,\n",
       " -0.873487651348114,\n",
       " -0.3241155445575714,\n",
       " -0.10882815718650818,\n",
       " 0.24245424568653107,\n",
       " -0.2772134840488434,\n",
       " 0.10021370649337769,\n",
       " 0.21823550760746002,\n",
       " 0.18160279095172882,\n",
       " 0.48677223920822144,\n",
       " 0.17916251718997955,\n",
       " -0.16899532079696655,\n",
       " -0.17601926624774933,\n",
       " 0.41788291931152344,\n",
       " -0.06866268068552017,\n",
       " 0.296078622341156,\n",
       " -0.4317943751811981,\n",
       " -0.13419754803180695,\n",
       " -0.8369889259338379,\n",
       " 0.42213791608810425,\n",
       " -0.5626455545425415,\n",
       " 0.1345367133617401,\n",
       " 0.43702465295791626,\n",
       " -0.32421091198921204,\n",
       " 0.0028959610499441624,\n",
       " -0.19611358642578125,\n",
       " -0.5884252786636353,\n",
       " -0.4924357533454895,\n",
       " 0.15527431666851044,\n",
       " -0.5412684082984924,\n",
       " -0.5271993279457092,\n",
       " -0.05955730751156807,\n",
       " -0.6810088753700256,\n",
       " -0.11806588619947433,\n",
       " -0.0557992123067379,\n",
       " -0.11257988959550858,\n",
       " 0.1253509521484375,\n",
       " -0.43865513801574707,\n",
       " -0.4123888909816742,\n",
       " 0.11259981244802475,\n",
       " 0.28063642978668213,\n",
       " -0.13743862509727478,\n",
       " 8.47421646118164,\n",
       " 0.10901390761137009,\n",
       " 0.07020536810159683,\n",
       " -0.5386170744895935,\n",
       " 0.5236940979957581,\n",
       " 0.8010274767875671,\n",
       " -0.497197687625885,\n",
       " -0.10234430432319641,\n",
       " -0.15084673464298248,\n",
       " -0.23427720367908478,\n",
       " 0.21932728588581085,\n",
       " 0.007223065011203289,\n",
       " -0.42745864391326904,\n",
       " 0.0479443185031414,\n",
       " 0.23097367584705353,\n",
       " 0.42797112464904785,\n",
       " 0.020866530016064644,\n",
       " 0.27917808294296265,\n",
       " -0.3988308906555176,\n",
       " -0.10837012529373169,\n",
       " -0.473964124917984,\n",
       " -0.26885858178138733,\n",
       " 0.1573140174150467,\n",
       " 0.029419515281915665,\n",
       " 0.15722593665122986,\n",
       " -0.09133612364530563,\n",
       " -0.3559229075908661,\n",
       " -0.13761772215366364,\n",
       " 0.01869393140077591,\n",
       " -1.3555645942687988,\n",
       " -0.009238660335540771,\n",
       " -0.07228591293096542,\n",
       " 0.03153202310204506,\n",
       " -0.2844584286212921,\n",
       " -0.4419257640838623,\n",
       " -0.3545481562614441,\n",
       " -0.14971117675304413,\n",
       " 0.5128132104873657,\n",
       " -0.07042086869478226,\n",
       " 0.12589937448501587,\n",
       " 0.30773407220840454,\n",
       " 0.21682001650333405,\n",
       " -0.2239425778388977,\n",
       " 0.34498587250709534,\n",
       " 0.14524075388908386,\n",
       " -0.41537532210350037,\n",
       " 0.11892527341842651,\n",
       " -0.4401054382324219,\n",
       " -0.07321804016828537,\n",
       " -0.5077600479125977,\n",
       " 0.27705127000808716,\n",
       " 0.1364479511976242,\n",
       " -0.6789302229881287,\n",
       " -0.8462740778923035,\n",
       " 0.007990243844687939,\n",
       " 0.3063376545906067,\n",
       " 0.3160132169723511,\n",
       " -0.13954392075538635,\n",
       " 0.3388480246067047,\n",
       " -0.09870564937591553,\n",
       " -0.5042886734008789,\n",
       " -0.7811052203178406,\n",
       " 0.13537272810935974,\n",
       " -0.2547382712364197,\n",
       " -0.07022929936647415,\n",
       " -0.04264502972364426,\n",
       " 0.4027727544307709,\n",
       " -0.17420591413974762,\n",
       " -0.2719341516494751,\n",
       " 0.5058580040931702,\n",
       " 0.14706219732761383,\n",
       " 0.6186916828155518,\n",
       " 0.22956840693950653,\n",
       " 0.27549663186073303,\n",
       " -0.2814484238624573,\n",
       " -0.14293377101421356,\n",
       " 0.012626596726477146,\n",
       " 0.0715726763010025,\n",
       " -0.24660862982273102,\n",
       " -0.31762781739234924,\n",
       " 0.20227524638175964,\n",
       " 0.362743079662323,\n",
       " 0.09697748720645905,\n",
       " -0.2817639410495758,\n",
       " -0.15276573598384857,\n",
       " -0.27138233184814453,\n",
       " -0.07252278923988342,\n",
       " -0.918788731098175,\n",
       " 0.493783175945282,\n",
       " -0.25347384810447693,\n",
       " 0.3518712520599365,\n",
       " -0.35334157943725586,\n",
       " 0.5841824412345886,\n",
       " 0.14540420472621918,\n",
       " 0.47978803515434265,\n",
       " -0.13278858363628387,\n",
       " -0.13709501922130585,\n",
       " 0.6318339109420776,\n",
       " 0.6257461905479431,\n",
       " -0.4563755989074707,\n",
       " -1.0354244709014893,\n",
       " 0.500029444694519,\n",
       " -0.21029719710350037,\n",
       " -0.6225699782371521,\n",
       " -0.21596024930477142,\n",
       " 0.043059419840574265,\n",
       " -0.3847329318523407,\n",
       " 0.6812018156051636,\n",
       " -0.014398355968296528,\n",
       " -0.13076290488243103,\n",
       " -0.010716930031776428,\n",
       " 0.184260755777359,\n",
       " 0.729566752910614,\n",
       " -0.48561564087867737,\n",
       " -0.43079715967178345,\n",
       " -0.02071637101471424,\n",
       " 0.1558392196893692,\n",
       " 0.19706830382347107,\n",
       " 0.4864279627799988,\n",
       " -0.04499762877821922,\n",
       " 0.34122341871261597,\n",
       " 0.08522886782884598,\n",
       " 0.23104242980480194,\n",
       " -0.33547842502593994,\n",
       " 0.5953606367111206,\n",
       " -0.22783541679382324,\n",
       " -0.30607491731643677,\n",
       " -0.3550505042076111,\n",
       " 0.10157819837331772,\n",
       " -0.05623863264918327,\n",
       " -0.10468127578496933,\n",
       " -0.05041499808430672,\n",
       " -0.47384190559387207,\n",
       " 0.6906585693359375,\n",
       " 0.6575160026550293,\n",
       " 0.2523493766784668,\n",
       " -0.5730103254318237,\n",
       " 0.10576881468296051,\n",
       " -0.041167840361595154,\n",
       " -0.4426535665988922,\n",
       " 0.11095523834228516,\n",
       " -0.054635681211948395,\n",
       " 0.4179644286632538,\n",
       " -0.1510075032711029,\n",
       " 0.34031882882118225,\n",
       " -0.16714641451835632,\n",
       " 0.18545308709144592,\n",
       " 0.3716621398925781,\n",
       " 0.2914729118347168,\n",
       " -0.5178200602531433,\n",
       " 0.35516804456710815,\n",
       " -0.11241264641284943,\n",
       " 0.5023932456970215,\n",
       " -0.4182467758655548,\n",
       " -0.4746907651424408,\n",
       " -0.20249608159065247,\n",
       " -0.21802128851413727,\n",
       " -0.08945377171039581,\n",
       " -0.3734264373779297,\n",
       " -0.48929131031036377,\n",
       " 0.10899360477924347,\n",
       " 0.07877117395401001,\n",
       " -0.19128990173339844,\n",
       " 0.1705261766910553,\n",
       " -0.404240220785141,\n",
       " 0.4539666473865509,\n",
       " -0.012704944238066673,\n",
       " 0.0760563537478447,\n",
       " -0.11671071499586105,\n",
       " 0.12178260087966919,\n",
       " -0.16676700115203857,\n",
       " 0.5650444030761719,\n",
       " -0.16927839815616608,\n",
       " 0.20731310546398163,\n",
       " -0.0374755933880806,\n",
       " -0.3828766644001007,\n",
       " -0.06082748621702194,\n",
       " 0.4228150248527527,\n",
       " -0.33145150542259216,\n",
       " -0.18687443435192108,\n",
       " -0.006800609640777111,\n",
       " 0.5047857165336609,\n",
       " 0.14211459457874298,\n",
       " -0.40864327549934387,\n",
       " -0.05168744921684265,\n",
       " -0.2848902642726898,\n",
       " -0.20915964245796204,\n",
       " -0.364371120929718,\n",
       " 0.38757479190826416,\n",
       " -0.10253935307264328,\n",
       " -0.30322638154029846,\n",
       " 0.1566791981458664,\n",
       " 0.12877312302589417,\n",
       " -0.41541656851768494,\n",
       " 0.26007914543151855,\n",
       " -0.10353369265794754,\n",
       " 0.0395197793841362,\n",
       " -0.3227342367172241,\n",
       " 0.5648877024650574,\n",
       " 0.28757137060165405,\n",
       " -0.025210559368133545,\n",
       " -0.08977288007736206,\n",
       " -0.20603249967098236,\n",
       " -0.16592223942279816,\n",
       " -0.005567209329456091,\n",
       " 0.28304052352905273,\n",
       " 0.42257946729660034,\n",
       " 0.15225796401500702,\n",
       " 0.3162173330783844,\n",
       " -0.06541158258914948,\n",
       " 0.3726973533630371,\n",
       " 0.48115187883377075,\n",
       " -0.752618670463562,\n",
       " 1.3744266033172607,\n",
       " -0.048988230526447296,\n",
       " 0.13327887654304504,\n",
       " -0.10427722334861755,\n",
       " 0.25190579891204834,\n",
       " -0.3764706254005432,\n",
       " -0.7651714086532593,\n",
       " -0.1758650243282318,\n",
       " -0.5056378841400146,\n",
       " -1.4150577783584595,\n",
       " -0.5930370092391968,\n",
       " 0.1683516949415207,\n",
       " -0.19612808525562286,\n",
       " -0.02875206246972084,\n",
       " -0.13801535964012146,\n",
       " -0.422069787979126,\n",
       " 0.1854662448167801,\n",
       " 0.18520091474056244,\n",
       " 0.008645527996122837,\n",
       " -0.29394906759262085,\n",
       " -0.5700415372848511,\n",
       " -0.40230855345726013,\n",
       " 0.5057453513145447,\n",
       " 0.4288671910762787,\n",
       " 0.4302923083305359,\n",
       " -0.23904326558113098,\n",
       " 0.1703178882598877,\n",
       " -0.5610405206680298,\n",
       " 0.15846963226795197,\n",
       " 0.4895632565021515,\n",
       " -0.027064168825745583,\n",
       " 0.7986564040184021,\n",
       " -0.10658713430166245,\n",
       " -0.21164676547050476,\n",
       " -0.32753056287765503,\n",
       " -0.5076915621757507,\n",
       " -0.5699224472045898,\n",
       " -0.3247801661491394,\n",
       " -0.1602051705121994,\n",
       " 0.6072978973388672,\n",
       " 0.2624698877334595,\n",
       " -0.654714047908783,\n",
       " 0.3554518222808838,\n",
       " 0.06906852126121521,\n",
       " -0.07714177668094635,\n",
       " -0.6775729656219482,\n",
       " -0.2574218809604645,\n",
       " -0.2431335747241974,\n",
       " -0.4075648784637451,\n",
       " -0.18141143023967743,\n",
       " -0.9951376914978027,\n",
       " -0.2779182195663452,\n",
       " -0.10455946624279022,\n",
       " 0.07604345679283142,\n",
       " 0.007703443057835102,\n",
       " 0.1579941064119339,\n",
       " -0.43728601932525635,\n",
       " -0.23220829665660858,\n",
       " 0.4157516062259674,\n",
       " 0.11031690984964371,\n",
       " 0.09364522248506546,\n",
       " -0.07163634151220322,\n",
       " 0.30431318283081055,\n",
       " -0.35911911725997925,\n",
       " 0.18655270338058472,\n",
       " -0.27048563957214355,\n",
       " 0.05055427923798561,\n",
       " -0.005650394596159458,\n",
       " -0.37305328249931335,\n",
       " -0.08598214387893677,\n",
       " -0.194731205701828,\n",
       " -0.02747156471014023,\n",
       " -0.13374176621437073,\n",
       " 0.15157844126224518,\n",
       " -0.4177519381046295,\n",
       " -1.135566234588623,\n",
       " -0.08564729988574982,\n",
       " -0.4433667063713074,\n",
       " 1.3424363136291504,\n",
       " 0.6537745594978333,\n",
       " -0.3078828454017639,\n",
       " -0.07136645913124084,\n",
       " 0.14713002741336823,\n",
       " -0.47451797127723694,\n",
       " -0.0418376699090004,\n",
       " 0.7038583755493164,\n",
       " -0.3640834391117096,\n",
       " 0.0684468224644661,\n",
       " -0.09190656244754791,\n",
       " -0.47042831778526306,\n",
       " 0.21465574204921722,\n",
       " 0.1649044305086136,\n",
       " -0.1813853532075882,\n",
       " 0.19624857604503632,\n",
       " 0.05309102684259415,\n",
       " -0.1155506819486618,\n",
       " 0.1912897229194641,\n",
       " -0.04780523478984833,\n",
       " 0.14497150480747223,\n",
       " -0.3580917418003082,\n",
       " 0.34587517380714417,\n",
       " 0.36453109979629517,\n",
       " -0.12032627314329147,\n",
       " 0.22454269230365753,\n",
       " 0.2593023478984833,\n",
       " -0.5048993825912476,\n",
       " 0.028820611536502838,\n",
       " -0.6198493838310242,\n",
       " 0.1204635351896286,\n",
       " -0.2993977665901184,\n",
       " -0.22442905604839325,\n",
       " -0.041663944721221924,\n",
       " 0.1892281472682953,\n",
       " -0.1076454371213913,\n",
       " -0.6577392220497131,\n",
       " -0.30005717277526855,\n",
       " -1.191941261291504,\n",
       " 0.5128546953201294,\n",
       " 0.015921950340270996,\n",
       " -0.48493003845214844,\n",
       " 0.32340529561042786,\n",
       " -0.04354432225227356,\n",
       " -0.2091856449842453,\n",
       " -0.5945611000061035,\n",
       " 0.490032821893692,\n",
       " -0.529810905456543,\n",
       " -0.6572948694229126,\n",
       " -0.11671599745750427,\n",
       " 0.3439636826515198,\n",
       " -0.3380447030067444,\n",
       " 0.43465375900268555,\n",
       " 0.7023933529853821,\n",
       " -0.8983028531074524,\n",
       " 0.06534313410520554,\n",
       " -0.2406165897846222,\n",
       " 0.28127917647361755,\n",
       " -0.057753439992666245,\n",
       " -0.18571069836616516,\n",
       " 0.2608356177806854,\n",
       " -0.15118934214115143,\n",
       " 0.24736326932907104,\n",
       " -0.19430725276470184,\n",
       " -0.10617296397686005,\n",
       " 0.07267210632562637,\n",
       " 0.7717809081077576,\n",
       " 0.1487244814634323,\n",
       " -0.11776886880397797,\n",
       " 0.7084254622459412,\n",
       " 0.07979333400726318,\n",
       " 0.15783476829528809,\n",
       " 0.03945913910865784,\n",
       " 0.07178504765033722,\n",
       " 1.0968788862228394,\n",
       " -0.20934605598449707,\n",
       " 0.21779915690422058,\n",
       " 0.02378973178565502,\n",
       " -0.2935684025287628,\n",
       " -0.756351888179779,\n",
       " -0.4283739924430847,\n",
       " -0.21973061561584473,\n",
       " 0.24319331347942352,\n",
       " -0.022034190595149994,\n",
       " -0.0953461080789566,\n",
       " 0.18236954510211945,\n",
       " 1.0465208292007446,\n",
       " 0.3721455931663513,\n",
       " -0.3948945105075836,\n",
       " 0.2957400977611542,\n",
       " -0.06577327102422714,\n",
       " 0.7753447890281677,\n",
       " 0.3184162974357605,\n",
       " 0.1217828318476677,\n",
       " -0.1722572296857834,\n",
       " 0.0939989984035492,\n",
       " -0.07493051141500473,\n",
       " -0.09722337126731873,\n",
       " -0.053183797746896744,\n",
       " -0.14797988533973694,\n",
       " -0.002247043652459979,\n",
       " 0.7671266794204712,\n",
       " -0.15400303900241852,\n",
       " -0.014412036165595055,\n",
       " 0.6480302810668945,\n",
       " -0.6759693622589111,\n",
       " 0.436421275138855,\n",
       " -0.2995971143245697,\n",
       " -0.09339584410190582,\n",
       " -0.23272080719470978,\n",
       " -0.30181974172592163,\n",
       " -0.29244762659072876,\n",
       " -0.19131261110305786,\n",
       " -0.13010352849960327,\n",
       " 0.04467318207025528,\n",
       " 0.033799707889556885,\n",
       " -0.3991038501262665,\n",
       " 0.32677119970321655,\n",
       " 0.27389591932296753,\n",
       " -0.07263610512018204,\n",
       " -0.13407808542251587,\n",
       " -0.07340241968631744,\n",
       " -0.20331591367721558,\n",
       " -0.3496836721897125,\n",
       " -0.11310648173093796,\n",
       " 0.5817109942436218,\n",
       " 0.19884909689426422,\n",
       " -0.6506502032279968,\n",
       " -0.6523847579956055,\n",
       " 0.12209775298833847,\n",
       " 0.47645923495292664,\n",
       " 0.30117908120155334,\n",
       " 0.08172616362571716,\n",
       " 0.324869304895401,\n",
       " 0.29471826553344727,\n",
       " 0.1823311299085617,\n",
       " -1.543124794960022,\n",
       " -0.7677878737449646,\n",
       " -0.012822724878787994,\n",
       " -0.014504174701869488,\n",
       " 0.3233652114868164,\n",
       " -0.48749372363090515,\n",
       " 0.21746957302093506,\n",
       " -0.07212811708450317,\n",
       " 0.33439093828201294,\n",
       " -0.5836148858070374,\n",
       " 0.4354952275753021,\n",
       " 0.09725186973810196,\n",
       " -0.2524491846561432,\n",
       " -0.4260760247707367,\n",
       " 0.27240169048309326,\n",
       " -0.48921945691108704,\n",
       " -0.1976272463798523,\n",
       " -0.33974897861480713,\n",
       " 0.17728038132190704,\n",
       " -0.0572822280228138,\n",
       " -0.024566225707530975,\n",
       " -0.06117019057273865,\n",
       " 0.31903448700904846,\n",
       " -0.4646778106689453,\n",
       " 0.06986881792545319,\n",
       " -0.0791650041937828,\n",
       " -0.7634007930755615,\n",
       " -0.07541229575872421,\n",
       " 0.2989461421966553,\n",
       " 0.06371451169252396,\n",
       " 0.1956101357936859,\n",
       " 0.011067410930991173,\n",
       " -0.13256347179412842,\n",
       " -0.8706499934196472,\n",
       " -0.18953703343868256,\n",
       " -0.5562310218811035,\n",
       " -0.15979044139385223,\n",
       " 0.5425325036048889,\n",
       " -0.025926021859049797,\n",
       " 0.015382589772343636,\n",
       " 0.1009308472275734,\n",
       " -0.3334381878376007,\n",
       " -0.07667732238769531,\n",
       " -0.21953727304935455,\n",
       " 0.21463117003440857,\n",
       " 0.4206066429615021,\n",
       " 0.21323823928833008,\n",
       " -0.6021776795387268,\n",
       " -0.4494318962097168,\n",
       " 0.3280780017375946,\n",
       " 0.48139891028404236,\n",
       " 0.3068296015262604,\n",
       " 0.21623611450195312,\n",
       " 0.4980253279209137,\n",
       " 0.08663006871938705,\n",
       " -0.11480027437210083,\n",
       " 0.18976224958896637,\n",
       " 0.2948918342590332,\n",
       " -0.10224027931690216,\n",
       " -0.9598856568336487,\n",
       " 0.23609016835689545,\n",
       " -0.09449237585067749,\n",
       " 0.052506282925605774,\n",
       " -0.4203580915927887,\n",
       " -0.014314880594611168,\n",
       " -0.3170241415500641,\n",
       " -0.3228985071182251,\n",
       " -0.41275885701179504,\n",
       " -0.4065886437892914,\n",
       " 0.10225266218185425,\n",
       " -0.36485883593559265,\n",
       " -0.20608201622962952,\n",
       " -0.3664376139640808,\n",
       " 0.2031165063381195,\n",
       " -0.5020965337753296,\n",
       " 0.006560862995684147,\n",
       " -0.4327579140663147,\n",
       " -0.7991798520088196,\n",
       " 0.47519272565841675,\n",
       " 0.1221391037106514,\n",
       " 0.5166019201278687,\n",
       " -0.11837384849786758,\n",
       " 0.15444740653038025,\n",
       " 0.3979053795337677,\n",
       " -0.3819642961025238,\n",
       " 0.1364331692457199,\n",
       " -0.27177104353904724,\n",
       " 0.24424880743026733,\n",
       " 0.24457956850528717,\n",
       " -0.13818921148777008,\n",
       " -0.8619312644004822,\n",
       " 0.12838990986347198,\n",
       " -0.2219124138355255,\n",
       " -0.24219830334186554,\n",
       " -0.2603321969509125,\n",
       " -0.24046792089939117,\n",
       " -0.29341715574264526,\n",
       " 0.09885213524103165,\n",
       " -0.34413111209869385,\n",
       " 0.27094605565071106,\n",
       " 0.6767832040786743,\n",
       " -0.25314146280288696,\n",
       " 0.7090892195701599,\n",
       " 0.793147087097168,\n",
       " 0.643509566783905,\n",
       " -0.33879899978637695,\n",
       " -0.8716253042221069,\n",
       " -0.03905327990651131,\n",
       " 0.45547258853912354,\n",
       " 0.5474456548690796,\n",
       " -0.053520381450653076,\n",
       " -0.3482794463634491,\n",
       " 0.02524126134812832,\n",
       " 0.0987047553062439,\n",
       " -0.09527818113565445,\n",
       " -0.08553871512413025,\n",
       " -0.2815529704093933,\n",
       " -0.33780089020729065,\n",
       " 0.6386598944664001,\n",
       " -0.0019684869330376387,\n",
       " -0.0937768965959549,\n",
       " 0.06594476103782654,\n",
       " 0.43350672721862793,\n",
       " -0.18855518102645874,\n",
       " -0.38848111033439636,\n",
       " 0.3666113317012787,\n",
       " -0.3717459738254547,\n",
       " -0.44297534227371216,\n",
       " 0.6192563772201538,\n",
       " 0.19556336104869843,\n",
       " -0.015747833997011185,\n",
       " -0.2588522434234619,\n",
       " 0.07169564068317413,\n",
       " -0.5307450294494629,\n",
       " 0.12014860659837723,\n",
       " -0.31492874026298523,\n",
       " -0.841177761554718,\n",
       " -0.03229428827762604,\n",
       " -0.21861550211906433,\n",
       " 0.4277530312538147,\n",
       " 0.40869441628456116,\n",
       " -0.3415469825267792,\n",
       " -0.15202663838863373,\n",
       " 0.1032106876373291,\n",
       " -0.5464094877243042,\n",
       " 0.226554736495018,\n",
       " 0.6326739192008972,\n",
       " 0.5590580701828003,\n",
       " 0.713014543056488,\n",
       " 0.36437979340553284,\n",
       " 0.21068967878818512,\n",
       " -0.44774535298347473,\n",
       " -0.5472168326377869,\n",
       " -0.009900790639221668,\n",
       " -0.17737682163715363,\n",
       " 0.7196383476257324,\n",
       " 0.49717676639556885,\n",
       " -0.14673037827014923,\n",
       " 0.06507443636655807,\n",
       " 0.284119576215744,\n",
       " 0.03567321226000786,\n",
       " 0.010790597647428513,\n",
       " 0.08898676931858063,\n",
       " -0.31101688742637634,\n",
       " -0.2676905691623688,\n",
       " -0.08849021792411804,\n",
       " -0.4241234064102173,\n",
       " 0.06617065519094467,\n",
       " 0.33747053146362305,\n",
       " -0.3142704367637634,\n",
       " -0.18126878142356873,\n",
       " -0.8179307579994202,\n",
       " -0.47334423661231995,\n",
       " 0.11920563131570816,\n",
       " 0.04801587015390396,\n",
       " -0.6814460158348083,\n",
       " 0.09129006415605545,\n",
       " 0.31331995129585266,\n",
       " 0.7902142405509949,\n",
       " 0.15507149696350098,\n",
       " -0.10465960949659348,\n",
       " -0.051776204258203506,\n",
       " 0.33946001529693604,\n",
       " -0.3125368356704712,\n",
       " -0.462578684091568,\n",
       " -0.005067466292530298,\n",
       " -0.4592827558517456,\n",
       " 0.3261919915676117,\n",
       " -0.5555254220962524,\n",
       " -0.7634145617485046,\n",
       " 0.32126912474632263,\n",
       " 0.7286916971206665,\n",
       " 0.9992658495903015,\n",
       " -0.03945173695683479,\n",
       " 0.15494279563426971,\n",
       " 0.038682278245687485,\n",
       " -0.27815255522727966,\n",
       " -0.4349535405635834,\n",
       " -0.1364632546901703,\n",
       " 0.27643072605133057,\n",
       " -0.022062858566641808,\n",
       " -0.13151656091213226,\n",
       " 0.41284507513046265,\n",
       " -0.8946599364280701,\n",
       " -0.306143581867218,\n",
       " 0.11493527889251709,\n",
       " 0.1682986468076706,\n",
       " 0.4786636233329773,\n",
       " 0.07821895182132721,\n",
       " -0.23646283149719238,\n",
       " -0.04447539523243904,\n",
       " -0.04970366880297661,\n",
       " 0.2522223889827728,\n",
       " 0.32259687781333923,\n",
       " 0.5478056073188782,\n",
       " -0.7076260447502136,\n",
       " -1.005781888961792,\n",
       " 10.818924903869629,\n",
       " -0.280402272939682,\n",
       " 0.02537461742758751,\n",
       " -0.15910522639751434,\n",
       " -0.33873799443244934,\n",
       " -0.22995877265930176,\n",
       " -0.010394599288702011,\n",
       " 0.1065545454621315,\n",
       " 0.4237334728240967,\n",
       " -1.075863242149353,\n",
       " -0.3571873605251312,\n",
       " -0.010278966277837753,\n",
       " 0.6221082806587219,\n",
       " 0.31156110763549805,\n",
       " 0.5605683922767639,\n",
       " 0.12335339188575745,\n",
       " 0.7439298629760742,\n",
       " 0.28444623947143555,\n",
       " -0.04775044694542885,\n",
       " -0.0948900431394577,\n",
       " 0.43551570177078247,\n",
       " -0.5664384365081787,\n",
       " -0.41544920206069946,\n",
       " -0.18838147819042206,\n",
       " 0.831204354763031,\n",
       " -0.05076887458562851,\n",
       " -0.7427150011062622,\n",
       " -0.582161009311676,\n",
       " 0.290500283241272,\n",
       " -0.29579827189445496,\n",
       " -0.5004895925521851,\n",
       " -0.3839000463485718,\n",
       " 0.12833073735237122,\n",
       " 0.3515210747718811,\n",
       " -0.20696742832660675,\n",
       " 0.5041478872299194,\n",
       " 0.10690060257911682,\n",
       " -0.1568886637687683,\n",
       " 0.21297024190425873,\n",
       " -0.17014223337173462,\n",
       " -0.059854403138160706,\n",
       " -1.0284223556518555,\n",
       " -0.328916996717453,\n",
       " 0.3532373309135437,\n",
       " 0.39424923062324524,\n",
       " 0.3358076214790344,\n",
       " 0.06827818602323532,\n",
       " 0.82314532995224,\n",
       " 0.3587897717952728,\n",
       " -0.08012261241674423,\n",
       " -0.3231280446052551,\n",
       " 0.17763689160346985,\n",
       " -0.5446596741676331,\n",
       " -0.24982117116451263,\n",
       " 1.704343318939209,\n",
       " 0.09250446408987045,\n",
       " -0.2513621151447296,\n",
       " 0.1259012073278427,\n",
       " -0.044435568153858185,\n",
       " -0.08944413810968399,\n",
       " -0.369159996509552,\n",
       " 3.9360921382904053,\n",
       " 0.2885613739490509,\n",
       " 0.39823445677757263,\n",
       " -0.29210925102233887,\n",
       " 0.14378800988197327,\n",
       " 0.20451311767101288,\n",
       " -0.1858789622783661,\n",
       " -0.16448049247264862,\n",
       " -0.3161230981349945,\n",
       " -0.26618924736976624,\n",
       " 0.32530534267425537,\n",
       " -0.07027255743741989]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增量训练BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     25\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, filename)\n\u001b[1;32m---> 26\u001b[0m         train_datasets\u001b[38;5;241m.\u001b[39mappend(\u001b[43mLineByLineTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 将 block_size 设置为 256，确保每个样本的长度不超过512个token\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 将所有数据集合并成一个\u001b[39;00m\n\u001b[0;32m     32\u001b[0m merged_train_dataset \u001b[38;5;241m=\u001b[39m ConcatDataset(train_datasets)\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:135\u001b[0m, in \u001b[0;36mLineByLineTextDataset.__init__\u001b[1;34m(self, tokenizer, file_path, block_size)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    133\u001b[0m     lines \u001b[38;5;241m=\u001b[39m [line \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39misspace())]\n\u001b[1;32m--> 135\u001b[0m batch_encoding \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples \u001b[38;5;241m=\u001b[39m batch_encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(e, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)} \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples]\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2829\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2828\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2829\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2915\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2910\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2911\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2912\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2913\u001b[0m         )\n\u001b[0;32m   2914\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2916\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2917\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2918\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2919\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2920\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2921\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2922\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2923\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2924\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2925\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2926\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2927\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2928\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2929\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2930\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2931\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2932\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2933\u001b[0m     )\n\u001b[0;32m   2934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2935\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2936\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2937\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2953\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2954\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3106\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3096\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3097\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3098\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3099\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3103\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3104\u001b[0m )\n\u001b[1;32m-> 3106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3107\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3108\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3109\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3110\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3111\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3112\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3113\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3114\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3115\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3116\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3117\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3118\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3119\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3120\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3121\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3122\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3123\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3124\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils.py:803\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    801\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 803\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    805\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils.py:770\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 770\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils.py:581\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()  \u001b[38;5;66;03m# don't split on any of the added tokens\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;66;03m# \"This is something<special_token_1>  else\"\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_trie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;66;03m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils.py:244\u001b[0m, in \u001b[0;36mTrie.split\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;66;03m# Longest cut is always the one with lower start so the first\u001b[39;00m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;66;03m# item so we need to break.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcut_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils.py:251\u001b[0m, in \u001b[0;36mTrie.cut_text\u001b[1;34m(self, text, offsets)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcut_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, offsets):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# We have all the offsets now, we just need to do the actual splitting.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# We need to eventually add the first part of the string and the eventual\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;66;03m# last part.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m     offsets\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(text))\n\u001b[1;32m--> 251\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    252\u001b[0m     start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m end \u001b[38;5;129;01min\u001b[39;00m offsets:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "# 设置GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer, model = load_pretrained_bert_model()\n",
    "\n",
    "# 设置微调参数\n",
    "output_dir = \"./fine_tuned_bert_model\"\n",
    "batch_size = 8\n",
    "num_train_epochs = 3\n",
    "\n",
    "# 准备用于微调的数据集\n",
    "data_dir = r\"C:\\Users\\LJH\\Desktop\\txt存放\\飞书\"  # 你的数据目录\n",
    "# 创建一个包含所有训练数据集的列表\n",
    "train_datasets = []\n",
    "\n",
    "# 遍历所有txt文件，并为每个文件创建一个LineByLineTextDataset\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        train_datasets.append(LineByLineTextDataset(\n",
    "            tokenizer=tokenizer,\n",
    "            file_path=file_path,\n",
    "            block_size=256,  # 将 block_size 设置为 256，确保每个样本的长度不超过512个token\n",
    "        ))\n",
    "# 将所有数据集合并成一个\n",
    "merged_train_dataset = ConcatDataset(train_datasets)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# 设置微调参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# 使用Trainer进行微调\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=merged_train_dataset,\n",
    ")\n",
    "\n",
    "# 开始微调BERT模型\n",
    "trainer.train()\n",
    "\n",
    "# 保存微调后的BERT模型\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     25\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, filename)\n\u001b[1;32m---> 26\u001b[0m         train_datasets\u001b[38;5;241m.\u001b[39mappend(\u001b[43mLineByLineTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 将 block_size 设置为 256，确保每个样本的长度不超过512个token\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 将所有数据集合并成一个\u001b[39;00m\n\u001b[0;32m     33\u001b[0m merged_train_dataset \u001b[38;5;241m=\u001b[39m ConcatDataset(train_datasets)\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:135\u001b[0m, in \u001b[0;36mLineByLineTextDataset.__init__\u001b[1;34m(self, tokenizer, file_path, block_size)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    133\u001b[0m     lines \u001b[38;5;241m=\u001b[39m [line \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39misspace())]\n\u001b[1;32m--> 135\u001b[0m batch_encoding \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples \u001b[38;5;241m=\u001b[39m batch_encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(e, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)} \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples]\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2829\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2828\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2829\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2915\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2910\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2911\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2912\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2913\u001b[0m         )\n\u001b[0;32m   2914\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2916\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2917\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2918\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2919\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2920\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2921\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2922\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2923\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2924\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2925\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2926\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2927\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2928\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2929\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2930\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2931\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2932\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2933\u001b[0m     )\n\u001b[0;32m   2934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2935\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2936\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2937\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2953\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2954\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3106\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3096\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3097\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3098\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3099\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3103\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3104\u001b[0m )\n\u001b[1;32m-> 3106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3107\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3108\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3109\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3110\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3111\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3112\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3113\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3114\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3115\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3116\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3117\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3118\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3119\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3120\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3121\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3122\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3123\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3124\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils.py:803\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    801\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 803\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    805\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils.py:770\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 770\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 617\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:245\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[1;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[0;32m    243\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[0;32m    250\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:432\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n\u001b[1;32m--> 432\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize_chinese_chars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# prevents treating the same character with different unicode codepoints as different characters\u001b[39;00m\n\u001b[0;32m    434\u001b[0m unicode_normalized_text \u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNFC\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "File \u001b[1;32mc:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:490\u001b[0m, in \u001b[0;36mBasicTokenizer._tokenize_chinese_chars\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_chinese_char(cp):\n\u001b[0;32m    489\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 490\u001b[0m     \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(char)\n\u001b[0;32m    491\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# 设置GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer, model = load_pretrained_bert_model()\n",
    "\n",
    "# 设置微调参数\n",
    "output_dir = \"./fine_tuned_bert_model\"\n",
    "batch_size = 8\n",
    "num_train_epochs = 3\n",
    "\n",
    "# 准备用于微调的数据集\n",
    "data_dir = r\"C:\\Users\\LJH\\Desktop\\txt存放\\飞书\"  # 你的数据目录\n",
    "\n",
    "# 创建一个包含所有训练数据集的列表\n",
    "train_datasets = []\n",
    "\n",
    "# 遍历所有txt文件，并为每个文件创建一个LineByLineTextDataset\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        train_datasets.append(LineByLineTextDataset(\n",
    "            tokenizer=tokenizer,\n",
    "            file_path=file_path,\n",
    "            block_size=256,  # 将 block_size 设置为 256，确保每个样本的长度不超过512个token\n",
    "        ))\n",
    "\n",
    "# 将所有数据集合并成一个\n",
    "merged_train_dataset = ConcatDataset(train_datasets)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# 设置微调参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir='./logs',  # 设置日志文件存储目录\n",
    "    logging_steps=500,  # 每500个训练步骤记录一次日志\n",
    "    logging_first_step=True,  # 在第一次训练步骤时记录日志\n",
    "    logging_verbose=True  # 启用详细的日志记录\n",
    ")\n",
    "\n",
    "# 使用Trainer进行微调\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=merged_train_dataset,\n",
    ")\n",
    "\n",
    "# 开始微调BERT模型\n",
    "trainer.train()\n",
    "\n",
    "# 保存微调后的BERT模型\n",
    "model.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载增强训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJH\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1947: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at E:\\obsidian\\Master\\fund_stream_project\\codes\\bert\\bert_ft_checkpoint-84500\\pytorch_model.bin and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertModel, BertConfig\n",
    "import torch\n",
    "\n",
    "# 下载并加载预训练的中文BERT模型和分词器\n",
    "def load_pretrained_bert_model():\n",
    "    # tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "    # model = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "\n",
    "    # 指定模型权重文件路径和配置文件路径\n",
    "    model_weights_path = r\"E:\\obsidian\\Master\\fund_stream_project\\codes\\bert\\bert_ft_checkpoint-84500\\pytorch_model.bin\"\n",
    "    model_config_path = r\"E:\\obsidian\\Master\\fund_stream_project\\codes\\bert\\bert_ft_checkpoint-84500\\config.json\"\n",
    "        # 指定词汇表文件路径\n",
    "    vocab_file_path = r\"E:\\obsidian\\Master\\fund_stream_project\\codes\\bert\\model\\vocab.txt\"\n",
    "\n",
    "    # 使用词汇表文件路径加载分词器\n",
    "    tokenizer = BertTokenizer.from_pretrained(vocab_file_path)\n",
    "\n",
    "    # 加载BERT配置\n",
    "    config = BertConfig.from_pretrained(model_config_path)\n",
    "\n",
    "    # 使用本地路径加载BERT模型\n",
    "    model = BertModel.from_pretrained(model_weights_path, config=config)\n",
    "    return tokenizer, model\n",
    "\n",
    "# 加载预训练的BERT模型\n",
    "tokenizer, model = load_pretrained_bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.717182457447052,\n",
       " -0.41662341356277466,\n",
       " 0.5299432277679443,\n",
       " 0.2987190783023834,\n",
       " 0.11066993325948715,\n",
       " 0.019121956080198288,\n",
       " -0.35343948006629944,\n",
       " 0.4006330966949463,\n",
       " 0.09625176340341568,\n",
       " -0.22542014718055725,\n",
       " -1.0740147829055786,\n",
       " 0.295207679271698,\n",
       " 0.13141460716724396,\n",
       " -0.21347811818122864,\n",
       " -0.4536829888820648,\n",
       " 0.11440753936767578,\n",
       " -0.04552561417222023,\n",
       " 0.652789294719696,\n",
       " 0.4582778215408325,\n",
       " 0.06845243275165558,\n",
       " -0.11564276367425919,\n",
       " -0.06365945935249329,\n",
       " -0.12925027310848236,\n",
       " -0.3374503552913666,\n",
       " 0.34227195382118225,\n",
       " -0.7340418100357056,\n",
       " 0.03328901529312134,\n",
       " -0.9234859347343445,\n",
       " 0.31954947113990784,\n",
       " -0.4369802176952362,\n",
       " 0.3526992201805115,\n",
       " 0.4182863235473633,\n",
       " -0.26276642084121704,\n",
       " 0.15289083123207092,\n",
       " -0.04965375363826752,\n",
       " -0.5104486346244812,\n",
       " -0.08860406279563904,\n",
       " 0.4965195655822754,\n",
       " -0.45352956652641296,\n",
       " -0.8987458944320679,\n",
       " 0.12773944437503815,\n",
       " -0.4509150981903076,\n",
       " 0.0021445131860673428,\n",
       " -0.6528545022010803,\n",
       " -0.012520529329776764,\n",
       " -0.18454165756702423,\n",
       " -0.22670583426952362,\n",
       " -0.33363044261932373,\n",
       " 0.26275384426116943,\n",
       " 0.2846432626247406,\n",
       " 0.20278708636760712,\n",
       " 6.55330753326416,\n",
       " 0.1263836920261383,\n",
       " -0.13271154463291168,\n",
       " -0.53546541929245,\n",
       " 0.47732776403427124,\n",
       " 0.7447738647460938,\n",
       " -0.20771288871765137,\n",
       " -0.5592264533042908,\n",
       " -0.02998606488108635,\n",
       " -0.5645676851272583,\n",
       " 0.24910975992679596,\n",
       " -0.21584124863147736,\n",
       " -0.24290332198143005,\n",
       " -0.14952737092971802,\n",
       " 0.42719465494155884,\n",
       " 0.4765999913215637,\n",
       " 0.04147610440850258,\n",
       " 0.2678276300430298,\n",
       " -0.3738882839679718,\n",
       " 0.23678524792194366,\n",
       " -0.18046660721302032,\n",
       " -0.26498380303382874,\n",
       " 0.1686973124742508,\n",
       " -0.08082451671361923,\n",
       " 0.11510682851076126,\n",
       " 0.039325956255197525,\n",
       " -0.3488403558731079,\n",
       " -0.047397855669260025,\n",
       " 0.2622251510620117,\n",
       " -1.2599848508834839,\n",
       " -0.262048602104187,\n",
       " -0.24398860335350037,\n",
       " 0.2922787070274353,\n",
       " -0.34435099363327026,\n",
       " -0.07836663722991943,\n",
       " -0.3404134511947632,\n",
       " -0.10716703534126282,\n",
       " 0.06278020888566971,\n",
       " -0.4617566764354706,\n",
       " -0.14351043105125427,\n",
       " 0.17643631994724274,\n",
       " -0.04366085305809975,\n",
       " -0.35161301493644714,\n",
       " 0.5239177942276001,\n",
       " 0.22001005709171295,\n",
       " -0.28735262155532837,\n",
       " 0.29310140013694763,\n",
       " -0.4146172106266022,\n",
       " 0.4898760914802551,\n",
       " -0.45233261585235596,\n",
       " 0.26809969544410706,\n",
       " 0.15031468868255615,\n",
       " -0.34990614652633667,\n",
       " -0.6541234254837036,\n",
       " -0.10538622736930847,\n",
       " 0.588736891746521,\n",
       " 0.5019721984863281,\n",
       " -0.2186645269393921,\n",
       " 0.390292227268219,\n",
       " -0.21972179412841797,\n",
       " -0.4320378303527832,\n",
       " -0.3248988389968872,\n",
       " 0.49554359912872314,\n",
       " -0.2838687002658844,\n",
       " 0.2819855511188507,\n",
       " -0.11686014384031296,\n",
       " 0.34950920939445496,\n",
       " 0.11088912189006805,\n",
       " -0.46688851714134216,\n",
       " 0.6692975163459778,\n",
       " -0.05750022828578949,\n",
       " 0.4694462716579437,\n",
       " 0.22990289330482483,\n",
       " 0.03987796977162361,\n",
       " 0.038686443120241165,\n",
       " 0.08848962932825089,\n",
       " -0.07400523126125336,\n",
       " 0.22870774567127228,\n",
       " -0.34296637773513794,\n",
       " -0.4052714407444,\n",
       " 0.06526968628168106,\n",
       " 0.18670868873596191,\n",
       " -0.013571922667324543,\n",
       " -0.058219343423843384,\n",
       " -0.18980683386325836,\n",
       " -0.511892557144165,\n",
       " -0.2787306010723114,\n",
       " -0.9369215965270996,\n",
       " -0.023396054282784462,\n",
       " 0.2124069780111313,\n",
       " 0.20984673500061035,\n",
       " -0.484686940908432,\n",
       " 0.31690216064453125,\n",
       " -0.22328129410743713,\n",
       " 0.46262824535369873,\n",
       " -0.07721420377492905,\n",
       " -0.14541226625442505,\n",
       " 0.7746821641921997,\n",
       " 0.8507581353187561,\n",
       " -0.2783764898777008,\n",
       " -1.3182727098464966,\n",
       " 0.5460516810417175,\n",
       " -0.24814657866954803,\n",
       " -0.5558087229728699,\n",
       " -0.015384146012365818,\n",
       " -0.02996048703789711,\n",
       " -0.47209957242012024,\n",
       " 0.3203847110271454,\n",
       " 0.2820306122303009,\n",
       " -0.0781731978058815,\n",
       " 0.015671713277697563,\n",
       " 0.2540106773376465,\n",
       " 0.5487099885940552,\n",
       " -0.8260329961776733,\n",
       " -0.1690288931131363,\n",
       " -0.23956136405467987,\n",
       " 0.022979795932769775,\n",
       " 0.16650156676769257,\n",
       " 0.08793119341135025,\n",
       " -0.1847793161869049,\n",
       " 0.10779400169849396,\n",
       " 0.10006747394800186,\n",
       " -0.13545943796634674,\n",
       " -0.6716459393501282,\n",
       " 0.4305974543094635,\n",
       " -0.09880329668521881,\n",
       " 0.30734771490097046,\n",
       " -0.1554330587387085,\n",
       " 0.10489314049482346,\n",
       " -0.26540905237197876,\n",
       " -0.23839879035949707,\n",
       " -0.3173920512199402,\n",
       " -0.04976488649845123,\n",
       " 0.7482558488845825,\n",
       " 0.5873304605484009,\n",
       " 0.15642958879470825,\n",
       " -0.5713604092597961,\n",
       " 0.007720004767179489,\n",
       " 0.02912214957177639,\n",
       " -0.7218517065048218,\n",
       " 0.037272289395332336,\n",
       " 0.26959848403930664,\n",
       " 0.45513054728507996,\n",
       " -0.30202582478523254,\n",
       " 0.45935356616973877,\n",
       " -0.26593872904777527,\n",
       " 0.6168850064277649,\n",
       " 0.4835885167121887,\n",
       " 0.08340394496917725,\n",
       " -0.40407493710517883,\n",
       " 0.27554231882095337,\n",
       " -0.23842740058898926,\n",
       " 0.41067641973495483,\n",
       " -0.772370457649231,\n",
       " -0.022788556292653084,\n",
       " -0.3933478593826294,\n",
       " -0.0981999859213829,\n",
       " 0.09829501062631607,\n",
       " -0.31811532378196716,\n",
       " -0.25650155544281006,\n",
       " -0.02167946845293045,\n",
       " 0.18546158075332642,\n",
       " -0.1635611206293106,\n",
       " -0.236946702003479,\n",
       " 0.09613931179046631,\n",
       " 0.35542744398117065,\n",
       " -0.05128901079297066,\n",
       " 0.15662142634391785,\n",
       " -0.23341503739356995,\n",
       " -0.21677422523498535,\n",
       " -0.1701647937297821,\n",
       " 0.39315274357795715,\n",
       " 0.10693029314279556,\n",
       " 0.47888848185539246,\n",
       " 0.11967862397432327,\n",
       " -0.3338468372821808,\n",
       " -0.10309013724327087,\n",
       " 0.2630671560764313,\n",
       " -0.15827345848083496,\n",
       " -0.19990627467632294,\n",
       " -0.04361706227064133,\n",
       " 0.7963628768920898,\n",
       " 0.10477077960968018,\n",
       " -0.26945802569389343,\n",
       " -0.20366990566253662,\n",
       " -0.4302148222923279,\n",
       " 0.12416835874319077,\n",
       " -0.6856899857521057,\n",
       " 0.24286150932312012,\n",
       " -0.1880028396844864,\n",
       " -0.3877531588077545,\n",
       " 0.17226234078407288,\n",
       " 0.2339566946029663,\n",
       " -0.1672913283109665,\n",
       " 0.10597003251314163,\n",
       " 0.27401119470596313,\n",
       " 0.2419666349887848,\n",
       " -0.44734257459640503,\n",
       " 0.49266666173934937,\n",
       " -0.10199973732233047,\n",
       " -0.2773749828338623,\n",
       " -0.14986301958560944,\n",
       " -0.46221351623535156,\n",
       " 0.006577929947525263,\n",
       " 0.35267335176467896,\n",
       " -0.3240398168563843,\n",
       " 0.2361983209848404,\n",
       " 0.30904096364974976,\n",
       " 0.5368508100509644,\n",
       " -0.33580514788627625,\n",
       " 0.3527466654777527,\n",
       " 0.38965651392936707,\n",
       " -0.6001912951469421,\n",
       " 1.086446762084961,\n",
       " -0.05071037635207176,\n",
       " 0.39091163873672485,\n",
       " -0.3907566964626312,\n",
       " -0.1669495850801468,\n",
       " -0.38128724694252014,\n",
       " -0.8668519258499146,\n",
       " 0.3209504187107086,\n",
       " -0.904802680015564,\n",
       " -1.498908519744873,\n",
       " -0.850206732749939,\n",
       " 0.044197358191013336,\n",
       " -0.22564604878425598,\n",
       " 0.12439266592264175,\n",
       " -0.34964320063591003,\n",
       " -0.20317082107067108,\n",
       " -0.1835125833749771,\n",
       " -0.01573292538523674,\n",
       " -0.24608983099460602,\n",
       " -0.4506602883338928,\n",
       " -0.4753658175468445,\n",
       " -0.5566450357437134,\n",
       " 0.23813624680042267,\n",
       " -0.03227422013878822,\n",
       " 0.17950539290905,\n",
       " -0.2895115613937378,\n",
       " -0.3290623426437378,\n",
       " -0.4659823775291443,\n",
       " -0.20859400928020477,\n",
       " 0.5544397234916687,\n",
       " 0.818965494632721,\n",
       " 0.7237341403961182,\n",
       " -0.23343217372894287,\n",
       " -0.40347954630851746,\n",
       " -0.425245463848114,\n",
       " -0.6658491492271423,\n",
       " -0.4472583532333374,\n",
       " -0.4284341037273407,\n",
       " -0.2909124195575714,\n",
       " 0.6869739890098572,\n",
       " 0.6620774269104004,\n",
       " -0.870624840259552,\n",
       " 0.6606417298316956,\n",
       " 0.35175809264183044,\n",
       " -0.013707973062992096,\n",
       " -0.7668880820274353,\n",
       " -0.8032466769218445,\n",
       " -0.423311322927475,\n",
       " -0.4908207058906555,\n",
       " -0.29953524470329285,\n",
       " -0.9125329256057739,\n",
       " -0.3515845239162445,\n",
       " -0.2785603404045105,\n",
       " 0.07833898067474365,\n",
       " 0.1822318583726883,\n",
       " 0.11143498867750168,\n",
       " -0.4701665937900543,\n",
       " -0.1911243200302124,\n",
       " 0.737935483455658,\n",
       " 0.6237041354179382,\n",
       " 0.0027480092830955982,\n",
       " 0.25666531920433044,\n",
       " 0.30700865387916565,\n",
       " -0.20462021231651306,\n",
       " 0.2993448078632355,\n",
       " -0.019433770328760147,\n",
       " 0.004494525957852602,\n",
       " -0.24154362082481384,\n",
       " -0.28476279973983765,\n",
       " 0.12089019268751144,\n",
       " -0.15258441865444183,\n",
       " 0.315580815076828,\n",
       " 0.00621276069432497,\n",
       " -0.277288556098938,\n",
       " -0.3258422911167145,\n",
       " -1.0900038480758667,\n",
       " -0.20696163177490234,\n",
       " -0.5314779877662659,\n",
       " 1.1970350742340088,\n",
       " 0.7729747295379639,\n",
       " -0.12963205575942993,\n",
       " 0.0048987832851707935,\n",
       " -0.05217979475855827,\n",
       " -0.8251145482063293,\n",
       " -0.29113900661468506,\n",
       " 0.6026906967163086,\n",
       " -0.35715481638908386,\n",
       " 0.08820722252130508,\n",
       " -0.313083291053772,\n",
       " -0.6116479635238647,\n",
       " 0.08845111727714539,\n",
       " 0.0953589454293251,\n",
       " -0.13074585795402527,\n",
       " -0.2032758891582489,\n",
       " 0.3984573483467102,\n",
       " 0.12079327553510666,\n",
       " 0.08524397015571594,\n",
       " 0.35695210099220276,\n",
       " 0.20615200698375702,\n",
       " -0.10294874012470245,\n",
       " 0.5016213655471802,\n",
       " -0.26013192534446716,\n",
       " -0.33618390560150146,\n",
       " 0.09244551509618759,\n",
       " -0.017482757568359375,\n",
       " -0.7875790596008301,\n",
       " -0.04926171526312828,\n",
       " -0.6301862597465515,\n",
       " 0.2594398558139801,\n",
       " -0.5350119471549988,\n",
       " -0.25336524844169617,\n",
       " 0.01338285580277443,\n",
       " 0.259674608707428,\n",
       " 0.1378391981124878,\n",
       " -0.7656711339950562,\n",
       " -0.2815798819065094,\n",
       " -0.9955034255981445,\n",
       " 0.31318536400794983,\n",
       " 0.0925421491265297,\n",
       " -0.3623221814632416,\n",
       " 0.4135195016860962,\n",
       " 0.120184525847435,\n",
       " 0.20040255784988403,\n",
       " -1.1092495918273926,\n",
       " 0.27943354845046997,\n",
       " -0.317048579454422,\n",
       " -0.44188448786735535,\n",
       " -0.08574794232845306,\n",
       " 0.4073023498058319,\n",
       " -0.5722345113754272,\n",
       " 0.47635698318481445,\n",
       " 0.7376450300216675,\n",
       " -1.0201733112335205,\n",
       " 0.057941798120737076,\n",
       " -0.45713889598846436,\n",
       " 0.2685263454914093,\n",
       " -0.0035238072741776705,\n",
       " 0.049372777342796326,\n",
       " 0.43194007873535156,\n",
       " -0.30267441272735596,\n",
       " -0.02270461991429329,\n",
       " -0.36596789956092834,\n",
       " 0.2611321806907654,\n",
       " -0.05487198382616043,\n",
       " 0.8172292709350586,\n",
       " -0.021404752507805824,\n",
       " -0.011012330651283264,\n",
       " 0.8387079834938049,\n",
       " -0.05442063882946968,\n",
       " -0.2658706605434418,\n",
       " 0.02435278333723545,\n",
       " -0.16948264837265015,\n",
       " 0.6563025116920471,\n",
       " -0.3529748022556305,\n",
       " 0.3951476812362671,\n",
       " -0.18065392971038818,\n",
       " -0.0384545736014843,\n",
       " -0.7496436834335327,\n",
       " -0.4015635848045349,\n",
       " -0.13055624067783356,\n",
       " 0.11109449714422226,\n",
       " 0.08735117316246033,\n",
       " 0.11203520745038986,\n",
       " 0.651833176612854,\n",
       " 0.8329499959945679,\n",
       " 0.3024536967277527,\n",
       " -0.14383740723133087,\n",
       " 0.07491178065538406,\n",
       " -0.1867062747478485,\n",
       " 0.9890912175178528,\n",
       " 0.3733666241168976,\n",
       " -0.09013032168149948,\n",
       " -0.3200720548629761,\n",
       " 0.14114299416542053,\n",
       " 0.011873063631355762,\n",
       " -0.02901417203247547,\n",
       " -0.13860774040222168,\n",
       " -0.11068063229322433,\n",
       " 0.10064028203487396,\n",
       " 0.7197244167327881,\n",
       " -0.06911756098270416,\n",
       " 0.3337298333644867,\n",
       " 1.0410950183868408,\n",
       " -0.5478122234344482,\n",
       " 0.7492792010307312,\n",
       " -0.1627836972475052,\n",
       " 0.009011571295559406,\n",
       " -0.27722176909446716,\n",
       " -0.4031635522842407,\n",
       " -0.07364384829998016,\n",
       " 0.07175322622060776,\n",
       " 0.11156954616308212,\n",
       " -0.27776843309402466,\n",
       " -0.05664448440074921,\n",
       " 0.21145223081111908,\n",
       " 0.0664396658539772,\n",
       " -0.15447048842906952,\n",
       " 0.11746291816234589,\n",
       " 0.29665327072143555,\n",
       " -0.2611956298351288,\n",
       " -0.05583688244223595,\n",
       " -0.6024231910705566,\n",
       " -0.24534952640533447,\n",
       " 0.5798813700675964,\n",
       " 0.46774038672447205,\n",
       " -0.1089649349451065,\n",
       " -0.6919376850128174,\n",
       " 0.1800338625907898,\n",
       " 0.4293036162853241,\n",
       " 0.3281773030757904,\n",
       " -0.21062488853931427,\n",
       " 0.25571325421333313,\n",
       " 0.10511418431997299,\n",
       " 0.34443506598472595,\n",
       " -1.4690423011779785,\n",
       " -0.5865758657455444,\n",
       " 0.06750526279211044,\n",
       " 0.07362423092126846,\n",
       " -0.22890931367874146,\n",
       " -0.3829834759235382,\n",
       " 0.09393420815467834,\n",
       " -0.2896088659763336,\n",
       " 0.507807731628418,\n",
       " -0.3371599614620209,\n",
       " 0.47632989287376404,\n",
       " 0.3259495794773102,\n",
       " -0.18766336143016815,\n",
       " -0.6263319253921509,\n",
       " 0.5208345651626587,\n",
       " -0.19048576056957245,\n",
       " -0.39532172679901123,\n",
       " 0.10656619817018509,\n",
       " 0.11414771527051926,\n",
       " -0.3382454514503479,\n",
       " -0.2943430244922638,\n",
       " 0.28173917531967163,\n",
       " 0.3678509593009949,\n",
       " -0.29593899846076965,\n",
       " -0.03200832009315491,\n",
       " 0.06711289286613464,\n",
       " -0.8438496589660645,\n",
       " 0.12231803685426712,\n",
       " 0.06931670755147934,\n",
       " 0.16003742814064026,\n",
       " 0.21130026876926422,\n",
       " 0.18021340668201447,\n",
       " -0.1115051880478859,\n",
       " -0.878973662853241,\n",
       " -0.06975629925727844,\n",
       " -0.5694693326950073,\n",
       " -0.10106907784938812,\n",
       " 0.6108940243721008,\n",
       " 0.11355984956026077,\n",
       " -0.2574908435344696,\n",
       " 0.07028795778751373,\n",
       " -0.3118903636932373,\n",
       " 0.05635062977671623,\n",
       " -0.17485986649990082,\n",
       " 0.014763136394321918,\n",
       " 0.12017133086919785,\n",
       " 0.14889279007911682,\n",
       " -0.0810806155204773,\n",
       " -0.13691416382789612,\n",
       " 0.2829619348049164,\n",
       " 0.6604955792427063,\n",
       " 0.12472078949213028,\n",
       " 0.3405798077583313,\n",
       " 0.0026399025227874517,\n",
       " -0.06019052863121033,\n",
       " -0.3187600076198578,\n",
       " 0.25965559482574463,\n",
       " 0.3430425226688385,\n",
       " -0.26894330978393555,\n",
       " -0.7250682711601257,\n",
       " 0.3092072308063507,\n",
       " -0.41752249002456665,\n",
       " 0.11489401757717133,\n",
       " -0.093549445271492,\n",
       " 0.03854707255959511,\n",
       " -0.16684482991695404,\n",
       " -0.3129139244556427,\n",
       " -0.46345624327659607,\n",
       " -0.38132333755493164,\n",
       " 0.20443980395793915,\n",
       " -0.19276095926761627,\n",
       " -0.2890673875808716,\n",
       " -0.31943172216415405,\n",
       " -0.34494438767433167,\n",
       " -0.8301522135734558,\n",
       " -0.10816653072834015,\n",
       " 0.02684813179075718,\n",
       " -0.5310736894607544,\n",
       " 0.4535370469093323,\n",
       " 0.3331052362918854,\n",
       " 0.5039202570915222,\n",
       " 0.13981246948242188,\n",
       " 0.07764668762683868,\n",
       " 0.3042915463447571,\n",
       " -0.2205372154712677,\n",
       " -0.0629434734582901,\n",
       " -0.2725706696510315,\n",
       " 0.21830371022224426,\n",
       " 0.01590823009610176,\n",
       " 0.029136158525943756,\n",
       " -0.7246156930923462,\n",
       " -0.12691378593444824,\n",
       " -0.04998181015253067,\n",
       " -0.2304837554693222,\n",
       " -0.3095794916152954,\n",
       " -0.20500852167606354,\n",
       " -0.40776488184928894,\n",
       " 0.053130947053432465,\n",
       " 0.03671771287918091,\n",
       " 0.06626251339912415,\n",
       " 0.5776139497756958,\n",
       " -0.5319265127182007,\n",
       " 0.6835048794746399,\n",
       " 1.0801433324813843,\n",
       " 0.43550801277160645,\n",
       " -0.23134970664978027,\n",
       " -0.6699469089508057,\n",
       " 0.452988862991333,\n",
       " 0.5773909091949463,\n",
       " 0.4220232367515564,\n",
       " 0.016741514205932617,\n",
       " -0.45219674706459045,\n",
       " -0.23987078666687012,\n",
       " 0.04762176051735878,\n",
       " 0.14517582952976227,\n",
       " 0.041883744299411774,\n",
       " -0.3383459746837616,\n",
       " -0.19654928147792816,\n",
       " 0.8249169588088989,\n",
       " 0.004878324456512928,\n",
       " -0.0823301374912262,\n",
       " 0.4646424949169159,\n",
       " 0.48541077971458435,\n",
       " -0.598057210445404,\n",
       " -0.2585310935974121,\n",
       " -0.19755199551582336,\n",
       " -0.41479045152664185,\n",
       " -0.13185133039951324,\n",
       " 0.7263365983963013,\n",
       " 0.23975881934165955,\n",
       " 0.06651639938354492,\n",
       " -0.5239229798316956,\n",
       " -0.08858336508274078,\n",
       " -0.601791262626648,\n",
       " -0.049714669585227966,\n",
       " -0.13070464134216309,\n",
       " -0.9573764204978943,\n",
       " -0.09867306053638458,\n",
       " -0.39924532175064087,\n",
       " 0.10362841933965683,\n",
       " 0.6063649654388428,\n",
       " -0.342132568359375,\n",
       " 0.3750201463699341,\n",
       " -0.1350221186876297,\n",
       " -0.5178520083427429,\n",
       " 0.030241573229432106,\n",
       " 0.7219882607460022,\n",
       " 0.3804621398448944,\n",
       " 0.07614389806985855,\n",
       " 0.27350103855133057,\n",
       " -0.17519733309745789,\n",
       " -0.4074236750602722,\n",
       " -0.30805420875549316,\n",
       " 0.38746094703674316,\n",
       " 0.21199718117713928,\n",
       " 0.7410728931427002,\n",
       " 0.2536650598049164,\n",
       " -0.04947091266512871,\n",
       " 0.4639911651611328,\n",
       " 0.3448314666748047,\n",
       " 0.3111981153488159,\n",
       " 0.38302159309387207,\n",
       " 0.05147167667746544,\n",
       " -0.14038445055484772,\n",
       " -0.4405926465988159,\n",
       " -0.631957471370697,\n",
       " -0.1723937839269638,\n",
       " 0.1675313413143158,\n",
       " 0.2568698227405548,\n",
       " -0.19705002009868622,\n",
       " -0.060719333589076996,\n",
       " -0.4724685549736023,\n",
       " -0.512444794178009,\n",
       " -0.022167960181832314,\n",
       " -0.006912414915859699,\n",
       " -0.4647265374660492,\n",
       " 0.03690684959292412,\n",
       " 0.33409014344215393,\n",
       " 0.4586544930934906,\n",
       " 0.18951556086540222,\n",
       " -0.07914218306541443,\n",
       " 0.39312660694122314,\n",
       " 0.4137856364250183,\n",
       " -0.05170787498354912,\n",
       " -0.37182626128196716,\n",
       " -0.5165498852729797,\n",
       " -0.750690758228302,\n",
       " 0.37196990847587585,\n",
       " -0.31361788511276245,\n",
       " -0.5226236581802368,\n",
       " 0.5379875302314758,\n",
       " 0.917018711566925,\n",
       " 1.004354476928711,\n",
       " -0.012655680067837238,\n",
       " 0.35471439361572266,\n",
       " -0.2116619050502777,\n",
       " -0.3189251720905304,\n",
       " -0.2744234800338745,\n",
       " -0.165728360414505,\n",
       " 0.10740174353122711,\n",
       " -0.1062384694814682,\n",
       " -0.15498647093772888,\n",
       " 0.6724424958229065,\n",
       " -0.6717050075531006,\n",
       " -0.14581027626991272,\n",
       " 0.31038254499435425,\n",
       " 0.2567238211631775,\n",
       " 0.3579651713371277,\n",
       " 0.17325358092784882,\n",
       " 0.025838661938905716,\n",
       " -0.16178357601165771,\n",
       " -0.18904803693294525,\n",
       " -0.051440395414829254,\n",
       " 0.09480078518390656,\n",
       " 0.23960378766059875,\n",
       " -0.4913081228733063,\n",
       " -0.6769168972969055,\n",
       " 9.30703353881836,\n",
       " -0.32194623351097107,\n",
       " -0.1570422500371933,\n",
       " -0.05516229569911957,\n",
       " -0.655345618724823,\n",
       " -0.02984529547393322,\n",
       " -0.06020355969667435,\n",
       " 0.10218548029661179,\n",
       " 0.3244534432888031,\n",
       " -0.7187684774398804,\n",
       " 0.5133413672447205,\n",
       " -0.09346654266119003,\n",
       " 0.3936081826686859,\n",
       " 0.45553022623062134,\n",
       " 0.35605114698410034,\n",
       " 0.4434502124786377,\n",
       " 0.42912036180496216,\n",
       " 0.556399941444397,\n",
       " -0.004717131145298481,\n",
       " 0.09948363155126572,\n",
       " 0.6814901828765869,\n",
       " -0.12096341699361801,\n",
       " -0.034214943647384644,\n",
       " -0.3777768313884735,\n",
       " 0.6675972938537598,\n",
       " -0.36959001421928406,\n",
       " -0.2263190597295761,\n",
       " -0.6328545808792114,\n",
       " 0.18310703337192535,\n",
       " 0.023378759622573853,\n",
       " -0.36955365538597107,\n",
       " -0.4776822626590729,\n",
       " 0.108546182513237,\n",
       " 0.422634094953537,\n",
       " -0.1194649264216423,\n",
       " 0.28473296761512756,\n",
       " 0.3618995249271393,\n",
       " -0.31758755445480347,\n",
       " -0.0215004775673151,\n",
       " 0.020566439256072044,\n",
       " 0.25093889236450195,\n",
       " -0.8400729894638062,\n",
       " -0.4113275110721588,\n",
       " 0.012238847091794014,\n",
       " 0.6153299808502197,\n",
       " 0.2723466157913208,\n",
       " 0.1871693879365921,\n",
       " 1.3108936548233032,\n",
       " 0.3314903676509857,\n",
       " -0.010595566593110561,\n",
       " -0.39116016030311584,\n",
       " 0.4392717182636261,\n",
       " -0.7986807227134705,\n",
       " -0.6271615028381348,\n",
       " 1.3939228057861328,\n",
       " 0.1489470899105072,\n",
       " -0.30464279651641846,\n",
       " 0.03936294466257095,\n",
       " -0.2995881736278534,\n",
       " 0.06592610478401184,\n",
       " -0.7614346146583557,\n",
       " 3.5066285133361816,\n",
       " 0.4601325988769531,\n",
       " 0.2444094866514206,\n",
       " -0.3444565236568451,\n",
       " -0.009237592108547688,\n",
       " 0.17093580961227417,\n",
       " -0.0809619352221489,\n",
       " -0.05163738131523132,\n",
       " -0.48426318168640137,\n",
       " -0.16086480021476746,\n",
       " 0.10052420198917389,\n",
       " 0.01230806577950716]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 示例句子\n",
    "example_sentence = \"我爱自然语言处理，这是一个很有趣的领域，我希望能够深入学习并取得进步。\"\n",
    "\n",
    "\n",
    "\n",
    "# 对示例句子进行编码\n",
    "encoded_sentence = encode_sentence(example_sentence, tokenizer, model)\n",
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载微调后的BERT模型\n",
    "fine_tuned_model = BertForMaskedLM.from_pretrained(output_dir).to(device)\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "# 编码指定句子\n",
    "def encode_sentence(sentence):\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model(input_ids)\n",
    "    sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    return sentence_embedding\n",
    "\n",
    "# 测试句子编码\n",
    "test_sentence = \"这是一个测试句子\"\n",
    "encoded_sentence = encode_sentence(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对bert训练文本的处理\n",
    "1. 不能有空行\n",
    "2. 每一行不能超512token的限制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除空行\n",
    "with open(r\"C:\\Users\\LJH\\Desktop\\txt存放\\飞书 - 副本\\total.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 删除空行\n",
    "non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# 将非空行写入到新文件中\n",
    "with open(r\"C:\\Users\\LJH\\Desktop\\txt存放\\飞书 - 副本\\total_deleted_blank.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(non_empty_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line, max_length=450):\n",
    "    if len(line) <= max_length:\n",
    "        return [line]\n",
    "    else:\n",
    "        # 从450个字符的位置向前找到最近的“。”\n",
    "        index = max_length\n",
    "        while (index >= 0) and (line[index] != \"。\")and(line[index] != \"?\")and(line[index] != \"？\")and(line[index] != \"！\")and(line[index] != \"!\"):\n",
    "            index -= 1\n",
    "        \n",
    "        # 如果没有找到句号，则找逗号\n",
    "        if index == -1:\n",
    "            index = max_length\n",
    "            while (index >= 0) and (line[index] != \",\")and (line[index] != \"，\"):\n",
    "                index -= 1\n",
    "        # 如果都没找到，则直接将字符串切割\n",
    "        if index == -1:\n",
    "            return [line[:max_length]] + process_line(line[max_length:], max_length)\n",
    "        # 否则将句号之后的内容作为新行\n",
    "        else:\n",
    "            return [line[:index + 1]] + process_line(line[index + 1:], max_length)\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    processed_lines = []\n",
    "    for line in lines:\n",
    "        processed_lines.extend(process_line(line.strip()))\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(processed_lines))\n",
    "\n",
    "# 使用示例\n",
    "input_file = r\"C:\\Users\\LJH\\Desktop\\txt存放\\飞书 - 副本\\total_deleted_blank.txt\"\n",
    "output_file = r\"C:\\Users\\LJH\\Desktop\\txt存放\\飞书 - 副本\\total_deleted_blank_less_512.txt\"\n",
    "process_file(input_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
